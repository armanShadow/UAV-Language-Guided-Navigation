2025-03-21 00:39:37,256 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_exy3ycf8.log
2025-03-21 00:39:37,256 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-21 00:39:37,256 - training - INFO - Device: cuda:0
2025-03-21 00:39:37,439 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_p66tmg80.log
2025-03-21 00:39:37,439 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-21 00:39:37,440 - training - INFO - Device: cuda:0
2025-03-21 00:39:38,103 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-21 00:39:38,104 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 00:39:38,104 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 00:39:38,106 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 00:39:38,106 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 00:39:38,106 - training - INFO - Starting model initialization...
2025-03-21 00:39:38,671 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-21 00:39:38,671 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 00:39:38,671 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 00:39:38,671 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 00:39:38,671 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 00:39:38,671 - training - INFO - Starting model initialization...
2025-03-21 00:39:48,493 - training - INFO - Per-GPU batch size: 32
2025-03-21 00:39:48,501 - training - INFO - Starting epoch 1/200000
2025-03-21 00:39:48,881 - training - INFO - Per-GPU batch size: 32
2025-03-21 00:39:48,887 - training - INFO - Starting epoch 1/200000
2025-03-21 00:39:52,869 - training - ERROR - Critical error in training batch 0: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 10.75 GiB total capacity; 7.21 GiB already allocated; 37.81 MiB free; 8.60 GiB allowed; 7.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:39:52,875 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 361, in forward
    context_layer = torch.matmul(attention_probs, value_layer)
RuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 10.75 GiB total capacity; 7.21 GiB already allocated; 37.81 MiB free; 8.60 GiB allowed; 7.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:39:52,875 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 10.75 GiB total capacity; 7.21 GiB already allocated; 37.81 MiB free; 8.60 GiB allowed; 7.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:39:52,876 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 361, in forward
    context_layer = torch.matmul(attention_probs, value_layer)
RuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 10.75 GiB total capacity; 7.21 GiB already allocated; 37.81 MiB free; 8.60 GiB allowed; 7.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:39:52,876 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 10.75 GiB total capacity; 7.21 GiB already allocated; 37.81 MiB free; 8.60 GiB allowed; 7.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:39:52,876 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 361, in forward
    context_layer = torch.matmul(attention_probs, value_layer)
RuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 10.75 GiB total capacity; 7.21 GiB already allocated; 37.81 MiB free; 8.60 GiB allowed; 7.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:39:52,965 - training - ERROR - Critical error in training batch 0: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 10.75 GiB total capacity; 1.24 GiB already allocated; 13.81 MiB free; 8.60 GiB allowed; 1.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:39:52,967 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 327, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
RuntimeError: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 10.75 GiB total capacity; 1.24 GiB already allocated; 13.81 MiB free; 8.60 GiB allowed; 1.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:39:52,967 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 10.75 GiB total capacity; 1.24 GiB already allocated; 13.81 MiB free; 8.60 GiB allowed; 1.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:39:52,968 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 327, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
RuntimeError: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 10.75 GiB total capacity; 1.24 GiB already allocated; 13.81 MiB free; 8.60 GiB allowed; 1.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:39:52,968 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 10.75 GiB total capacity; 1.24 GiB already allocated; 13.81 MiB free; 8.60 GiB allowed; 1.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:39:52,968 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 327, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
RuntimeError: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 10.75 GiB total capacity; 1.24 GiB already allocated; 13.81 MiB free; 8.60 GiB allowed; 1.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:44:13,829 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_4r4vkg03.log
2025-03-21 00:44:13,829 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 2
2025-03-21 00:44:13,829 - training - INFO - Device: cuda:0
2025-03-21 00:44:14,607 - training - INFO - Training on 3 GPUs, distributed mode: True
2025-03-21 00:44:14,607 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 00:44:14,608 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 00:44:14,610 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 00:44:14,610 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 00:44:14,840 - training - INFO - Starting model initialization...
2025-03-21 00:44:25,466 - training - INFO - Per-GPU batch size: 32 (global batch size: 64)
2025-03-21 00:44:26,285 - training - INFO - Starting epoch 1/200000
2025-03-21 00:44:36,424 - training - ERROR - Critical error in training batch 0: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 10.75 GiB total capacity; 8.09 GiB already allocated; 287.81 MiB free; 8.60 GiB allowed; 8.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:44:36,430 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 348, in forward
    attention_scores = attention_scores + attention_mask
RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 10.75 GiB total capacity; 8.09 GiB already allocated; 287.81 MiB free; 8.60 GiB allowed; 8.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:44:36,430 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 10.75 GiB total capacity; 8.09 GiB already allocated; 287.81 MiB free; 8.60 GiB allowed; 8.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:44:36,431 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 348, in forward
    attention_scores = attention_scores + attention_mask
RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 10.75 GiB total capacity; 8.09 GiB already allocated; 287.81 MiB free; 8.60 GiB allowed; 8.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:44:36,431 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 10.75 GiB total capacity; 8.09 GiB already allocated; 287.81 MiB free; 8.60 GiB allowed; 8.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:44:36,431 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 348, in forward
    attention_scores = attention_scores + attention_mask
RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 10.75 GiB total capacity; 8.09 GiB already allocated; 287.81 MiB free; 8.60 GiB allowed; 8.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:44:36,650 - training - ERROR - Critical error in training batch 0: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 1; 10.75 GiB total capacity; 8.10 GiB already allocated; 1.18 GiB free; 8.60 GiB allowed; 8.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:44:36,653 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 348, in forward
    attention_scores = attention_scores + attention_mask
RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 1; 10.75 GiB total capacity; 8.10 GiB already allocated; 1.18 GiB free; 8.60 GiB allowed; 8.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:44:36,653 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 1; 10.75 GiB total capacity; 8.10 GiB already allocated; 1.18 GiB free; 8.60 GiB allowed; 8.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:44:36,654 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 348, in forward
    attention_scores = attention_scores + attention_mask
RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 1; 10.75 GiB total capacity; 8.10 GiB already allocated; 1.18 GiB free; 8.60 GiB allowed; 8.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:44:36,654 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 1; 10.75 GiB total capacity; 8.10 GiB already allocated; 1.18 GiB free; 8.60 GiB allowed; 8.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:44:36,654 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 348, in forward
    attention_scores = attention_scores + attention_mask
RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 1; 10.75 GiB total capacity; 8.10 GiB already allocated; 1.18 GiB free; 8.60 GiB allowed; 8.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:45:49,436 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_r_q97fis.log
2025-03-21 00:45:49,436 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 2
2025-03-21 00:45:49,436 - training - INFO - Device: cuda:0
2025-03-21 00:45:49,998 - training - INFO - Training on 3 GPUs, distributed mode: True
2025-03-21 00:45:49,998 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 00:45:49,998 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 00:45:49,998 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 00:45:49,998 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 00:45:50,223 - training - INFO - Starting model initialization...
2025-03-21 00:46:00,470 - training - INFO - Per-GPU batch size: 16 (global batch size: 32)
2025-03-21 00:46:00,487 - training - INFO - Starting epoch 1/200000
2025-03-21 00:46:09,098 - training - ERROR - Critical error in training batch 0: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 10.75 GiB total capacity; 8.45 GiB already allocated; 203.81 MiB free; 8.60 GiB allowed; 8.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:46:09,101 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 345, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 10.75 GiB total capacity; 8.45 GiB already allocated; 203.81 MiB free; 8.60 GiB allowed; 8.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:46:09,101 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 10.75 GiB total capacity; 8.45 GiB already allocated; 203.81 MiB free; 8.60 GiB allowed; 8.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:46:09,101 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 345, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 10.75 GiB total capacity; 8.45 GiB already allocated; 203.81 MiB free; 8.60 GiB allowed; 8.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:46:09,101 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 10.75 GiB total capacity; 8.45 GiB already allocated; 203.81 MiB free; 8.60 GiB allowed; 8.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:46:09,101 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 345, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 10.75 GiB total capacity; 8.45 GiB already allocated; 203.81 MiB free; 8.60 GiB allowed; 8.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:46:09,308 - training - ERROR - Critical error in training batch 0: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 1; 10.75 GiB total capacity; 8.46 GiB already allocated; 1.11 GiB free; 8.60 GiB allowed; 8.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:46:09,311 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 345, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 1; 10.75 GiB total capacity; 8.46 GiB already allocated; 1.11 GiB free; 8.60 GiB allowed; 8.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:46:09,311 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 1; 10.75 GiB total capacity; 8.46 GiB already allocated; 1.11 GiB free; 8.60 GiB allowed; 8.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:46:09,311 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 345, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 1; 10.75 GiB total capacity; 8.46 GiB already allocated; 1.11 GiB free; 8.60 GiB allowed; 8.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:46:09,312 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 1; 10.75 GiB total capacity; 8.46 GiB already allocated; 1.11 GiB free; 8.60 GiB allowed; 8.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:46:09,312 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 345, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 1; 10.75 GiB total capacity; 8.46 GiB already allocated; 1.11 GiB free; 8.60 GiB allowed; 8.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:47:43,360 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_usi9_i3u.log
2025-03-21 00:47:43,360 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 2
2025-03-21 00:47:43,360 - training - INFO - Device: cuda:0
2025-03-21 00:47:43,986 - training - INFO - Training on 3 GPUs, distributed mode: True
2025-03-21 00:47:43,986 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 00:47:43,986 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 00:47:43,986 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 00:47:43,986 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 00:47:44,202 - training - INFO - Starting model initialization...
2025-03-21 00:47:54,242 - training - INFO - Per-GPU batch size: 4 (global batch size: 8)
2025-03-21 00:47:54,260 - training - INFO - Starting epoch 1/200000
2025-03-21 00:48:04,236 - training - INFO - GPU Memory: GPU 0: 2515.5MB/4814.0MB, GPU 1: 2524.6MB/4826.0MB, Total: 5040.1MB allocated, 9640.0MB reserved, Mean: 2520.0MB allocated, 4820.0MB reserved
2025-03-21 00:48:04,237 - training - INFO - Epoch: 1/200000, Batch: 0/326, Loss: 10.3579, Throughput: 0.80 samples/sec
2025-03-21 00:48:51,895 - training - INFO - GPU Memory: GPU 0: 4016.2MB/7950.0MB, GPU 1: 4028.8MB/7958.0MB, Total: 8044.9MB allocated, 15908.0MB reserved, Mean: 4022.5MB allocated, 7954.0MB reserved
2025-03-21 00:48:51,896 - training - INFO - Epoch: 1/200000, Batch: 108/326, Loss: 9.6987, Throughput: 15.13 samples/sec
2025-03-21 00:49:39,691 - training - INFO - GPU Memory: GPU 0: 4018.0MB/7950.0MB, GPU 1: 4026.6MB/7958.0MB, Total: 8044.6MB allocated, 15908.0MB reserved, Mean: 4022.3MB allocated, 7954.0MB reserved
2025-03-21 00:49:39,692 - training - INFO - Epoch: 1/200000, Batch: 216/326, Loss: 9.2922, Throughput: 16.47 samples/sec
2025-03-21 00:50:18,725 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_yvjes29_.log
2025-03-21 00:50:18,725 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 3
2025-03-21 00:50:18,725 - training - INFO - Device: cuda:0
2025-03-21 00:50:19,353 - training - INFO - Training on 3 GPUs, distributed mode: True
2025-03-21 00:50:19,353 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 00:50:19,353 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 00:50:19,354 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 00:50:19,354 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 00:50:20,226 - training - INFO - Starting model initialization...
2025-03-21 00:50:30,310 - training - INFO - Per-GPU batch size: 4 (global batch size: 12)
2025-03-21 00:50:31,189 - training - INFO - Starting epoch 1/200000
2025-03-21 00:50:41,371 - training - INFO - GPU Memory: GPU 0: 2515.5MB/4814.0MB, GPU 1: 2528.5MB/4854.0MB, GPU 2: 2524.6MB/4826.0MB, Total: 7568.6MB allocated, 14494.0MB reserved, Mean: 2522.9MB allocated, 4831.3MB reserved
2025-03-21 00:50:41,371 - training - INFO - Epoch: 1/200000, Batch: 0/217, Loss: 10.3431, Throughput: 1.18 samples/sec
2025-03-21 00:50:49,421 - training - ERROR - Critical error in training batch 13: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 10.75 GiB total capacity; 7.45 GiB already allocated; 117.81 MiB free; 8.60 GiB allowed; 7.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:50:49,422 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 162, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 10.75 GiB total capacity; 7.45 GiB already allocated; 117.81 MiB free; 8.60 GiB allowed; 7.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:50:49,422 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 10.75 GiB total capacity; 7.45 GiB already allocated; 117.81 MiB free; 8.60 GiB allowed; 7.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:50:49,423 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 162, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 10.75 GiB total capacity; 7.45 GiB already allocated; 117.81 MiB free; 8.60 GiB allowed; 7.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:50:49,423 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 10.75 GiB total capacity; 7.45 GiB already allocated; 117.81 MiB free; 8.60 GiB allowed; 7.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:50:49,423 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 162, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 10.75 GiB total capacity; 7.45 GiB already allocated; 117.81 MiB free; 8.60 GiB allowed; 7.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:52:10,153 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_eleuh7sw.log
2025-03-21 00:52:10,153 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 3
2025-03-21 00:52:10,153 - training - INFO - Device: cuda:0
2025-03-21 00:52:10,815 - training - INFO - Training on 3 GPUs, distributed mode: True
2025-03-21 00:52:10,815 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 00:52:10,815 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 00:52:10,815 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 00:52:10,815 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 00:52:11,310 - training - INFO - Starting model initialization...
2025-03-21 00:52:21,623 - training - INFO - Per-GPU batch size: 4 (global batch size: 12)
2025-03-21 00:52:21,642 - training - INFO - Starting epoch 1/200000
2025-03-21 00:52:30,412 - training - INFO - GPU Memory: GPU 0: 2515.5MB/4814.0MB, GPU 1: 2523.1MB/4826.0MB, GPU 2: 2530.0MB/4854.0MB, Total: 7568.6MB allocated, 14494.0MB reserved, Mean: 2522.9MB allocated, 4831.3MB reserved
2025-03-21 00:52:30,413 - training - INFO - Epoch: 1/200000, Batch: 0/217, Loss: 10.2957, Throughput: 1.37 samples/sec
2025-03-21 00:52:58,437 - training - ERROR - Critical error in training batch 43: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 10.75 GiB total capacity; 7.45 GiB already allocated; 107.81 MiB free; 8.60 GiB allowed; 7.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:52:58,439 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 162, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 10.75 GiB total capacity; 7.45 GiB already allocated; 107.81 MiB free; 8.60 GiB allowed; 7.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:52:58,440 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 10.75 GiB total capacity; 7.45 GiB already allocated; 107.81 MiB free; 8.60 GiB allowed; 7.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:52:58,440 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 162, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 10.75 GiB total capacity; 7.45 GiB already allocated; 107.81 MiB free; 8.60 GiB allowed; 7.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 00:52:58,440 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 10.75 GiB total capacity; 7.45 GiB already allocated; 107.81 MiB free; 8.60 GiB allowed; 7.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 00:52:58,440 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 162, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 10.75 GiB total capacity; 7.45 GiB already allocated; 107.81 MiB free; 8.60 GiB allowed; 7.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:00:07,035 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_rfsic5xa.log
2025-03-21 01:00:07,036 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 3
2025-03-21 01:00:07,036 - training - INFO - Device: cuda:0
2025-03-21 01:00:07,725 - training - INFO - Training on 3 GPUs, distributed mode: True
2025-03-21 01:00:07,725 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 01:00:07,725 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 01:00:07,727 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 01:00:07,727 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 01:00:14,359 - training - INFO - Starting model initialization...
2025-03-21 01:00:22,127 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 2; 10.75 GiB total capacity; 840.09 MiB already allocated; 41.81 MiB free; 7.52 GiB allowed; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:00:22,129 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 569, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 2; 10.75 GiB total capacity; 840.09 MiB already allocated; 41.81 MiB free; 7.52 GiB allowed; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:00:22,470 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 1; 10.75 GiB total capacity; 840.09 MiB already allocated; 45.81 MiB free; 7.52 GiB allowed; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:00:22,470 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 569, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 1; 10.75 GiB total capacity; 840.09 MiB already allocated; 45.81 MiB free; 7.52 GiB allowed; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:05:15,544 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_xg5cowyh.log
2025-03-21 01:05:15,544 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 3
2025-03-21 01:05:15,544 - training - INFO - Device: cuda:0
2025-03-21 01:05:16,752 - training - INFO - Training on 3 GPUs, distributed mode: True
2025-03-21 01:05:16,752 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 01:05:16,753 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 01:05:16,754 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 01:05:16,755 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 01:05:21,148 - training - INFO - Starting model initialization...
2025-03-21 01:05:31,109 - training - INFO - Per-GPU batch size: 4 (global batch size: 12)
2025-03-21 01:05:31,999 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 1; 10.75 GiB total capacity; 840.09 MiB already allocated; 45.81 MiB free; 7.52 GiB allowed; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:05:31,999 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 569, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 1; 10.75 GiB total capacity; 840.09 MiB already allocated; 45.81 MiB free; 7.52 GiB allowed; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:05:32,023 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 2; 10.75 GiB total capacity; 840.09 MiB already allocated; 41.81 MiB free; 7.52 GiB allowed; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:05:32,024 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 569, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 2; 10.75 GiB total capacity; 840.09 MiB already allocated; 41.81 MiB free; 7.52 GiB allowed; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:05:52,072 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_7hyo_7yj.log
2025-03-21 01:05:52,073 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 3
2025-03-21 01:05:52,073 - training - INFO - Device: cuda:0
2025-03-21 01:05:52,698 - training - INFO - Training on 3 GPUs, distributed mode: True
2025-03-21 01:05:52,698 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 01:05:52,698 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 01:05:52,698 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 01:05:52,698 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 01:06:00,526 - training - INFO - Starting model initialization...
2025-03-21 01:06:11,244 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-21 01:06:11,246 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 566, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-21 01:06:11,263 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-21 01:06:11,263 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 566, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-21 01:06:14,305 - training - INFO - Per-GPU batch size: 4 (global batch size: 12)
2025-03-21 01:06:55,672 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_efjngt0l.log
2025-03-21 01:06:55,673 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 3
2025-03-21 01:06:55,673 - training - INFO - Device: cuda:0
2025-03-21 01:06:56,331 - training - INFO - Training on 3 GPUs, distributed mode: True
2025-03-21 01:06:56,331 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 01:06:56,331 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 01:06:56,331 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 01:06:56,331 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 01:07:08,825 - training - INFO - Starting model initialization...
2025-03-21 01:07:13,097 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.75 GiB total capacity; 263.89 MiB already allocated; 1.81 MiB free; 264.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:07:13,098 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 566, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.75 GiB total capacity; 263.89 MiB already allocated; 1.81 MiB free; 264.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:07:13,098 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-21 01:07:13,098 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 566, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-21 01:07:13,099 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-21 01:07:13,099 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 566, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-21 01:07:37,822 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_uo0l0o0z.log
2025-03-21 01:07:37,822 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 3
2025-03-21 01:07:37,822 - training - INFO - Device: cuda:0
2025-03-21 01:07:38,407 - training - INFO - Training on 3 GPUs, distributed mode: True
2025-03-21 01:07:38,407 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 01:07:38,407 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 01:07:38,407 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 01:07:38,407 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 01:07:38,731 - training - INFO - Starting model initialization...
2025-03-21 01:07:42,979 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 264.14 MiB already allocated; 17.81 MiB free; 7.52 GiB allowed; 266.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:07:42,979 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 566, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 264.14 MiB already allocated; 17.81 MiB free; 7.52 GiB allowed; 266.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:07:43,115 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-21 01:07:43,142 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-21 01:07:43,485 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 566, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-21 01:07:43,485 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 566, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-21 01:08:00,244 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_es012e03.log
2025-03-21 01:08:00,244 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 3
2025-03-21 01:08:00,244 - training - INFO - Device: cuda:0
2025-03-21 01:08:01,434 - training - INFO - Training on 3 GPUs, distributed mode: True
2025-03-21 01:08:01,434 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 01:08:01,434 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 01:08:01,434 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 01:08:01,434 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 01:08:13,828 - training - INFO - Starting model initialization...
2025-03-21 01:08:18,239 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 280.90 MiB already allocated; 15.81 MiB free; 286.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:08:18,239 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 566, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 280.90 MiB already allocated; 15.81 MiB free; 286.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:08:18,343 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-21 01:08:18,355 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-21 01:08:18,396 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 566, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-21 01:08:18,396 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 566, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-21 01:09:35,095 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_v1hu7whg.log
2025-03-21 01:09:35,096 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-21 01:09:35,096 - training - INFO - Device: cuda:0
2025-03-21 01:09:35,241 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_ajkhr9i8.log
2025-03-21 01:09:35,241 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_qux_56wr.log
2025-03-21 01:09:35,242 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-21 01:09:35,242 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-21 01:09:35,242 - training - INFO - Device: cuda:0
2025-03-21 01:09:35,242 - training - INFO - Device: cuda:0
2025-03-21 01:09:36,194 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-21 01:09:36,194 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 01:09:36,194 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 01:09:36,194 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 01:09:36,194 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 01:09:36,194 - training - INFO - Starting model initialization...
2025-03-21 01:09:36,203 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-21 01:09:36,203 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 01:09:36,203 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 01:09:36,203 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 01:09:36,204 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 01:09:36,204 - training - INFO - Starting model initialization...
2025-03-21 01:09:36,210 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-21 01:09:36,210 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 01:09:36,210 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 01:09:36,210 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 01:09:36,210 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 01:09:36,210 - training - INFO - Starting model initialization...
2025-03-21 01:10:04,963 - training - INFO - Per-GPU batch size: 4
2025-03-21 01:10:04,970 - training - INFO - Starting epoch 1/200000
2025-03-21 01:10:05,024 - training - INFO - Per-GPU batch size: 4
2025-03-21 01:10:05,030 - training - INFO - Starting epoch 1/200000
2025-03-21 01:10:05,279 - training - INFO - Per-GPU batch size: 4
2025-03-21 01:10:05,286 - training - INFO - Starting epoch 1/200000
2025-03-21 01:10:06,713 - training - ERROR - Critical error in training batch 0: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 10.75 GiB total capacity; 2.73 GiB already allocated; 13.81 MiB free; 8.60 GiB allowed; 2.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:10:06,718 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 345, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 10.75 GiB total capacity; 2.73 GiB already allocated; 13.81 MiB free; 8.60 GiB allowed; 2.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:10:06,718 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 10.75 GiB total capacity; 2.73 GiB already allocated; 13.81 MiB free; 8.60 GiB allowed; 2.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:10:06,719 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 345, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 10.75 GiB total capacity; 2.73 GiB already allocated; 13.81 MiB free; 8.60 GiB allowed; 2.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:10:06,719 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 10.75 GiB total capacity; 2.73 GiB already allocated; 13.81 MiB free; 8.60 GiB allowed; 2.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:10:06,719 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 345, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 10.75 GiB total capacity; 2.73 GiB already allocated; 13.81 MiB free; 8.60 GiB allowed; 2.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:10:07,068 - training - ERROR - Critical error in training batch 0: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.75 GiB total capacity; 3.69 GiB already allocated; 3.81 MiB free; 8.60 GiB allowed; 3.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:10:07,079 - training - ERROR - Critical error in training batch 0: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
2025-03-21 01:10:07,157 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 289, in forward
    mixed_query_layer = self.query(hidden_states)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`

2025-03-21 01:10:07,157 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 457, in forward
    x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 466, in _sa_block
    x = self.self_attn(x, x, x,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1038, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5358, in multi_head_attention_forward
    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5041, in _scaled_dot_product_attention
    output = torch.bmm(attn, v)
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.75 GiB total capacity; 3.69 GiB already allocated; 3.81 MiB free; 8.60 GiB allowed; 3.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:10:07,157 - training - ERROR - Fatal error in training loop: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
2025-03-21 01:10:07,157 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.75 GiB total capacity; 3.69 GiB already allocated; 3.81 MiB free; 8.60 GiB allowed; 3.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:10:07,158 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 289, in forward
    mixed_query_layer = self.query(hidden_states)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`

2025-03-21 01:10:07,158 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 457, in forward
    x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 466, in _sa_block
    x = self.self_attn(x, x, x,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1038, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5358, in multi_head_attention_forward
    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5041, in _scaled_dot_product_attention
    output = torch.bmm(attn, v)
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.75 GiB total capacity; 3.69 GiB already allocated; 3.81 MiB free; 8.60 GiB allowed; 3.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:10:07,159 - training - ERROR - Fatal error in main function: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
2025-03-21 01:10:07,159 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.75 GiB total capacity; 3.69 GiB already allocated; 3.81 MiB free; 8.60 GiB allowed; 3.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:10:07,159 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 289, in forward
    mixed_query_layer = self.query(hidden_states)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`

2025-03-21 01:10:07,159 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 457, in forward
    x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 466, in _sa_block
    x = self.self_attn(x, x, x,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1038, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5358, in multi_head_attention_forward
    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5041, in _scaled_dot_product_attention
    output = torch.bmm(attn, v)
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.75 GiB total capacity; 3.69 GiB already allocated; 3.81 MiB free; 8.60 GiB allowed; 3.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:10:35,930 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error__351zluo.log
2025-03-21 01:10:35,930 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-21 01:10:35,930 - training - INFO - Device: cuda:0
2025-03-21 01:10:36,221 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_u1uyo1g7.log
2025-03-21 01:10:36,221 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_1r9r1o4u.log
2025-03-21 01:10:36,222 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-21 01:10:36,222 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-21 01:10:36,222 - training - INFO - Device: cuda:0
2025-03-21 01:10:36,222 - training - INFO - Device: cuda:0
2025-03-21 01:10:36,552 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-21 01:10:36,552 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 01:10:36,552 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 01:10:36,552 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 01:10:36,552 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 01:10:36,552 - training - INFO - Starting model initialization...
2025-03-21 01:10:36,855 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-21 01:10:36,855 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 01:10:36,855 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 01:10:36,855 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 01:10:36,855 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 01:10:36,855 - training - INFO - Starting model initialization...
2025-03-21 01:10:39,481 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-21 01:10:39,483 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 01:10:39,483 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 01:10:39,483 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 01:10:39,483 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 01:10:39,483 - training - INFO - Starting model initialization...
2025-03-21 01:10:46,690 - training - INFO - Per-GPU batch size: 4
2025-03-21 01:10:46,699 - training - INFO - Starting epoch 1/200000
2025-03-21 01:10:47,220 - training - INFO - Per-GPU batch size: 4
2025-03-21 01:10:47,226 - training - INFO - Starting epoch 1/200000
2025-03-21 01:10:48,940 - training - ERROR - Critical error in training batch 0: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 10.75 GiB total capacity; 1.94 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 1.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:10:48,943 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 348, in forward
    attention_scores = attention_scores + attention_mask
RuntimeError: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 10.75 GiB total capacity; 1.94 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 1.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:10:48,943 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 10.75 GiB total capacity; 1.94 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 1.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:10:48,943 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 348, in forward
    attention_scores = attention_scores + attention_mask
RuntimeError: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 10.75 GiB total capacity; 1.94 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 1.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:10:48,943 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 10.75 GiB total capacity; 1.94 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 1.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:10:48,943 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 348, in forward
    attention_scores = attention_scores + attention_mask
RuntimeError: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 10.75 GiB total capacity; 1.94 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 1.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:10:49,054 - training - ERROR - Critical error in training batch 0: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 10.75 GiB total capacity; 4.54 GiB already allocated; 93.81 MiB free; 8.60 GiB allowed; 4.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:10:49,055 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 162, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 10.75 GiB total capacity; 4.54 GiB already allocated; 93.81 MiB free; 8.60 GiB allowed; 4.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:10:49,055 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 10.75 GiB total capacity; 4.54 GiB already allocated; 93.81 MiB free; 8.60 GiB allowed; 4.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:10:49,055 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 162, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 10.75 GiB total capacity; 4.54 GiB already allocated; 93.81 MiB free; 8.60 GiB allowed; 4.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:10:49,056 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 10.75 GiB total capacity; 4.54 GiB already allocated; 93.81 MiB free; 8.60 GiB allowed; 4.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:10:49,056 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 162, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 10.75 GiB total capacity; 4.54 GiB already allocated; 93.81 MiB free; 8.60 GiB allowed; 4.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:10:50,504 - training - INFO - Per-GPU batch size: 4
2025-03-21 01:10:50,552 - training - INFO - Starting epoch 1/200000
2025-03-21 01:11:21,846 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_s0hwmmkf.log
2025-03-21 01:11:21,846 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 3
2025-03-21 01:11:21,846 - training - INFO - Device: cuda:0
2025-03-21 01:11:22,408 - training - INFO - Training on 3 GPUs, distributed mode: True
2025-03-21 01:11:22,408 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 01:11:22,408 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 01:11:22,408 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 01:11:22,408 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 01:11:22,704 - training - INFO - Starting model initialization...
2025-03-21 01:11:33,291 - training - INFO - Per-GPU batch size: 4 (global batch size: 12)
2025-03-21 01:11:33,310 - training - INFO - Starting epoch 1/200000
2025-03-21 01:11:41,292 - training - INFO - GPU Memory: GPU 0: 2514.7MB/4814.0MB, GPU 1: 2530.8MB/4854.0MB, GPU 2: 2526.8MB/4826.0MB, Total: 7572.3MB allocated, 14494.0MB reserved, Mean: 2524.1MB allocated, 4831.3MB reserved
2025-03-21 01:11:41,293 - training - INFO - Epoch: 1/200000, Batch: 0/217, Loss: 10.3822, Throughput: 1.50 samples/sec
2025-03-21 01:12:28,164 - training - INFO - GPU Memory: GPU 0: 4015.6MB/7950.0MB, GPU 1: 4022.1MB/7970.0MB, GPU 2: 4030.8MB/7958.0MB, Total: 12068.5MB allocated, 23878.0MB reserved, Mean: 4022.8MB allocated, 7959.3MB reserved
2025-03-21 01:12:28,166 - training - INFO - Epoch: 1/200000, Batch: 72/217, Loss: 9.7998, Throughput: 15.97 samples/sec
2025-03-21 01:12:57,312 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_gmnqk1kj.log
2025-03-21 01:12:57,312 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 4
2025-03-21 01:12:57,312 - training - INFO - Device: cuda:0
2025-03-21 01:12:57,953 - training - INFO - Training on 4 GPUs, distributed mode: True
2025-03-21 01:12:57,953 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-21 01:12:57,953 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-21 01:12:57,953 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-21 01:12:57,953 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-21 01:12:58,732 - training - INFO - Starting model initialization...
2025-03-21 01:13:08,702 - training - INFO - Per-GPU batch size: 4 (global batch size: 16)
2025-03-21 01:13:08,723 - training - INFO - Starting epoch 1/200000
2025-03-21 01:13:17,498 - training - INFO - GPU Memory: GPU 0: 2521.1MB/4782.0MB, GPU 1: 2526.8MB/4826.0MB, GPU 2: 2530.8MB/4854.0MB, GPU 3: 2526.8MB/4826.0MB, Total: 10105.5MB allocated, 19288.0MB reserved, Mean: 2526.4MB allocated, 4822.0MB reserved
2025-03-21 01:13:17,500 - training - INFO - Epoch: 1/200000, Batch: 0/163, Loss: 10.3444, Throughput: 1.82 samples/sec
2025-03-21 01:13:19,398 - training - ERROR - Critical error in training batch 3: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.75 GiB total capacity; 6.78 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:13:19,401 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 458, in forward
    x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 475, in _mha_block
    x = self.multihead_attn(x, mem, mem,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1038, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5358, in multi_head_attention_forward
    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5039, in _scaled_dot_product_attention
    attn = dropout(attn, p=dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.75 GiB total capacity; 6.78 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:13:19,401 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.75 GiB total capacity; 6.78 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:13:19,401 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 458, in forward
    x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 475, in _mha_block
    x = self.multihead_attn(x, mem, mem,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1038, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5358, in multi_head_attention_forward
    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5039, in _scaled_dot_product_attention
    attn = dropout(attn, p=dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.75 GiB total capacity; 6.78 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:13:19,401 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.75 GiB total capacity; 6.78 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-21 01:13:19,401 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 458, in forward
    x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 475, in _mha_block
    x = self.multihead_attn(x, mem, mem,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1038, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5358, in multi_head_attention_forward
    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5039, in _scaled_dot_product_attention
    attn = dropout(attn, p=dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.75 GiB total capacity; 6.78 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-21 01:43:48,734 - training - ERROR - Critical error in training batch 3: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=886, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804853 milliseconds before timing out.
2025-03-21 01:43:48,738 - training - ERROR - Critical error in training batch 3: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=886, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804679 milliseconds before timing out.
2025-03-21 01:43:48,736 - training - ERROR - Critical error in training batch 3: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=886, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804854 milliseconds before timing out.
2025-03-21 01:43:48,742 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 162, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=886, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804853 milliseconds before timing out.

2025-03-21 01:43:48,742 - training - ERROR - Fatal error in training loop: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=886, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804853 milliseconds before timing out.
2025-03-21 01:43:48,742 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 162, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=886, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804854 milliseconds before timing out.

2025-03-21 01:43:48,742 - training - ERROR - Fatal error in training loop: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=886, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804854 milliseconds before timing out.
2025-03-21 01:43:48,742 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 162, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=886, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804853 milliseconds before timing out.

2025-03-21 01:43:48,743 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 162, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=886, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804854 milliseconds before timing out.

2025-03-21 01:43:48,743 - training - ERROR - Fatal error in main function: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=886, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804853 milliseconds before timing out.
2025-03-21 01:43:48,743 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 162, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=886, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804853 milliseconds before timing out.

2025-03-21 01:43:48,743 - training - ERROR - Fatal error in main function: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=886, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804854 milliseconds before timing out.
2025-03-21 01:43:48,743 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 162, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=886, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804679 milliseconds before timing out.

2025-03-21 01:43:48,743 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 162, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=886, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804854 milliseconds before timing out.

2025-03-21 01:43:48,743 - training - ERROR - Fatal error in training loop: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=886, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804679 milliseconds before timing out.
2025-03-21 01:43:48,743 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 162, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=886, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804679 milliseconds before timing out.

2025-03-21 01:43:48,744 - training - ERROR - Fatal error in main function: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=886, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804679 milliseconds before timing out.
2025-03-21 01:43:48,744 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 162, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=886, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804679 milliseconds before timing out.

2025-03-24 18:10:14,968 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_hom3kmk0.log
2025-03-24 18:10:14,968 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 4
2025-03-24 18:10:14,968 - training - INFO - Device: cuda:0
2025-03-24 18:10:15,534 - training - INFO - Training on 4 GPUs, distributed mode: True
2025-03-24 18:10:15,534 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 18:10:15,534 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 18:10:15,536 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 18:10:15,536 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 18:10:16,123 - training - INFO - Starting model initialization...
2025-03-24 18:10:26,866 - training - INFO - Per-GPU batch size: 4 (global batch size: 16)
2025-03-24 18:10:26,886 - training - INFO - Starting epoch 1/200000
2025-03-24 18:10:34,297 - training - INFO - GPU Memory: GPU 0: 2521.1MB/4782.0MB, GPU 1: 2530.8MB/4854.0MB, GPU 2: 2530.8MB/4854.0MB, GPU 3: 2526.8MB/4826.0MB, Total: 10109.4MB allocated, 19316.0MB reserved, Mean: 2527.4MB allocated, 4829.0MB reserved
2025-03-24 18:10:34,299 - training - INFO - Epoch: 1/200000, Batch: 0/163, Loss: 10.3303, Throughput: 2.16 samples/sec
2025-03-24 18:10:36,158 - training - ERROR - Critical error in training batch 3: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.75 GiB total capacity; 6.79 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:10:36,164 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 458, in forward
    x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 475, in _mha_block
    x = self.multihead_attn(x, mem, mem,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1038, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5358, in multi_head_attention_forward
    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5039, in _scaled_dot_product_attention
    attn = dropout(attn, p=dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.75 GiB total capacity; 6.79 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:10:36,164 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.75 GiB total capacity; 6.79 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:10:36,164 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 458, in forward
    x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 475, in _mha_block
    x = self.multihead_attn(x, mem, mem,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1038, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5358, in multi_head_attention_forward
    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5039, in _scaled_dot_product_attention
    attn = dropout(attn, p=dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.75 GiB total capacity; 6.79 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:10:36,165 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.75 GiB total capacity; 6.79 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:10:36,165 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 458, in forward
    x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 475, in _mha_block
    x = self.multihead_attn(x, mem, mem,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1038, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5358, in multi_head_attention_forward
    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5039, in _scaled_dot_product_attention
    attn = dropout(attn, p=dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.75 GiB total capacity; 6.79 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:29:54,033 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_or8i_lz6.log
2025-03-24 18:29:54,033 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 4
2025-03-24 18:29:54,033 - training - INFO - Device: cuda:0
2025-03-24 18:29:54,553 - training - INFO - Training on 4 GPUs, distributed mode: True
2025-03-24 18:29:54,553 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 18:29:54,553 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 18:29:54,555 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 18:29:54,555 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 18:29:55,469 - training - INFO - Starting model initialization...
2025-03-24 18:30:06,069 - training - INFO - Per-GPU batch size: 4 (global batch size: 16)
2025-03-24 18:30:06,089 - training - INFO - Starting epoch 1/200000
2025-03-24 18:30:14,194 - training - INFO - GPU Memory: GPU 0: 2521.1MB/4782.0MB, GPU 1: 2530.0MB/4854.0MB, GPU 2: 2530.0MB/4854.0MB, GPU 3: 2526.8MB/4826.0MB, Total: 10107.9MB allocated, 19316.0MB reserved, Mean: 2527.0MB allocated, 4829.0MB reserved
2025-03-24 18:30:14,199 - training - INFO - Epoch: 1/200000, Batch: 0/163, Loss: 10.3715, Throughput: 1.97 samples/sec
2025-03-24 18:30:16,080 - training - ERROR - Critical error in training batch 3: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.75 GiB total capacity; 6.79 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:30:16,086 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 458, in forward
    x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 475, in _mha_block
    x = self.multihead_attn(x, mem, mem,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1038, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5358, in multi_head_attention_forward
    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5039, in _scaled_dot_product_attention
    attn = dropout(attn, p=dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.75 GiB total capacity; 6.79 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:30:16,086 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.75 GiB total capacity; 6.79 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:30:16,086 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 458, in forward
    x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 475, in _mha_block
    x = self.multihead_attn(x, mem, mem,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1038, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5358, in multi_head_attention_forward
    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5039, in _scaled_dot_product_attention
    attn = dropout(attn, p=dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.75 GiB total capacity; 6.79 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:30:16,087 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.75 GiB total capacity; 6.79 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:30:16,087 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 458, in forward
    x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 475, in _mha_block
    x = self.multihead_attn(x, mem, mem,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1038, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5358, in multi_head_attention_forward
    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 5039, in _scaled_dot_product_attention
    attn = dropout(attn, p=dropout_p)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.75 GiB total capacity; 6.79 GiB already allocated; 25.81 MiB free; 8.60 GiB allowed; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:34:14,902 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_6zkqxsuy.log
2025-03-24 18:34:14,903 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-24 18:34:14,903 - training - INFO - Device: cuda:0
2025-03-24 18:34:15,576 - training - INFO - Training on 4 GPUs, distributed mode: True
2025-03-24 18:34:15,576 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 18:34:15,577 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 18:34:15,579 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 18:34:15,579 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 18:34:15,777 - training - INFO - Starting model initialization...
2025-03-24 18:34:25,304 - training - INFO - Per-GPU batch size: 4 (global batch size: 4)
2025-03-24 18:34:25,310 - training - INFO - Starting epoch 1/200000
2025-03-24 18:34:27,580 - training - INFO - GPU Memory: GPU 0: 2505.0MB/4764.0MB, Total: 2505.0MB allocated, 4764.0MB reserved, Mean: 2505.0MB allocated, 4764.0MB reserved
2025-03-24 18:34:27,581 - training - INFO - Epoch: 1/200000, Batch: 0/651, Loss: 10.3724, Throughput: 1.76 samples/sec
2025-03-24 18:34:28,136 - training - ERROR - Critical error in training batch 3: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 6.85 GiB already allocated; 9.81 MiB free; 8.60 GiB allowed; 6.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:34:28,142 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 459, in forward
    x = self.norm3(x + self._ff_block(x))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 484, in _ff_block
    return self.dropout3(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 6.85 GiB already allocated; 9.81 MiB free; 8.60 GiB allowed; 6.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:34:28,142 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 6.85 GiB already allocated; 9.81 MiB free; 8.60 GiB allowed; 6.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:34:28,142 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 459, in forward
    x = self.norm3(x + self._ff_block(x))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 484, in _ff_block
    return self.dropout3(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 6.85 GiB already allocated; 9.81 MiB free; 8.60 GiB allowed; 6.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:34:28,142 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 6.85 GiB already allocated; 9.81 MiB free; 8.60 GiB allowed; 6.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:34:28,142 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 459, in forward
    x = self.norm3(x + self._ff_block(x))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 484, in _ff_block
    return self.dropout3(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 6.85 GiB already allocated; 9.81 MiB free; 8.60 GiB allowed; 6.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:37:26,649 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_5ik1c9v6.log
2025-03-24 18:37:26,650 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-24 18:37:26,650 - training - INFO - Device: cuda:0
2025-03-24 18:37:27,178 - training - INFO - Training on 4 GPUs, distributed mode: True
2025-03-24 18:37:27,178 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 18:37:27,178 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 18:37:27,180 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 18:37:27,180 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 18:37:27,350 - training - INFO - Starting model initialization...
2025-03-24 18:37:37,125 - training - INFO - Per-GPU batch size: 4 (global batch size: 4)
2025-03-24 18:37:37,131 - training - INFO - Starting epoch 1/200000
2025-03-24 18:37:39,672 - training - INFO - GPU Memory: GPU 0: 2505.8MB/4786.0MB, Total: 2505.8MB allocated, 4786.0MB reserved, Mean: 2505.8MB allocated, 4786.0MB reserved
2025-03-24 18:37:39,672 - training - INFO - Epoch: 1/200000, Batch: 0/651, Loss: 10.3422, Throughput: 1.57 samples/sec
2025-03-24 18:37:40,199 - training - ERROR - Critical error in training batch 3: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 6.85 GiB already allocated; 7.81 MiB free; 8.60 GiB allowed; 6.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:37:40,205 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 459, in forward
    x = self.norm3(x + self._ff_block(x))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 484, in _ff_block
    return self.dropout3(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 6.85 GiB already allocated; 7.81 MiB free; 8.60 GiB allowed; 6.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:37:40,205 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 6.85 GiB already allocated; 7.81 MiB free; 8.60 GiB allowed; 6.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:37:40,205 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 459, in forward
    x = self.norm3(x + self._ff_block(x))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 484, in _ff_block
    return self.dropout3(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 6.85 GiB already allocated; 7.81 MiB free; 8.60 GiB allowed; 6.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:37:40,206 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 6.85 GiB already allocated; 7.81 MiB free; 8.60 GiB allowed; 6.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:37:40,206 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 668, in main
    train_model(
  File "train.py", line 311, in train_model
    raise e
  File "train.py", line 210, in train_model
    raise e
  File "train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 459, in forward
    x = self.norm3(x + self._ff_block(x))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 484, in _ff_block
    return self.dropout3(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 6.85 GiB already allocated; 7.81 MiB free; 8.60 GiB allowed; 6.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:38:36,770 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_3za_icpd.log
2025-03-24 18:38:36,770 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 3
2025-03-24 18:38:36,771 - training - INFO - Device: cuda:0
2025-03-24 18:38:37,326 - training - INFO - Training on 4 GPUs, distributed mode: True
2025-03-24 18:38:37,326 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 18:38:37,326 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 18:38:37,326 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 18:38:37,326 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 18:38:41,871 - training - INFO - Starting model initialization...
2025-03-24 18:38:50,978 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 254.00 MiB (GPU 1; 10.75 GiB total capacity; 930.21 MiB already allocated; 55.81 MiB free; 7.52 GiB allowed; 976.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:38:50,980 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 577, in main
    model = DDP(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 643, in __init__
    self._sync_params_and_buffers(authoritative_rank=0)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 667, in _sync_params_and_buffers
    self._distributed_broadcast_coalesced(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1543, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(
RuntimeError: CUDA out of memory. Tried to allocate 254.00 MiB (GPU 1; 10.75 GiB total capacity; 930.21 MiB already allocated; 55.81 MiB free; 7.52 GiB allowed; 976.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:38:50,979 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 254.00 MiB (GPU 2; 10.75 GiB total capacity; 930.21 MiB already allocated; 71.81 MiB free; 7.52 GiB allowed; 976.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:38:50,981 - training - ERROR - Traceback (most recent call last):
  File "train.py", line 577, in main
    model = DDP(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 643, in __init__
    self._sync_params_and_buffers(authoritative_rank=0)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 667, in _sync_params_and_buffers
    self._distributed_broadcast_coalesced(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1543, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(
RuntimeError: CUDA out of memory. Tried to allocate 254.00 MiB (GPU 2; 10.75 GiB total capacity; 930.21 MiB already allocated; 71.81 MiB free; 7.52 GiB allowed; 976.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:38:53,272 - training - INFO - Per-GPU batch size: 4 (global batch size: 12)
2025-03-24 18:40:27,293 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_4o79a62o.log
2025-03-24 18:40:27,293 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-24 18:40:27,293 - training - INFO - Device: cuda:0
2025-03-24 18:40:27,433 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_ja5dp9gq.log
2025-03-24 18:40:27,433 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_wj3drq6s.log
2025-03-24 18:40:27,433 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-24 18:40:27,433 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-24 18:40:27,434 - training - INFO - Device: cuda:0
2025-03-24 18:40:27,434 - training - INFO - Device: cuda:0
2025-03-24 18:40:28,172 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-24 18:40:28,172 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 18:40:28,172 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 18:40:28,172 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 18:40:28,172 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 18:40:28,172 - training - INFO - Starting model initialization...
2025-03-24 18:40:28,198 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-24 18:40:28,198 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-24 18:40:28,198 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 18:40:28,198 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 18:40:28,198 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 18:40:28,198 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 18:40:28,198 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 18:40:28,198 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 18:40:28,198 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 18:40:28,198 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 18:40:28,198 - training - INFO - Starting model initialization...
2025-03-24 18:40:28,198 - training - INFO - Starting model initialization...
2025-03-24 18:40:56,710 - training - INFO - Per-GPU batch size: 4
2025-03-24 18:40:56,710 - training - INFO - Per-GPU batch size: 4
2025-03-24 18:40:56,719 - training - INFO - Starting epoch 1/200000
2025-03-24 18:40:56,719 - training - INFO - Starting epoch 1/200000
2025-03-24 18:40:57,292 - training - INFO - Per-GPU batch size: 4
2025-03-24 18:40:57,300 - training - INFO - Starting epoch 1/200000
2025-03-24 18:40:59,148 - training - ERROR - Critical error in training batch 0: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 0; 10.75 GiB total capacity; 3.05 GiB already allocated; 9.81 MiB free; 8.60 GiB allowed; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:40:59,164 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 220, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 447, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 443, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 0; 10.75 GiB total capacity; 3.05 GiB already allocated; 9.81 MiB free; 8.60 GiB allowed; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:40:59,165 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 0; 10.75 GiB total capacity; 3.05 GiB already allocated; 9.81 MiB free; 8.60 GiB allowed; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:40:59,165 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 210, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 220, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 447, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 443, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 0; 10.75 GiB total capacity; 3.05 GiB already allocated; 9.81 MiB free; 8.60 GiB allowed; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:40:59,166 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 0; 10.75 GiB total capacity; 3.05 GiB already allocated; 9.81 MiB free; 8.60 GiB allowed; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:40:59,166 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 668, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 311, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 210, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 220, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 447, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 443, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 0; 10.75 GiB total capacity; 3.05 GiB already allocated; 9.81 MiB free; 8.60 GiB allowed; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:40:59,351 - training - ERROR - Critical error in training batch 0: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.75 GiB total capacity; 3.36 GiB already allocated; 5.81 MiB free; 8.60 GiB allowed; 3.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:40:59,352 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 459, in forward
    x = self.norm3(x + self._ff_block(x))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 483, in _ff_block
    x = self.linear2(self.dropout(self.activation(self.linear1(x))))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1442, in relu
    result = torch.relu(input)
RuntimeError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.75 GiB total capacity; 3.36 GiB already allocated; 5.81 MiB free; 8.60 GiB allowed; 3.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:40:59,353 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.75 GiB total capacity; 3.36 GiB already allocated; 5.81 MiB free; 8.60 GiB allowed; 3.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:40:59,353 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 210, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 459, in forward
    x = self.norm3(x + self._ff_block(x))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 483, in _ff_block
    x = self.linear2(self.dropout(self.activation(self.linear1(x))))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1442, in relu
    result = torch.relu(input)
RuntimeError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.75 GiB total capacity; 3.36 GiB already allocated; 5.81 MiB free; 8.60 GiB allowed; 3.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:40:59,353 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.75 GiB total capacity; 3.36 GiB already allocated; 5.81 MiB free; 8.60 GiB allowed; 3.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:40:59,353 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 668, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 311, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 210, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 237, in forward
    decoder_output = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 252, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 459, in forward
    x = self.norm3(x + self._ff_block(x))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 483, in _ff_block
    x = self.linear2(self.dropout(self.activation(self.linear1(x))))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1442, in relu
    result = torch.relu(input)
RuntimeError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.75 GiB total capacity; 3.36 GiB already allocated; 5.81 MiB free; 8.60 GiB allowed; 3.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:40:59,408 - training - ERROR - Critical error in training batch 0: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
2025-03-24 18:40:59,412 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 289, in forward
    mixed_query_layer = self.query(hidden_states)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`

2025-03-24 18:40:59,412 - training - ERROR - Fatal error in training loop: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
2025-03-24 18:40:59,413 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 210, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 289, in forward
    mixed_query_layer = self.query(hidden_states)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`

2025-03-24 18:40:59,413 - training - ERROR - Fatal error in main function: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
2025-03-24 18:40:59,413 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 668, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 311, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 210, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 148, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 212, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1017, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 606, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 289, in forward
    mixed_query_layer = self.query(hidden_states)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`

2025-03-24 18:50:12,288 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_vs_hzke9.log
2025-03-24 18:50:12,288 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-24 18:50:12,288 - training - INFO - Device: cuda:0
2025-03-24 18:50:12,837 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-24 18:50:12,838 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 18:50:12,838 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 18:50:12,840 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 18:50:12,840 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 18:50:15,803 - training - INFO - Starting model initialization...
2025-03-24 18:50:25,337 - training - INFO - Per-GPU batch size: 2 (global batch size: 2)
2025-03-24 18:50:25,345 - training - INFO - Starting epoch 1/200000
2025-03-24 18:50:27,909 - training - INFO - GPU Memory: GPU 0: 2484.2MB/4992.0MB, Total: 2484.2MB allocated, 4992.0MB reserved, Mean: 2484.2MB allocated, 4992.0MB reserved
2025-03-24 18:50:27,910 - training - INFO - Epoch: 1/200000, Batch: 0/1302, Loss: 10.3211, Throughput: 0.78 samples/sec
2025-03-24 18:51:36,205 - training - INFO - GPU Memory: GPU 0: 3974.7MB/6160.0MB, Total: 3974.7MB allocated, 6160.0MB reserved, Mean: 3974.7MB allocated, 6160.0MB reserved
2025-03-24 18:51:36,207 - training - INFO - Epoch: 1/200000, Batch: 434/1302, Loss: 9.2542, Throughput: 12.28 samples/sec
2025-03-24 18:52:44,413 - training - INFO - GPU Memory: GPU 0: 3974.7MB/6160.0MB, Total: 3974.7MB allocated, 6160.0MB reserved, Mean: 3974.7MB allocated, 6160.0MB reserved
2025-03-24 18:52:44,413 - training - INFO - Epoch: 1/200000, Batch: 868/1302, Loss: 8.3825, Throughput: 12.50 samples/sec
2025-03-24 18:53:40,917 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_i2liu3ff.log
2025-03-24 18:53:40,917 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 9
2025-03-24 18:53:40,917 - training - INFO - Device: cuda:0
2025-03-24 18:53:41,499 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-24 18:53:41,499 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 18:53:41,499 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 18:53:41,501 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 18:53:41,501 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 18:53:46,619 - training - INFO - Starting model initialization...
2025-03-24 18:53:54,570 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 10.75 GiB total capacity; 57.75 MiB already allocated; 3.81 MiB free; 68.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:53:54,577 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 566, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 10.75 GiB total capacity; 57.75 MiB already allocated; 3.81 MiB free; 68.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:53:54,578 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-24 18:53:54,579 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 566, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-24 18:53:54,580 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-24 18:53:54,580 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 566, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-24 18:53:54,587 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-24 18:53:54,587 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 566, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-24 18:53:54,588 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-24 18:53:54,589 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 566, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-24 18:53:54,613 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 23.16 MiB already allocated; 1.81 MiB free; 24.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:53:54,613 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 566, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 23.16 MiB already allocated; 1.81 MiB free; 24.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:53:54,616 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.75 GiB total capacity; 9.75 MiB already allocated; 1.81 MiB free; 22.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:53:54,616 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 566, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.75 GiB total capacity; 9.75 MiB already allocated; 1.81 MiB free; 22.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:53:54,633 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 43.22 MiB already allocated; 1.81 MiB free; 46.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 18:53:54,633 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 566, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 43.22 MiB already allocated; 1.81 MiB free; 46.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 18:53:58,032 - training - INFO - Per-GPU batch size: 2 (global batch size: 18)
2025-03-24 18:55:43,510 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_cs32sr3n.log
2025-03-24 18:55:43,510 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 4
2025-03-24 18:55:43,510 - training - INFO - Device: cuda:0
2025-03-24 18:55:44,001 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-24 18:55:44,001 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 18:55:44,001 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 18:55:44,002 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 18:55:44,002 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 18:55:48,178 - training - INFO - Starting model initialization...
2025-03-24 18:55:58,225 - training - INFO - Per-GPU batch size: 2 (global batch size: 8)
2025-03-24 18:55:58,817 - training - INFO - Starting epoch 1/200000
2025-03-24 18:56:09,075 - training - INFO - GPU Memory: GPU 0: 2495.6MB/5008.0MB, GPU 1: 2497.8MB/4996.0MB, GPU 2: 2493.4MB/5012.0MB, GPU 3: 2497.8MB/4996.0MB, Total: 9984.7MB allocated, 20012.0MB reserved, Mean: 2496.2MB allocated, 5003.0MB reserved
2025-03-24 18:56:09,076 - training - INFO - Epoch: 1/200000, Batch: 0/326, Loss: 10.3079, Throughput: 0.78 samples/sec
2025-03-24 20:10:59,965 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_nsvr1lnc.log
2025-03-24 20:10:59,965 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 4
2025-03-24 20:10:59,965 - training - INFO - Device: cuda:0
2025-03-24 20:11:00,508 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-24 20:11:00,509 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 20:11:00,509 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 20:11:00,511 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 20:11:00,511 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 20:11:04,109 - training - INFO - Starting model initialization...
2025-03-24 20:11:15,017 - training - INFO - Per-GPU batch size: 1 (global batch size: 4)
2025-03-24 20:11:15,037 - training - INFO - Starting epoch 1/200000
2025-03-24 20:11:23,889 - training - INFO - GPU Memory: GPU 0: 3193.7MB/4556.0MB, GPU 1: 3201.6MB/4572.0MB, GPU 2: 3202.7MB/4568.0MB, GPU 3: 3201.6MB/4572.0MB, Total: 12799.7MB allocated, 18268.0MB reserved, Mean: 3199.9MB allocated, 4567.0MB reserved
2025-03-24 20:11:23,889 - training - INFO - Epoch: 1/200000, Batch: 0/651, Loss: 10.3430, Throughput: 0.45 samples/sec
2025-03-24 20:15:43,779 - training - INFO - GPU Memory: GPU 0: 3193.7MB/4856.0MB, GPU 1: 3201.7MB/4862.0MB, GPU 2: 3202.7MB/4858.0MB, GPU 3: 3201.7MB/4862.0MB, Total: 12799.8MB allocated, 19438.0MB reserved, Mean: 3200.0MB allocated, 4859.5MB reserved
2025-03-24 20:15:43,782 - training - INFO - Epoch: 1/200000, Batch: 217/651, Loss: 8.8599, Throughput: 3.24 samples/sec
2025-03-24 20:20:02,843 - training - INFO - GPU Memory: GPU 0: 3193.7MB/4856.0MB, GPU 1: 3201.7MB/4862.0MB, GPU 2: 3202.7MB/4858.0MB, GPU 3: 3201.7MB/4862.0MB, Total: 12799.8MB allocated, 19438.0MB reserved, Mean: 3200.0MB allocated, 4859.5MB reserved
2025-03-24 20:20:02,845 - training - INFO - Epoch: 1/200000, Batch: 434/651, Loss: 7.7098, Throughput: 3.30 samples/sec
2025-03-24 20:21:45,528 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_ujgp7pgy.log
2025-03-24 20:21:45,529 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 4
2025-03-24 20:21:45,529 - training - INFO - Device: cuda:0
2025-03-24 20:21:46,023 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-24 20:21:46,023 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 20:21:46,023 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 20:21:46,025 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 20:21:46,025 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 20:21:53,893 - training - INFO - Starting model initialization...
2025-03-24 20:22:01,268 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-24 20:22:01,269 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 564, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-24 20:22:01,276 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-24 20:22:01,276 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 564, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-24 20:22:01,314 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-24 20:22:01,314 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 564, in main
    model = AnsweringAgent(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 116, in __init__
    self.feature_extractor = FeatureExtractor(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 50, in __init__
    self._init_darknet(config)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 221, in _init_darknet
    new_state = torch.load(config.data.darknet_weights_path)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/opt/conda/lib/python3.8/site-packages/torch/serialization.py", line 158, in _cuda_deserialize
    return obj.cuda(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/_utils.py", line 79, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 661, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-24 20:24:50,791 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_mgg82ko5.log
2025-03-24 20:24:50,791 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 4
2025-03-24 20:24:50,791 - training - INFO - Device: cuda:0
2025-03-24 20:24:51,297 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-24 20:24:51,297 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 20:24:51,297 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 20:24:51,303 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 20:24:51,303 - training - INFO - Dataset preprocessing complete. Took 0.01 seconds.
2025-03-24 20:24:59,038 - training - INFO - Starting model initialization...
2025-03-24 20:25:10,712 - training - INFO - Per-GPU batch size: 1 (global batch size: 4)
2025-03-24 20:25:10,720 - training - INFO - Starting epoch 1/200000
2025-03-24 20:25:15,871 - training - INFO - GPU Memory: GPU 0: 3204.5MB/4382.0MB, GPU 1: 3201.6MB/4572.0MB, GPU 2: 3202.8MB/4568.0MB, GPU 3: 3201.6MB/4572.0MB, Total: 12810.5MB allocated, 18094.0MB reserved, Mean: 3202.6MB allocated, 4523.5MB reserved
2025-03-24 20:25:15,871 - training - INFO - Epoch: 1/200000, Batch: 0/651, Loss: 10.3659, Throughput: 0.78 samples/sec
2025-03-24 20:25:16,061 - training - ERROR - Critical error in training batch 1: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 10.75 GiB total capacity; 4.25 GiB already allocated; 17.81 MiB free; 7.52 GiB allowed; 4.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 20:25:16,066 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 145, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 247, in forward
    output = self.output_projection(decoder_output)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 10.75 GiB total capacity; 4.25 GiB already allocated; 17.81 MiB free; 7.52 GiB allowed; 4.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 20:25:16,066 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 10.75 GiB total capacity; 4.25 GiB already allocated; 17.81 MiB free; 7.52 GiB allowed; 4.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 20:25:16,066 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 208, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 145, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 247, in forward
    output = self.output_projection(decoder_output)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 10.75 GiB total capacity; 4.25 GiB already allocated; 17.81 MiB free; 7.52 GiB allowed; 4.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 20:25:16,067 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 10.75 GiB total capacity; 4.25 GiB already allocated; 17.81 MiB free; 7.52 GiB allowed; 4.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-24 20:25:16,067 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 666, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 309, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 208, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 145, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 247, in forward
    output = self.output_projection(decoder_output)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 10.75 GiB total capacity; 4.25 GiB already allocated; 17.81 MiB free; 7.52 GiB allowed; 4.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-24 20:55:49,800 - training - ERROR - Critical error in training batch 1: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=600, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804770 milliseconds before timing out.
2025-03-24 20:55:49,802 - training - ERROR - Critical error in training batch 1: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=600, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804743 milliseconds before timing out.
2025-03-24 20:55:49,802 - training - ERROR - Critical error in training batch 1: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=600, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804859 milliseconds before timing out.
2025-03-24 20:55:49,812 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 167, in train_model
    dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1314, in all_reduce
    work = default_pg.allreduce([tensor], opts)
RuntimeError: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=600, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804770 milliseconds before timing out.

2025-03-24 20:55:49,812 - training - ERROR - Fatal error in training loop: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=600, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804770 milliseconds before timing out.
2025-03-24 20:55:49,812 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 208, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 167, in train_model
    dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1314, in all_reduce
    work = default_pg.allreduce([tensor], opts)
RuntimeError: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=600, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804770 milliseconds before timing out.

2025-03-24 20:55:49,813 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 167, in train_model
    dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1314, in all_reduce
    work = default_pg.allreduce([tensor], opts)
RuntimeError: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=600, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804743 milliseconds before timing out.

2025-03-24 20:55:49,813 - training - ERROR - Fatal error in training loop: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=600, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804743 milliseconds before timing out.
2025-03-24 20:55:49,813 - training - ERROR - Fatal error in main function: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=600, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804770 milliseconds before timing out.
2025-03-24 20:55:49,813 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 666, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 309, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 208, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 167, in train_model
    dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1314, in all_reduce
    work = default_pg.allreduce([tensor], opts)
RuntimeError: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=600, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804770 milliseconds before timing out.

2025-03-24 20:55:49,813 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 208, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 167, in train_model
    dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1314, in all_reduce
    work = default_pg.allreduce([tensor], opts)
RuntimeError: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=600, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804743 milliseconds before timing out.

2025-03-24 20:55:49,814 - training - ERROR - Fatal error in main function: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=600, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804743 milliseconds before timing out.
2025-03-24 20:55:49,814 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 666, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 309, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 208, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 167, in train_model
    dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1314, in all_reduce
    work = default_pg.allreduce([tensor], opts)
RuntimeError: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=600, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804743 milliseconds before timing out.

2025-03-24 20:55:49,814 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 167, in train_model
    dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1314, in all_reduce
    work = default_pg.allreduce([tensor], opts)
RuntimeError: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=600, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804859 milliseconds before timing out.

2025-03-24 20:55:49,814 - training - ERROR - Fatal error in training loop: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=600, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804859 milliseconds before timing out.
2025-03-24 20:55:49,815 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 208, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 167, in train_model
    dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1314, in all_reduce
    work = default_pg.allreduce([tensor], opts)
RuntimeError: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=600, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804859 milliseconds before timing out.

2025-03-24 20:55:49,815 - training - ERROR - Fatal error in main function: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=600, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804859 milliseconds before timing out.
2025-03-24 20:55:49,815 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 666, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 309, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 208, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 167, in train_model
    dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1314, in all_reduce
    work = default_pg.allreduce([tensor], opts)
RuntimeError: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=600, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804859 milliseconds before timing out.

2025-03-24 22:08:08,907 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_g1v9mvox.log
2025-03-24 22:08:08,907 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 4
2025-03-24 22:08:08,907 - training - INFO - Device: cuda:0
2025-03-24 22:08:09,605 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-24 22:08:09,605 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 22:08:09,605 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 22:08:09,607 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 22:08:09,607 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 22:08:13,199 - training - INFO - Starting model initialization...
2025-03-24 22:08:22,625 - training - INFO - Per-GPU batch size: 1 (global batch size: 4)
2025-03-24 22:08:22,633 - training - INFO - Starting epoch 1/200000
2025-03-24 22:08:26,830 - training - INFO - GPU Memory: GPU 0: 1527.0MB/4390.0MB, GPU 1: 1527.0MB/4386.0MB, GPU 2: 1527.0MB/4390.0MB, GPU 3: 1527.0MB/4386.0MB, Total: 6108.0MB allocated, 17552.0MB reserved, Mean: 1527.0MB allocated, 4388.0MB reserved
2025-03-24 22:08:26,830 - training - INFO - Epoch: 1/200000, Batch: 0/651, Loss: 10.5083, Throughput: 0.95 samples/sec
2025-03-24 22:12:33,121 - training - INFO - GPU Memory: GPU 0: 2846.3MB/4446.0MB, GPU 1: 2844.8MB/4440.0MB, GPU 2: 2845.1MB/4442.0MB, GPU 3: 2845.6MB/4446.0MB, Total: 11381.8MB allocated, 17774.0MB reserved, Mean: 2845.4MB allocated, 4443.5MB reserved
2025-03-24 22:12:33,123 - training - INFO - Epoch: 1/200000, Batch: 217/651, Loss: 5.6327, Throughput: 3.48 samples/sec
2025-03-24 22:13:25,582 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_873jq_99.log
2025-03-24 22:13:25,582 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 8
2025-03-24 22:13:25,582 - training - INFO - Device: cuda:0
2025-03-24 22:13:26,116 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-24 22:13:26,116 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 22:13:26,116 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 22:13:26,118 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 22:13:26,118 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 22:13:30,812 - training - INFO - Starting model initialization...
2025-03-24 22:13:40,354 - training - INFO - Per-GPU batch size: 1 (global batch size: 8)
2025-03-24 22:13:40,361 - training - INFO - Starting epoch 1/200000
2025-03-24 22:13:45,360 - training - INFO - GPU Memory: GPU 0: 2843.5MB/4386.0MB, GPU 1: 2843.5MB/4386.0MB, GPU 2: 2842.3MB/4390.0MB, GPU 3: 2843.5MB/4386.0MB, GPU 4: 2842.3MB/4392.0MB, GPU 5: 2843.5MB/4386.0MB, GPU 6: 2843.5MB/4386.0MB, GPU 7: 2842.3MB/4392.0MB, Total: 22744.5MB allocated, 35104.0MB reserved, Mean: 2843.1MB allocated, 4388.0MB reserved
2025-03-24 22:13:45,362 - training - INFO - Epoch: 1/200000, Batch: 0/326, Loss: 11.4178, Throughput: 1.60 samples/sec
2025-03-24 22:16:01,217 - training - INFO - GPU Memory: GPU 0: 2843.5MB/4310.0MB, GPU 1: 2843.5MB/4310.0MB, GPU 2: 2842.4MB/4306.0MB, GPU 3: 2843.5MB/4310.0MB, GPU 4: 2842.4MB/4306.0MB, GPU 5: 2843.5MB/4310.0MB, GPU 6: 2843.5MB/4310.0MB, GPU 7: 2842.4MB/4306.0MB, Total: 22744.8MB allocated, 34468.0MB reserved, Mean: 2843.1MB allocated, 4308.5MB reserved
2025-03-24 22:16:01,219 - training - INFO - Epoch: 1/200000, Batch: 108/326, Loss: 5.9202, Throughput: 6.19 samples/sec
2025-03-24 22:18:16,840 - training - INFO - GPU Memory: GPU 0: 2843.5MB/4310.0MB, GPU 1: 2843.5MB/4310.0MB, GPU 2: 2842.4MB/4306.0MB, GPU 3: 2843.5MB/4310.0MB, GPU 4: 2842.4MB/4306.0MB, GPU 5: 2843.5MB/4310.0MB, GPU 6: 2843.5MB/4310.0MB, GPU 7: 2842.4MB/4306.0MB, Total: 22744.8MB allocated, 34468.0MB reserved, Mean: 2843.1MB allocated, 4308.5MB reserved
2025-03-24 22:18:16,842 - training - INFO - Epoch: 1/200000, Batch: 216/326, Loss: 5.3236, Throughput: 6.28 samples/sec
2025-03-24 22:20:32,366 - training - INFO - GPU Memory: GPU 0: 2843.5MB/4310.0MB, GPU 1: 2843.5MB/4310.0MB, GPU 2: 2842.4MB/4306.0MB, GPU 3: 2843.5MB/4310.0MB, GPU 4: 2842.4MB/4306.0MB, GPU 5: 2843.5MB/4310.0MB, GPU 6: 2843.5MB/4310.0MB, GPU 7: 2842.4MB/4306.0MB, Total: 22744.8MB allocated, 34468.0MB reserved, Mean: 2843.1MB allocated, 4308.5MB reserved
2025-03-24 22:20:32,368 - training - INFO - Epoch: 1/200000, Batch: 324/326, Loss: 5.0663, Throughput: 6.31 samples/sec
2025-03-24 22:20:33,713 - training - INFO - Epoch 1 completed in 413.35s. Average loss: 5.0760
2025-03-24 22:20:33,717 - training - INFO - Starting epoch 2/200000
2025-03-24 22:20:34,904 - training - INFO - GPU Memory: GPU 0: 2843.5MB/4310.0MB, GPU 1: 2843.5MB/4310.0MB, GPU 2: 2842.4MB/4306.0MB, GPU 3: 2843.5MB/4310.0MB, GPU 4: 2842.4MB/4306.0MB, GPU 5: 2843.5MB/4310.0MB, GPU 6: 2843.5MB/4310.0MB, GPU 7: 2842.4MB/4306.0MB, Total: 22744.8MB allocated, 34468.0MB reserved, Mean: 2843.1MB allocated, 4308.5MB reserved
2025-03-24 22:20:34,904 - training - INFO - Epoch: 2/200000, Batch: 0/326, Loss: 3.9881, Throughput: 6.74 samples/sec
2025-03-24 22:22:50,186 - training - INFO - GPU Memory: GPU 0: 2843.5MB/4310.0MB, GPU 1: 2843.5MB/4310.0MB, GPU 2: 2842.4MB/4306.0MB, GPU 3: 2843.5MB/4310.0MB, GPU 4: 2842.4MB/4306.0MB, GPU 5: 2843.5MB/4310.0MB, GPU 6: 2843.5MB/4310.0MB, GPU 7: 2842.4MB/4306.0MB, Total: 22744.8MB allocated, 34468.0MB reserved, Mean: 2843.1MB allocated, 4308.5MB reserved
2025-03-24 22:22:50,188 - training - INFO - Epoch: 2/200000, Batch: 108/326, Loss: 4.4612, Throughput: 6.39 samples/sec
2025-03-24 22:25:05,465 - training - INFO - GPU Memory: GPU 0: 2843.5MB/4310.0MB, GPU 1: 2843.5MB/4310.0MB, GPU 2: 2842.4MB/4306.0MB, GPU 3: 2843.5MB/4310.0MB, GPU 4: 2842.4MB/4306.0MB, GPU 5: 2843.5MB/4310.0MB, GPU 6: 2843.5MB/4310.0MB, GPU 7: 2842.4MB/4306.0MB, Total: 22744.8MB allocated, 34468.0MB reserved, Mean: 2843.1MB allocated, 4308.5MB reserved
2025-03-24 22:25:05,467 - training - INFO - Epoch: 2/200000, Batch: 216/326, Loss: 4.4094, Throughput: 6.39 samples/sec
2025-03-24 22:27:20,873 - training - INFO - GPU Memory: GPU 0: 2843.5MB/4310.0MB, GPU 1: 2843.5MB/4310.0MB, GPU 2: 2842.4MB/4306.0MB, GPU 3: 2843.5MB/4310.0MB, GPU 4: 2842.4MB/4306.0MB, GPU 5: 2843.5MB/4310.0MB, GPU 6: 2843.5MB/4310.0MB, GPU 7: 2842.4MB/4306.0MB, Total: 22744.8MB allocated, 34468.0MB reserved, Mean: 2843.1MB allocated, 4308.5MB reserved
2025-03-24 22:27:20,875 - training - INFO - Epoch: 2/200000, Batch: 324/326, Loss: 4.3749, Throughput: 6.39 samples/sec
2025-03-24 22:27:22,219 - training - INFO - Epoch 2 completed in 408.50s. Average loss: 4.3783
2025-03-24 22:27:22,222 - training - INFO - Starting epoch 3/200000
2025-03-24 22:27:23,430 - training - INFO - GPU Memory: GPU 0: 2843.5MB/4310.0MB, GPU 1: 2843.5MB/4310.0MB, GPU 2: 2842.4MB/4306.0MB, GPU 3: 2843.5MB/4310.0MB, GPU 4: 2842.4MB/4306.0MB, GPU 5: 2843.5MB/4310.0MB, GPU 6: 2843.5MB/4310.0MB, GPU 7: 2842.4MB/4306.0MB, Total: 22744.8MB allocated, 34468.0MB reserved, Mean: 2843.1MB allocated, 4308.5MB reserved
2025-03-24 22:27:23,430 - training - INFO - Epoch: 3/200000, Batch: 0/326, Loss: 4.6935, Throughput: 6.62 samples/sec
2025-03-24 22:29:39,034 - training - INFO - GPU Memory: GPU 0: 2843.5MB/4310.0MB, GPU 1: 2843.5MB/4310.0MB, GPU 2: 2842.4MB/4306.0MB, GPU 3: 2843.5MB/4310.0MB, GPU 4: 2842.4MB/4306.0MB, GPU 5: 2843.5MB/4310.0MB, GPU 6: 2843.5MB/4310.0MB, GPU 7: 2842.4MB/4306.0MB, Total: 22744.8MB allocated, 34468.0MB reserved, Mean: 2843.1MB allocated, 4308.5MB reserved
2025-03-24 22:29:39,036 - training - INFO - Epoch: 3/200000, Batch: 108/326, Loss: 4.2206, Throughput: 6.37 samples/sec
2025-03-24 22:31:54,219 - training - INFO - GPU Memory: GPU 0: 2843.5MB/4310.0MB, GPU 1: 2843.5MB/4310.0MB, GPU 2: 2842.4MB/4306.0MB, GPU 3: 2843.5MB/4310.0MB, GPU 4: 2842.4MB/4306.0MB, GPU 5: 2843.5MB/4310.0MB, GPU 6: 2843.5MB/4310.0MB, GPU 7: 2842.4MB/4306.0MB, Total: 22744.8MB allocated, 34468.0MB reserved, Mean: 2843.1MB allocated, 4308.5MB reserved
2025-03-24 22:31:54,222 - training - INFO - Epoch: 3/200000, Batch: 216/326, Loss: 4.2459, Throughput: 6.38 samples/sec
2025-03-24 22:33:01,178 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_j_hjmuve.log
2025-03-24 22:33:01,179 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 8
2025-03-24 22:33:01,179 - training - INFO - Device: cuda:0
2025-03-24 22:33:01,725 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-24 22:33:01,725 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 22:33:01,725 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 22:33:01,727 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 22:33:01,727 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 22:33:11,339 - training - INFO - Starting model initialization...
2025-03-24 22:33:22,957 - training - INFO - Per-GPU batch size: 1 (global batch size: 8)
2025-03-24 22:33:22,973 - training - INFO - Starting epoch 1/200000
2025-03-24 22:33:31,325 - training - INFO - GPU Memory: GPU 0: 2843.5MB/3920.0MB, GPU 1: 2843.5MB/3920.0MB, GPU 2: 2842.3MB/3904.0MB, GPU 3: 2843.5MB/3920.0MB, GPU 4: 2843.5MB/3920.0MB, GPU 5: 2843.5MB/3920.0MB, GPU 6: 2842.3MB/3904.0MB, GPU 7: 2843.5MB/4386.0MB, Total: 22745.7MB allocated, 31794.0MB reserved, Mean: 2843.2MB allocated, 3974.2MB reserved
2025-03-24 22:33:31,325 - training - INFO - Epoch: 1/200000, Batch: 0/326, Loss: 11.1472, Throughput: 0.96 samples/sec
2025-03-24 22:44:58,689 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_c4adn8z6.log
2025-03-24 22:44:58,689 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 8
2025-03-24 22:44:58,689 - training - INFO - Device: cuda:0
2025-03-24 22:44:59,194 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-24 22:44:59,195 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 22:44:59,195 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 22:44:59,197 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 22:44:59,197 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 22:45:08,517 - training - INFO - Starting model initialization...
2025-03-24 22:45:20,460 - training - INFO - Per-GPU batch size: 1 (global batch size: 8)
2025-03-24 22:45:20,469 - training - INFO - Starting epoch 1/200000
2025-03-24 22:45:20,469 - training - INFO - Starting profiling for the first 10 batches...
2025-03-24 22:45:29,723 - training - INFO - GPU Memory: GPU 0: 2842.3MB/3904.0MB, GPU 1: 2843.5MB/3920.0MB, GPU 2: 2843.5MB/3920.0MB, GPU 3: 2843.5MB/3920.0MB, GPU 4: 2843.5MB/3920.0MB, GPU 5: 2843.5MB/3920.0MB, GPU 6: 2843.5MB/3920.0MB, GPU 7: 2843.5MB/4386.0MB, Total: 22746.9MB allocated, 31810.0MB reserved, Mean: 2843.4MB allocated, 3976.2MB reserved
2025-03-24 22:45:29,725 - training - INFO - Epoch: 1/200000, Batch: 0/326, Loss: 11.3785, Throughput: 0.86 samples/sec
2025-03-24 23:12:57,104 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_ghv_zsj2.log
2025-03-24 23:12:57,104 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 4
2025-03-24 23:12:57,104 - training - INFO - Device: cuda:0
2025-03-24 23:12:57,614 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-24 23:12:57,614 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 23:12:57,614 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 23:12:57,616 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 23:12:57,616 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 23:13:02,317 - training - INFO - Starting model initialization...
2025-03-24 23:13:12,046 - training - INFO - Per-GPU batch size: 4 (global batch size: 16)
2025-03-24 23:13:12,054 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-24 23:13:12,058 - training - INFO - Starting epoch 1/200000
2025-03-24 23:13:12,058 - training - INFO - Starting profiling for the first 10 batches...
2025-03-24 23:13:17,724 - training - INFO - GPU Memory: GPU 0: 2269.5MB/4730.0MB, GPU 1: 2271.5MB/4770.0MB, GPU 2: 2272.2MB/4770.0MB, GPU 3: 2272.2MB/4770.0MB, Total: 9085.4MB allocated, 19040.0MB reserved, Mean: 2271.4MB allocated, 4760.0MB reserved
2025-03-24 23:13:17,725 - training - INFO - Epoch: 1/200000, Batch: 0/163, Loss: 11.0846, Throughput: 2.82 samples/sec
2025-03-24 23:17:04,499 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_4lxsuz1u.log
2025-03-24 23:17:04,500 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 4
2025-03-24 23:17:04,500 - training - INFO - Device: cuda:0
2025-03-24 23:17:05,183 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-24 23:17:05,183 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 23:17:05,183 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 23:17:05,185 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 23:17:05,185 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 23:17:08,544 - training - INFO - Starting model initialization...
2025-03-24 23:17:17,939 - training - INFO - Per-GPU batch size: 4 (global batch size: 16)
2025-03-24 23:17:17,947 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-24 23:17:17,950 - training - INFO - Starting epoch 1/200000
2025-03-24 23:17:17,950 - training - INFO - Starting profiling for the first 10 batches...
2025-03-24 23:17:23,154 - training - INFO - GPU Memory: GPU 0: 2268.0MB/4730.0MB, GPU 1: 2271.1MB/4770.0MB, GPU 2: 2272.2MB/4770.0MB, GPU 3: 2272.2MB/4770.0MB, Total: 9083.6MB allocated, 19040.0MB reserved, Mean: 2270.9MB allocated, 4760.0MB reserved
2025-03-24 23:17:23,154 - training - INFO - Epoch: 1/200000, Batch: 0/163, Loss: 10.5378, Throughput: 3.07 samples/sec
2025-03-24 23:25:28,525 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_0i9f6nt4.log
2025-03-24 23:25:28,525 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 4
2025-03-24 23:25:28,525 - training - INFO - Device: cuda:0
2025-03-24 23:25:29,121 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-24 23:25:29,121 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-24 23:25:29,121 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-24 23:25:29,123 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-24 23:25:29,123 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-24 23:25:32,795 - training - INFO - Starting model initialization...
2025-03-24 23:25:42,359 - training - INFO - Per-GPU batch size: 4 (global batch size: 16)
2025-03-24 23:25:42,368 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-24 23:25:42,371 - training - INFO - Starting epoch 1/200000
2025-03-24 23:25:47,784 - training - INFO - GPU Memory: GPU 0: 2260.7MB/4748.0MB, GPU 1: 2256.4MB/4708.0MB, GPU 2: 2259.6MB/4748.0MB, GPU 3: 2260.7MB/4748.0MB, Total: 9037.4MB allocated, 18952.0MB reserved, Mean: 2259.4MB allocated, 4738.0MB reserved
2025-03-24 23:25:47,784 - training - INFO - Epoch: 1/200000, Batch: 0/163, Loss: 11.0081, Throughput: 2.96 samples/sec
2025-03-24 23:26:23,262 - training - INFO - GPU Memory: GPU 0: 3583.6MB/7510.0MB, GPU 1: 3581.5MB/7512.0MB, GPU 2: 3577.1MB/7510.0MB, GPU 3: 3577.1MB/7516.0MB, Total: 14319.4MB allocated, 30048.0MB reserved, Mean: 3579.8MB allocated, 7512.0MB reserved
2025-03-24 23:26:23,264 - training - INFO - Epoch: 1/200000, Batch: 54/163, Loss: 8.2133, Throughput: 21.52 samples/sec
2025-03-24 23:26:58,961 - training - INFO - GPU Memory: GPU 0: 3585.4MB/7516.0MB, GPU 1: 3578.2MB/7516.0MB, GPU 2: 3588.0MB/7524.0MB, GPU 3: 3581.8MB/7512.0MB, Total: 14333.3MB allocated, 30068.0MB reserved, Mean: 3583.3MB allocated, 7517.0MB reserved
2025-03-24 23:26:58,963 - training - INFO - Epoch: 1/200000, Batch: 108/163, Loss: 7.2359, Throughput: 22.77 samples/sec
2025-03-24 23:27:35,092 - training - INFO - GPU Memory: GPU 0: 3571.9MB/6122.0MB, GPU 1: 3576.4MB/6328.0MB, GPU 2: 3576.9MB/6090.0MB, GPU 3: 3559.1MB/6060.0MB, Total: 14284.3MB allocated, 24600.0MB reserved, Mean: 3571.1MB allocated, 6150.0MB reserved
2025-03-24 23:27:35,093 - training - INFO - Epoch: 1/200000, Batch: 162/163, Loss: 6.6897, Throughput: 23.14 samples/sec
2025-03-24 23:27:35,093 - training - INFO - Epoch 1 completed in 112.72s. Average loss: 6.6775
2025-03-24 23:27:35,104 - training - INFO - Starting epoch 2/200000
2025-03-24 23:27:35,759 - training - INFO - GPU Memory: GPU 0: 3579.3MB/7032.0MB, GPU 1: 3576.5MB/7046.0MB, GPU 2: 3585.4MB/7032.0MB, GPU 3: 3577.9MB/7036.0MB, Total: 14319.1MB allocated, 28146.0MB reserved, Mean: 3579.8MB allocated, 7036.5MB reserved
2025-03-24 23:27:35,759 - training - INFO - Epoch: 2/200000, Batch: 0/163, Loss: 5.3424, Throughput: 24.42 samples/sec
2025-03-24 23:28:11,359 - training - INFO - GPU Memory: GPU 0: 3583.2MB/7626.0MB, GPU 1: 3581.9MB/7520.0MB, GPU 2: 3581.9MB/7640.0MB, GPU 3: 3580.5MB/7628.0MB, Total: 14327.5MB allocated, 30414.0MB reserved, Mean: 3581.9MB allocated, 7603.5MB reserved
2025-03-24 23:28:11,359 - training - INFO - Epoch: 2/200000, Batch: 54/163, Loss: 5.2814, Throughput: 24.27 samples/sec
2025-03-24 23:28:47,131 - training - INFO - GPU Memory: GPU 0: 3581.4MB/7512.0MB, GPU 1: 3579.6MB/7520.0MB, GPU 2: 3580.0MB/7504.0MB, GPU 3: 3577.8MB/7510.0MB, Total: 14318.8MB allocated, 30046.0MB reserved, Mean: 3579.7MB allocated, 7511.5MB reserved
2025-03-24 23:28:47,132 - training - INFO - Epoch: 2/200000, Batch: 108/163, Loss: 5.1672, Throughput: 24.21 samples/sec
2025-03-24 23:29:22,393 - training - INFO - GPU Memory: GPU 0: 3565.4MB/7518.0MB, GPU 1: 3568.3MB/7510.0MB, GPU 2: 3579.6MB/7510.0MB, GPU 3: 3567.9MB/7510.0MB, Total: 14281.2MB allocated, 30048.0MB reserved, Mean: 3570.3MB allocated, 7512.0MB reserved
2025-03-24 23:29:22,394 - training - INFO - Epoch: 2/200000, Batch: 162/163, Loss: 5.0897, Throughput: 24.31 samples/sec
2025-03-24 23:29:22,394 - training - INFO - Epoch 2 completed in 107.29s. Average loss: 5.0869
2025-03-24 23:29:22,401 - training - INFO - Starting epoch 3/200000
2025-03-24 23:29:23,030 - training - INFO - GPU Memory: GPU 0: 3576.1MB/7518.0MB, GPU 1: 3578.9MB/7510.0MB, GPU 2: 3586.5MB/7510.0MB, GPU 3: 3578.6MB/7510.0MB, Total: 14320.1MB allocated, 30048.0MB reserved, Mean: 3580.0MB allocated, 7512.0MB reserved
2025-03-24 23:29:23,031 - training - INFO - Epoch: 3/200000, Batch: 0/163, Loss: 4.6956, Throughput: 25.42 samples/sec
2025-03-24 23:29:58,508 - training - INFO - GPU Memory: GPU 0: 3581.3MB/7510.0MB, GPU 1: 3584.0MB/7524.0MB, GPU 2: 3581.5MB/7514.0MB, GPU 3: 3574.9MB/7510.0MB, Total: 14321.7MB allocated, 30058.0MB reserved, Mean: 3580.4MB allocated, 7514.5MB reserved
2025-03-24 23:29:58,508 - training - INFO - Epoch: 3/200000, Batch: 54/163, Loss: 4.8319, Throughput: 24.37 samples/sec
2025-03-24 23:30:34,294 - training - INFO - GPU Memory: GPU 0: 3580.7MB/7518.0MB, GPU 1: 3580.1MB/7520.0MB, GPU 2: 3579.0MB/7508.0MB, GPU 3: 3579.0MB/7510.0MB, Total: 14318.7MB allocated, 30056.0MB reserved, Mean: 3579.7MB allocated, 7514.0MB reserved
2025-03-24 23:30:34,294 - training - INFO - Epoch: 3/200000, Batch: 108/163, Loss: 4.8071, Throughput: 24.26 samples/sec
2025-03-24 23:31:09,516 - training - INFO - GPU Memory: GPU 0: 3569.1MB/7504.0MB, GPU 1: 3572.6MB/7510.0MB, GPU 2: 3568.2MB/7502.0MB, GPU 3: 3570.4MB/7514.0MB, Total: 14280.4MB allocated, 30030.0MB reserved, Mean: 3570.1MB allocated, 7507.5MB reserved
2025-03-24 23:31:09,517 - training - INFO - Epoch: 3/200000, Batch: 162/163, Loss: 4.7953, Throughput: 24.35 samples/sec
2025-03-24 23:31:09,517 - training - INFO - Epoch 3 completed in 107.12s. Average loss: 4.7789
2025-03-24 23:31:09,525 - training - INFO - Starting epoch 4/200000
2025-03-24 23:31:10,185 - training - INFO - GPU Memory: GPU 0: 3579.8MB/7504.0MB, GPU 1: 3581.7MB/7510.0MB, GPU 2: 3578.3MB/7502.0MB, GPU 3: 3577.8MB/7514.0MB, Total: 14317.7MB allocated, 30030.0MB reserved, Mean: 3579.4MB allocated, 7507.5MB reserved
2025-03-24 23:31:10,186 - training - INFO - Epoch: 4/200000, Batch: 0/163, Loss: 4.5087, Throughput: 24.24 samples/sec
2025-03-24 23:31:45,633 - training - INFO - GPU Memory: GPU 0: 3584.6MB/7522.0MB, GPU 1: 3580.4MB/7526.0MB, GPU 2: 3580.4MB/7518.0MB, GPU 3: 3582.9MB/7508.0MB, Total: 14328.3MB allocated, 30074.0MB reserved, Mean: 3582.1MB allocated, 7518.5MB reserved
2025-03-24 23:31:45,633 - training - INFO - Epoch: 4/200000, Batch: 54/163, Loss: 4.6965, Throughput: 24.37 samples/sec
2025-03-24 23:32:21,566 - training - INFO - GPU Memory: GPU 0: 3579.6MB/7516.0MB, GPU 1: 3580.1MB/7528.0MB, GPU 2: 3585.9MB/7514.0MB, GPU 3: 3577.7MB/7508.0MB, Total: 14323.3MB allocated, 30066.0MB reserved, Mean: 3580.8MB allocated, 7516.5MB reserved
2025-03-24 23:32:21,567 - training - INFO - Epoch: 4/200000, Batch: 108/163, Loss: 4.6660, Throughput: 24.21 samples/sec
2025-03-24 23:32:56,759 - training - INFO - GPU Memory: GPU 0: 3565.6MB/7520.0MB, GPU 1: 3566.8MB/7516.0MB, GPU 2: 3576.8MB/7512.0MB, GPU 3: 3564.0MB/7506.0MB, Total: 14273.2MB allocated, 30054.0MB reserved, Mean: 3568.3MB allocated, 7513.5MB reserved
2025-03-24 23:32:56,759 - training - INFO - Epoch: 4/200000, Batch: 162/163, Loss: 4.6437, Throughput: 24.32 samples/sec
2025-03-24 23:32:56,760 - training - INFO - Epoch 4 completed in 107.23s. Average loss: 4.6268
2025-03-24 23:32:56,772 - training - INFO - Starting epoch 5/200000
2025-03-24 23:32:57,405 - training - INFO - GPU Memory: GPU 0: 3576.3MB/7520.0MB, GPU 1: 3577.4MB/7516.0MB, GPU 2: 3583.3MB/7512.0MB, GPU 3: 3577.2MB/7506.0MB, Total: 14314.3MB allocated, 30054.0MB reserved, Mean: 3578.6MB allocated, 7513.5MB reserved
2025-03-24 23:32:57,405 - training - INFO - Epoch: 5/200000, Batch: 0/163, Loss: 4.1444, Throughput: 25.28 samples/sec
2025-03-24 23:33:32,964 - training - INFO - GPU Memory: GPU 0: 3576.7MB/7506.0MB, GPU 1: 3583.4MB/7510.0MB, GPU 2: 3588.0MB/7508.0MB, GPU 3: 3578.2MB/7512.0MB, Total: 14326.4MB allocated, 30036.0MB reserved, Mean: 3581.6MB allocated, 7509.0MB reserved
2025-03-24 23:33:32,965 - training - INFO - Epoch: 5/200000, Batch: 54/163, Loss: 4.5424, Throughput: 24.31 samples/sec
2025-03-24 23:34:08,710 - training - INFO - GPU Memory: GPU 0: 3578.5MB/7510.0MB, GPU 1: 3586.8MB/7518.0MB, GPU 2: 3582.0MB/7520.0MB, GPU 3: 3577.4MB/7512.0MB, Total: 14324.6MB allocated, 30060.0MB reserved, Mean: 3581.2MB allocated, 7515.0MB reserved
2025-03-24 23:34:08,710 - training - INFO - Epoch: 5/200000, Batch: 108/163, Loss: 4.5294, Throughput: 24.24 samples/sec
2025-03-24 23:34:43,950 - training - INFO - GPU Memory: GPU 0: 3566.1MB/7514.0MB, GPU 1: 3571.6MB/7528.0MB, GPU 2: 3570.5MB/7498.0MB, GPU 3: 3565.1MB/7506.0MB, Total: 14273.2MB allocated, 30046.0MB reserved, Mean: 3568.3MB allocated, 7511.5MB reserved
2025-03-24 23:34:43,950 - training - INFO - Epoch: 5/200000, Batch: 162/163, Loss: 4.5193, Throughput: 24.33 samples/sec
2025-03-24 23:34:43,951 - training - INFO - Epoch 5 completed in 107.18s. Average loss: 4.5241
2025-03-24 23:34:43,960 - training - INFO - Starting epoch 6/200000
2025-03-24 23:34:44,574 - training - INFO - GPU Memory: GPU 0: 3579.7MB/7514.0MB, GPU 1: 3581.6MB/7528.0MB, GPU 2: 3579.9MB/7498.0MB, GPU 3: 3576.2MB/7506.0MB, Total: 14317.5MB allocated, 30046.0MB reserved, Mean: 3579.4MB allocated, 7511.5MB reserved
2025-03-24 23:34:44,574 - training - INFO - Epoch: 6/200000, Batch: 0/163, Loss: 4.2633, Throughput: 26.04 samples/sec
2025-03-24 23:35:20,070 - training - INFO - GPU Memory: GPU 0: 3580.3MB/7512.0MB, GPU 1: 3579.6MB/7536.0MB, GPU 2: 3577.7MB/7504.0MB, GPU 3: 3579.7MB/7516.0MB, Total: 14317.4MB allocated, 30068.0MB reserved, Mean: 3579.3MB allocated, 7517.0MB reserved
2025-03-24 23:35:20,071 - training - INFO - Epoch: 6/200000, Batch: 54/163, Loss: 4.4802, Throughput: 24.37 samples/sec
2025-03-24 23:35:55,863 - training - INFO - GPU Memory: GPU 0: 3578.1MB/7514.0MB, GPU 1: 3585.9MB/7516.0MB, GPU 2: 3585.6MB/7512.0MB, GPU 3: 3577.8MB/7504.0MB, Total: 14327.4MB allocated, 30046.0MB reserved, Mean: 3581.8MB allocated, 7511.5MB reserved
2025-03-24 23:35:55,863 - training - INFO - Epoch: 6/200000, Batch: 108/163, Loss: 4.4739, Throughput: 24.25 samples/sec
2025-03-24 23:36:31,081 - training - INFO - GPU Memory: GPU 0: 3565.4MB/7512.0MB, GPU 1: 3570.0MB/7526.0MB, GPU 2: 3574.2MB/7514.0MB, GPU 3: 3572.7MB/7502.0MB, Total: 14282.4MB allocated, 30054.0MB reserved, Mean: 3570.6MB allocated, 7513.5MB reserved
2025-03-24 23:36:31,081 - training - INFO - Epoch: 6/200000, Batch: 162/163, Loss: 4.4550, Throughput: 24.35 samples/sec
2025-03-24 23:36:31,081 - training - INFO - Epoch 6 completed in 107.12s. Average loss: 4.4534
2025-03-24 23:36:31,090 - training - INFO - Starting epoch 7/200000
2025-03-24 23:36:31,742 - training - INFO - GPU Memory: GPU 0: 3575.1MB/7512.0MB, GPU 1: 3575.3MB/7526.0MB, GPU 2: 3588.8MB/7514.0MB, GPU 3: 3581.9MB/7502.0MB, Total: 14321.1MB allocated, 30054.0MB reserved, Mean: 3580.3MB allocated, 7513.5MB reserved
2025-03-24 23:36:31,742 - training - INFO - Epoch: 7/200000, Batch: 0/163, Loss: 4.3057, Throughput: 24.55 samples/sec
2025-03-24 23:37:07,121 - training - INFO - GPU Memory: GPU 0: 3577.5MB/7518.0MB, GPU 1: 3587.8MB/7522.0MB, GPU 2: 3574.6MB/7512.0MB, GPU 3: 3576.5MB/7520.0MB, Total: 14316.3MB allocated, 30072.0MB reserved, Mean: 3579.1MB allocated, 7518.0MB reserved
2025-03-24 23:37:07,122 - training - INFO - Epoch: 7/200000, Batch: 54/163, Loss: 4.4170, Throughput: 24.42 samples/sec
2025-03-24 23:37:42,775 - training - INFO - GPU Memory: GPU 0: 3573.8MB/7506.0MB, GPU 1: 3585.7MB/7522.0MB, GPU 2: 3584.6MB/7504.0MB, GPU 3: 3577.0MB/7504.0MB, Total: 14321.1MB allocated, 30036.0MB reserved, Mean: 3580.3MB allocated, 7509.0MB reserved
2025-03-24 23:37:42,775 - training - INFO - Epoch: 7/200000, Batch: 108/163, Loss: 4.4051, Throughput: 24.33 samples/sec
2025-03-24 23:38:18,099 - training - INFO - GPU Memory: GPU 0: 3567.2MB/7502.0MB, GPU 1: 3569.7MB/7522.0MB, GPU 2: 3577.7MB/7522.0MB, GPU 3: 3569.8MB/7498.0MB, Total: 14284.4MB allocated, 30044.0MB reserved, Mean: 3571.1MB allocated, 7511.0MB reserved
2025-03-24 23:38:18,099 - training - INFO - Epoch: 7/200000, Batch: 162/163, Loss: 4.3717, Throughput: 24.37 samples/sec
2025-03-24 23:38:18,100 - training - INFO - Epoch 7 completed in 107.01s. Average loss: 4.4013
2025-03-24 23:38:18,110 - training - INFO - Starting epoch 8/200000
2025-03-24 23:38:18,746 - training - INFO - GPU Memory: GPU 0: 3577.3MB/7502.0MB, GPU 1: 3581.1MB/7522.0MB, GPU 2: 3582.4MB/7522.0MB, GPU 3: 3580.5MB/7498.0MB, Total: 14321.4MB allocated, 30044.0MB reserved, Mean: 3580.3MB allocated, 7511.0MB reserved
2025-03-24 23:38:18,746 - training - INFO - Epoch: 8/200000, Batch: 0/163, Loss: 4.2210, Throughput: 25.13 samples/sec
2025-03-24 23:38:54,230 - training - INFO - GPU Memory: GPU 0: 3576.6MB/7510.0MB, GPU 1: 3575.2MB/7516.0MB, GPU 2: 3577.1MB/7520.0MB, GPU 3: 3571.6MB/7498.0MB, Total: 14300.4MB allocated, 30044.0MB reserved, Mean: 3575.1MB allocated, 7511.0MB reserved
2025-03-24 23:38:54,231 - training - INFO - Epoch: 8/200000, Batch: 54/163, Loss: 4.4496, Throughput: 24.36 samples/sec
2025-03-24 23:39:30,074 - training - INFO - GPU Memory: GPU 0: 3578.8MB/7518.0MB, GPU 1: 3581.0MB/7520.0MB, GPU 2: 3580.9MB/7504.0MB, GPU 3: 3580.2MB/7504.0MB, Total: 14320.9MB allocated, 30046.0MB reserved, Mean: 3580.2MB allocated, 7511.5MB reserved
2025-03-24 23:39:30,075 - training - INFO - Epoch: 8/200000, Batch: 108/163, Loss: 4.4081, Throughput: 24.23 samples/sec
2025-03-24 23:40:05,202 - training - INFO - GPU Memory: GPU 0: 3562.6MB/7514.0MB, GPU 1: 3567.4MB/7518.0MB, GPU 2: 3576.1MB/7508.0MB, GPU 3: 3561.4MB/7520.0MB, Total: 14267.6MB allocated, 30060.0MB reserved, Mean: 3566.9MB allocated, 7515.0MB reserved
2025-03-24 23:40:05,202 - training - INFO - Epoch: 8/200000, Batch: 162/163, Loss: 4.3926, Throughput: 24.35 samples/sec
2025-03-24 23:40:05,203 - training - INFO - Epoch 8 completed in 107.09s. Average loss: 4.3610
2025-03-24 23:40:05,209 - training - INFO - Starting epoch 9/200000
2025-03-24 23:40:05,862 - training - INFO - GPU Memory: GPU 0: 3574.8MB/7514.0MB, GPU 1: 3578.0MB/7518.0MB, GPU 2: 3586.5MB/7508.0MB, GPU 3: 3577.5MB/7520.0MB, Total: 14316.9MB allocated, 30060.0MB reserved, Mean: 3579.2MB allocated, 7515.0MB reserved
2025-03-24 23:40:05,863 - training - INFO - Epoch: 9/200000, Batch: 0/163, Loss: 4.6847, Throughput: 24.50 samples/sec
2025-03-24 23:40:41,350 - training - INFO - GPU Memory: GPU 0: 3580.0MB/7512.0MB, GPU 1: 3586.0MB/7520.0MB, GPU 2: 3580.2MB/7494.0MB, GPU 3: 3577.6MB/7502.0MB, Total: 14323.8MB allocated, 30028.0MB reserved, Mean: 3580.9MB allocated, 7507.0MB reserved
2025-03-24 23:40:41,350 - training - INFO - Epoch: 9/200000, Batch: 54/163, Loss: 4.3619, Throughput: 24.35 samples/sec
2025-03-24 23:41:17,074 - training - INFO - GPU Memory: GPU 0: 3578.2MB/7514.0MB, GPU 1: 3582.4MB/7528.0MB, GPU 2: 3587.9MB/7506.0MB, GPU 3: 3576.8MB/7512.0MB, Total: 14325.4MB allocated, 30060.0MB reserved, Mean: 3581.4MB allocated, 7515.0MB reserved
2025-03-24 23:41:17,074 - training - INFO - Epoch: 9/200000, Batch: 108/163, Loss: 4.3359, Throughput: 24.27 samples/sec
2025-03-24 23:41:52,484 - training - INFO - GPU Memory: GPU 0: 3560.4MB/7512.0MB, GPU 1: 3566.8MB/7516.0MB, GPU 2: 3576.2MB/7508.0MB, GPU 3: 3568.3MB/7512.0MB, Total: 14271.7MB allocated, 30048.0MB reserved, Mean: 3567.9MB allocated, 7512.0MB reserved
2025-03-24 23:41:52,484 - training - INFO - Epoch: 9/200000, Batch: 162/163, Loss: 4.3205, Throughput: 24.31 samples/sec
2025-03-24 23:41:52,485 - training - INFO - Epoch 9 completed in 107.28s. Average loss: 4.3201
2025-03-24 23:41:52,493 - training - INFO - Starting epoch 10/200000
2025-03-24 23:41:53,147 - training - INFO - GPU Memory: GPU 0: 3571.1MB/7512.0MB, GPU 1: 3577.5MB/7516.0MB, GPU 2: 3588.8MB/7508.0MB, GPU 3: 3579.0MB/7512.0MB, Total: 14316.3MB allocated, 30048.0MB reserved, Mean: 3579.1MB allocated, 7512.0MB reserved
2025-03-24 23:41:53,148 - training - INFO - Epoch: 10/200000, Batch: 0/163, Loss: 3.7004, Throughput: 24.47 samples/sec
2025-03-24 23:42:28,764 - training - INFO - GPU Memory: GPU 0: 3579.5MB/7500.0MB, GPU 1: 3579.2MB/7516.0MB, GPU 2: 3583.4MB/7520.0MB, GPU 3: 3572.2MB/7514.0MB, Total: 14314.3MB allocated, 30050.0MB reserved, Mean: 3578.6MB allocated, 7512.5MB reserved
2025-03-24 23:42:28,764 - training - INFO - Epoch: 10/200000, Batch: 54/163, Loss: 4.3056, Throughput: 24.26 samples/sec
2025-03-24 23:43:04,499 - training - INFO - GPU Memory: GPU 0: 3577.0MB/7504.0MB, GPU 1: 3579.1MB/7512.0MB, GPU 2: 3582.2MB/7510.0MB, GPU 3: 3575.9MB/7506.0MB, Total: 14314.2MB allocated, 30032.0MB reserved, Mean: 3578.5MB allocated, 7508.0MB reserved
2025-03-24 23:43:04,499 - training - INFO - Epoch: 10/200000, Batch: 108/163, Loss: 4.2870, Throughput: 24.22 samples/sec
2025-03-24 23:43:39,800 - training - INFO - GPU Memory: GPU 0: 3568.7MB/7510.0MB, GPU 1: 3570.7MB/7518.0MB, GPU 2: 3568.9MB/7506.0MB, GPU 3: 3566.2MB/7508.0MB, Total: 14274.5MB allocated, 30042.0MB reserved, Mean: 3568.6MB allocated, 7510.5MB reserved
2025-03-24 23:43:39,800 - training - INFO - Epoch: 10/200000, Batch: 162/163, Loss: 4.2864, Throughput: 24.30 samples/sec
2025-03-24 23:43:39,800 - training - INFO - Epoch 10 completed in 107.31s. Average loss: 4.2836
2025-03-24 23:43:39,806 - training - INFO - Starting epoch 11/200000
2025-03-24 23:43:40,441 - training - INFO - GPU Memory: GPU 0: 3579.4MB/7510.0MB, GPU 1: 3581.4MB/7518.0MB, GPU 2: 3576.8MB/7506.0MB, GPU 3: 3576.9MB/7508.0MB, Total: 14314.4MB allocated, 30042.0MB reserved, Mean: 3578.6MB allocated, 7510.5MB reserved
2025-03-24 23:43:40,441 - training - INFO - Epoch: 11/200000, Batch: 0/163, Loss: 4.7938, Throughput: 25.22 samples/sec
2025-03-24 23:44:15,756 - training - INFO - GPU Memory: GPU 0: 3574.2MB/7512.0MB, GPU 1: 3579.0MB/7520.0MB, GPU 2: 3582.3MB/7500.0MB, GPU 3: 3578.5MB/7520.0MB, Total: 14313.9MB allocated, 30052.0MB reserved, Mean: 3578.5MB allocated, 7513.0MB reserved
2025-03-24 23:44:15,757 - training - INFO - Epoch: 11/200000, Batch: 54/163, Loss: 4.2745, Throughput: 24.48 samples/sec
2025-03-24 23:44:51,453 - training - INFO - GPU Memory: GPU 0: 3579.6MB/7508.0MB, GPU 1: 3579.7MB/7514.0MB, GPU 2: 3585.6MB/7514.0MB, GPU 3: 3574.1MB/7510.0MB, Total: 14318.9MB allocated, 30046.0MB reserved, Mean: 3579.7MB allocated, 7511.5MB reserved
2025-03-24 23:44:51,454 - training - INFO - Epoch: 11/200000, Batch: 108/163, Loss: 4.2942, Throughput: 24.34 samples/sec
2025-03-24 23:45:26,845 - training - INFO - GPU Memory: GPU 0: 3563.6MB/7502.0MB, GPU 1: 3570.7MB/7512.0MB, GPU 2: 3565.9MB/7508.0MB, GPU 3: 3572.2MB/7504.0MB, Total: 14272.4MB allocated, 30026.0MB reserved, Mean: 3568.1MB allocated, 7506.5MB reserved
2025-03-24 23:45:26,845 - training - INFO - Epoch: 11/200000, Batch: 162/163, Loss: 4.2688, Throughput: 24.37 samples/sec
2025-03-24 23:45:26,846 - training - INFO - Epoch 11 completed in 107.04s. Average loss: 4.2642
2025-03-24 23:45:26,856 - training - INFO - Starting epoch 12/200000
2025-03-24 23:45:27,494 - training - INFO - GPU Memory: GPU 0: 3577.3MB/7502.0MB, GPU 1: 3581.3MB/7512.0MB, GPU 2: 3577.1MB/7508.0MB, GPU 3: 3581.4MB/7504.0MB, Total: 14317.1MB allocated, 30026.0MB reserved, Mean: 3579.3MB allocated, 7506.5MB reserved
2025-03-24 23:45:27,494 - training - INFO - Epoch: 12/200000, Batch: 0/163, Loss: 4.7846, Throughput: 25.05 samples/sec
2025-03-24 23:46:03,032 - training - INFO - GPU Memory: GPU 0: 3575.2MB/7522.0MB, GPU 1: 3581.6MB/7526.0MB, GPU 2: 3581.3MB/7504.0MB, GPU 3: 3578.1MB/7510.0MB, Total: 14316.3MB allocated, 30062.0MB reserved, Mean: 3579.1MB allocated, 7515.5MB reserved
2025-03-24 23:46:03,032 - training - INFO - Epoch: 12/200000, Batch: 54/163, Loss: 4.2295, Throughput: 24.33 samples/sec
2025-03-24 23:46:38,908 - training - INFO - GPU Memory: GPU 0: 3579.0MB/7500.0MB, GPU 1: 3586.5MB/7512.0MB, GPU 2: 3589.2MB/7504.0MB, GPU 3: 3574.9MB/7512.0MB, Total: 14329.6MB allocated, 30028.0MB reserved, Mean: 3582.4MB allocated, 7507.0MB reserved
2025-03-24 23:46:38,909 - training - INFO - Epoch: 12/200000, Batch: 108/163, Loss: 4.2637, Throughput: 24.20 samples/sec
2025-03-24 23:47:14,200 - training - INFO - GPU Memory: GPU 0: 3569.8MB/7510.0MB, GPU 1: 3571.7MB/7526.0MB, GPU 2: 3573.8MB/7516.0MB, GPU 3: 3570.6MB/7508.0MB, Total: 14285.8MB allocated, 30060.0MB reserved, Mean: 3571.5MB allocated, 7515.0MB reserved
2025-03-24 23:47:14,200 - training - INFO - Epoch: 12/200000, Batch: 162/163, Loss: 4.2507, Throughput: 24.30 samples/sec
2025-03-24 23:47:14,201 - training - INFO - Epoch 12 completed in 107.35s. Average loss: 4.2281
2025-03-24 23:47:14,213 - training - INFO - Starting epoch 13/200000
2025-03-24 23:47:14,845 - training - INFO - GPU Memory: GPU 0: 3581.0MB/7510.0MB, GPU 1: 3585.3MB/7526.0MB, GPU 2: 3582.5MB/7516.0MB, GPU 3: 3578.4MB/7508.0MB, Total: 14327.3MB allocated, 30060.0MB reserved, Mean: 3581.8MB allocated, 7515.0MB reserved
2025-03-24 23:47:14,845 - training - INFO - Epoch: 13/200000, Batch: 0/163, Loss: 4.3888, Throughput: 25.31 samples/sec
2025-03-24 23:47:50,337 - training - INFO - GPU Memory: GPU 0: 3575.6MB/7510.0MB, GPU 1: 3587.3MB/7520.0MB, GPU 2: 3584.9MB/7508.0MB, GPU 3: 3577.9MB/7518.0MB, Total: 14325.7MB allocated, 30056.0MB reserved, Mean: 3581.4MB allocated, 7514.0MB reserved
2025-03-24 23:47:50,337 - training - INFO - Epoch: 13/200000, Batch: 54/163, Loss: 4.1596, Throughput: 24.36 samples/sec
2025-03-24 23:48:26,188 - training - INFO - GPU Memory: GPU 0: 3574.7MB/7510.0MB, GPU 1: 3583.9MB/7522.0MB, GPU 2: 3582.3MB/7524.0MB, GPU 3: 3581.4MB/7506.0MB, Total: 14322.3MB allocated, 30062.0MB reserved, Mean: 3580.6MB allocated, 7515.5MB reserved
2025-03-24 23:48:26,188 - training - INFO - Epoch: 13/200000, Batch: 108/163, Loss: 4.1944, Throughput: 24.23 samples/sec
2025-03-24 23:49:01,507 - training - INFO - GPU Memory: GPU 0: 3563.7MB/7504.0MB, GPU 1: 3570.7MB/7518.0MB, GPU 2: 3574.3MB/7516.0MB, GPU 3: 3562.7MB/7514.0MB, Total: 14271.4MB allocated, 30052.0MB reserved, Mean: 3567.8MB allocated, 7513.0MB reserved
2025-03-24 23:49:01,507 - training - INFO - Epoch: 13/200000, Batch: 162/163, Loss: 4.2086, Throughput: 24.31 samples/sec
2025-03-24 23:49:01,508 - training - INFO - Epoch 13 completed in 107.29s. Average loss: 4.2082
2025-03-24 23:49:01,515 - training - INFO - Starting epoch 14/200000
2025-03-24 23:49:02,162 - training - INFO - GPU Memory: GPU 0: 3575.4MB/7504.0MB, GPU 1: 3577.2MB/7518.0MB, GPU 2: 3586.7MB/7516.0MB, GPU 3: 3572.7MB/7514.0MB, Total: 14312.0MB allocated, 30052.0MB reserved, Mean: 3578.0MB allocated, 7513.0MB reserved
2025-03-24 23:49:02,162 - training - INFO - Epoch: 14/200000, Batch: 0/163, Loss: 4.1435, Throughput: 24.74 samples/sec
2025-03-24 23:49:37,814 - training - INFO - GPU Memory: GPU 0: 3578.7MB/7508.0MB, GPU 1: 3582.2MB/7510.0MB, GPU 2: 3585.0MB/7502.0MB, GPU 3: 3582.6MB/7514.0MB, Total: 14328.5MB allocated, 30034.0MB reserved, Mean: 3582.1MB allocated, 7508.5MB reserved
2025-03-24 23:49:37,814 - training - INFO - Epoch: 14/200000, Batch: 54/163, Loss: 4.2098, Throughput: 24.24 samples/sec
2025-03-24 23:50:13,739 - training - INFO - GPU Memory: GPU 0: 3581.6MB/7518.0MB, GPU 1: 3580.5MB/7516.0MB, GPU 2: 3582.8MB/7522.0MB, GPU 3: 3577.3MB/7516.0MB, Total: 14322.2MB allocated, 30072.0MB reserved, Mean: 3580.6MB allocated, 7518.0MB reserved
2025-03-24 23:50:13,739 - training - INFO - Epoch: 14/200000, Batch: 108/163, Loss: 4.1933, Throughput: 24.15 samples/sec
2025-03-24 23:50:49,063 - training - INFO - GPU Memory: GPU 0: 3565.3MB/7516.0MB, GPU 1: 3574.7MB/7528.0MB, GPU 2: 3575.2MB/7508.0MB, GPU 3: 3565.8MB/7504.0MB, Total: 14281.0MB allocated, 30056.0MB reserved, Mean: 3570.3MB allocated, 7514.0MB reserved
2025-03-24 23:50:49,063 - training - INFO - Epoch: 14/200000, Batch: 162/163, Loss: 4.1960, Throughput: 24.25 samples/sec
2025-03-24 23:50:49,064 - training - INFO - Epoch 14 completed in 107.55s. Average loss: 4.1817
2025-03-24 23:50:49,070 - training - INFO - Starting epoch 15/200000
2025-03-24 23:50:49,700 - training - INFO - GPU Memory: GPU 0: 3575.6MB/7516.0MB, GPU 1: 3585.3MB/7528.0MB, GPU 2: 3583.8MB/7508.0MB, GPU 3: 3576.5MB/7504.0MB, Total: 14321.2MB allocated, 30056.0MB reserved, Mean: 3580.3MB allocated, 7514.0MB reserved
2025-03-24 23:50:49,700 - training - INFO - Epoch: 15/200000, Batch: 0/163, Loss: 4.1279, Throughput: 25.39 samples/sec
2025-03-24 23:51:25,147 - training - INFO - GPU Memory: GPU 0: 3574.5MB/7510.0MB, GPU 1: 3578.7MB/7512.0MB, GPU 2: 3579.1MB/7506.0MB, GPU 3: 3577.8MB/7510.0MB, Total: 14310.0MB allocated, 30038.0MB reserved, Mean: 3577.5MB allocated, 7509.5MB reserved
2025-03-24 23:51:25,147 - training - INFO - Epoch: 15/200000, Batch: 54/163, Loss: 4.1976, Throughput: 24.39 samples/sec
2025-03-24 23:52:01,031 - training - INFO - GPU Memory: GPU 0: 3579.1MB/7506.0MB, GPU 1: 3579.3MB/7516.0MB, GPU 2: 3588.0MB/7506.0MB, GPU 3: 3577.4MB/7512.0MB, Total: 14323.7MB allocated, 30040.0MB reserved, Mean: 3580.9MB allocated, 7510.0MB reserved
2025-03-24 23:52:01,031 - training - INFO - Epoch: 15/200000, Batch: 108/163, Loss: 4.1738, Throughput: 24.24 samples/sec
2025-03-24 23:52:36,324 - training - INFO - GPU Memory: GPU 0: 3566.7MB/7516.0MB, GPU 1: 3577.0MB/7518.0MB, GPU 2: 3566.5MB/7516.0MB, GPU 3: 3570.9MB/7508.0MB, Total: 14281.1MB allocated, 30058.0MB reserved, Mean: 3570.3MB allocated, 7514.5MB reserved
2025-03-24 23:52:36,325 - training - INFO - Epoch: 15/200000, Batch: 162/163, Loss: 4.1684, Throughput: 24.32 samples/sec
2025-03-24 23:52:36,325 - training - INFO - Epoch 15 completed in 107.26s. Average loss: 4.1666
2025-03-24 23:52:36,335 - training - INFO - Starting epoch 16/200000
2025-03-24 23:52:36,963 - training - INFO - GPU Memory: GPU 0: 3583.6MB/7516.0MB, GPU 1: 3587.6MB/7518.0MB, GPU 2: 3578.6MB/7516.0MB, GPU 3: 3581.6MB/7508.0MB, Total: 14331.4MB allocated, 30058.0MB reserved, Mean: 3582.8MB allocated, 7514.5MB reserved
2025-03-24 23:52:36,963 - training - INFO - Epoch: 16/200000, Batch: 0/163, Loss: 3.2714, Throughput: 25.50 samples/sec
2025-03-24 23:53:12,451 - training - INFO - GPU Memory: GPU 0: 3577.3MB/7520.0MB, GPU 1: 3581.9MB/7520.0MB, GPU 2: 3578.9MB/7510.0MB, GPU 3: 3575.2MB/7518.0MB, Total: 14313.4MB allocated, 30068.0MB reserved, Mean: 3578.3MB allocated, 7517.0MB reserved
2025-03-24 23:53:12,451 - training - INFO - Epoch: 16/200000, Batch: 54/163, Loss: 4.1135, Throughput: 24.37 samples/sec
2025-03-24 23:53:48,261 - training - INFO - GPU Memory: GPU 0: 3578.4MB/7504.0MB, GPU 1: 3576.8MB/7522.0MB, GPU 2: 3580.4MB/7508.0MB, GPU 3: 3579.6MB/7510.0MB, Total: 14315.2MB allocated, 30044.0MB reserved, Mean: 3578.8MB allocated, 7511.0MB reserved
2025-03-24 23:53:48,262 - training - INFO - Epoch: 16/200000, Batch: 108/163, Loss: 4.1340, Throughput: 24.25 samples/sec
2025-03-24 23:54:23,448 - training - INFO - GPU Memory: GPU 0: 3567.9MB/7510.0MB, GPU 1: 3568.8MB/7514.0MB, GPU 2: 3569.1MB/7514.0MB, GPU 3: 3570.1MB/7512.0MB, Total: 14275.9MB allocated, 30050.0MB reserved, Mean: 3569.0MB allocated, 7512.5MB reserved
2025-03-24 23:54:23,448 - training - INFO - Epoch: 16/200000, Batch: 162/163, Loss: 4.1319, Throughput: 24.35 samples/sec
2025-03-24 23:54:23,449 - training - INFO - Epoch 16 completed in 107.11s. Average loss: 4.1515
2025-03-24 23:54:23,457 - training - INFO - Starting epoch 17/200000
2025-03-24 23:54:24,089 - training - INFO - GPU Memory: GPU 0: 3578.7MB/7510.0MB, GPU 1: 3582.7MB/7514.0MB, GPU 2: 3577.8MB/7514.0MB, GPU 3: 3575.7MB/7512.0MB, Total: 14315.0MB allocated, 30050.0MB reserved, Mean: 3578.7MB allocated, 7512.5MB reserved
2025-03-24 23:54:24,089 - training - INFO - Epoch: 17/200000, Batch: 0/163, Loss: 3.7404, Throughput: 25.35 samples/sec
2025-03-24 23:54:59,448 - training - INFO - GPU Memory: GPU 0: 3579.8MB/7510.0MB, GPU 1: 3586.6MB/7522.0MB, GPU 2: 3584.2MB/7512.0MB, GPU 3: 3577.5MB/7512.0MB, Total: 14328.1MB allocated, 30056.0MB reserved, Mean: 3582.0MB allocated, 7514.0MB reserved
2025-03-24 23:54:59,449 - training - INFO - Epoch: 17/200000, Batch: 54/163, Loss: 4.1501, Throughput: 24.45 samples/sec
2025-03-24 23:55:35,186 - training - INFO - GPU Memory: GPU 0: 3577.8MB/7518.0MB, GPU 1: 3589.0MB/7526.0MB, GPU 2: 3577.4MB/7508.0MB, GPU 3: 3574.5MB/7510.0MB, Total: 14318.7MB allocated, 30062.0MB reserved, Mean: 3579.7MB allocated, 7515.5MB reserved
2025-03-24 23:55:35,187 - training - INFO - Epoch: 17/200000, Batch: 108/163, Loss: 4.0839, Throughput: 24.31 samples/sec
2025-03-24 23:56:10,323 - training - INFO - GPU Memory: GPU 0: 3562.9MB/7514.0MB, GPU 1: 3570.2MB/7532.0MB, GPU 2: 3564.3MB/7504.0MB, GPU 3: 3566.9MB/7516.0MB, Total: 14264.3MB allocated, 30066.0MB reserved, Mean: 3566.1MB allocated, 7516.5MB reserved
2025-03-24 23:56:10,323 - training - INFO - Epoch: 17/200000, Batch: 162/163, Loss: 4.0877, Throughput: 24.40 samples/sec
2025-03-24 23:56:10,324 - training - INFO - Epoch 17 completed in 106.87s. Average loss: 4.1187
2025-03-24 23:56:10,332 - training - INFO - Starting epoch 18/200000
2025-03-24 23:56:10,964 - training - INFO - GPU Memory: GPU 0: 3578.3MB/7514.0MB, GPU 1: 3580.9MB/7532.0MB, GPU 2: 3584.8MB/7504.0MB, GPU 3: 3576.8MB/7516.0MB, Total: 14320.9MB allocated, 30066.0MB reserved, Mean: 3580.2MB allocated, 7516.5MB reserved
2025-03-24 23:56:10,965 - training - INFO - Epoch: 18/200000, Batch: 0/163, Loss: 3.8362, Throughput: 25.30 samples/sec
2025-03-24 23:56:46,331 - training - INFO - GPU Memory: GPU 0: 3579.0MB/7514.0MB, GPU 1: 3576.4MB/7520.0MB, GPU 2: 3586.4MB/7514.0MB, GPU 3: 3578.0MB/7502.0MB, Total: 14319.8MB allocated, 30050.0MB reserved, Mean: 3580.0MB allocated, 7512.5MB reserved
2025-03-24 23:56:46,331 - training - INFO - Epoch: 18/200000, Batch: 54/163, Loss: 4.0172, Throughput: 24.45 samples/sec
2025-03-24 23:57:22,184 - training - INFO - GPU Memory: GPU 0: 3578.8MB/7510.0MB, GPU 1: 3583.9MB/7520.0MB, GPU 2: 3586.0MB/7508.0MB, GPU 3: 3579.6MB/7514.0MB, Total: 14328.3MB allocated, 30052.0MB reserved, Mean: 3582.1MB allocated, 7513.0MB reserved
2025-03-24 23:57:22,184 - training - INFO - Epoch: 18/200000, Batch: 108/163, Loss: 4.0451, Throughput: 24.27 samples/sec
2025-03-24 23:57:57,392 - training - INFO - GPU Memory: GPU 0: 3569.6MB/7508.0MB, GPU 1: 3572.0MB/7518.0MB, GPU 2: 3567.5MB/7518.0MB, GPU 3: 3567.4MB/7510.0MB, Total: 14276.7MB allocated, 30054.0MB reserved, Mean: 3569.2MB allocated, 7513.5MB reserved
2025-03-24 23:57:57,392 - training - INFO - Epoch: 18/200000, Batch: 162/163, Loss: 4.0422, Throughput: 24.36 samples/sec
2025-03-24 23:57:57,393 - training - INFO - Epoch 18 completed in 107.06s. Average loss: 4.0901
2025-03-24 23:57:57,397 - training - INFO - Starting epoch 19/200000
2025-03-24 23:57:58,047 - training - INFO - GPU Memory: GPU 0: 3576.8MB/7508.0MB, GPU 1: 3582.7MB/7518.0MB, GPU 2: 3585.4MB/7518.0MB, GPU 3: 3579.0MB/7510.0MB, Total: 14323.9MB allocated, 30054.0MB reserved, Mean: 3581.0MB allocated, 7513.5MB reserved
2025-03-24 23:57:58,047 - training - INFO - Epoch: 19/200000, Batch: 0/163, Loss: 3.2578, Throughput: 24.62 samples/sec
2025-03-24 23:58:33,540 - training - INFO - GPU Memory: GPU 0: 3583.7MB/7498.0MB, GPU 1: 3583.1MB/7516.0MB, GPU 2: 3582.9MB/7512.0MB, GPU 3: 3578.7MB/7512.0MB, Total: 14328.4MB allocated, 30038.0MB reserved, Mean: 3582.1MB allocated, 7509.5MB reserved
2025-03-24 23:58:33,541 - training - INFO - Epoch: 19/200000, Batch: 54/163, Loss: 4.1448, Throughput: 24.35 samples/sec
2025-03-24 23:59:09,556 - training - INFO - GPU Memory: GPU 0: 3577.7MB/7514.0MB, GPU 1: 3583.7MB/7522.0MB, GPU 2: 3585.4MB/7508.0MB, GPU 3: 3571.6MB/7512.0MB, Total: 14318.4MB allocated, 30056.0MB reserved, Mean: 3579.6MB allocated, 7514.0MB reserved
2025-03-24 23:59:09,556 - training - INFO - Epoch: 19/200000, Batch: 108/163, Loss: 4.0988, Throughput: 24.17 samples/sec
2025-03-24 23:59:45,425 - training - INFO - GPU Memory: GPU 0: 3567.0MB/7514.0MB, GPU 1: 3571.4MB/7528.0MB, GPU 2: 3566.7MB/7518.0MB, GPU 3: 3562.1MB/7510.0MB, Total: 14267.2MB allocated, 30070.0MB reserved, Mean: 3566.8MB allocated, 7517.5MB reserved
2025-03-24 23:59:45,426 - training - INFO - Epoch: 19/200000, Batch: 162/163, Loss: 4.0727, Throughput: 24.14 samples/sec
2025-03-24 23:59:45,426 - training - INFO - Epoch 19 completed in 108.03s. Average loss: 4.0785
2025-03-24 23:59:45,432 - training - INFO - Starting epoch 20/200000
2025-03-24 23:59:46,065 - training - INFO - GPU Memory: GPU 0: 3577.7MB/7514.0MB, GPU 1: 3581.4MB/7528.0MB, GPU 2: 3580.9MB/7518.0MB, GPU 3: 3572.8MB/7510.0MB, Total: 14312.8MB allocated, 30070.0MB reserved, Mean: 3578.2MB allocated, 7517.5MB reserved
2025-03-24 23:59:46,065 - training - INFO - Epoch: 20/200000, Batch: 0/163, Loss: 2.8769, Throughput: 25.29 samples/sec
2025-03-25 00:00:21,586 - training - INFO - GPU Memory: GPU 0: 3582.6MB/7504.0MB, GPU 1: 3579.0MB/7516.0MB, GPU 2: 3582.6MB/7522.0MB, GPU 3: 3573.1MB/7514.0MB, Total: 14317.4MB allocated, 30056.0MB reserved, Mean: 3579.3MB allocated, 7514.0MB reserved
2025-03-25 00:00:21,586 - training - INFO - Epoch: 20/200000, Batch: 54/163, Loss: 3.9511, Throughput: 24.34 samples/sec
2025-03-25 00:00:57,398 - training - INFO - GPU Memory: GPU 0: 3581.3MB/7518.0MB, GPU 1: 3582.8MB/7516.0MB, GPU 2: 3576.4MB/7508.0MB, GPU 3: 3578.1MB/7504.0MB, Total: 14318.5MB allocated, 30046.0MB reserved, Mean: 3579.6MB allocated, 7511.5MB reserved
2025-03-25 00:00:57,398 - training - INFO - Epoch: 20/200000, Batch: 108/163, Loss: 3.9979, Throughput: 24.23 samples/sec
2025-03-25 00:01:32,647 - training - INFO - GPU Memory: GPU 0: 3567.4MB/7522.0MB, GPU 1: 3571.2MB/7508.0MB, GPU 2: 3573.0MB/7506.0MB, GPU 3: 3568.0MB/7514.0MB, Total: 14279.7MB allocated, 30050.0MB reserved, Mean: 3569.9MB allocated, 7512.5MB reserved
2025-03-25 00:01:32,647 - training - INFO - Epoch: 20/200000, Batch: 162/163, Loss: 4.0312, Throughput: 24.33 samples/sec
2025-03-25 00:01:32,647 - training - INFO - Epoch 20 completed in 107.22s. Average loss: 4.0513
2025-03-25 00:01:32,654 - training - INFO - Starting epoch 21/200000
2025-03-25 00:01:33,292 - training - INFO - GPU Memory: GPU 0: 3577.6MB/7522.0MB, GPU 1: 3581.9MB/7508.0MB, GPU 2: 3582.4MB/7506.0MB, GPU 3: 3578.7MB/7514.0MB, Total: 14320.6MB allocated, 30050.0MB reserved, Mean: 3580.1MB allocated, 7512.5MB reserved
2025-03-25 00:01:33,292 - training - INFO - Epoch: 21/200000, Batch: 0/163, Loss: 3.6787, Throughput: 25.08 samples/sec
2025-03-25 00:02:08,913 - training - INFO - GPU Memory: GPU 0: 3583.8MB/7512.0MB, GPU 1: 3582.3MB/7514.0MB, GPU 2: 3578.8MB/7512.0MB, GPU 3: 3575.2MB/7516.0MB, Total: 14320.2MB allocated, 30054.0MB reserved, Mean: 3580.0MB allocated, 7513.5MB reserved
2025-03-25 00:02:08,913 - training - INFO - Epoch: 21/200000, Batch: 54/163, Loss: 3.9797, Throughput: 24.27 samples/sec
2025-03-25 00:02:44,772 - training - INFO - GPU Memory: GPU 0: 3579.9MB/7516.0MB, GPU 1: 3579.4MB/7532.0MB, GPU 2: 3582.6MB/7510.0MB, GPU 3: 3580.7MB/7510.0MB, Total: 14322.7MB allocated, 30068.0MB reserved, Mean: 3580.7MB allocated, 7517.0MB reserved
2025-03-25 00:02:44,772 - training - INFO - Epoch: 21/200000, Batch: 108/163, Loss: 4.0185, Throughput: 24.18 samples/sec
2025-03-25 00:03:20,100 - training - INFO - GPU Memory: GPU 0: 3562.9MB/7514.0MB, GPU 1: 3566.9MB/7518.0MB, GPU 2: 3570.1MB/7508.0MB, GPU 3: 3566.4MB/7512.0MB, Total: 14266.3MB allocated, 30052.0MB reserved, Mean: 3566.6MB allocated, 7513.0MB reserved
2025-03-25 00:03:20,101 - training - INFO - Epoch: 21/200000, Batch: 162/163, Loss: 4.0199, Throughput: 24.27 samples/sec
2025-03-25 00:03:20,102 - training - INFO - Epoch 21 completed in 107.45s. Average loss: 4.0344
2025-03-25 00:03:20,112 - training - INFO - Starting epoch 22/200000
2025-03-25 00:03:20,753 - training - INFO - GPU Memory: GPU 0: 3575.6MB/7514.0MB, GPU 1: 3577.6MB/7518.0MB, GPU 2: 3586.4MB/7508.0MB, GPU 3: 3579.3MB/7512.0MB, Total: 14318.9MB allocated, 30052.0MB reserved, Mean: 3579.7MB allocated, 7513.0MB reserved
2025-03-25 00:03:20,753 - training - INFO - Epoch: 22/200000, Batch: 0/163, Loss: 4.0416, Throughput: 24.94 samples/sec
2025-03-25 00:03:56,182 - training - INFO - GPU Memory: GPU 0: 3575.5MB/7506.0MB, GPU 1: 3582.1MB/7522.0MB, GPU 2: 3583.3MB/7502.0MB, GPU 3: 3574.7MB/7508.0MB, Total: 14315.6MB allocated, 30038.0MB reserved, Mean: 3578.9MB allocated, 7509.5MB reserved
2025-03-25 00:03:56,182 - training - INFO - Epoch: 22/200000, Batch: 54/163, Loss: 4.0484, Throughput: 24.40 samples/sec
2025-03-25 00:04:31,865 - training - INFO - GPU Memory: GPU 0: 3575.6MB/7506.0MB, GPU 1: 3576.0MB/7518.0MB, GPU 2: 3580.8MB/7508.0MB, GPU 3: 3572.2MB/7504.0MB, Total: 14304.7MB allocated, 30036.0MB reserved, Mean: 3576.2MB allocated, 7509.0MB reserved
2025-03-25 00:04:31,865 - training - INFO - Epoch: 22/200000, Batch: 108/163, Loss: 4.0464, Throughput: 24.31 samples/sec
2025-03-25 00:05:07,092 - training - INFO - GPU Memory: GPU 0: 3564.1MB/7506.0MB, GPU 1: 3569.2MB/7516.0MB, GPU 2: 3574.8MB/7510.0MB, GPU 3: 3565.8MB/7508.0MB, Total: 14274.0MB allocated, 30040.0MB reserved, Mean: 3568.5MB allocated, 7510.0MB reserved
2025-03-25 00:05:07,093 - training - INFO - Epoch: 22/200000, Batch: 162/163, Loss: 4.0720, Throughput: 24.38 samples/sec
2025-03-25 00:05:07,093 - training - INFO - Epoch 22 completed in 106.98s. Average loss: 4.0167
2025-03-25 00:05:07,102 - training - INFO - Starting epoch 23/200000
2025-03-25 00:05:07,741 - training - INFO - GPU Memory: GPU 0: 3574.8MB/7506.0MB, GPU 1: 3584.1MB/7516.0MB, GPU 2: 3587.2MB/7510.0MB, GPU 3: 3575.7MB/7508.0MB, Total: 14321.9MB allocated, 30040.0MB reserved, Mean: 3580.5MB allocated, 7510.0MB reserved
2025-03-25 00:05:07,742 - training - INFO - Epoch: 23/200000, Batch: 0/163, Loss: 3.9441, Throughput: 25.04 samples/sec
2025-03-25 00:05:43,198 - training - INFO - GPU Memory: GPU 0: 3576.4MB/7520.0MB, GPU 1: 3585.5MB/7520.0MB, GPU 2: 3581.9MB/7510.0MB, GPU 3: 3579.0MB/7510.0MB, Total: 14322.7MB allocated, 30060.0MB reserved, Mean: 3580.7MB allocated, 7515.0MB reserved
2025-03-25 00:05:43,199 - training - INFO - Epoch: 23/200000, Batch: 54/163, Loss: 3.9066, Throughput: 24.38 samples/sec
2025-03-25 00:06:18,936 - training - INFO - GPU Memory: GPU 0: 3583.3MB/7506.0MB, GPU 1: 3584.8MB/7520.0MB, GPU 2: 3586.5MB/7516.0MB, GPU 3: 3581.5MB/7514.0MB, Total: 14336.1MB allocated, 30056.0MB reserved, Mean: 3584.0MB allocated, 7514.0MB reserved
2025-03-25 00:06:18,936 - training - INFO - Epoch: 23/200000, Batch: 108/163, Loss: 3.9430, Throughput: 24.28 samples/sec
2025-03-25 00:06:54,404 - training - INFO - GPU Memory: GPU 0: 3568.6MB/7520.0MB, GPU 1: 3572.2MB/7528.0MB, GPU 2: 3575.7MB/7510.0MB, GPU 3: 3561.7MB/7514.0MB, Total: 14278.2MB allocated, 30072.0MB reserved, Mean: 3569.6MB allocated, 7518.0MB reserved
2025-03-25 00:06:54,405 - training - INFO - Epoch: 23/200000, Batch: 162/163, Loss: 3.9280, Throughput: 24.31 samples/sec
2025-03-25 00:06:54,405 - training - INFO - Epoch 23 completed in 107.30s. Average loss: 3.9923
2025-03-25 00:06:54,417 - training - INFO - Starting epoch 24/200000
2025-03-25 00:06:55,058 - training - INFO - GPU Memory: GPU 0: 3579.2MB/7520.0MB, GPU 1: 3584.7MB/7528.0MB, GPU 2: 3581.6MB/7510.0MB, GPU 3: 3578.4MB/7514.0MB, Total: 14323.9MB allocated, 30072.0MB reserved, Mean: 3581.0MB allocated, 7518.0MB reserved
2025-03-25 00:06:55,058 - training - INFO - Epoch: 24/200000, Batch: 0/163, Loss: 4.1955, Throughput: 24.96 samples/sec
2025-03-25 00:07:31,004 - training - INFO - GPU Memory: GPU 0: 3577.5MB/7514.0MB, GPU 1: 3583.7MB/7528.0MB, GPU 2: 3583.7MB/7510.0MB, GPU 3: 3576.5MB/7522.0MB, Total: 14321.3MB allocated, 30074.0MB reserved, Mean: 3580.3MB allocated, 7518.5MB reserved
2025-03-25 00:07:31,005 - training - INFO - Epoch: 24/200000, Batch: 54/163, Loss: 4.0306, Throughput: 24.05 samples/sec
2025-03-25 00:08:06,814 - training - INFO - GPU Memory: GPU 0: 3576.4MB/7510.0MB, GPU 1: 3584.0MB/7524.0MB, GPU 2: 3585.4MB/7508.0MB, GPU 3: 3582.1MB/7500.0MB, Total: 14327.9MB allocated, 30042.0MB reserved, Mean: 3582.0MB allocated, 7510.5MB reserved
2025-03-25 00:08:06,814 - training - INFO - Epoch: 24/200000, Batch: 108/163, Loss: 3.9614, Throughput: 24.09 samples/sec
2025-03-25 00:08:42,016 - training - INFO - GPU Memory: GPU 0: 3572.6MB/7514.0MB, GPU 1: 3575.1MB/7524.0MB, GPU 2: 3569.5MB/7510.0MB, GPU 3: 3565.1MB/7518.0MB, Total: 14282.4MB allocated, 30066.0MB reserved, Mean: 3570.6MB allocated, 7516.5MB reserved
2025-03-25 00:08:42,016 - training - INFO - Epoch: 24/200000, Batch: 162/163, Loss: 3.9762, Throughput: 24.24 samples/sec
2025-03-25 00:08:42,017 - training - INFO - Epoch 24 completed in 107.60s. Average loss: 3.9808
2025-03-25 00:08:42,023 - training - INFO - Starting epoch 25/200000
2025-03-25 00:08:42,659 - training - INFO - GPU Memory: GPU 0: 3584.7MB/7514.0MB, GPU 1: 3585.9MB/7524.0MB, GPU 2: 3580.2MB/7510.0MB, GPU 3: 3577.1MB/7518.0MB, Total: 14327.8MB allocated, 30066.0MB reserved, Mean: 3581.9MB allocated, 7516.5MB reserved
2025-03-25 00:08:42,659 - training - INFO - Epoch: 25/200000, Batch: 0/163, Loss: 3.9055, Throughput: 25.15 samples/sec
2025-03-25 00:09:17,904 - training - INFO - GPU Memory: GPU 0: 3571.2MB/7512.0MB, GPU 1: 3585.2MB/7524.0MB, GPU 2: 3575.1MB/7520.0MB, GPU 3: 3579.1MB/7514.0MB, Total: 14310.6MB allocated, 30070.0MB reserved, Mean: 3577.7MB allocated, 7517.5MB reserved
2025-03-25 00:09:17,904 - training - INFO - Epoch: 25/200000, Batch: 54/163, Loss: 4.0137, Throughput: 24.53 samples/sec
2025-03-25 00:09:53,681 - training - INFO - GPU Memory: GPU 0: 3579.6MB/7504.0MB, GPU 1: 3582.1MB/7526.0MB, GPU 2: 3587.1MB/7504.0MB, GPU 3: 3573.4MB/7512.0MB, Total: 14322.1MB allocated, 30046.0MB reserved, Mean: 3580.5MB allocated, 7511.5MB reserved
2025-03-25 00:09:53,682 - training - INFO - Epoch: 25/200000, Batch: 108/163, Loss: 3.9882, Throughput: 24.34 samples/sec
2025-03-25 00:10:28,962 - training - INFO - GPU Memory: GPU 0: 3567.3MB/7510.0MB, GPU 1: 3574.2MB/7518.0MB, GPU 2: 3574.5MB/7516.0MB, GPU 3: 3563.6MB/7508.0MB, Total: 14279.6MB allocated, 30052.0MB reserved, Mean: 3569.9MB allocated, 7513.0MB reserved
2025-03-25 00:10:28,962 - training - INFO - Epoch: 25/200000, Batch: 162/163, Loss: 3.9835, Throughput: 24.39 samples/sec
2025-03-25 00:10:28,963 - training - INFO - Epoch 25 completed in 106.94s. Average loss: 3.9587
2025-03-25 00:10:28,974 - training - INFO - Starting epoch 26/200000
2025-03-25 00:10:29,637 - training - INFO - GPU Memory: GPU 0: 3575.1MB/7510.0MB, GPU 1: 3584.4MB/7518.0MB, GPU 2: 3578.0MB/7516.0MB, GPU 3: 3574.2MB/7508.0MB, Total: 14311.7MB allocated, 30052.0MB reserved, Mean: 3577.9MB allocated, 7513.0MB reserved
2025-03-25 00:10:29,637 - training - INFO - Epoch: 26/200000, Batch: 0/163, Loss: 4.0065, Throughput: 24.13 samples/sec
2025-03-25 00:11:05,151 - training - INFO - GPU Memory: GPU 0: 3577.1MB/7510.0MB, GPU 1: 3580.2MB/7524.0MB, GPU 2: 3582.8MB/7514.0MB, GPU 3: 3577.6MB/7514.0MB, Total: 14317.7MB allocated, 30062.0MB reserved, Mean: 3579.4MB allocated, 7515.5MB reserved
2025-03-25 00:11:05,152 - training - INFO - Epoch: 26/200000, Batch: 54/163, Loss: 3.9169, Throughput: 24.32 samples/sec
2025-03-25 00:11:40,961 - training - INFO - GPU Memory: GPU 0: 3574.0MB/7514.0MB, GPU 1: 3578.4MB/7522.0MB, GPU 2: 3576.9MB/7506.0MB, GPU 3: 3575.9MB/7506.0MB, Total: 14305.1MB allocated, 30048.0MB reserved, Mean: 3576.3MB allocated, 7512.0MB reserved
2025-03-25 00:11:40,961 - training - INFO - Epoch: 26/200000, Batch: 108/163, Loss: 3.9513, Throughput: 24.23 samples/sec
2025-03-25 00:12:16,314 - training - INFO - GPU Memory: GPU 0: 3569.9MB/7504.0MB, GPU 1: 3564.8MB/7516.0MB, GPU 2: 3571.9MB/7512.0MB, GPU 3: 3570.9MB/7514.0MB, Total: 14277.5MB allocated, 30046.0MB reserved, Mean: 3569.4MB allocated, 7511.5MB reserved
2025-03-25 00:12:16,315 - training - INFO - Epoch: 26/200000, Batch: 162/163, Loss: 3.9466, Throughput: 24.30 samples/sec
2025-03-25 00:12:16,316 - training - INFO - Epoch 26 completed in 107.34s. Average loss: 3.9427
2025-03-25 00:12:16,326 - training - INFO - Starting epoch 27/200000
2025-03-25 00:12:16,958 - training - INFO - GPU Memory: GPU 0: 3580.1MB/7504.0MB, GPU 1: 3575.5MB/7516.0MB, GPU 2: 3581.2MB/7512.0MB, GPU 3: 3581.1MB/7514.0MB, Total: 14317.9MB allocated, 30046.0MB reserved, Mean: 3579.5MB allocated, 7511.5MB reserved
2025-03-25 00:12:16,959 - training - INFO - Epoch: 27/200000, Batch: 0/163, Loss: 3.4522, Throughput: 25.31 samples/sec
2025-03-25 00:12:52,447 - training - INFO - GPU Memory: GPU 0: 3576.0MB/7506.0MB, GPU 1: 3582.6MB/7520.0MB, GPU 2: 3582.6MB/7510.0MB, GPU 3: 3576.9MB/7514.0MB, Total: 14318.0MB allocated, 30050.0MB reserved, Mean: 3579.5MB allocated, 7512.5MB reserved
2025-03-25 00:12:52,447 - training - INFO - Epoch: 27/200000, Batch: 54/163, Loss: 3.8951, Throughput: 24.36 samples/sec
2025-03-25 00:13:28,219 - training - INFO - GPU Memory: GPU 0: 3574.0MB/7506.0MB, GPU 1: 3579.8MB/7518.0MB, GPU 2: 3581.1MB/7516.0MB, GPU 3: 3574.6MB/7512.0MB, Total: 14309.5MB allocated, 30052.0MB reserved, Mean: 3577.4MB allocated, 7513.0MB reserved
2025-03-25 00:13:28,220 - training - INFO - Epoch: 27/200000, Batch: 108/163, Loss: 3.9517, Throughput: 24.26 samples/sec
2025-03-25 00:14:03,459 - training - INFO - GPU Memory: GPU 0: 3568.4MB/7508.0MB, GPU 1: 3570.5MB/7518.0MB, GPU 2: 3573.2MB/7500.0MB, GPU 3: 3570.4MB/7514.0MB, Total: 14282.5MB allocated, 30040.0MB reserved, Mean: 3570.6MB allocated, 7510.0MB reserved
2025-03-25 00:14:03,459 - training - INFO - Epoch: 27/200000, Batch: 162/163, Loss: 3.9759, Throughput: 24.34 samples/sec
2025-03-25 00:14:03,460 - training - INFO - Epoch 27 completed in 107.13s. Average loss: 3.9262
2025-03-25 00:14:03,473 - training - INFO - Starting epoch 28/200000
2025-03-25 00:14:04,100 - training - INFO - GPU Memory: GPU 0: 3579.2MB/7508.0MB, GPU 1: 3581.2MB/7518.0MB, GPU 2: 3579.1MB/7500.0MB, GPU 3: 3580.6MB/7514.0MB, Total: 14320.1MB allocated, 30040.0MB reserved, Mean: 3580.0MB allocated, 7510.0MB reserved
2025-03-25 00:14:04,100 - training - INFO - Epoch: 28/200000, Batch: 0/163, Loss: 4.0753, Throughput: 25.52 samples/sec
2025-03-25 00:14:39,651 - training - INFO - GPU Memory: GPU 0: 3571.1MB/7500.0MB, GPU 1: 3576.7MB/7518.0MB, GPU 2: 3585.1MB/7522.0MB, GPU 3: 3580.7MB/7514.0MB, Total: 14313.5MB allocated, 30054.0MB reserved, Mean: 3578.4MB allocated, 7513.5MB reserved
2025-03-25 00:14:39,651 - training - INFO - Epoch: 28/200000, Batch: 54/163, Loss: 3.8314, Throughput: 24.32 samples/sec
2025-03-25 00:15:15,345 - training - INFO - GPU Memory: GPU 0: 3578.2MB/7500.0MB, GPU 1: 3583.9MB/7516.0MB, GPU 2: 3584.5MB/7504.0MB, GPU 3: 3583.7MB/7518.0MB, Total: 14330.4MB allocated, 30038.0MB reserved, Mean: 3582.6MB allocated, 7509.5MB reserved
2025-03-25 00:15:15,346 - training - INFO - Epoch: 28/200000, Batch: 108/163, Loss: 3.9105, Throughput: 24.27 samples/sec
2025-03-25 00:15:50,558 - training - INFO - GPU Memory: GPU 0: 3569.1MB/7510.0MB, GPU 1: 3568.0MB/7522.0MB, GPU 2: 3574.6MB/7506.0MB, GPU 3: 3568.5MB/7516.0MB, Total: 14280.3MB allocated, 30054.0MB reserved, Mean: 3570.1MB allocated, 7513.5MB reserved
2025-03-25 00:15:50,559 - training - INFO - Epoch: 28/200000, Batch: 162/163, Loss: 3.9144, Throughput: 24.35 samples/sec
2025-03-25 00:15:50,559 - training - INFO - Epoch 28 completed in 107.09s. Average loss: 3.9110
2025-03-25 00:15:50,570 - training - INFO - Starting epoch 29/200000
2025-03-25 00:15:51,215 - training - INFO - GPU Memory: GPU 0: 3579.8MB/7510.0MB, GPU 1: 3578.7MB/7522.0MB, GPU 2: 3585.5MB/7506.0MB, GPU 3: 3579.7MB/7516.0MB, Total: 14323.7MB allocated, 30054.0MB reserved, Mean: 3580.9MB allocated, 7513.5MB reserved
2025-03-25 00:15:51,215 - training - INFO - Epoch: 29/200000, Batch: 0/163, Loss: 4.3130, Throughput: 24.82 samples/sec
2025-03-25 00:16:26,667 - training - INFO - GPU Memory: GPU 0: 3575.0MB/7514.0MB, GPU 1: 3582.7MB/7522.0MB, GPU 2: 3585.1MB/7508.0MB, GPU 3: 3578.2MB/7516.0MB, Total: 14320.9MB allocated, 30060.0MB reserved, Mean: 3580.2MB allocated, 7515.0MB reserved
2025-03-25 00:16:26,667 - training - INFO - Epoch: 29/200000, Batch: 54/163, Loss: 4.0374, Throughput: 24.38 samples/sec
2025-03-25 00:17:02,377 - training - INFO - GPU Memory: GPU 0: 3576.6MB/7506.0MB, GPU 1: 3583.5MB/7522.0MB, GPU 2: 3584.1MB/7510.0MB, GPU 3: 3578.7MB/7506.0MB, Total: 14323.0MB allocated, 30044.0MB reserved, Mean: 3580.7MB allocated, 7511.0MB reserved
2025-03-25 00:17:02,377 - training - INFO - Epoch: 29/200000, Batch: 108/163, Loss: 3.9623, Throughput: 24.29 samples/sec
2025-03-25 00:17:37,551 - training - INFO - GPU Memory: GPU 0: 3567.6MB/7508.0MB, GPU 1: 3566.8MB/7516.0MB, GPU 2: 3572.5MB/7514.0MB, GPU 3: 3568.9MB/7508.0MB, Total: 14275.8MB allocated, 30046.0MB reserved, Mean: 3569.0MB allocated, 7511.5MB reserved
2025-03-25 00:17:37,552 - training - INFO - Epoch: 29/200000, Batch: 162/163, Loss: 3.9134, Throughput: 24.38 samples/sec
2025-03-25 00:17:37,552 - training - INFO - Epoch 29 completed in 106.98s. Average loss: 3.8944
2025-03-25 00:17:37,558 - training - INFO - Starting epoch 30/200000
2025-03-25 00:17:38,198 - training - INFO - GPU Memory: GPU 0: 3578.2MB/7508.0MB, GPU 1: 3578.7MB/7516.0MB, GPU 2: 3581.2MB/7514.0MB, GPU 3: 3579.6MB/7508.0MB, Total: 14317.8MB allocated, 30046.0MB reserved, Mean: 3579.4MB allocated, 7511.5MB reserved
2025-03-25 00:17:38,198 - training - INFO - Epoch: 30/200000, Batch: 0/163, Loss: 4.4353, Throughput: 25.01 samples/sec
2025-03-25 00:18:13,510 - training - INFO - GPU Memory: GPU 0: 3574.7MB/7506.0MB, GPU 1: 3585.9MB/7530.0MB, GPU 2: 3582.0MB/7518.0MB, GPU 3: 3573.1MB/7514.0MB, Total: 14315.7MB allocated, 30068.0MB reserved, Mean: 3578.9MB allocated, 7517.0MB reserved
2025-03-25 00:18:13,510 - training - INFO - Epoch: 30/200000, Batch: 54/163, Loss: 3.9749, Throughput: 24.48 samples/sec
2025-03-25 00:18:49,156 - training - INFO - GPU Memory: GPU 0: 3577.0MB/7508.0MB, GPU 1: 3578.5MB/7526.0MB, GPU 2: 3582.6MB/7514.0MB, GPU 3: 3584.0MB/7502.0MB, Total: 14322.1MB allocated, 30050.0MB reserved, Mean: 3580.5MB allocated, 7512.5MB reserved
2025-03-25 00:18:49,156 - training - INFO - Epoch: 30/200000, Batch: 108/163, Loss: 3.9218, Throughput: 24.36 samples/sec
2025-03-25 00:19:24,321 - training - INFO - GPU Memory: GPU 0: 3568.8MB/7512.0MB, GPU 1: 3571.3MB/7516.0MB, GPU 2: 3574.7MB/7514.0MB, GPU 3: 3567.4MB/7512.0MB, Total: 14282.2MB allocated, 30054.0MB reserved, Mean: 3570.6MB allocated, 7513.5MB reserved
2025-03-25 00:19:24,322 - training - INFO - Epoch: 30/200000, Batch: 162/163, Loss: 3.9152, Throughput: 24.43 samples/sec
2025-03-25 00:19:24,322 - training - INFO - Epoch 30 completed in 106.76s. Average loss: 3.8866
2025-03-25 00:19:24,326 - training - INFO - Starting epoch 31/200000
2025-03-25 00:19:24,966 - training - INFO - GPU Memory: GPU 0: 3581.2MB/7512.0MB, GPU 1: 3582.0MB/7516.0MB, GPU 2: 3581.6MB/7514.0MB, GPU 3: 3578.6MB/7512.0MB, Total: 14323.4MB allocated, 30054.0MB reserved, Mean: 3580.9MB allocated, 7513.5MB reserved
2025-03-25 00:19:24,967 - training - INFO - Epoch: 31/200000, Batch: 0/163, Loss: 3.0899, Throughput: 24.98 samples/sec
2025-03-25 00:20:00,436 - training - INFO - GPU Memory: GPU 0: 3582.2MB/7512.0MB, GPU 1: 3577.2MB/7508.0MB, GPU 2: 3576.9MB/7502.0MB, GPU 3: 3573.6MB/7512.0MB, Total: 14309.8MB allocated, 30034.0MB reserved, Mean: 3577.5MB allocated, 7508.5MB reserved
2025-03-25 00:20:00,436 - training - INFO - Epoch: 31/200000, Batch: 54/163, Loss: 3.8768, Throughput: 24.37 samples/sec
2025-03-25 00:20:36,612 - training - INFO - GPU Memory: GPU 0: 3583.2MB/7512.0MB, GPU 1: 3582.7MB/7516.0MB, GPU 2: 3578.7MB/7504.0MB, GPU 3: 3576.0MB/7508.0MB, Total: 14320.6MB allocated, 30040.0MB reserved, Mean: 3580.2MB allocated, 7510.0MB reserved
2025-03-25 00:20:36,613 - training - INFO - Epoch: 31/200000, Batch: 108/163, Loss: 3.8441, Throughput: 24.13 samples/sec
2025-03-25 00:21:12,367 - training - INFO - GPU Memory: GPU 0: 3562.6MB/7520.0MB, GPU 1: 3572.0MB/7526.0MB, GPU 2: 3578.2MB/7496.0MB, GPU 3: 3562.8MB/7504.0MB, Total: 14275.5MB allocated, 30046.0MB reserved, Mean: 3568.9MB allocated, 7511.5MB reserved
2025-03-25 00:21:12,368 - training - INFO - Epoch: 31/200000, Batch: 162/163, Loss: 3.8479, Throughput: 24.14 samples/sec
2025-03-25 00:21:12,369 - training - INFO - Epoch 31 completed in 108.04s. Average loss: 3.8729
2025-03-25 00:21:12,380 - training - INFO - Starting epoch 32/200000
2025-03-25 00:21:13,020 - training - INFO - GPU Memory: GPU 0: 3573.2MB/7520.0MB, GPU 1: 3582.7MB/7526.0MB, GPU 2: 3578.5MB/7496.0MB, GPU 3: 3573.2MB/7504.0MB, Total: 14307.6MB allocated, 30046.0MB reserved, Mean: 3576.9MB allocated, 7511.5MB reserved
2025-03-25 00:21:13,021 - training - INFO - Epoch: 32/200000, Batch: 0/163, Loss: 4.3107, Throughput: 25.00 samples/sec
2025-03-25 00:21:48,663 - training - INFO - GPU Memory: GPU 0: 3579.1MB/7512.0MB, GPU 1: 3583.8MB/7526.0MB, GPU 2: 3581.0MB/7506.0MB, GPU 3: 3581.8MB/7510.0MB, Total: 14325.7MB allocated, 30054.0MB reserved, Mean: 3581.4MB allocated, 7513.5MB reserved
2025-03-25 00:21:48,663 - training - INFO - Epoch: 32/200000, Batch: 54/163, Loss: 3.8738, Throughput: 24.25 samples/sec
2025-03-25 00:22:24,317 - training - INFO - GPU Memory: GPU 0: 3572.5MB/7510.0MB, GPU 1: 3574.4MB/7524.0MB, GPU 2: 3582.4MB/7510.0MB, GPU 3: 3579.1MB/7516.0MB, Total: 14308.3MB allocated, 30060.0MB reserved, Mean: 3577.1MB allocated, 7515.0MB reserved
2025-03-25 00:22:24,317 - training - INFO - Epoch: 32/200000, Batch: 108/163, Loss: 3.8250, Throughput: 24.24 samples/sec
2025-03-25 00:22:59,616 - training - INFO - GPU Memory: GPU 0: 3564.1MB/7504.0MB, GPU 1: 3563.1MB/7508.0MB, GPU 2: 3572.2MB/7506.0MB, GPU 3: 3567.4MB/7518.0MB, Total: 14266.9MB allocated, 30036.0MB reserved, Mean: 3566.7MB allocated, 7509.0MB reserved
2025-03-25 00:22:59,617 - training - INFO - Epoch: 32/200000, Batch: 162/163, Loss: 3.8310, Throughput: 24.32 samples/sec
2025-03-25 00:22:59,618 - training - INFO - Epoch 32 completed in 107.24s. Average loss: 3.8498
2025-03-25 00:22:59,628 - training - INFO - Starting epoch 33/200000
2025-03-25 00:23:00,278 - training - INFO - GPU Memory: GPU 0: 3574.8MB/7504.0MB, GPU 1: 3573.8MB/7508.0MB, GPU 2: 3584.1MB/7506.0MB, GPU 3: 3578.6MB/7518.0MB, Total: 14311.4MB allocated, 30036.0MB reserved, Mean: 3577.8MB allocated, 7509.0MB reserved
2025-03-25 00:23:00,278 - training - INFO - Epoch: 33/200000, Batch: 0/163, Loss: 4.0039, Throughput: 24.63 samples/sec
2025-03-25 00:23:35,831 - training - INFO - GPU Memory: GPU 0: 3576.8MB/7506.0MB, GPU 1: 3585.4MB/7522.0MB, GPU 2: 3576.4MB/7506.0MB, GPU 3: 3573.0MB/7514.0MB, Total: 14311.6MB allocated, 30048.0MB reserved, Mean: 3577.9MB allocated, 7512.0MB reserved
2025-03-25 00:23:35,832 - training - INFO - Epoch: 33/200000, Batch: 54/163, Loss: 3.8995, Throughput: 24.31 samples/sec
2025-03-25 00:24:11,631 - training - INFO - GPU Memory: GPU 0: 3571.2MB/7502.0MB, GPU 1: 3578.6MB/7526.0MB, GPU 2: 3574.9MB/7508.0MB, GPU 3: 3580.9MB/7504.0MB, Total: 14305.6MB allocated, 30040.0MB reserved, Mean: 3576.4MB allocated, 7510.0MB reserved
2025-03-25 00:24:11,631 - training - INFO - Epoch: 33/200000, Batch: 108/163, Loss: 3.9293, Throughput: 24.22 samples/sec
2025-03-25 00:24:46,843 - training - INFO - GPU Memory: GPU 0: 3572.0MB/7500.0MB, GPU 1: 3571.2MB/7514.0MB, GPU 2: 3577.1MB/7512.0MB, GPU 3: 3566.1MB/7516.0MB, Total: 14286.4MB allocated, 30042.0MB reserved, Mean: 3571.6MB allocated, 7510.5MB reserved
2025-03-25 00:24:46,844 - training - INFO - Epoch: 33/200000, Batch: 162/163, Loss: 3.8926, Throughput: 24.32 samples/sec
2025-03-25 00:24:46,844 - training - INFO - Epoch 33 completed in 107.22s. Average loss: 3.8346
2025-03-25 00:24:46,853 - training - INFO - Starting epoch 34/200000
2025-03-25 00:24:47,481 - training - INFO - GPU Memory: GPU 0: 3582.8MB/7500.0MB, GPU 1: 3580.4MB/7514.0MB, GPU 2: 3587.6MB/7512.0MB, GPU 3: 3580.1MB/7516.0MB, Total: 14331.0MB allocated, 30042.0MB reserved, Mean: 3582.7MB allocated, 7510.5MB reserved
2025-03-25 00:24:47,481 - training - INFO - Epoch: 34/200000, Batch: 0/163, Loss: 3.3999, Throughput: 25.48 samples/sec
2025-03-25 00:25:23,059 - training - INFO - GPU Memory: GPU 0: 3581.1MB/7516.0MB, GPU 1: 3580.6MB/7524.0MB, GPU 2: 3590.2MB/7506.0MB, GPU 3: 3577.6MB/7498.0MB, Total: 14329.5MB allocated, 30044.0MB reserved, Mean: 3582.4MB allocated, 7511.0MB reserved
2025-03-25 00:25:23,059 - training - INFO - Epoch: 34/200000, Batch: 54/163, Loss: 3.8971, Throughput: 24.31 samples/sec
2025-03-25 00:25:59,541 - training - INFO - GPU Memory: GPU 0: 3578.4MB/7514.0MB, GPU 1: 3580.8MB/7520.0MB, GPU 2: 3585.8MB/7524.0MB, GPU 3: 3578.0MB/7508.0MB, Total: 14322.9MB allocated, 30066.0MB reserved, Mean: 3580.7MB allocated, 7516.5MB reserved
2025-03-25 00:25:59,541 - training - INFO - Epoch: 34/200000, Batch: 108/163, Loss: 3.8750, Throughput: 23.99 samples/sec
2025-03-25 00:26:34,655 - training - INFO - GPU Memory: GPU 0: 3565.8MB/7514.0MB, GPU 1: 3568.6MB/7510.0MB, GPU 2: 3578.7MB/7524.0MB, GPU 3: 3562.2MB/7506.0MB, Total: 14275.3MB allocated, 30054.0MB reserved, Mean: 3568.8MB allocated, 7513.5MB reserved
2025-03-25 00:26:34,655 - training - INFO - Epoch: 34/200000, Batch: 162/163, Loss: 3.8776, Throughput: 24.19 samples/sec
2025-03-25 00:26:34,655 - training - INFO - Epoch 34 completed in 107.80s. Average loss: 3.8274
2025-03-25 00:26:34,662 - training - INFO - Starting epoch 35/200000
2025-03-25 00:26:35,290 - training - INFO - GPU Memory: GPU 0: 3576.0MB/7514.0MB, GPU 1: 3579.3MB/7510.0MB, GPU 2: 3582.4MB/7524.0MB, GPU 3: 3573.4MB/7506.0MB, Total: 14311.0MB allocated, 30054.0MB reserved, Mean: 3577.7MB allocated, 7513.5MB reserved
2025-03-25 00:26:35,290 - training - INFO - Epoch: 35/200000, Batch: 0/163, Loss: 4.1113, Throughput: 25.49 samples/sec
2025-03-25 00:27:10,835 - training - INFO - GPU Memory: GPU 0: 3573.1MB/7516.0MB, GPU 1: 3580.3MB/7514.0MB, GPU 2: 3575.1MB/7502.0MB, GPU 3: 3575.0MB/7510.0MB, Total: 14303.4MB allocated, 30042.0MB reserved, Mean: 3575.8MB allocated, 7510.5MB reserved
2025-03-25 00:27:10,835 - training - INFO - Epoch: 35/200000, Batch: 54/163, Loss: 3.7029, Throughput: 24.33 samples/sec
2025-03-25 00:27:46,649 - training - INFO - GPU Memory: GPU 0: 3575.0MB/7496.0MB, GPU 1: 3579.5MB/7512.0MB, GPU 2: 3581.0MB/7508.0MB, GPU 3: 3578.6MB/7516.0MB, Total: 14314.1MB allocated, 30032.0MB reserved, Mean: 3578.5MB allocated, 7508.0MB reserved
2025-03-25 00:27:46,649 - training - INFO - Epoch: 35/200000, Batch: 108/163, Loss: 3.7446, Throughput: 24.23 samples/sec
2025-03-25 00:28:21,844 - training - INFO - GPU Memory: GPU 0: 3563.9MB/7504.0MB, GPU 1: 3571.5MB/7520.0MB, GPU 2: 3569.7MB/7504.0MB, GPU 3: 3571.4MB/7514.0MB, Total: 14276.4MB allocated, 30042.0MB reserved, Mean: 3569.1MB allocated, 7510.5MB reserved
2025-03-25 00:28:21,845 - training - INFO - Epoch: 35/200000, Batch: 162/163, Loss: 3.7748, Throughput: 24.33 samples/sec
2025-03-25 00:28:21,845 - training - INFO - Epoch 35 completed in 107.18s. Average loss: 3.7966
2025-03-25 00:28:21,852 - training - INFO - Starting epoch 36/200000
2025-03-25 00:28:22,483 - training - INFO - GPU Memory: GPU 0: 3575.4MB/7504.0MB, GPU 1: 3579.9MB/7520.0MB, GPU 2: 3580.3MB/7504.0MB, GPU 3: 3583.1MB/7514.0MB, Total: 14318.6MB allocated, 30042.0MB reserved, Mean: 3579.6MB allocated, 7510.5MB reserved
2025-03-25 00:28:22,483 - training - INFO - Epoch: 36/200000, Batch: 0/163, Loss: 4.5138, Throughput: 25.35 samples/sec
2025-03-25 00:28:57,913 - training - INFO - GPU Memory: GPU 0: 3573.2MB/7516.0MB, GPU 1: 3575.7MB/7524.0MB, GPU 2: 3581.1MB/7506.0MB, GPU 3: 3575.8MB/7514.0MB, Total: 14305.7MB allocated, 30060.0MB reserved, Mean: 3576.4MB allocated, 7515.0MB reserved
2025-03-25 00:28:57,914 - training - INFO - Epoch: 36/200000, Batch: 54/163, Loss: 3.8387, Throughput: 24.40 samples/sec
2025-03-25 00:29:33,860 - training - INFO - GPU Memory: GPU 0: 3574.6MB/7504.0MB, GPU 1: 3577.9MB/7504.0MB, GPU 2: 3584.6MB/7508.0MB, GPU 3: 3577.0MB/7522.0MB, Total: 14314.1MB allocated, 30038.0MB reserved, Mean: 3578.5MB allocated, 7509.5MB reserved
2025-03-25 00:29:33,860 - training - INFO - Epoch: 36/200000, Batch: 108/163, Loss: 3.8441, Throughput: 24.22 samples/sec
2025-03-25 00:30:09,096 - training - INFO - GPU Memory: GPU 0: 3566.6MB/7508.0MB, GPU 1: 3570.1MB/7516.0MB, GPU 2: 3562.9MB/7518.0MB, GPU 3: 3567.4MB/7502.0MB, Total: 14267.0MB allocated, 30044.0MB reserved, Mean: 3566.7MB allocated, 7511.0MB reserved
2025-03-25 00:30:09,096 - training - INFO - Epoch: 36/200000, Batch: 162/163, Loss: 3.7780, Throughput: 24.32 samples/sec
2025-03-25 00:30:09,097 - training - INFO - Epoch 36 completed in 107.25s. Average loss: 3.7957
2025-03-25 00:30:09,107 - training - INFO - Starting epoch 37/200000
2025-03-25 00:30:09,737 - training - INFO - GPU Memory: GPU 0: 3577.5MB/7508.0MB, GPU 1: 3580.9MB/7516.0MB, GPU 2: 3578.3MB/7518.0MB, GPU 3: 3582.2MB/7502.0MB, Total: 14318.9MB allocated, 30044.0MB reserved, Mean: 3579.7MB allocated, 7511.0MB reserved
2025-03-25 00:30:09,737 - training - INFO - Epoch: 37/200000, Batch: 0/163, Loss: 3.1694, Throughput: 25.40 samples/sec
2025-03-25 00:30:45,225 - training - INFO - GPU Memory: GPU 0: 3577.7MB/7512.0MB, GPU 1: 3585.1MB/7520.0MB, GPU 2: 3580.0MB/7520.0MB, GPU 3: 3583.2MB/7510.0MB, Total: 14326.0MB allocated, 30062.0MB reserved, Mean: 3581.5MB allocated, 7515.5MB reserved
2025-03-25 00:30:45,225 - training - INFO - Epoch: 37/200000, Batch: 54/163, Loss: 3.7073, Throughput: 24.36 samples/sec
2025-03-25 00:31:21,173 - training - INFO - GPU Memory: GPU 0: 3575.2MB/7510.0MB, GPU 1: 3580.5MB/7518.0MB, GPU 2: 3583.1MB/7510.0MB, GPU 3: 3578.9MB/7512.0MB, Total: 14317.8MB allocated, 30050.0MB reserved, Mean: 3579.4MB allocated, 7512.5MB reserved
2025-03-25 00:31:21,173 - training - INFO - Epoch: 37/200000, Batch: 108/163, Loss: 3.7226, Throughput: 24.20 samples/sec
2025-03-25 00:31:56,902 - training - INFO - GPU Memory: GPU 0: 3568.1MB/7502.0MB, GPU 1: 3568.6MB/7518.0MB, GPU 2: 3576.7MB/7516.0MB, GPU 3: 3569.4MB/7510.0MB, Total: 14282.7MB allocated, 30046.0MB reserved, Mean: 3570.7MB allocated, 7511.5MB reserved
2025-03-25 00:31:56,902 - training - INFO - Epoch: 37/200000, Batch: 162/163, Loss: 3.7856, Throughput: 24.19 samples/sec
2025-03-25 00:31:56,902 - training - INFO - Epoch 37 completed in 107.80s. Average loss: 3.7811
2025-03-25 00:31:56,908 - training - INFO - Starting epoch 38/200000
2025-03-25 00:31:57,558 - training - INFO - GPU Memory: GPU 0: 3579.0MB/7502.0MB, GPU 1: 3579.2MB/7518.0MB, GPU 2: 3585.2MB/7516.0MB, GPU 3: 3576.4MB/7510.0MB, Total: 14319.8MB allocated, 30046.0MB reserved, Mean: 3579.9MB allocated, 7511.5MB reserved
2025-03-25 00:31:57,558 - training - INFO - Epoch: 38/200000, Batch: 0/163, Loss: 4.2712, Throughput: 24.59 samples/sec
2025-03-25 00:32:33,066 - training - INFO - GPU Memory: GPU 0: 3574.5MB/7514.0MB, GPU 1: 3581.7MB/7518.0MB, GPU 2: 3580.5MB/7508.0MB, GPU 3: 3577.6MB/7512.0MB, Total: 14314.3MB allocated, 30052.0MB reserved, Mean: 3578.6MB allocated, 7513.0MB reserved
2025-03-25 00:32:33,066 - training - INFO - Epoch: 38/200000, Batch: 54/163, Loss: 3.7479, Throughput: 24.34 samples/sec
2025-03-25 00:33:08,829 - training - INFO - GPU Memory: GPU 0: 3579.7MB/7506.0MB, GPU 1: 3577.4MB/7514.0MB, GPU 2: 3581.7MB/7510.0MB, GPU 3: 3579.2MB/7508.0MB, Total: 14318.2MB allocated, 30038.0MB reserved, Mean: 3579.5MB allocated, 7509.5MB reserved
2025-03-25 00:33:08,829 - training - INFO - Epoch: 38/200000, Batch: 108/163, Loss: 3.7661, Throughput: 24.25 samples/sec
2025-03-25 00:33:44,135 - training - INFO - GPU Memory: GPU 0: 3566.0MB/7508.0MB, GPU 1: 3568.4MB/7514.0MB, GPU 2: 3572.1MB/7516.0MB, GPU 3: 3569.9MB/7510.0MB, Total: 14276.3MB allocated, 30048.0MB reserved, Mean: 3569.1MB allocated, 7512.0MB reserved
2025-03-25 00:33:44,135 - training - INFO - Epoch: 38/200000, Batch: 162/163, Loss: 3.7487, Throughput: 24.32 samples/sec
2025-03-25 00:33:44,136 - training - INFO - Epoch 38 completed in 107.23s. Average loss: 3.7579
2025-03-25 00:33:44,147 - training - INFO - Starting epoch 39/200000
2025-03-25 00:33:44,786 - training - INFO - GPU Memory: GPU 0: 3576.6MB/7508.0MB, GPU 1: 3582.5MB/7514.0MB, GPU 2: 3586.8MB/7516.0MB, GPU 3: 3574.9MB/7510.0MB, Total: 14320.8MB allocated, 30048.0MB reserved, Mean: 3580.2MB allocated, 7512.0MB reserved
2025-03-25 00:33:44,787 - training - INFO - Epoch: 39/200000, Batch: 0/163, Loss: 4.2955, Throughput: 25.04 samples/sec
2025-03-25 00:34:20,231 - training - INFO - GPU Memory: GPU 0: 3578.2MB/7516.0MB, GPU 1: 3580.8MB/7530.0MB, GPU 2: 3581.1MB/7512.0MB, GPU 3: 3574.7MB/7512.0MB, Total: 14314.8MB allocated, 30070.0MB reserved, Mean: 3578.7MB allocated, 7517.5MB reserved
2025-03-25 00:34:20,231 - training - INFO - Epoch: 39/200000, Batch: 54/163, Loss: 3.8212, Throughput: 24.39 samples/sec
2025-03-25 00:34:56,071 - training - INFO - GPU Memory: GPU 0: 3576.9MB/7514.0MB, GPU 1: 3580.0MB/7524.0MB, GPU 2: 3581.8MB/7518.0MB, GPU 3: 3578.5MB/7514.0MB, Total: 14317.2MB allocated, 30070.0MB reserved, Mean: 3579.3MB allocated, 7517.5MB reserved
2025-03-25 00:34:56,071 - training - INFO - Epoch: 39/200000, Batch: 108/163, Loss: 3.7192, Throughput: 24.25 samples/sec
2025-03-25 00:35:31,338 - training - INFO - GPU Memory: GPU 0: 3567.8MB/7508.0MB, GPU 1: 3575.1MB/7526.0MB, GPU 2: 3577.0MB/7522.0MB, GPU 3: 3568.6MB/7514.0MB, Total: 14288.4MB allocated, 30070.0MB reserved, Mean: 3572.1MB allocated, 7517.5MB reserved
2025-03-25 00:35:31,338 - training - INFO - Epoch: 39/200000, Batch: 162/163, Loss: 3.7558, Throughput: 24.33 samples/sec
2025-03-25 00:35:31,339 - training - INFO - Epoch 39 completed in 107.19s. Average loss: 3.7441
2025-03-25 00:35:31,349 - training - INFO - Starting epoch 40/200000
2025-03-25 00:35:32,025 - training - INFO - GPU Memory: GPU 0: 3579.5MB/7508.0MB, GPU 1: 3584.5MB/7526.0MB, GPU 2: 3587.4MB/7522.0MB, GPU 3: 3579.2MB/7514.0MB, Total: 14330.6MB allocated, 30070.0MB reserved, Mean: 3582.6MB allocated, 7517.5MB reserved
2025-03-25 00:35:32,025 - training - INFO - Epoch: 40/200000, Batch: 0/163, Loss: 4.0343, Throughput: 23.68 samples/sec
2025-03-25 00:36:07,384 - training - INFO - GPU Memory: GPU 0: 3577.7MB/7512.0MB, GPU 1: 3580.6MB/7516.0MB, GPU 2: 3580.5MB/7502.0MB, GPU 3: 3574.4MB/7500.0MB, Total: 14313.1MB allocated, 30030.0MB reserved, Mean: 3578.3MB allocated, 7507.5MB reserved
2025-03-25 00:36:07,384 - training - INFO - Epoch: 40/200000, Batch: 54/163, Loss: 3.7490, Throughput: 24.42 samples/sec
2025-03-25 00:36:43,505 - training - INFO - GPU Memory: GPU 0: 3582.7MB/7518.0MB, GPU 1: 3579.4MB/7520.0MB, GPU 2: 3574.9MB/7502.0MB, GPU 3: 3580.7MB/7504.0MB, Total: 14317.8MB allocated, 30044.0MB reserved, Mean: 3579.5MB allocated, 7511.0MB reserved
2025-03-25 00:36:43,506 - training - INFO - Epoch: 40/200000, Batch: 108/163, Loss: 3.7305, Throughput: 24.17 samples/sec
2025-03-25 00:37:19,202 - training - INFO - GPU Memory: GPU 0: 3566.5MB/7516.0MB, GPU 1: 3565.1MB/7524.0MB, GPU 2: 3567.3MB/7504.0MB, GPU 3: 3563.4MB/7520.0MB, Total: 14262.2MB allocated, 30064.0MB reserved, Mean: 3565.6MB allocated, 7516.0MB reserved
2025-03-25 00:37:19,202 - training - INFO - Epoch: 40/200000, Batch: 162/163, Loss: 3.6758, Throughput: 24.18 samples/sec
2025-03-25 00:37:19,203 - training - INFO - Epoch 40 completed in 107.85s. Average loss: 3.7386
2025-03-25 00:37:19,217 - training - INFO - Starting epoch 41/200000
2025-03-25 00:37:19,869 - training - INFO - GPU Memory: GPU 0: 3581.4MB/7516.0MB, GPU 1: 3575.7MB/7524.0MB, GPU 2: 3581.3MB/7504.0MB, GPU 3: 3572.6MB/7520.0MB, Total: 14311.1MB allocated, 30064.0MB reserved, Mean: 3577.8MB allocated, 7516.0MB reserved
2025-03-25 00:37:19,870 - training - INFO - Epoch: 41/200000, Batch: 0/163, Loss: 3.7727, Throughput: 24.53 samples/sec
2025-03-25 00:37:55,314 - training - INFO - GPU Memory: GPU 0: 3579.5MB/7506.0MB, GPU 1: 3585.8MB/7524.0MB, GPU 2: 3572.6MB/7510.0MB, GPU 3: 3573.9MB/7510.0MB, Total: 14311.8MB allocated, 30050.0MB reserved, Mean: 3577.9MB allocated, 7512.5MB reserved
2025-03-25 00:37:55,314 - training - INFO - Epoch: 41/200000, Batch: 54/163, Loss: 3.7902, Throughput: 24.38 samples/sec
2025-03-25 00:38:31,017 - training - INFO - GPU Memory: GPU 0: 3571.7MB/7524.0MB, GPU 1: 3582.2MB/7518.0MB, GPU 2: 3585.2MB/7506.0MB, GPU 3: 3576.2MB/7502.0MB, Total: 14315.4MB allocated, 30050.0MB reserved, Mean: 3578.8MB allocated, 7512.5MB reserved
2025-03-25 00:38:31,018 - training - INFO - Epoch: 41/200000, Batch: 108/163, Loss: 3.7588, Throughput: 24.29 samples/sec
2025-03-25 00:39:06,288 - training - INFO - GPU Memory: GPU 0: 3564.7MB/7506.0MB, GPU 1: 3574.6MB/7522.0MB, GPU 2: 3575.0MB/7514.0MB, GPU 3: 3563.2MB/7502.0MB, Total: 14277.5MB allocated, 30044.0MB reserved, Mean: 3569.4MB allocated, 7511.0MB reserved
2025-03-25 00:39:06,288 - training - INFO - Epoch: 41/200000, Batch: 162/163, Loss: 3.7459, Throughput: 24.36 samples/sec
2025-03-25 00:39:06,289 - training - INFO - Epoch 41 completed in 107.07s. Average loss: 3.7216
2025-03-25 00:39:06,295 - training - INFO - Starting epoch 42/200000
2025-03-25 00:39:06,946 - training - INFO - GPU Memory: GPU 0: 3574.9MB/7506.0MB, GPU 1: 3585.4MB/7522.0MB, GPU 2: 3583.8MB/7514.0MB, GPU 3: 3576.1MB/7502.0MB, Total: 14320.2MB allocated, 30044.0MB reserved, Mean: 3580.0MB allocated, 7511.0MB reserved
2025-03-25 00:39:06,946 - training - INFO - Epoch: 42/200000, Batch: 0/163, Loss: 4.6236, Throughput: 24.60 samples/sec
2025-03-25 00:39:42,431 - training - INFO - GPU Memory: GPU 0: 3579.4MB/7508.0MB, GPU 1: 3588.5MB/7522.0MB, GPU 2: 3584.0MB/7506.0MB, GPU 3: 3574.7MB/7516.0MB, Total: 14326.6MB allocated, 30052.0MB reserved, Mean: 3581.6MB allocated, 7513.0MB reserved
2025-03-25 00:39:42,431 - training - INFO - Epoch: 42/200000, Batch: 54/163, Loss: 3.6686, Throughput: 24.35 samples/sec
2025-03-25 00:40:18,322 - training - INFO - GPU Memory: GPU 0: 3573.2MB/7506.0MB, GPU 1: 3580.7MB/7522.0MB, GPU 2: 3584.7MB/7514.0MB, GPU 3: 3577.1MB/7508.0MB, Total: 14315.7MB allocated, 30050.0MB reserved, Mean: 3578.9MB allocated, 7512.5MB reserved
2025-03-25 00:40:18,322 - training - INFO - Epoch: 42/200000, Batch: 108/163, Loss: 3.7122, Throughput: 24.21 samples/sec
2025-03-25 00:40:53,588 - training - INFO - GPU Memory: GPU 0: 3567.0MB/7498.0MB, GPU 1: 3571.1MB/7520.0MB, GPU 2: 3572.5MB/7516.0MB, GPU 3: 3563.6MB/7522.0MB, Total: 14274.2MB allocated, 30056.0MB reserved, Mean: 3568.5MB allocated, 7514.0MB reserved
2025-03-25 00:40:53,589 - training - INFO - Epoch: 42/200000, Batch: 162/163, Loss: 3.7198, Throughput: 24.31 samples/sec
2025-03-25 00:40:53,589 - training - INFO - Epoch 42 completed in 107.29s. Average loss: 3.7047
2025-03-25 00:40:53,600 - training - INFO - Starting epoch 43/200000
2025-03-25 00:40:54,245 - training - INFO - GPU Memory: GPU 0: 3578.6MB/7498.0MB, GPU 1: 3581.8MB/7520.0MB, GPU 2: 3582.1MB/7516.0MB, GPU 3: 3573.8MB/7522.0MB, Total: 14316.4MB allocated, 30056.0MB reserved, Mean: 3579.1MB allocated, 7514.0MB reserved
2025-03-25 00:40:54,245 - training - INFO - Epoch: 43/200000, Batch: 0/163, Loss: 3.7446, Throughput: 24.82 samples/sec
2025-03-25 00:41:29,683 - training - INFO - GPU Memory: GPU 0: 3578.5MB/7510.0MB, GPU 1: 3580.5MB/7510.0MB, GPU 2: 3585.5MB/7504.0MB, GPU 3: 3579.4MB/7504.0MB, Total: 14323.9MB allocated, 30028.0MB reserved, Mean: 3581.0MB allocated, 7507.0MB reserved
2025-03-25 00:41:29,684 - training - INFO - Epoch: 43/200000, Batch: 54/163, Loss: 3.6331, Throughput: 24.39 samples/sec
2025-03-25 00:42:05,360 - training - INFO - GPU Memory: GPU 0: 3579.5MB/7514.0MB, GPU 1: 3585.0MB/7518.0MB, GPU 2: 3580.8MB/7506.0MB, GPU 3: 3581.0MB/7514.0MB, Total: 14326.2MB allocated, 30052.0MB reserved, Mean: 3581.6MB allocated, 7513.0MB reserved
2025-03-25 00:42:05,360 - training - INFO - Epoch: 43/200000, Batch: 108/163, Loss: 3.6721, Throughput: 24.30 samples/sec
2025-03-25 00:42:40,470 - training - INFO - GPU Memory: GPU 0: 3562.9MB/7508.0MB, GPU 1: 3572.8MB/7526.0MB, GPU 2: 3570.8MB/7506.0MB, GPU 3: 3567.6MB/7508.0MB, Total: 14274.1MB allocated, 30048.0MB reserved, Mean: 3568.5MB allocated, 7512.0MB reserved
2025-03-25 00:42:40,470 - training - INFO - Epoch: 43/200000, Batch: 162/163, Loss: 3.7153, Throughput: 24.40 samples/sec
2025-03-25 00:42:40,471 - training - INFO - Epoch 43 completed in 106.87s. Average loss: 3.6905
2025-03-25 00:42:40,482 - training - INFO - Starting epoch 44/200000
2025-03-25 00:42:41,158 - training - INFO - GPU Memory: GPU 0: 3573.1MB/7508.0MB, GPU 1: 3585.0MB/7526.0MB, GPU 2: 3580.6MB/7506.0MB, GPU 3: 3580.4MB/7508.0MB, Total: 14319.1MB allocated, 30048.0MB reserved, Mean: 3579.8MB allocated, 7512.0MB reserved
2025-03-25 00:42:41,158 - training - INFO - Epoch: 44/200000, Batch: 0/163, Loss: 4.7015, Throughput: 23.67 samples/sec
2025-03-25 00:43:16,943 - training - INFO - GPU Memory: GPU 0: 3580.2MB/7512.0MB, GPU 1: 3584.0MB/7524.0MB, GPU 2: 3583.7MB/7516.0MB, GPU 3: 3575.2MB/7506.0MB, Total: 14323.2MB allocated, 30058.0MB reserved, Mean: 3580.8MB allocated, 7514.5MB reserved
2025-03-25 00:43:16,944 - training - INFO - Epoch: 44/200000, Batch: 54/163, Loss: 3.7071, Throughput: 24.14 samples/sec
2025-03-25 00:43:52,782 - training - INFO - GPU Memory: GPU 0: 3582.3MB/7510.0MB, GPU 1: 3576.5MB/7514.0MB, GPU 2: 3578.3MB/7520.0MB, GPU 3: 3576.8MB/7512.0MB, Total: 14313.9MB allocated, 30056.0MB reserved, Mean: 3578.5MB allocated, 7514.0MB reserved
2025-03-25 00:43:52,782 - training - INFO - Epoch: 44/200000, Batch: 108/163, Loss: 3.6639, Throughput: 24.12 samples/sec
2025-03-25 00:44:28,011 - training - INFO - GPU Memory: GPU 0: 3570.8MB/7518.0MB, GPU 1: 3567.7MB/7516.0MB, GPU 2: 3569.6MB/7500.0MB, GPU 3: 3567.0MB/7512.0MB, Total: 14275.1MB allocated, 30046.0MB reserved, Mean: 3568.8MB allocated, 7511.5MB reserved
2025-03-25 00:44:28,012 - training - INFO - Epoch: 44/200000, Batch: 162/163, Loss: 3.7157, Throughput: 24.25 samples/sec
2025-03-25 00:44:28,012 - training - INFO - Epoch 44 completed in 107.53s. Average loss: 3.6769
2025-03-25 00:44:28,018 - training - INFO - Starting epoch 45/200000
2025-03-25 00:44:28,680 - training - INFO - GPU Memory: GPU 0: 3575.2MB/7518.0MB, GPU 1: 3578.4MB/7516.0MB, GPU 2: 3585.7MB/7500.0MB, GPU 3: 3578.8MB/7512.0MB, Total: 14318.2MB allocated, 30046.0MB reserved, Mean: 3579.5MB allocated, 7511.5MB reserved
2025-03-25 00:44:28,680 - training - INFO - Epoch: 45/200000, Batch: 0/163, Loss: 3.9124, Throughput: 24.18 samples/sec
2025-03-25 00:45:04,204 - training - INFO - GPU Memory: GPU 0: 3577.4MB/7516.0MB, GPU 1: 3584.5MB/7524.0MB, GPU 2: 3578.9MB/7522.0MB, GPU 3: 3578.4MB/7504.0MB, Total: 14319.2MB allocated, 30066.0MB reserved, Mean: 3579.8MB allocated, 7516.5MB reserved
2025-03-25 00:45:04,204 - training - INFO - Epoch: 45/200000, Batch: 54/163, Loss: 3.6844, Throughput: 24.32 samples/sec
2025-03-25 00:45:40,134 - training - INFO - GPU Memory: GPU 0: 3574.7MB/7510.0MB, GPU 1: 3585.8MB/7522.0MB, GPU 2: 3581.3MB/7506.0MB, GPU 3: 3577.1MB/7504.0MB, Total: 14318.9MB allocated, 30042.0MB reserved, Mean: 3579.7MB allocated, 7510.5MB reserved
2025-03-25 00:45:40,134 - training - INFO - Epoch: 45/200000, Batch: 108/163, Loss: 3.7224, Throughput: 24.18 samples/sec
2025-03-25 00:46:15,831 - training - INFO - GPU Memory: GPU 0: 3569.3MB/7506.0MB, GPU 1: 3569.6MB/7522.0MB, GPU 2: 3569.5MB/7518.0MB, GPU 3: 3568.2MB/7504.0MB, Total: 14276.5MB allocated, 30050.0MB reserved, Mean: 3569.1MB allocated, 7512.5MB reserved
2025-03-25 00:46:15,831 - training - INFO - Epoch: 45/200000, Batch: 162/163, Loss: 3.7130, Throughput: 24.19 samples/sec
2025-03-25 00:46:15,832 - training - INFO - Epoch 45 completed in 107.81s. Average loss: 3.6577
2025-03-25 00:46:15,843 - training - INFO - Starting epoch 46/200000
2025-03-25 00:46:16,461 - training - INFO - GPU Memory: GPU 0: 3580.0MB/7506.0MB, GPU 1: 3580.5MB/7522.0MB, GPU 2: 3583.1MB/7518.0MB, GPU 3: 3579.1MB/7504.0MB, Total: 14322.7MB allocated, 30050.0MB reserved, Mean: 3580.7MB allocated, 7512.5MB reserved
2025-03-25 00:46:16,461 - training - INFO - Epoch: 46/200000, Batch: 0/163, Loss: 3.6538, Throughput: 25.91 samples/sec
2025-03-25 00:46:52,092 - training - INFO - GPU Memory: GPU 0: 3581.1MB/7504.0MB, GPU 1: 3577.1MB/7514.0MB, GPU 2: 3583.4MB/7506.0MB, GPU 3: 3574.0MB/7510.0MB, Total: 14315.6MB allocated, 30034.0MB reserved, Mean: 3578.9MB allocated, 7508.5MB reserved
2025-03-25 00:46:52,093 - training - INFO - Epoch: 46/200000, Batch: 54/163, Loss: 3.6804, Throughput: 24.28 samples/sec
2025-03-25 00:47:27,899 - training - INFO - GPU Memory: GPU 0: 3581.0MB/7520.0MB, GPU 1: 3580.8MB/7508.0MB, GPU 2: 3585.6MB/7516.0MB, GPU 3: 3576.2MB/7516.0MB, Total: 14323.6MB allocated, 30060.0MB reserved, Mean: 3580.9MB allocated, 7515.0MB reserved
2025-03-25 00:47:27,899 - training - INFO - Epoch: 46/200000, Batch: 108/163, Loss: 3.6975, Throughput: 24.20 samples/sec
2025-03-25 00:48:03,186 - training - INFO - GPU Memory: GPU 0: 3566.6MB/7516.0MB, GPU 1: 3576.1MB/7512.0MB, GPU 2: 3574.1MB/7508.0MB, GPU 3: 3568.3MB/7514.0MB, Total: 14285.2MB allocated, 30050.0MB reserved, Mean: 3571.3MB allocated, 7512.5MB reserved
2025-03-25 00:48:03,186 - training - INFO - Epoch: 46/200000, Batch: 162/163, Loss: 3.6861, Throughput: 24.30 samples/sec
2025-03-25 00:48:03,187 - training - INFO - Epoch 46 completed in 107.34s. Average loss: 3.6547
2025-03-25 00:48:03,199 - training - INFO - Starting epoch 47/200000
2025-03-25 00:48:03,837 - training - INFO - GPU Memory: GPU 0: 3577.1MB/7516.0MB, GPU 1: 3587.9MB/7512.0MB, GPU 2: 3580.2MB/7508.0MB, GPU 3: 3578.9MB/7514.0MB, Total: 14324.1MB allocated, 30050.0MB reserved, Mean: 3581.0MB allocated, 7512.5MB reserved
2025-03-25 00:48:03,837 - training - INFO - Epoch: 47/200000, Batch: 0/163, Loss: 3.4146, Throughput: 25.09 samples/sec
2025-03-25 00:48:39,243 - training - INFO - GPU Memory: GPU 0: 3580.5MB/7506.0MB, GPU 1: 3585.7MB/7520.0MB, GPU 2: 3580.5MB/7514.0MB, GPU 3: 3580.6MB/7518.0MB, Total: 14327.2MB allocated, 30058.0MB reserved, Mean: 3581.8MB allocated, 7514.5MB reserved
2025-03-25 00:48:39,243 - training - INFO - Epoch: 47/200000, Batch: 54/163, Loss: 3.6099, Throughput: 24.42 samples/sec
2025-03-25 00:49:14,928 - training - INFO - GPU Memory: GPU 0: 3579.3MB/7514.0MB, GPU 1: 3581.4MB/7520.0MB, GPU 2: 3580.3MB/7506.0MB, GPU 3: 3580.2MB/7522.0MB, Total: 14321.2MB allocated, 30062.0MB reserved, Mean: 3580.3MB allocated, 7515.5MB reserved
2025-03-25 00:49:14,928 - training - INFO - Epoch: 47/200000, Batch: 108/163, Loss: 3.6131, Throughput: 24.31 samples/sec
2025-03-25 00:49:50,201 - training - INFO - GPU Memory: GPU 0: 3567.4MB/7508.0MB, GPU 1: 3579.6MB/7516.0MB, GPU 2: 3575.6MB/7508.0MB, GPU 3: 3570.1MB/7516.0MB, Total: 14292.7MB allocated, 30048.0MB reserved, Mean: 3573.2MB allocated, 7512.0MB reserved
2025-03-25 00:49:50,201 - training - INFO - Epoch: 47/200000, Batch: 162/163, Loss: 3.6367, Throughput: 24.37 samples/sec
2025-03-25 00:49:50,201 - training - INFO - Epoch 47 completed in 107.00s. Average loss: 3.6311
2025-03-25 00:49:50,207 - training - INFO - Starting epoch 48/200000
2025-03-25 00:49:50,838 - training - INFO - GPU Memory: GPU 0: 3577.6MB/7508.0MB, GPU 1: 3589.8MB/7516.0MB, GPU 2: 3584.4MB/7508.0MB, GPU 3: 3581.6MB/7516.0MB, Total: 14333.4MB allocated, 30048.0MB reserved, Mean: 3583.4MB allocated, 7512.0MB reserved
2025-03-25 00:49:50,838 - training - INFO - Epoch: 48/200000, Batch: 0/163, Loss: 4.3101, Throughput: 25.34 samples/sec
2025-03-25 00:50:26,370 - training - INFO - GPU Memory: GPU 0: 3575.9MB/7514.0MB, GPU 1: 3582.6MB/7522.0MB, GPU 2: 3584.4MB/7512.0MB, GPU 3: 3575.8MB/7514.0MB, Total: 14318.6MB allocated, 30062.0MB reserved, Mean: 3579.7MB allocated, 7515.5MB reserved
2025-03-25 00:50:26,370 - training - INFO - Epoch: 48/200000, Batch: 54/163, Loss: 3.6493, Throughput: 24.33 samples/sec
2025-03-25 00:51:02,107 - training - INFO - GPU Memory: GPU 0: 3576.5MB/7502.0MB, GPU 1: 3581.7MB/7512.0MB, GPU 2: 3587.9MB/7512.0MB, GPU 3: 3574.8MB/7516.0MB, Total: 14320.9MB allocated, 30042.0MB reserved, Mean: 3580.2MB allocated, 7510.5MB reserved
2025-03-25 00:51:02,107 - training - INFO - Epoch: 48/200000, Batch: 108/163, Loss: 3.6177, Throughput: 24.26 samples/sec
2025-03-25 00:51:37,186 - training - INFO - GPU Memory: GPU 0: 3571.9MB/7512.0MB, GPU 1: 3574.6MB/7520.0MB, GPU 2: 3564.9MB/7520.0MB, GPU 3: 3565.8MB/7504.0MB, Total: 14277.1MB allocated, 30056.0MB reserved, Mean: 3569.3MB allocated, 7514.0MB reserved
2025-03-25 00:51:37,187 - training - INFO - Epoch: 48/200000, Batch: 162/163, Loss: 3.5833, Throughput: 24.38 samples/sec
2025-03-25 00:51:37,187 - training - INFO - Epoch 48 completed in 106.98s. Average loss: 3.6187
2025-03-25 00:51:37,192 - training - INFO - Starting epoch 49/200000
2025-03-25 00:51:37,831 - training - INFO - GPU Memory: GPU 0: 3580.1MB/7512.0MB, GPU 1: 3585.2MB/7520.0MB, GPU 2: 3578.3MB/7520.0MB, GPU 3: 3574.2MB/7504.0MB, Total: 14317.9MB allocated, 30056.0MB reserved, Mean: 3579.5MB allocated, 7514.0MB reserved
2025-03-25 00:51:37,831 - training - INFO - Epoch: 49/200000, Batch: 0/163, Loss: 3.4465, Throughput: 25.04 samples/sec
2025-03-25 00:52:13,132 - training - INFO - GPU Memory: GPU 0: 3579.2MB/7506.0MB, GPU 1: 3580.4MB/7524.0MB, GPU 2: 3575.4MB/7514.0MB, GPU 3: 3585.0MB/7516.0MB, Total: 14320.0MB allocated, 30060.0MB reserved, Mean: 3580.0MB allocated, 7515.0MB reserved
2025-03-25 00:52:13,132 - training - INFO - Epoch: 49/200000, Batch: 54/163, Loss: 3.6140, Throughput: 24.49 samples/sec
2025-03-25 00:52:48,863 - training - INFO - GPU Memory: GPU 0: 3576.5MB/7506.0MB, GPU 1: 3582.5MB/7508.0MB, GPU 2: 3580.9MB/7504.0MB, GPU 3: 3576.5MB/7522.0MB, Total: 14316.4MB allocated, 30040.0MB reserved, Mean: 3579.1MB allocated, 7510.0MB reserved
2025-03-25 00:52:48,863 - training - INFO - Epoch: 49/200000, Batch: 108/163, Loss: 3.6553, Throughput: 24.33 samples/sec
2025-03-25 00:53:23,992 - training - INFO - GPU Memory: GPU 0: 3569.2MB/7506.0MB, GPU 1: 3571.3MB/7524.0MB, GPU 2: 3567.3MB/7502.0MB, GPU 3: 3571.9MB/7514.0MB, Total: 14279.7MB allocated, 30046.0MB reserved, Mean: 3569.9MB allocated, 7511.5MB reserved
2025-03-25 00:53:23,993 - training - INFO - Epoch: 49/200000, Batch: 162/163, Loss: 3.6385, Throughput: 24.42 samples/sec
2025-03-25 00:53:23,993 - training - INFO - Epoch 49 completed in 106.80s. Average loss: 3.6111
2025-03-25 00:53:24,002 - training - INFO - Starting epoch 50/200000
2025-03-25 00:53:24,649 - training - INFO - GPU Memory: GPU 0: 3579.9MB/7506.0MB, GPU 1: 3578.6MB/7524.0MB, GPU 2: 3582.0MB/7502.0MB, GPU 3: 3582.3MB/7514.0MB, Total: 14322.9MB allocated, 30046.0MB reserved, Mean: 3580.7MB allocated, 7511.5MB reserved
2025-03-25 00:53:24,649 - training - INFO - Epoch: 50/200000, Batch: 0/163, Loss: 3.4097, Throughput: 24.72 samples/sec
2025-03-25 00:54:00,065 - training - INFO - GPU Memory: GPU 0: 3577.4MB/7516.0MB, GPU 1: 3584.9MB/7518.0MB, GPU 2: 3577.5MB/7502.0MB, GPU 3: 3574.6MB/7508.0MB, Total: 14314.3MB allocated, 30044.0MB reserved, Mean: 3578.6MB allocated, 7511.0MB reserved
2025-03-25 00:54:00,066 - training - INFO - Epoch: 50/200000, Batch: 54/163, Loss: 3.5992, Throughput: 24.40 samples/sec
2025-03-25 00:54:35,778 - training - INFO - GPU Memory: GPU 0: 3576.7MB/7506.0MB, GPU 1: 3574.1MB/7528.0MB, GPU 2: 3586.4MB/7504.0MB, GPU 3: 3581.2MB/7510.0MB, Total: 14318.5MB allocated, 30048.0MB reserved, Mean: 3579.6MB allocated, 7512.0MB reserved
2025-03-25 00:54:35,778 - training - INFO - Epoch: 50/200000, Batch: 108/163, Loss: 3.5999, Throughput: 24.30 samples/sec
2025-03-25 00:55:11,028 - training - INFO - GPU Memory: GPU 0: 3564.0MB/7508.0MB, GPU 1: 3576.5MB/7516.0MB, GPU 2: 3567.5MB/7504.0MB, GPU 3: 3571.3MB/7526.0MB, Total: 14279.3MB allocated, 30054.0MB reserved, Mean: 3569.8MB allocated, 7513.5MB reserved
2025-03-25 00:55:11,028 - training - INFO - Epoch: 50/200000, Batch: 162/163, Loss: 3.5828, Throughput: 24.37 samples/sec
2025-03-25 00:55:11,029 - training - INFO - Epoch 50 completed in 107.03s. Average loss: 3.6056
2025-03-25 00:55:11,034 - training - INFO - Starting epoch 51/200000
2025-03-25 00:55:11,686 - training - INFO - GPU Memory: GPU 0: 3575.7MB/7508.0MB, GPU 1: 3587.2MB/7516.0MB, GPU 2: 3582.2MB/7504.0MB, GPU 3: 3577.6MB/7526.0MB, Total: 14322.7MB allocated, 30054.0MB reserved, Mean: 3580.7MB allocated, 7513.5MB reserved
2025-03-25 00:55:11,687 - training - INFO - Epoch: 51/200000, Batch: 0/163, Loss: 3.2372, Throughput: 24.52 samples/sec
2025-03-25 00:55:47,081 - training - INFO - GPU Memory: GPU 0: 3575.8MB/7508.0MB, GPU 1: 3579.2MB/7516.0MB, GPU 2: 3584.6MB/7508.0MB, GPU 3: 3575.9MB/7520.0MB, Total: 14315.4MB allocated, 30052.0MB reserved, Mean: 3578.9MB allocated, 7513.0MB reserved
2025-03-25 00:55:47,082 - training - INFO - Epoch: 51/200000, Batch: 54/163, Loss: 3.5995, Throughput: 24.41 samples/sec
2025-03-25 00:56:22,735 - training - INFO - GPU Memory: GPU 0: 3574.8MB/7512.0MB, GPU 1: 3578.5MB/7514.0MB, GPU 2: 3577.2MB/7510.0MB, GPU 3: 3577.0MB/7506.0MB, Total: 14307.6MB allocated, 30042.0MB reserved, Mean: 3576.9MB allocated, 7510.5MB reserved
2025-03-25 00:56:22,736 - training - INFO - Epoch: 51/200000, Batch: 108/163, Loss: 3.5506, Throughput: 24.32 samples/sec
2025-03-25 00:56:57,866 - training - INFO - GPU Memory: GPU 0: 3573.3MB/7504.0MB, GPU 1: 3570.1MB/7522.0MB, GPU 2: 3570.2MB/7504.0MB, GPU 3: 3574.8MB/7508.0MB, Total: 14288.3MB allocated, 30038.0MB reserved, Mean: 3572.1MB allocated, 7509.5MB reserved
2025-03-25 00:56:57,866 - training - INFO - Epoch: 51/200000, Batch: 162/163, Loss: 3.5780, Throughput: 24.41 samples/sec
2025-03-25 00:56:57,867 - training - INFO - Epoch 51 completed in 106.83s. Average loss: 3.5718
2025-03-25 00:56:57,876 - training - INFO - Starting epoch 52/200000
2025-03-25 00:56:58,503 - training - INFO - GPU Memory: GPU 0: 3584.0MB/7504.0MB, GPU 1: 3582.0MB/7522.0MB, GPU 2: 3577.9MB/7504.0MB, GPU 3: 3585.5MB/7508.0MB, Total: 14329.3MB allocated, 30038.0MB reserved, Mean: 3582.3MB allocated, 7509.5MB reserved
2025-03-25 00:56:58,503 - training - INFO - Epoch: 52/200000, Batch: 0/163, Loss: 3.7610, Throughput: 25.53 samples/sec
2025-03-25 00:57:33,902 - training - INFO - GPU Memory: GPU 0: 3577.4MB/7514.0MB, GPU 1: 3588.0MB/7524.0MB, GPU 2: 3582.8MB/7510.0MB, GPU 3: 3573.9MB/7504.0MB, Total: 14322.0MB allocated, 30052.0MB reserved, Mean: 3580.5MB allocated, 7513.0MB reserved
2025-03-25 00:57:33,902 - training - INFO - Epoch: 52/200000, Batch: 54/163, Loss: 3.4836, Throughput: 24.43 samples/sec
2025-03-25 00:58:09,611 - training - INFO - GPU Memory: GPU 0: 3581.2MB/7514.0MB, GPU 1: 3582.2MB/7528.0MB, GPU 2: 3582.2MB/7506.0MB, GPU 3: 3578.8MB/7510.0MB, Total: 14324.4MB allocated, 30058.0MB reserved, Mean: 3581.1MB allocated, 7514.5MB reserved
2025-03-25 00:58:09,611 - training - INFO - Epoch: 52/200000, Batch: 108/163, Loss: 3.5234, Throughput: 24.31 samples/sec
2025-03-25 00:58:44,872 - training - INFO - GPU Memory: GPU 0: 3564.3MB/7506.0MB, GPU 1: 3571.5MB/7522.0MB, GPU 2: 3568.6MB/7506.0MB, GPU 3: 3572.7MB/7516.0MB, Total: 14277.1MB allocated, 30050.0MB reserved, Mean: 3569.3MB allocated, 7512.5MB reserved
2025-03-25 00:58:44,872 - training - INFO - Epoch: 52/200000, Batch: 162/163, Loss: 3.5341, Throughput: 24.37 samples/sec
2025-03-25 00:58:44,873 - training - INFO - Epoch 52 completed in 107.00s. Average loss: 3.5603
2025-03-25 00:58:44,878 - training - INFO - Starting epoch 53/200000
2025-03-25 00:58:45,527 - training - INFO - GPU Memory: GPU 0: 3576.6MB/7506.0MB, GPU 1: 3582.2MB/7522.0MB, GPU 2: 3578.5MB/7506.0MB, GPU 3: 3582.9MB/7516.0MB, Total: 14320.2MB allocated, 30050.0MB reserved, Mean: 3580.0MB allocated, 7512.5MB reserved
2025-03-25 00:58:45,528 - training - INFO - Epoch: 53/200000, Batch: 0/163, Loss: 2.3680, Throughput: 24.64 samples/sec
2025-03-25 00:59:21,034 - training - INFO - GPU Memory: GPU 0: 3578.0MB/7508.0MB, GPU 1: 3581.2MB/7526.0MB, GPU 2: 3586.7MB/7506.0MB, GPU 3: 3577.0MB/7514.0MB, Total: 14322.8MB allocated, 30054.0MB reserved, Mean: 3580.7MB allocated, 7513.5MB reserved
2025-03-25 00:59:21,034 - training - INFO - Epoch: 53/200000, Batch: 54/163, Loss: 3.5615, Throughput: 24.34 samples/sec
2025-03-25 00:59:56,778 - training - INFO - GPU Memory: GPU 0: 3575.1MB/7510.0MB, GPU 1: 3582.9MB/7522.0MB, GPU 2: 3586.3MB/7510.0MB, GPU 3: 3575.7MB/7514.0MB, Total: 14320.0MB allocated, 30056.0MB reserved, Mean: 3580.0MB allocated, 7514.0MB reserved
2025-03-25 00:59:56,778 - training - INFO - Epoch: 53/200000, Batch: 108/163, Loss: 3.5332, Throughput: 24.26 samples/sec
2025-03-25 01:00:32,297 - training - INFO - GPU Memory: GPU 0: 3564.6MB/7500.0MB, GPU 1: 3571.2MB/7522.0MB, GPU 2: 3572.2MB/7510.0MB, GPU 3: 3568.3MB/7502.0MB, Total: 14276.3MB allocated, 30034.0MB reserved, Mean: 3569.1MB allocated, 7508.5MB reserved
2025-03-25 01:00:32,297 - training - INFO - Epoch: 53/200000, Batch: 162/163, Loss: 3.5349, Throughput: 24.28 samples/sec
2025-03-25 01:00:32,297 - training - INFO - Epoch 53 completed in 107.42s. Average loss: 3.5378
2025-03-25 01:00:32,304 - training - INFO - Starting epoch 54/200000
2025-03-25 01:00:32,965 - training - INFO - GPU Memory: GPU 0: 3575.3MB/7500.0MB, GPU 1: 3581.9MB/7522.0MB, GPU 2: 3586.1MB/7510.0MB, GPU 3: 3579.0MB/7502.0MB, Total: 14322.3MB allocated, 30034.0MB reserved, Mean: 3580.6MB allocated, 7508.5MB reserved
2025-03-25 01:00:32,965 - training - INFO - Epoch: 54/200000, Batch: 0/163, Loss: 3.9061, Throughput: 24.21 samples/sec
2025-03-25 01:01:08,746 - training - INFO - GPU Memory: GPU 0: 3575.6MB/7508.0MB, GPU 1: 3581.2MB/7518.0MB, GPU 2: 3587.7MB/7512.0MB, GPU 3: 3577.7MB/7516.0MB, Total: 14322.2MB allocated, 30054.0MB reserved, Mean: 3580.5MB allocated, 7513.5MB reserved
2025-03-25 01:01:08,746 - training - INFO - Epoch: 54/200000, Batch: 54/163, Loss: 3.4999, Throughput: 24.15 samples/sec
2025-03-25 01:01:44,829 - training - INFO - GPU Memory: GPU 0: 3579.5MB/7502.0MB, GPU 1: 3582.7MB/7524.0MB, GPU 2: 3585.7MB/7504.0MB, GPU 3: 3579.1MB/7506.0MB, Total: 14327.0MB allocated, 30036.0MB reserved, Mean: 3581.7MB allocated, 7509.0MB reserved
2025-03-25 01:01:44,829 - training - INFO - Epoch: 54/200000, Batch: 108/163, Loss: 3.5077, Throughput: 24.05 samples/sec
2025-03-25 01:02:20,093 - training - INFO - GPU Memory: GPU 0: 3563.2MB/7512.0MB, GPU 1: 3572.1MB/7516.0MB, GPU 2: 3566.3MB/7514.0MB, GPU 3: 3571.4MB/7512.0MB, Total: 14273.0MB allocated, 30054.0MB reserved, Mean: 3568.3MB allocated, 7513.5MB reserved
2025-03-25 01:02:20,093 - training - INFO - Epoch: 54/200000, Batch: 162/163, Loss: 3.5051, Throughput: 24.20 samples/sec
2025-03-25 01:02:20,094 - training - INFO - Epoch 54 completed in 107.79s. Average loss: 3.5303
2025-03-25 01:02:20,104 - training - INFO - Starting epoch 55/200000
2025-03-25 01:02:20,726 - training - INFO - GPU Memory: GPU 0: 3573.1MB/7512.0MB, GPU 1: 3582.7MB/7516.0MB, GPU 2: 3581.3MB/7514.0MB, GPU 3: 3575.8MB/7512.0MB, Total: 14313.0MB allocated, 30054.0MB reserved, Mean: 3578.3MB allocated, 7513.5MB reserved
2025-03-25 01:02:20,726 - training - INFO - Epoch: 55/200000, Batch: 0/163, Loss: 3.3982, Throughput: 25.70 samples/sec
2025-03-25 01:02:56,293 - training - INFO - GPU Memory: GPU 0: 3577.0MB/7520.0MB, GPU 1: 3575.5MB/7508.0MB, GPU 2: 3583.7MB/7520.0MB, GPU 3: 3578.2MB/7512.0MB, Total: 14314.4MB allocated, 30060.0MB reserved, Mean: 3578.6MB allocated, 7515.0MB reserved
2025-03-25 01:02:56,293 - training - INFO - Epoch: 55/200000, Batch: 54/163, Loss: 3.4960, Throughput: 24.32 samples/sec
2025-03-25 01:03:32,134 - training - INFO - GPU Memory: GPU 0: 3580.9MB/7514.0MB, GPU 1: 3580.7MB/7514.0MB, GPU 2: 3584.0MB/7514.0MB, GPU 3: 3578.0MB/7512.0MB, Total: 14323.6MB allocated, 30054.0MB reserved, Mean: 3580.9MB allocated, 7513.5MB reserved
2025-03-25 01:03:32,134 - training - INFO - Epoch: 55/200000, Batch: 108/163, Loss: 3.5192, Throughput: 24.21 samples/sec
2025-03-25 01:04:07,220 - training - INFO - GPU Memory: GPU 0: 3569.3MB/7516.0MB, GPU 1: 3565.0MB/7516.0MB, GPU 2: 3570.6MB/7514.0MB, GPU 3: 3563.8MB/7506.0MB, Total: 14268.7MB allocated, 30052.0MB reserved, Mean: 3567.2MB allocated, 7513.0MB reserved
2025-03-25 01:04:07,220 - training - INFO - Epoch: 55/200000, Batch: 162/163, Loss: 3.5395, Throughput: 24.35 samples/sec
2025-03-25 01:04:07,221 - training - INFO - Epoch 55 completed in 107.12s. Average loss: 3.5278
2025-03-25 01:04:07,226 - training - INFO - Starting epoch 56/200000
2025-03-25 01:04:07,864 - training - INFO - GPU Memory: GPU 0: 3580.0MB/7516.0MB, GPU 1: 3580.7MB/7516.0MB, GPU 2: 3579.5MB/7514.0MB, GPU 3: 3578.1MB/7506.0MB, Total: 14318.3MB allocated, 30052.0MB reserved, Mean: 3579.6MB allocated, 7513.0MB reserved
2025-03-25 01:04:07,864 - training - INFO - Epoch: 56/200000, Batch: 0/163, Loss: 3.0024, Throughput: 25.09 samples/sec
2025-03-25 01:04:43,198 - training - INFO - GPU Memory: GPU 0: 3579.2MB/7508.0MB, GPU 1: 3586.1MB/7520.0MB, GPU 2: 3582.2MB/7512.0MB, GPU 3: 3579.3MB/7506.0MB, Total: 14326.8MB allocated, 30046.0MB reserved, Mean: 3581.7MB allocated, 7511.5MB reserved
2025-03-25 01:04:43,199 - training - INFO - Epoch: 56/200000, Batch: 54/163, Loss: 3.3988, Throughput: 24.46 samples/sec
2025-03-25 01:05:18,878 - training - INFO - GPU Memory: GPU 0: 3578.7MB/7520.0MB, GPU 1: 3584.6MB/7524.0MB, GPU 2: 3587.9MB/7512.0MB, GPU 3: 3577.0MB/7514.0MB, Total: 14328.2MB allocated, 30070.0MB reserved, Mean: 3582.0MB allocated, 7517.5MB reserved
2025-03-25 01:05:18,878 - training - INFO - Epoch: 56/200000, Batch: 108/163, Loss: 3.4425, Throughput: 24.34 samples/sec
2025-03-25 01:05:54,183 - training - INFO - GPU Memory: GPU 0: 3565.1MB/7510.0MB, GPU 1: 3567.1MB/7514.0MB, GPU 2: 3573.5MB/7518.0MB, GPU 3: 3566.6MB/7508.0MB, Total: 14272.3MB allocated, 30050.0MB reserved, Mean: 3568.1MB allocated, 7512.5MB reserved
2025-03-25 01:05:54,184 - training - INFO - Epoch: 56/200000, Batch: 162/163, Loss: 3.4729, Throughput: 24.38 samples/sec
2025-03-25 01:05:54,184 - training - INFO - Epoch 56 completed in 106.96s. Average loss: 3.5066
2025-03-25 01:05:54,196 - training - INFO - Starting epoch 57/200000
2025-03-25 01:05:54,846 - training - INFO - GPU Memory: GPU 0: 3578.2MB/7510.0MB, GPU 1: 3577.7MB/7514.0MB, GPU 2: 3583.7MB/7518.0MB, GPU 3: 3576.3MB/7508.0MB, Total: 14316.0MB allocated, 30050.0MB reserved, Mean: 3579.0MB allocated, 7512.5MB reserved
2025-03-25 01:05:54,847 - training - INFO - Epoch: 57/200000, Batch: 0/163, Loss: 3.7464, Throughput: 24.59 samples/sec
2025-03-25 01:06:30,390 - training - INFO - GPU Memory: GPU 0: 3580.0MB/7510.0MB, GPU 1: 3578.6MB/7510.0MB, GPU 2: 3581.3MB/7518.0MB, GPU 3: 3575.7MB/7518.0MB, Total: 14315.6MB allocated, 30056.0MB reserved, Mean: 3578.9MB allocated, 7514.0MB reserved
2025-03-25 01:06:30,390 - training - INFO - Epoch: 57/200000, Batch: 54/163, Loss: 3.5048, Throughput: 24.31 samples/sec
2025-03-25 01:07:06,256 - training - INFO - GPU Memory: GPU 0: 3573.2MB/7516.0MB, GPU 1: 3579.3MB/7514.0MB, GPU 2: 3580.4MB/7504.0MB, GPU 3: 3579.2MB/7510.0MB, Total: 14312.2MB allocated, 30044.0MB reserved, Mean: 3578.0MB allocated, 7511.0MB reserved
2025-03-25 01:07:06,256 - training - INFO - Epoch: 57/200000, Batch: 108/163, Loss: 3.5180, Throughput: 24.20 samples/sec
2025-03-25 01:07:41,443 - training - INFO - GPU Memory: GPU 0: 3564.6MB/7506.0MB, GPU 1: 3570.1MB/7512.0MB, GPU 2: 3568.8MB/7508.0MB, GPU 3: 3570.1MB/7514.0MB, Total: 14273.6MB allocated, 30040.0MB reserved, Mean: 3568.4MB allocated, 7510.0MB reserved
2025-03-25 01:07:41,443 - training - INFO - Epoch: 57/200000, Batch: 162/163, Loss: 3.5118, Throughput: 24.32 samples/sec
2025-03-25 01:07:41,444 - training - INFO - Epoch 57 completed in 107.25s. Average loss: 3.4904
2025-03-25 01:07:41,450 - training - INFO - Starting epoch 58/200000
2025-03-25 01:07:42,088 - training - INFO - GPU Memory: GPU 0: 3577.0MB/7506.0MB, GPU 1: 3580.8MB/7512.0MB, GPU 2: 3575.6MB/7508.0MB, GPU 3: 3580.8MB/7514.0MB, Total: 14314.2MB allocated, 30040.0MB reserved, Mean: 3578.5MB allocated, 7510.0MB reserved
2025-03-25 01:07:42,089 - training - INFO - Epoch: 58/200000, Batch: 0/163, Loss: 3.5314, Throughput: 25.08 samples/sec
2025-03-25 01:08:17,525 - training - INFO - GPU Memory: GPU 0: 3582.6MB/7508.0MB, GPU 1: 3582.1MB/7522.0MB, GPU 2: 3579.4MB/7514.0MB, GPU 3: 3578.4MB/7510.0MB, Total: 14322.4MB allocated, 30054.0MB reserved, Mean: 3580.6MB allocated, 7513.5MB reserved
2025-03-25 01:08:17,526 - training - INFO - Epoch: 58/200000, Batch: 54/163, Loss: 3.5258, Throughput: 24.39 samples/sec
2025-03-25 01:08:53,644 - training - INFO - GPU Memory: GPU 0: 3578.2MB/7514.0MB, GPU 1: 3581.6MB/7518.0MB, GPU 2: 3582.7MB/7504.0MB, GPU 3: 3576.5MB/7516.0MB, Total: 14318.9MB allocated, 30052.0MB reserved, Mean: 3579.7MB allocated, 7513.0MB reserved
2025-03-25 01:08:53,644 - training - INFO - Epoch: 58/200000, Batch: 108/163, Loss: 3.5015, Throughput: 24.16 samples/sec
2025-03-25 01:09:29,354 - training - INFO - GPU Memory: GPU 0: 3565.3MB/7520.0MB, GPU 1: 3569.6MB/7526.0MB, GPU 2: 3579.0MB/7510.0MB, GPU 3: 3568.1MB/7510.0MB, Total: 14281.9MB allocated, 30066.0MB reserved, Mean: 3570.5MB allocated, 7516.5MB reserved
2025-03-25 01:09:29,354 - training - INFO - Epoch: 58/200000, Batch: 162/163, Loss: 3.4973, Throughput: 24.17 samples/sec
2025-03-25 01:09:29,354 - training - INFO - Epoch 58 completed in 107.90s. Average loss: 3.4710
2025-03-25 01:09:29,363 - training - INFO - Starting epoch 59/200000
2025-03-25 01:09:30,002 - training - INFO - GPU Memory: GPU 0: 3577.2MB/7520.0MB, GPU 1: 3580.2MB/7526.0MB, GPU 2: 3589.9MB/7510.0MB, GPU 3: 3577.1MB/7510.0MB, Total: 14324.4MB allocated, 30066.0MB reserved, Mean: 3581.1MB allocated, 7516.5MB reserved
2025-03-25 01:09:30,002 - training - INFO - Epoch: 59/200000, Batch: 0/163, Loss: 3.0491, Throughput: 25.06 samples/sec
2025-03-25 01:10:05,331 - training - INFO - GPU Memory: GPU 0: 3573.0MB/7510.0MB, GPU 1: 3580.9MB/7512.0MB, GPU 2: 3584.7MB/7514.0MB, GPU 3: 3578.6MB/7512.0MB, Total: 14317.1MB allocated, 30048.0MB reserved, Mean: 3579.3MB allocated, 7512.0MB reserved
2025-03-25 01:10:05,331 - training - INFO - Epoch: 59/200000, Batch: 54/163, Loss: 3.3677, Throughput: 24.47 samples/sec
2025-03-25 01:10:41,051 - training - INFO - GPU Memory: GPU 0: 3576.1MB/7506.0MB, GPU 1: 3578.7MB/7520.0MB, GPU 2: 3581.8MB/7510.0MB, GPU 3: 3574.6MB/7512.0MB, Total: 14311.2MB allocated, 30048.0MB reserved, Mean: 3577.8MB allocated, 7512.0MB reserved
2025-03-25 01:10:41,051 - training - INFO - Epoch: 59/200000, Batch: 108/163, Loss: 3.3704, Throughput: 24.33 samples/sec
2025-03-25 01:11:16,253 - training - INFO - GPU Memory: GPU 0: 3565.7MB/7506.0MB, GPU 1: 3579.9MB/7524.0MB, GPU 2: 3566.0MB/7510.0MB, GPU 3: 3564.0MB/7510.0MB, Total: 14275.6MB allocated, 30050.0MB reserved, Mean: 3568.9MB allocated, 7512.5MB reserved
2025-03-25 01:11:16,253 - training - INFO - Epoch: 59/200000, Batch: 162/163, Loss: 3.3923, Throughput: 24.40 samples/sec
2025-03-25 01:11:16,254 - training - INFO - Epoch 59 completed in 106.89s. Average loss: 3.4682
2025-03-25 01:11:16,261 - training - INFO - Starting epoch 60/200000
2025-03-25 01:11:16,912 - training - INFO - GPU Memory: GPU 0: 3575.2MB/7506.0MB, GPU 1: 3590.6MB/7524.0MB, GPU 2: 3585.4MB/7510.0MB, GPU 3: 3580.7MB/7510.0MB, Total: 14331.9MB allocated, 30050.0MB reserved, Mean: 3583.0MB allocated, 7512.5MB reserved
2025-03-25 01:11:16,912 - training - INFO - Epoch: 60/200000, Batch: 0/163, Loss: 2.9989, Throughput: 24.59 samples/sec
2025-03-25 01:11:52,485 - training - INFO - GPU Memory: GPU 0: 3585.7MB/7506.0MB, GPU 1: 3581.2MB/7514.0MB, GPU 2: 3584.9MB/7514.0MB, GPU 3: 3578.2MB/7520.0MB, Total: 14330.0MB allocated, 30054.0MB reserved, Mean: 3582.5MB allocated, 7513.5MB reserved
2025-03-25 01:11:52,485 - training - INFO - Epoch: 60/200000, Batch: 54/163, Loss: 3.3952, Throughput: 24.29 samples/sec
2025-03-25 01:12:28,243 - training - INFO - GPU Memory: GPU 0: 3575.1MB/7518.0MB, GPU 1: 3581.5MB/7512.0MB, GPU 2: 3580.5MB/7506.0MB, GPU 3: 3578.3MB/7506.0MB, Total: 14315.5MB allocated, 30042.0MB reserved, Mean: 3578.9MB allocated, 7510.5MB reserved
2025-03-25 01:12:28,243 - training - INFO - Epoch: 60/200000, Batch: 108/163, Loss: 3.4027, Throughput: 24.23 samples/sec
2025-03-25 01:13:03,491 - training - INFO - GPU Memory: GPU 0: 3568.5MB/7510.0MB, GPU 1: 3567.9MB/7516.0MB, GPU 2: 3571.6MB/7512.0MB, GPU 3: 3563.0MB/7508.0MB, Total: 14270.9MB allocated, 30046.0MB reserved, Mean: 3567.7MB allocated, 7511.5MB reserved
2025-03-25 01:13:03,491 - training - INFO - Epoch: 60/200000, Batch: 162/163, Loss: 3.3993, Throughput: 24.32 samples/sec
2025-03-25 01:13:03,492 - training - INFO - Epoch 60 completed in 107.23s. Average loss: 3.4406
2025-03-25 01:13:03,500 - training - INFO - Starting epoch 61/200000
2025-03-25 01:13:04,124 - training - INFO - GPU Memory: GPU 0: 3579.4MB/7510.0MB, GPU 1: 3578.7MB/7516.0MB, GPU 2: 3581.8MB/7512.0MB, GPU 3: 3575.6MB/7508.0MB, Total: 14315.5MB allocated, 30046.0MB reserved, Mean: 3578.9MB allocated, 7511.5MB reserved
2025-03-25 01:13:04,125 - training - INFO - Epoch: 61/200000, Batch: 0/163, Loss: 3.9448, Throughput: 25.62 samples/sec
2025-03-25 01:13:39,667 - training - INFO - GPU Memory: GPU 0: 3578.9MB/7508.0MB, GPU 1: 3579.6MB/7526.0MB, GPU 2: 3578.4MB/7506.0MB, GPU 3: 3581.4MB/7516.0MB, Total: 14318.3MB allocated, 30056.0MB reserved, Mean: 3579.6MB allocated, 7514.0MB reserved
2025-03-25 01:13:39,667 - training - INFO - Epoch: 61/200000, Batch: 54/163, Loss: 3.5738, Throughput: 24.33 samples/sec
2025-03-25 01:14:15,428 - training - INFO - GPU Memory: GPU 0: 3575.3MB/7514.0MB, GPU 1: 3584.9MB/7518.0MB, GPU 2: 3581.1MB/7500.0MB, GPU 3: 3576.9MB/7520.0MB, Total: 14318.3MB allocated, 30052.0MB reserved, Mean: 3579.6MB allocated, 7513.0MB reserved
2025-03-25 01:14:15,428 - training - INFO - Epoch: 61/200000, Batch: 108/163, Loss: 3.4892, Throughput: 24.25 samples/sec
2025-03-25 01:14:50,706 - training - INFO - GPU Memory: GPU 0: 3568.2MB/7512.0MB, GPU 1: 3568.4MB/7522.0MB, GPU 2: 3568.8MB/7500.0MB, GPU 3: 3566.9MB/7512.0MB, Total: 14272.3MB allocated, 30046.0MB reserved, Mean: 3568.1MB allocated, 7511.5MB reserved
2025-03-25 01:14:50,706 - training - INFO - Epoch: 61/200000, Batch: 162/163, Loss: 3.5042, Throughput: 24.33 samples/sec
2025-03-25 01:14:50,707 - training - INFO - Epoch 61 completed in 107.21s. Average loss: 3.4448
2025-03-25 01:14:50,711 - training - INFO - Starting epoch 62/200000
2025-03-25 01:14:51,358 - training - INFO - GPU Memory: GPU 0: 3578.6MB/7512.0MB, GPU 1: 3579.1MB/7522.0MB, GPU 2: 3579.9MB/7500.0MB, GPU 3: 3581.2MB/7512.0MB, Total: 14318.9MB allocated, 30046.0MB reserved, Mean: 3579.7MB allocated, 7511.5MB reserved
2025-03-25 01:14:51,359 - training - INFO - Epoch: 62/200000, Batch: 0/163, Loss: 2.6292, Throughput: 24.74 samples/sec
2025-03-25 01:15:26,905 - training - INFO - GPU Memory: GPU 0: 3576.1MB/7496.0MB, GPU 1: 3576.1MB/7520.0MB, GPU 2: 3579.5MB/7518.0MB, GPU 3: 3580.2MB/7516.0MB, Total: 14311.9MB allocated, 30050.0MB reserved, Mean: 3578.0MB allocated, 7512.5MB reserved
2025-03-25 01:15:26,905 - training - INFO - Epoch: 62/200000, Batch: 54/163, Loss: 3.3275, Throughput: 24.31 samples/sec
2025-03-25 01:16:02,728 - training - INFO - GPU Memory: GPU 0: 3578.8MB/7506.0MB, GPU 1: 3583.3MB/7514.0MB, GPU 2: 3580.9MB/7502.0MB, GPU 3: 3576.0MB/7518.0MB, Total: 14318.9MB allocated, 30040.0MB reserved, Mean: 3579.7MB allocated, 7510.0MB reserved
2025-03-25 01:16:02,728 - training - INFO - Epoch: 62/200000, Batch: 108/163, Loss: 3.3259, Throughput: 24.22 samples/sec
2025-03-25 01:16:37,928 - training - INFO - GPU Memory: GPU 0: 3573.6MB/7506.0MB, GPU 1: 3571.8MB/7526.0MB, GPU 2: 3574.0MB/7510.0MB, GPU 3: 3565.6MB/7506.0MB, Total: 14285.0MB allocated, 30048.0MB reserved, Mean: 3571.2MB allocated, 7512.0MB reserved
2025-03-25 01:16:37,929 - training - INFO - Epoch: 62/200000, Batch: 162/163, Loss: 3.3410, Throughput: 24.32 samples/sec
2025-03-25 01:16:37,929 - training - INFO - Epoch 62 completed in 107.22s. Average loss: 3.4249
2025-03-25 01:16:37,941 - training - INFO - Starting epoch 63/200000
2025-03-25 01:16:38,590 - training - INFO - GPU Memory: GPU 0: 3584.5MB/7506.0MB, GPU 1: 3582.5MB/7526.0MB, GPU 2: 3587.7MB/7510.0MB, GPU 3: 3577.8MB/7506.0MB, Total: 14332.4MB allocated, 30048.0MB reserved, Mean: 3583.1MB allocated, 7512.0MB reserved
2025-03-25 01:16:38,590 - training - INFO - Epoch: 63/200000, Batch: 0/163, Loss: 2.5767, Throughput: 24.67 samples/sec
2025-03-25 01:17:13,927 - training - INFO - GPU Memory: GPU 0: 3580.1MB/7516.0MB, GPU 1: 3577.8MB/7514.0MB, GPU 2: 3584.7MB/7504.0MB, GPU 3: 3580.6MB/7514.0MB, Total: 14323.2MB allocated, 30048.0MB reserved, Mean: 3580.8MB allocated, 7512.0MB reserved
2025-03-25 01:17:13,927 - training - INFO - Epoch: 63/200000, Batch: 54/163, Loss: 3.3104, Throughput: 24.45 samples/sec
2025-03-25 01:17:49,633 - training - INFO - GPU Memory: GPU 0: 3579.1MB/7516.0MB, GPU 1: 3579.5MB/7510.0MB, GPU 2: 3582.5MB/7514.0MB, GPU 3: 3576.4MB/7512.0MB, Total: 14317.5MB allocated, 30052.0MB reserved, Mean: 3579.4MB allocated, 7513.0MB reserved
2025-03-25 01:17:49,634 - training - INFO - Epoch: 63/200000, Batch: 108/163, Loss: 3.4320, Throughput: 24.33 samples/sec
2025-03-25 01:18:24,913 - training - INFO - GPU Memory: GPU 0: 3566.2MB/7512.0MB, GPU 1: 3570.3MB/7510.0MB, GPU 2: 3575.4MB/7506.0MB, GPU 3: 3567.9MB/7516.0MB, Total: 14279.8MB allocated, 30044.0MB reserved, Mean: 3569.9MB allocated, 7511.0MB reserved
2025-03-25 01:18:24,913 - training - INFO - Epoch: 63/200000, Batch: 162/163, Loss: 3.4309, Throughput: 24.38 samples/sec
2025-03-25 01:18:24,914 - training - INFO - Epoch 63 completed in 106.97s. Average loss: 3.4140
2025-03-25 01:18:24,919 - training - INFO - Starting epoch 64/200000
2025-03-25 01:18:25,584 - training - INFO - GPU Memory: GPU 0: 3576.9MB/7512.0MB, GPU 1: 3578.2MB/7510.0MB, GPU 2: 3584.9MB/7506.0MB, GPU 3: 3579.7MB/7516.0MB, Total: 14319.6MB allocated, 30044.0MB reserved, Mean: 3579.9MB allocated, 7511.0MB reserved
2025-03-25 01:18:25,585 - training - INFO - Epoch: 64/200000, Batch: 0/163, Loss: 3.6795, Throughput: 24.06 samples/sec
2025-03-25 01:19:00,966 - training - INFO - GPU Memory: GPU 0: 3580.6MB/7508.0MB, GPU 1: 3576.9MB/7508.0MB, GPU 2: 3578.7MB/7512.0MB, GPU 3: 3573.4MB/7518.0MB, Total: 14309.6MB allocated, 30046.0MB reserved, Mean: 3577.4MB allocated, 7511.5MB reserved
2025-03-25 01:19:00,966 - training - INFO - Epoch: 64/200000, Batch: 54/163, Loss: 3.4233, Throughput: 24.41 samples/sec
2025-03-25 01:19:36,841 - training - INFO - GPU Memory: GPU 0: 3579.2MB/7516.0MB, GPU 1: 3584.2MB/7512.0MB, GPU 2: 3582.5MB/7508.0MB, GPU 3: 3572.1MB/7502.0MB, Total: 14318.1MB allocated, 30038.0MB reserved, Mean: 3579.5MB allocated, 7509.5MB reserved
2025-03-25 01:19:36,842 - training - INFO - Epoch: 64/200000, Batch: 108/163, Loss: 3.4238, Throughput: 24.25 samples/sec
2025-03-25 01:20:12,235 - training - INFO - GPU Memory: GPU 0: 3569.0MB/7514.0MB, GPU 1: 3569.5MB/7518.0MB, GPU 2: 3573.2MB/7514.0MB, GPU 3: 3570.5MB/7506.0MB, Total: 14282.2MB allocated, 30052.0MB reserved, Mean: 3570.6MB allocated, 7513.0MB reserved
2025-03-25 01:20:12,235 - training - INFO - Epoch: 64/200000, Batch: 162/163, Loss: 3.4057, Throughput: 24.30 samples/sec
2025-03-25 01:20:12,236 - training - INFO - Epoch 64 completed in 107.32s. Average loss: 3.3960
2025-03-25 01:20:12,242 - training - INFO - Starting epoch 65/200000
2025-03-25 01:20:12,888 - training - INFO - GPU Memory: GPU 0: 3579.1MB/7514.0MB, GPU 1: 3576.0MB/7518.0MB, GPU 2: 3584.1MB/7514.0MB, GPU 3: 3579.4MB/7506.0MB, Total: 14318.7MB allocated, 30052.0MB reserved, Mean: 3579.7MB allocated, 7513.0MB reserved
2025-03-25 01:20:12,889 - training - INFO - Epoch: 65/200000, Batch: 0/163, Loss: 3.5828, Throughput: 24.77 samples/sec
2025-03-25 01:20:48,230 - training - INFO - GPU Memory: GPU 0: 3579.6MB/7514.0MB, GPU 1: 3584.9MB/7530.0MB, GPU 2: 3577.9MB/7516.0MB, GPU 3: 3575.1MB/7508.0MB, Total: 14317.5MB allocated, 30068.0MB reserved, Mean: 3579.4MB allocated, 7517.0MB reserved
2025-03-25 01:20:48,230 - training - INFO - Epoch: 65/200000, Batch: 54/163, Loss: 3.3539, Throughput: 24.45 samples/sec
2025-03-25 01:21:24,111 - training - INFO - GPU Memory: GPU 0: 3583.7MB/7516.0MB, GPU 1: 3583.2MB/7526.0MB, GPU 2: 3578.4MB/7510.0MB, GPU 3: 3578.5MB/7504.0MB, Total: 14323.7MB allocated, 30056.0MB reserved, Mean: 3580.9MB allocated, 7514.0MB reserved
2025-03-25 01:21:24,111 - training - INFO - Epoch: 65/200000, Batch: 108/163, Loss: 3.3426, Throughput: 24.27 samples/sec
2025-03-25 01:21:59,466 - training - INFO - GPU Memory: GPU 0: 3566.2MB/7522.0MB, GPU 1: 3573.9MB/7520.0MB, GPU 2: 3570.6MB/7504.0MB, GPU 3: 3567.9MB/7512.0MB, Total: 14278.7MB allocated, 30058.0MB reserved, Mean: 3569.7MB allocated, 7514.5MB reserved
2025-03-25 01:21:59,466 - training - INFO - Epoch: 65/200000, Batch: 162/163, Loss: 3.3406, Throughput: 24.32 samples/sec
2025-03-25 01:21:59,467 - training - INFO - Epoch 65 completed in 107.22s. Average loss: 3.3789
2025-03-25 01:21:59,478 - training - INFO - Starting epoch 66/200000
2025-03-25 01:22:00,099 - training - INFO - GPU Memory: GPU 0: 3576.9MB/7522.0MB, GPU 1: 3576.9MB/7520.0MB, GPU 2: 3582.4MB/7504.0MB, GPU 3: 3579.4MB/7512.0MB, Total: 14315.5MB allocated, 30058.0MB reserved, Mean: 3578.9MB allocated, 7514.5MB reserved
2025-03-25 01:22:00,099 - training - INFO - Epoch: 66/200000, Batch: 0/163, Loss: 4.0718, Throughput: 25.75 samples/sec
2025-03-25 01:22:35,669 - training - INFO - GPU Memory: GPU 0: 3577.6MB/7512.0MB, GPU 1: 3577.5MB/7520.0MB, GPU 2: 3585.8MB/7506.0MB, GPU 3: 3582.4MB/7516.0MB, Total: 14323.3MB allocated, 30054.0MB reserved, Mean: 3580.8MB allocated, 7513.5MB reserved
2025-03-25 01:22:35,669 - training - INFO - Epoch: 66/200000, Batch: 54/163, Loss: 3.4106, Throughput: 24.32 samples/sec
2025-03-25 01:23:11,508 - training - INFO - GPU Memory: GPU 0: 3577.5MB/7510.0MB, GPU 1: 3583.7MB/7514.0MB, GPU 2: 3577.5MB/7516.0MB, GPU 3: 3579.6MB/7514.0MB, Total: 14318.4MB allocated, 30054.0MB reserved, Mean: 3579.6MB allocated, 7513.5MB reserved
2025-03-25 01:23:11,508 - training - INFO - Epoch: 66/200000, Batch: 108/163, Loss: 3.3101, Throughput: 24.21 samples/sec
2025-03-25 01:23:46,716 - training - INFO - GPU Memory: GPU 0: 3565.6MB/7508.0MB, GPU 1: 3573.2MB/7524.0MB, GPU 2: 3572.3MB/7504.0MB, GPU 3: 3564.1MB/7514.0MB, Total: 14275.1MB allocated, 30050.0MB reserved, Mean: 3568.8MB allocated, 7512.5MB reserved
2025-03-25 01:23:46,717 - training - INFO - Epoch: 66/200000, Batch: 162/163, Loss: 3.3445, Throughput: 24.32 samples/sec
2025-03-25 01:23:46,717 - training - INFO - Epoch 66 completed in 107.24s. Average loss: 3.3673
2025-03-25 01:23:46,728 - training - INFO - Starting epoch 67/200000
2025-03-25 01:23:47,368 - training - INFO - GPU Memory: GPU 0: 3578.0MB/7508.0MB, GPU 1: 3582.2MB/7524.0MB, GPU 2: 3584.0MB/7504.0MB, GPU 3: 3574.7MB/7514.0MB, Total: 14318.9MB allocated, 30050.0MB reserved, Mean: 3579.7MB allocated, 7512.5MB reserved
2025-03-25 01:23:47,368 - training - INFO - Epoch: 67/200000, Batch: 0/163, Loss: 3.2394, Throughput: 25.01 samples/sec
2025-03-25 01:24:22,998 - training - INFO - GPU Memory: GPU 0: 3574.4MB/7512.0MB, GPU 1: 3582.6MB/7522.0MB, GPU 2: 3575.6MB/7522.0MB, GPU 3: 3575.6MB/7512.0MB, Total: 14308.2MB allocated, 30068.0MB reserved, Mean: 3577.0MB allocated, 7517.0MB reserved
2025-03-25 01:24:22,998 - training - INFO - Epoch: 67/200000, Batch: 54/163, Loss: 3.3086, Throughput: 24.26 samples/sec
2025-03-25 01:24:58,812 - training - INFO - GPU Memory: GPU 0: 3572.6MB/7512.0MB, GPU 1: 3581.5MB/7528.0MB, GPU 2: 3577.0MB/7512.0MB, GPU 3: 3576.5MB/7506.0MB, Total: 14307.7MB allocated, 30058.0MB reserved, Mean: 3576.9MB allocated, 7514.5MB reserved
2025-03-25 01:24:58,812 - training - INFO - Epoch: 67/200000, Batch: 108/163, Loss: 3.3156, Throughput: 24.19 samples/sec
2025-03-25 01:25:34,000 - training - INFO - GPU Memory: GPU 0: 3569.4MB/7510.0MB, GPU 1: 3572.1MB/7516.0MB, GPU 2: 3569.9MB/7506.0MB, GPU 3: 3563.7MB/7510.0MB, Total: 14275.1MB allocated, 30042.0MB reserved, Mean: 3568.8MB allocated, 7510.5MB reserved
2025-03-25 01:25:34,000 - training - INFO - Epoch: 67/200000, Batch: 162/163, Loss: 3.2697, Throughput: 24.31 samples/sec
2025-03-25 01:25:34,001 - training - INFO - Epoch 67 completed in 107.27s. Average loss: 3.3638
2025-03-25 01:25:34,014 - training - INFO - Starting epoch 68/200000
2025-03-25 01:25:34,665 - training - INFO - GPU Memory: GPU 0: 3581.4MB/7510.0MB, GPU 1: 3579.3MB/7516.0MB, GPU 2: 3586.6MB/7506.0MB, GPU 3: 3578.1MB/7510.0MB, Total: 14325.4MB allocated, 30042.0MB reserved, Mean: 3581.3MB allocated, 7510.5MB reserved
2025-03-25 01:25:34,665 - training - INFO - Epoch: 68/200000, Batch: 0/163, Loss: 3.0859, Throughput: 24.56 samples/sec
2025-03-25 01:26:10,192 - training - INFO - GPU Memory: GPU 0: 3587.0MB/7504.0MB, GPU 1: 3580.9MB/7512.0MB, GPU 2: 3587.0MB/7508.0MB, GPU 3: 3577.9MB/7510.0MB, Total: 14332.8MB allocated, 30034.0MB reserved, Mean: 3583.2MB allocated, 7508.5MB reserved
2025-03-25 01:26:10,192 - training - INFO - Epoch: 68/200000, Batch: 54/163, Loss: 3.4220, Throughput: 24.32 samples/sec
2025-03-25 01:26:45,942 - training - INFO - GPU Memory: GPU 0: 3575.6MB/7528.0MB, GPU 1: 3586.0MB/7520.0MB, GPU 2: 3582.9MB/7512.0MB, GPU 3: 3575.6MB/7508.0MB, Total: 14320.2MB allocated, 30068.0MB reserved, Mean: 3580.0MB allocated, 7517.0MB reserved
2025-03-25 01:26:45,942 - training - INFO - Epoch: 68/200000, Batch: 108/163, Loss: 3.3731, Throughput: 24.25 samples/sec
2025-03-25 01:27:21,194 - training - INFO - GPU Memory: GPU 0: 3568.3MB/7504.0MB, GPU 1: 3573.7MB/7526.0MB, GPU 2: 3574.2MB/7512.0MB, GPU 3: 3561.6MB/7516.0MB, Total: 14277.8MB allocated, 30058.0MB reserved, Mean: 3569.5MB allocated, 7514.5MB reserved
2025-03-25 01:27:21,195 - training - INFO - Epoch: 68/200000, Batch: 162/163, Loss: 3.3523, Throughput: 24.33 samples/sec
2025-03-25 01:27:21,195 - training - INFO - Epoch 68 completed in 107.18s. Average loss: 3.3518
2025-03-25 01:27:21,202 - training - INFO - Starting epoch 69/200000
2025-03-25 01:27:21,833 - training - INFO - GPU Memory: GPU 0: 3577.7MB/7504.0MB, GPU 1: 3584.6MB/7526.0MB, GPU 2: 3584.9MB/7512.0MB, GPU 3: 3573.2MB/7516.0MB, Total: 14320.5MB allocated, 30058.0MB reserved, Mean: 3580.1MB allocated, 7514.5MB reserved
2025-03-25 01:27:21,834 - training - INFO - Epoch: 69/200000, Batch: 0/163, Loss: 3.3884, Throughput: 25.34 samples/sec
2025-03-25 01:27:57,393 - training - INFO - GPU Memory: GPU 0: 3575.5MB/7508.0MB, GPU 1: 3579.3MB/7522.0MB, GPU 2: 3582.2MB/7514.0MB, GPU 3: 3571.2MB/7502.0MB, Total: 14308.3MB allocated, 30046.0MB reserved, Mean: 3577.1MB allocated, 7511.5MB reserved
2025-03-25 01:27:57,394 - training - INFO - Epoch: 69/200000, Batch: 54/163, Loss: 3.3970, Throughput: 24.32 samples/sec
2025-03-25 01:28:33,137 - training - INFO - GPU Memory: GPU 0: 3580.4MB/7512.0MB, GPU 1: 3583.3MB/7530.0MB, GPU 2: 3580.2MB/7514.0MB, GPU 3: 3572.3MB/7500.0MB, Total: 14316.2MB allocated, 30056.0MB reserved, Mean: 3579.0MB allocated, 7514.0MB reserved
2025-03-25 01:28:33,137 - training - INFO - Epoch: 69/200000, Batch: 108/163, Loss: 3.3679, Throughput: 24.24 samples/sec
2025-03-25 01:29:08,464 - training - INFO - GPU Memory: GPU 0: 3567.8MB/7514.0MB, GPU 1: 3574.1MB/7524.0MB, GPU 2: 3571.8MB/7506.0MB, GPU 3: 3562.9MB/7510.0MB, Total: 14276.6MB allocated, 30054.0MB reserved, Mean: 3569.2MB allocated, 7513.5MB reserved
2025-03-25 01:29:08,465 - training - INFO - Epoch: 69/200000, Batch: 162/163, Loss: 3.3724, Throughput: 24.31 samples/sec
2025-03-25 01:29:08,465 - training - INFO - Epoch 69 completed in 107.26s. Average loss: 3.3370
2025-03-25 01:29:08,471 - training - INFO - Starting epoch 70/200000
2025-03-25 01:29:09,110 - training - INFO - GPU Memory: GPU 0: 3580.2MB/7514.0MB, GPU 1: 3584.8MB/7524.0MB, GPU 2: 3582.0MB/7506.0MB, GPU 3: 3574.6MB/7510.0MB, Total: 14321.6MB allocated, 30054.0MB reserved, Mean: 3580.4MB allocated, 7513.5MB reserved
2025-03-25 01:29:09,111 - training - INFO - Epoch: 70/200000, Batch: 0/163, Loss: 3.3241, Throughput: 25.04 samples/sec
2025-03-25 01:29:44,610 - training - INFO - GPU Memory: GPU 0: 3577.4MB/7502.0MB, GPU 1: 3578.6MB/7516.0MB, GPU 2: 3590.0MB/7522.0MB, GPU 3: 3578.0MB/7510.0MB, Total: 14324.0MB allocated, 30050.0MB reserved, Mean: 3581.0MB allocated, 7512.5MB reserved
2025-03-25 01:29:44,610 - training - INFO - Epoch: 70/200000, Batch: 54/163, Loss: 3.2574, Throughput: 24.35 samples/sec
2025-03-25 01:30:20,421 - training - INFO - GPU Memory: GPU 0: 3575.6MB/7506.0MB, GPU 1: 3584.6MB/7522.0MB, GPU 2: 3577.5MB/7522.0MB, GPU 3: 3581.6MB/7512.0MB, Total: 14319.3MB allocated, 30062.0MB reserved, Mean: 3579.8MB allocated, 7515.5MB reserved
2025-03-25 01:30:20,422 - training - INFO - Epoch: 70/200000, Batch: 108/163, Loss: 3.2524, Throughput: 24.24 samples/sec
2025-03-25 01:30:55,702 - training - INFO - GPU Memory: GPU 0: 3565.7MB/7504.0MB, GPU 1: 3567.2MB/7530.0MB, GPU 2: 3574.8MB/7504.0MB, GPU 3: 3569.9MB/7516.0MB, Total: 14277.6MB allocated, 30054.0MB reserved, Mean: 3569.4MB allocated, 7513.5MB reserved
2025-03-25 01:30:55,702 - training - INFO - Epoch: 70/200000, Batch: 162/163, Loss: 3.2926, Throughput: 24.32 samples/sec
2025-03-25 01:30:55,703 - training - INFO - Epoch 70 completed in 107.23s. Average loss: 3.3161
2025-03-25 01:30:55,713 - training - INFO - Starting epoch 71/200000
2025-03-25 01:30:56,360 - training - INFO - GPU Memory: GPU 0: 3577.4MB/7504.0MB, GPU 1: 3581.4MB/7530.0MB, GPU 2: 3586.1MB/7504.0MB, GPU 3: 3577.6MB/7516.0MB, Total: 14322.5MB allocated, 30054.0MB reserved, Mean: 3580.6MB allocated, 7513.5MB reserved
2025-03-25 01:30:56,361 - training - INFO - Epoch: 71/200000, Batch: 0/163, Loss: 2.8713, Throughput: 24.73 samples/sec
2025-03-25 01:31:32,148 - training - INFO - GPU Memory: GPU 0: 3572.8MB/7506.0MB, GPU 1: 3584.3MB/7520.0MB, GPU 2: 3579.5MB/7522.0MB, GPU 3: 3576.3MB/7510.0MB, Total: 14312.9MB allocated, 30058.0MB reserved, Mean: 3578.2MB allocated, 7514.5MB reserved
2025-03-25 01:31:32,148 - training - INFO - Epoch: 71/200000, Batch: 54/163, Loss: 3.2338, Throughput: 24.15 samples/sec
2025-03-25 01:32:08,234 - training - INFO - GPU Memory: GPU 0: 3576.3MB/7502.0MB, GPU 1: 3585.0MB/7524.0MB, GPU 2: 3585.2MB/7504.0MB, GPU 3: 3578.2MB/7516.0MB, Total: 14324.8MB allocated, 30046.0MB reserved, Mean: 3581.2MB allocated, 7511.5MB reserved
2025-03-25 01:32:08,234 - training - INFO - Epoch: 71/200000, Batch: 108/163, Loss: 3.2601, Throughput: 24.05 samples/sec
2025-03-25 01:32:43,808 - training - INFO - GPU Memory: GPU 0: 3559.7MB/7516.0MB, GPU 1: 3569.5MB/7512.0MB, GPU 2: 3571.6MB/7522.0MB, GPU 3: 3567.3MB/7506.0MB, Total: 14268.2MB allocated, 30056.0MB reserved, Mean: 3567.0MB allocated, 7514.0MB reserved
2025-03-25 01:32:43,808 - training - INFO - Epoch: 71/200000, Batch: 162/163, Loss: 3.2949, Throughput: 24.13 samples/sec
2025-03-25 01:32:43,809 - training - INFO - Epoch 71 completed in 108.10s. Average loss: 3.3094
2025-03-25 01:32:43,817 - training - INFO - Starting epoch 72/200000
2025-03-25 01:32:44,463 - training - INFO - GPU Memory: GPU 0: 3569.9MB/7516.0MB, GPU 1: 3581.4MB/7512.0MB, GPU 2: 3582.5MB/7522.0MB, GPU 3: 3573.7MB/7506.0MB, Total: 14307.6MB allocated, 30056.0MB reserved, Mean: 3576.9MB allocated, 7514.0MB reserved
2025-03-25 01:32:44,463 - training - INFO - Epoch: 72/200000, Batch: 0/163, Loss: 2.9848, Throughput: 24.80 samples/sec
2025-03-25 01:33:19,867 - training - INFO - GPU Memory: GPU 0: 3577.7MB/7506.0MB, GPU 1: 3583.7MB/7524.0MB, GPU 2: 3576.4MB/7512.0MB, GPU 3: 3580.6MB/7518.0MB, Total: 14318.4MB allocated, 30060.0MB reserved, Mean: 3579.6MB allocated, 7515.0MB reserved
2025-03-25 01:33:19,867 - training - INFO - Epoch: 72/200000, Batch: 54/163, Loss: 3.2840, Throughput: 24.41 samples/sec
2025-03-25 01:33:55,692 - training - INFO - GPU Memory: GPU 0: 3575.5MB/7510.0MB, GPU 1: 3582.9MB/7530.0MB, GPU 2: 3581.6MB/7508.0MB, GPU 3: 3581.2MB/7520.0MB, Total: 14321.3MB allocated, 30068.0MB reserved, Mean: 3580.3MB allocated, 7517.0MB reserved
2025-03-25 01:33:55,693 - training - INFO - Epoch: 72/200000, Batch: 108/163, Loss: 3.3246, Throughput: 24.26 samples/sec
2025-03-25 01:34:30,851 - training - INFO - GPU Memory: GPU 0: 3575.6MB/7510.0MB, GPU 1: 3575.0MB/7516.0MB, GPU 2: 3573.9MB/7506.0MB, GPU 3: 3567.7MB/7512.0MB, Total: 14292.2MB allocated, 30044.0MB reserved, Mean: 3573.1MB allocated, 7511.0MB reserved
2025-03-25 01:34:30,852 - training - INFO - Epoch: 72/200000, Batch: 162/163, Loss: 3.2902, Throughput: 24.37 samples/sec
2025-03-25 01:34:30,852 - training - INFO - Epoch 72 completed in 107.03s. Average loss: 3.2971
2025-03-25 01:34:30,860 - training - INFO - Starting epoch 73/200000
2025-03-25 01:34:31,510 - training - INFO - GPU Memory: GPU 0: 3586.2MB/7510.0MB, GPU 1: 3585.2MB/7516.0MB, GPU 2: 3586.0MB/7506.0MB, GPU 3: 3579.3MB/7512.0MB, Total: 14336.7MB allocated, 30044.0MB reserved, Mean: 3584.2MB allocated, 7511.0MB reserved
2025-03-25 01:34:31,510 - training - INFO - Epoch: 73/200000, Batch: 0/163, Loss: 3.5298, Throughput: 24.63 samples/sec
2025-03-25 01:35:06,991 - training - INFO - GPU Memory: GPU 0: 3580.5MB/7512.0MB, GPU 1: 3576.9MB/7520.0MB, GPU 2: 3583.1MB/7514.0MB, GPU 3: 3577.2MB/7512.0MB, Total: 14317.6MB allocated, 30058.0MB reserved, Mean: 3579.4MB allocated, 7514.5MB reserved
2025-03-25 01:35:06,992 - training - INFO - Epoch: 73/200000, Batch: 54/163, Loss: 3.3011, Throughput: 24.36 samples/sec
2025-03-25 01:35:42,686 - training - INFO - GPU Memory: GPU 0: 3576.5MB/7508.0MB, GPU 1: 3584.7MB/7512.0MB, GPU 2: 3582.2MB/7500.0MB, GPU 3: 3579.4MB/7508.0MB, Total: 14322.8MB allocated, 30028.0MB reserved, Mean: 3580.7MB allocated, 7507.0MB reserved
2025-03-25 01:35:42,686 - training - INFO - Epoch: 73/200000, Batch: 108/163, Loss: 3.2531, Throughput: 24.28 samples/sec
2025-03-25 01:36:17,930 - training - INFO - GPU Memory: GPU 0: 3567.6MB/7512.0MB, GPU 1: 3571.8MB/7524.0MB, GPU 2: 3569.3MB/7512.0MB, GPU 3: 3567.6MB/7518.0MB, Total: 14276.3MB allocated, 30066.0MB reserved, Mean: 3569.1MB allocated, 7516.5MB reserved
2025-03-25 01:36:17,930 - training - INFO - Epoch: 73/200000, Batch: 162/163, Loss: 3.2383, Throughput: 24.36 samples/sec
2025-03-25 01:36:17,931 - training - INFO - Epoch 73 completed in 107.07s. Average loss: 3.2864
2025-03-25 01:36:17,939 - training - INFO - Starting epoch 74/200000
2025-03-25 01:36:18,566 - training - INFO - GPU Memory: GPU 0: 3577.7MB/7512.0MB, GPU 1: 3582.5MB/7524.0MB, GPU 2: 3585.2MB/7512.0MB, GPU 3: 3575.6MB/7518.0MB, Total: 14321.0MB allocated, 30066.0MB reserved, Mean: 3580.2MB allocated, 7516.5MB reserved
2025-03-25 01:36:18,567 - training - INFO - Epoch: 74/200000, Batch: 0/163, Loss: 3.5038, Throughput: 25.51 samples/sec
2025-03-25 01:36:53,937 - training - INFO - GPU Memory: GPU 0: 3578.4MB/7508.0MB, GPU 1: 3578.8MB/7528.0MB, GPU 2: 3591.4MB/7512.0MB, GPU 3: 3570.1MB/7512.0MB, Total: 14318.7MB allocated, 30060.0MB reserved, Mean: 3579.7MB allocated, 7515.0MB reserved
2025-03-25 01:36:53,938 - training - INFO - Epoch: 74/200000, Batch: 54/163, Loss: 3.2650, Throughput: 24.45 samples/sec
2025-03-25 01:37:29,763 - training - INFO - GPU Memory: GPU 0: 3581.7MB/7510.0MB, GPU 1: 3577.1MB/7516.0MB, GPU 2: 3582.5MB/7526.0MB, GPU 3: 3576.7MB/7506.0MB, Total: 14318.0MB allocated, 30058.0MB reserved, Mean: 3579.5MB allocated, 7514.5MB reserved
2025-03-25 01:37:29,764 - training - INFO - Epoch: 74/200000, Batch: 108/163, Loss: 3.2999, Throughput: 24.28 samples/sec
2025-03-25 01:38:05,090 - training - INFO - GPU Memory: GPU 0: 3567.1MB/7510.0MB, GPU 1: 3569.4MB/7510.0MB, GPU 2: 3570.0MB/7510.0MB, GPU 3: 3571.3MB/7506.0MB, Total: 14277.8MB allocated, 30036.0MB reserved, Mean: 3569.4MB allocated, 7509.0MB reserved
2025-03-25 01:38:05,090 - training - INFO - Epoch: 74/200000, Batch: 162/163, Loss: 3.2991, Throughput: 24.34 samples/sec
2025-03-25 01:38:05,091 - training - INFO - Epoch 74 completed in 107.15s. Average loss: 3.2698
2025-03-25 01:38:05,102 - training - INFO - Starting epoch 75/200000
2025-03-25 01:38:05,720 - training - INFO - GPU Memory: GPU 0: 3578.0MB/7510.0MB, GPU 1: 3577.8MB/7510.0MB, GPU 2: 3579.9MB/7510.0MB, GPU 3: 3582.0MB/7506.0MB, Total: 14317.7MB allocated, 30036.0MB reserved, Mean: 3579.4MB allocated, 7509.0MB reserved
2025-03-25 01:38:05,721 - training - INFO - Epoch: 75/200000, Batch: 0/163, Loss: 3.9057, Throughput: 25.86 samples/sec
2025-03-25 01:38:41,503 - training - INFO - GPU Memory: GPU 0: 3579.4MB/7506.0MB, GPU 1: 3585.0MB/7524.0MB, GPU 2: 3581.4MB/7504.0MB, GPU 3: 3579.6MB/7520.0MB, Total: 14325.4MB allocated, 30054.0MB reserved, Mean: 3581.3MB allocated, 7513.5MB reserved
2025-03-25 01:38:41,504 - training - INFO - Epoch: 75/200000, Batch: 54/163, Loss: 3.3912, Throughput: 24.17 samples/sec
2025-03-25 01:39:17,754 - training - INFO - GPU Memory: GPU 0: 3578.2MB/7510.0MB, GPU 1: 3579.0MB/7520.0MB, GPU 2: 3579.5MB/7508.0MB, GPU 3: 3573.7MB/7516.0MB, Total: 14310.4MB allocated, 30054.0MB reserved, Mean: 3577.6MB allocated, 7513.5MB reserved
2025-03-25 01:39:17,754 - training - INFO - Epoch: 75/200000, Batch: 108/163, Loss: 3.3073, Throughput: 24.00 samples/sec
2025-03-25 01:39:53,440 - training - INFO - GPU Memory: GPU 0: 3567.3MB/7518.0MB, GPU 1: 3569.1MB/7524.0MB, GPU 2: 3568.1MB/7500.0MB, GPU 3: 3569.8MB/7500.0MB, Total: 14274.3MB allocated, 30042.0MB reserved, Mean: 3568.6MB allocated, 7510.5MB reserved
2025-03-25 01:39:53,440 - training - INFO - Epoch: 75/200000, Batch: 162/163, Loss: 3.2644, Throughput: 24.07 samples/sec
2025-03-25 01:39:53,441 - training - INFO - Epoch 75 completed in 108.34s. Average loss: 3.2672
2025-03-25 01:39:53,449 - training - INFO - Starting epoch 76/200000
2025-03-25 01:39:54,096 - training - INFO - GPU Memory: GPU 0: 3578.0MB/7518.0MB, GPU 1: 3579.8MB/7524.0MB, GPU 2: 3580.0MB/7500.0MB, GPU 3: 3580.5MB/7500.0MB, Total: 14318.2MB allocated, 30042.0MB reserved, Mean: 3579.6MB allocated, 7510.5MB reserved
2025-03-25 01:39:54,096 - training - INFO - Epoch: 76/200000, Batch: 0/163, Loss: 3.3392, Throughput: 24.70 samples/sec
2025-03-25 01:40:29,548 - training - INFO - GPU Memory: GPU 0: 3577.0MB/7510.0MB, GPU 1: 3583.2MB/7526.0MB, GPU 2: 3580.4MB/7512.0MB, GPU 3: 3570.3MB/7516.0MB, Total: 14310.9MB allocated, 30064.0MB reserved, Mean: 3577.7MB allocated, 7516.0MB reserved
2025-03-25 01:40:29,549 - training - INFO - Epoch: 76/200000, Batch: 54/163, Loss: 3.2018, Throughput: 24.38 samples/sec
2025-03-25 01:41:05,232 - training - INFO - GPU Memory: GPU 0: 3577.6MB/7510.0MB, GPU 1: 3580.7MB/7524.0MB, GPU 2: 3580.7MB/7510.0MB, GPU 3: 3580.7MB/7500.0MB, Total: 14319.8MB allocated, 30044.0MB reserved, Mean: 3579.9MB allocated, 7511.0MB reserved
2025-03-25 01:41:05,232 - training - INFO - Epoch: 76/200000, Batch: 108/163, Loss: 3.2231, Throughput: 24.30 samples/sec
2025-03-25 01:41:40,476 - training - INFO - GPU Memory: GPU 0: 3566.8MB/7514.0MB, GPU 1: 3564.1MB/7516.0MB, GPU 2: 3567.2MB/7500.0MB, GPU 3: 3563.1MB/7516.0MB, Total: 14261.2MB allocated, 30046.0MB reserved, Mean: 3565.3MB allocated, 7511.5MB reserved
2025-03-25 01:41:40,476 - training - INFO - Epoch: 76/200000, Batch: 162/163, Loss: 3.2522, Throughput: 24.37 samples/sec
2025-03-25 01:41:40,476 - training - INFO - Epoch 76 completed in 107.03s. Average loss: 3.2536
2025-03-25 01:41:40,481 - training - INFO - Starting epoch 77/200000
2025-03-25 01:41:41,125 - training - INFO - GPU Memory: GPU 0: 3577.6MB/7514.0MB, GPU 1: 3574.8MB/7516.0MB, GPU 2: 3581.2MB/7500.0MB, GPU 3: 3578.7MB/7516.0MB, Total: 14312.3MB allocated, 30046.0MB reserved, Mean: 3578.1MB allocated, 7511.5MB reserved
2025-03-25 01:41:41,125 - training - INFO - Epoch: 77/200000, Batch: 0/163, Loss: 2.6445, Throughput: 24.85 samples/sec
2025-03-25 01:42:16,691 - training - INFO - GPU Memory: GPU 0: 3580.7MB/7514.0MB, GPU 1: 3586.2MB/7526.0MB, GPU 2: 3590.5MB/7512.0MB, GPU 3: 3582.6MB/7526.0MB, Total: 14340.0MB allocated, 30078.0MB reserved, Mean: 3585.0MB allocated, 7519.5MB reserved
2025-03-25 01:42:16,691 - training - INFO - Epoch: 77/200000, Batch: 54/163, Loss: 3.1982, Throughput: 24.30 samples/sec
2025-03-25 01:42:52,421 - training - INFO - GPU Memory: GPU 0: 3581.0MB/7520.0MB, GPU 1: 3581.5MB/7522.0MB, GPU 2: 3576.8MB/7520.0MB, GPU 3: 3575.1MB/7518.0MB, Total: 14314.3MB allocated, 30080.0MB reserved, Mean: 3578.6MB allocated, 7520.0MB reserved
2025-03-25 01:42:52,422 - training - INFO - Epoch: 77/200000, Batch: 108/163, Loss: 3.2760, Throughput: 24.24 samples/sec
2025-03-25 01:43:27,662 - training - INFO - GPU Memory: GPU 0: 3568.2MB/7516.0MB, GPU 1: 3566.0MB/7518.0MB, GPU 2: 3572.0MB/7510.0MB, GPU 3: 3565.7MB/7506.0MB, Total: 14271.8MB allocated, 30050.0MB reserved, Mean: 3568.0MB allocated, 7512.5MB reserved
2025-03-25 01:43:27,662 - training - INFO - Epoch: 77/200000, Batch: 162/163, Loss: 3.2871, Throughput: 24.33 samples/sec
2025-03-25 01:43:27,663 - training - INFO - Epoch 77 completed in 107.18s. Average loss: 3.2408
2025-03-25 01:43:27,667 - training - INFO - Starting epoch 78/200000
2025-03-25 01:43:28,312 - training - INFO - GPU Memory: GPU 0: 3580.4MB/7516.0MB, GPU 1: 3576.7MB/7518.0MB, GPU 2: 3585.1MB/7510.0MB, GPU 3: 3568.5MB/7506.0MB, Total: 14310.7MB allocated, 30050.0MB reserved, Mean: 3577.7MB allocated, 7512.5MB reserved
2025-03-25 01:43:28,312 - training - INFO - Epoch: 78/200000, Batch: 0/163, Loss: 2.3695, Throughput: 24.80 samples/sec
2025-03-25 01:44:03,832 - training - INFO - GPU Memory: GPU 0: 3579.1MB/7520.0MB, GPU 1: 3581.3MB/7518.0MB, GPU 2: 3582.3MB/7506.0MB, GPU 3: 3582.2MB/7508.0MB, Total: 14324.9MB allocated, 30052.0MB reserved, Mean: 3581.2MB allocated, 7513.0MB reserved
2025-03-25 01:44:03,832 - training - INFO - Epoch: 78/200000, Batch: 54/163, Loss: 3.2546, Throughput: 24.33 samples/sec
2025-03-25 01:44:39,741 - training - INFO - GPU Memory: GPU 0: 3577.9MB/7516.0MB, GPU 1: 3578.5MB/7520.0MB, GPU 2: 3585.8MB/7508.0MB, GPU 3: 3574.0MB/7504.0MB, Total: 14316.2MB allocated, 30048.0MB reserved, Mean: 3579.1MB allocated, 7512.0MB reserved
2025-03-25 01:44:39,741 - training - INFO - Epoch: 78/200000, Batch: 108/163, Loss: 3.1805, Throughput: 24.20 samples/sec
2025-03-25 01:45:15,005 - training - INFO - GPU Memory: GPU 0: 3568.4MB/7522.0MB, GPU 1: 3569.1MB/7516.0MB, GPU 2: 3567.5MB/7506.0MB, GPU 3: 3565.4MB/7502.0MB, Total: 14270.5MB allocated, 30046.0MB reserved, Mean: 3567.6MB allocated, 7511.5MB reserved
2025-03-25 01:45:15,005 - training - INFO - Epoch: 78/200000, Batch: 162/163, Loss: 3.2298, Throughput: 24.30 samples/sec
2025-03-25 01:45:15,005 - training - INFO - Epoch 78 completed in 107.34s. Average loss: 3.2241
2025-03-25 01:45:15,010 - training - INFO - Starting epoch 79/200000
2025-03-25 01:45:15,637 - training - INFO - GPU Memory: GPU 0: 3579.0MB/7522.0MB, GPU 1: 3579.7MB/7516.0MB, GPU 2: 3577.7MB/7506.0MB, GPU 3: 3576.3MB/7502.0MB, Total: 14312.8MB allocated, 30046.0MB reserved, Mean: 3578.2MB allocated, 7511.5MB reserved
2025-03-25 01:45:15,637 - training - INFO - Epoch: 79/200000, Batch: 0/163, Loss: 3.7214, Throughput: 25.52 samples/sec
2025-03-25 01:45:51,280 - training - INFO - GPU Memory: GPU 0: 3581.2MB/7516.0MB, GPU 1: 3583.7MB/7522.0MB, GPU 2: 3584.4MB/7506.0MB, GPU 3: 3575.5MB/7510.0MB, Total: 14324.8MB allocated, 30054.0MB reserved, Mean: 3581.2MB allocated, 7513.5MB reserved
2025-03-25 01:45:51,281 - training - INFO - Epoch: 79/200000, Batch: 54/163, Loss: 3.2694, Throughput: 24.26 samples/sec
2025-03-25 01:46:27,062 - training - INFO - GPU Memory: GPU 0: 3577.5MB/7520.0MB, GPU 1: 3586.2MB/7528.0MB, GPU 2: 3579.4MB/7508.0MB, GPU 3: 3576.3MB/7510.0MB, Total: 14319.4MB allocated, 30066.0MB reserved, Mean: 3579.9MB allocated, 7516.5MB reserved
2025-03-25 01:46:27,062 - training - INFO - Epoch: 79/200000, Batch: 108/163, Loss: 3.2533, Throughput: 24.20 samples/sec
2025-03-25 01:47:02,313 - training - INFO - GPU Memory: GPU 0: 3566.3MB/7510.0MB, GPU 1: 3568.4MB/7518.0MB, GPU 2: 3571.3MB/7498.0MB, GPU 3: 3561.9MB/7508.0MB, Total: 14268.0MB allocated, 30034.0MB reserved, Mean: 3567.0MB allocated, 7508.5MB reserved
2025-03-25 01:47:02,313 - training - INFO - Epoch: 79/200000, Batch: 162/163, Loss: 3.2307, Throughput: 24.31 samples/sec
2025-03-25 01:47:02,313 - training - INFO - Epoch 79 completed in 107.30s. Average loss: 3.2024
2025-03-25 01:47:02,320 - training - INFO - Starting epoch 80/200000
2025-03-25 01:47:02,948 - training - INFO - GPU Memory: GPU 0: 3578.7MB/7510.0MB, GPU 1: 3579.2MB/7518.0MB, GPU 2: 3582.9MB/7498.0MB, GPU 3: 3573.6MB/7508.0MB, Total: 14314.4MB allocated, 30034.0MB reserved, Mean: 3578.6MB allocated, 7508.5MB reserved
2025-03-25 01:47:02,948 - training - INFO - Epoch: 80/200000, Batch: 0/163, Loss: 2.9784, Throughput: 25.47 samples/sec
2025-03-25 01:47:38,352 - training - INFO - GPU Memory: GPU 0: 3580.1MB/7516.0MB, GPU 1: 3579.8MB/7508.0MB, GPU 2: 3584.1MB/7504.0MB, GPU 3: 3574.3MB/7516.0MB, Total: 14318.3MB allocated, 30044.0MB reserved, Mean: 3579.6MB allocated, 7511.0MB reserved
2025-03-25 01:47:38,352 - training - INFO - Epoch: 80/200000, Batch: 54/163, Loss: 3.1373, Throughput: 24.42 samples/sec
2025-03-25 01:48:14,120 - training - INFO - GPU Memory: GPU 0: 3578.1MB/7512.0MB, GPU 1: 3584.2MB/7516.0MB, GPU 2: 3580.2MB/7506.0MB, GPU 3: 3580.6MB/7506.0MB, Total: 14323.1MB allocated, 30040.0MB reserved, Mean: 3580.8MB allocated, 7510.0MB reserved
2025-03-25 01:48:14,120 - training - INFO - Epoch: 80/200000, Batch: 108/163, Loss: 3.2004, Throughput: 24.29 samples/sec
2025-03-25 01:48:49,401 - training - INFO - GPU Memory: GPU 0: 3568.1MB/7510.0MB, GPU 1: 3567.5MB/7522.0MB, GPU 2: 3573.4MB/7516.0MB, GPU 3: 3565.4MB/7502.0MB, Total: 14274.3MB allocated, 30050.0MB reserved, Mean: 3568.6MB allocated, 7512.5MB reserved
2025-03-25 01:48:49,402 - training - INFO - Epoch: 80/200000, Batch: 162/163, Loss: 3.2274, Throughput: 24.36 samples/sec
2025-03-25 01:48:49,402 - training - INFO - Epoch 80 completed in 107.08s. Average loss: 3.1948
2025-03-25 01:48:49,409 - training - INFO - Starting epoch 81/200000
2025-03-25 01:48:50,036 - training - INFO - GPU Memory: GPU 0: 3576.0MB/7510.0MB, GPU 1: 3578.1MB/7522.0MB, GPU 2: 3582.8MB/7516.0MB, GPU 3: 3577.7MB/7502.0MB, Total: 14314.6MB allocated, 30050.0MB reserved, Mean: 3578.7MB allocated, 7512.5MB reserved
2025-03-25 01:48:50,036 - training - INFO - Epoch: 81/200000, Batch: 0/163, Loss: 2.6871, Throughput: 25.52 samples/sec
2025-03-25 01:49:25,484 - training - INFO - GPU Memory: GPU 0: 3576.9MB/7502.0MB, GPU 1: 3580.7MB/7516.0MB, GPU 2: 3583.2MB/7512.0MB, GPU 3: 3573.6MB/7514.0MB, Total: 14314.3MB allocated, 30044.0MB reserved, Mean: 3578.6MB allocated, 7511.0MB reserved
2025-03-25 01:49:25,484 - training - INFO - Epoch: 81/200000, Batch: 54/163, Loss: 3.2029, Throughput: 24.39 samples/sec
2025-03-25 01:50:01,311 - training - INFO - GPU Memory: GPU 0: 3573.9MB/7510.0MB, GPU 1: 3580.8MB/7518.0MB, GPU 2: 3586.7MB/7504.0MB, GPU 3: 3575.3MB/7502.0MB, Total: 14316.7MB allocated, 30034.0MB reserved, Mean: 3579.2MB allocated, 7508.5MB reserved
2025-03-25 01:50:01,311 - training - INFO - Epoch: 81/200000, Batch: 108/163, Loss: 3.2398, Throughput: 24.26 samples/sec
2025-03-25 01:50:36,969 - training - INFO - GPU Memory: GPU 0: 3571.2MB/7514.0MB, GPU 1: 3570.2MB/7518.0MB, GPU 2: 3570.6MB/7516.0MB, GPU 3: 3563.4MB/7512.0MB, Total: 14275.4MB allocated, 30060.0MB reserved, Mean: 3568.9MB allocated, 7515.0MB reserved
2025-03-25 01:50:36,969 - training - INFO - Epoch: 81/200000, Batch: 162/163, Loss: 3.1848, Throughput: 24.25 samples/sec
2025-03-25 01:50:36,970 - training - INFO - Epoch 81 completed in 107.56s. Average loss: 3.1845
2025-03-25 01:50:36,979 - training - INFO - Starting epoch 82/200000
2025-03-25 01:50:37,610 - training - INFO - GPU Memory: GPU 0: 3581.4MB/7514.0MB, GPU 1: 3581.1MB/7518.0MB, GPU 2: 3581.5MB/7516.0MB, GPU 3: 3577.6MB/7512.0MB, Total: 14321.6MB allocated, 30060.0MB reserved, Mean: 3580.4MB allocated, 7515.0MB reserved
2025-03-25 01:50:37,610 - training - INFO - Epoch: 82/200000, Batch: 0/163, Loss: 2.9597, Throughput: 25.34 samples/sec
2025-03-25 01:51:13,564 - training - INFO - GPU Memory: GPU 0: 3571.7MB/7514.0MB, GPU 1: 3582.2MB/7530.0MB, GPU 2: 3583.0MB/7516.0MB, GPU 3: 3578.5MB/7510.0MB, Total: 14315.5MB allocated, 30070.0MB reserved, Mean: 3578.9MB allocated, 7517.5MB reserved
2025-03-25 01:51:13,565 - training - INFO - Epoch: 82/200000, Batch: 54/163, Loss: 3.1818, Throughput: 24.05 samples/sec
2025-03-25 01:51:49,965 - training - INFO - GPU Memory: GPU 0: 3577.8MB/7512.0MB, GPU 1: 3582.2MB/7518.0MB, GPU 2: 3588.0MB/7516.0MB, GPU 3: 3579.1MB/7512.0MB, Total: 14327.2MB allocated, 30058.0MB reserved, Mean: 3581.8MB allocated, 7514.5MB reserved
2025-03-25 01:51:49,965 - training - INFO - Epoch: 82/200000, Batch: 108/163, Loss: 3.1842, Throughput: 23.89 samples/sec
2025-03-25 01:52:25,224 - training - INFO - GPU Memory: GPU 0: 3570.2MB/7510.0MB, GPU 1: 3572.6MB/7516.0MB, GPU 2: 3563.8MB/7510.0MB, GPU 3: 3567.8MB/7518.0MB, Total: 14274.4MB allocated, 30054.0MB reserved, Mean: 3568.6MB allocated, 7513.5MB reserved
2025-03-25 01:52:25,225 - training - INFO - Epoch: 82/200000, Batch: 162/163, Loss: 3.1894, Throughput: 24.09 samples/sec
2025-03-25 01:52:25,225 - training - INFO - Epoch 82 completed in 108.25s. Average loss: 3.1735
2025-03-25 01:52:25,232 - training - INFO - Starting epoch 83/200000
2025-03-25 01:52:25,882 - training - INFO - GPU Memory: GPU 0: 3580.1MB/7510.0MB, GPU 1: 3581.5MB/7516.0MB, GPU 2: 3580.6MB/7510.0MB, GPU 3: 3579.0MB/7518.0MB, Total: 14321.1MB allocated, 30054.0MB reserved, Mean: 3580.3MB allocated, 7513.5MB reserved
2025-03-25 01:52:25,882 - training - INFO - Epoch: 83/200000, Batch: 0/163, Loss: 2.6712, Throughput: 24.60 samples/sec
2025-03-25 01:53:01,676 - training - INFO - GPU Memory: GPU 0: 3576.2MB/7508.0MB, GPU 1: 3576.1MB/7512.0MB, GPU 2: 3585.7MB/7518.0MB, GPU 3: 3578.7MB/7512.0MB, Total: 14316.8MB allocated, 30050.0MB reserved, Mean: 3579.2MB allocated, 7512.5MB reserved
2025-03-25 01:53:01,676 - training - INFO - Epoch: 83/200000, Batch: 54/163, Loss: 3.0064, Throughput: 24.15 samples/sec
2025-03-25 01:53:37,961 - training - INFO - GPU Memory: GPU 0: 3577.5MB/7514.0MB, GPU 1: 3578.5MB/7516.0MB, GPU 2: 3585.3MB/7514.0MB, GPU 3: 3571.7MB/7508.0MB, Total: 14313.1MB allocated, 30052.0MB reserved, Mean: 3578.3MB allocated, 7513.0MB reserved
2025-03-25 01:53:37,961 - training - INFO - Epoch: 83/200000, Batch: 108/163, Loss: 3.1188, Throughput: 23.98 samples/sec
2025-03-25 01:54:13,477 - training - INFO - GPU Memory: GPU 0: 3569.4MB/7512.0MB, GPU 1: 3569.6MB/7512.0MB, GPU 2: 3573.5MB/7520.0MB, GPU 3: 3564.3MB/7508.0MB, Total: 14276.8MB allocated, 30052.0MB reserved, Mean: 3569.2MB allocated, 7513.0MB reserved
2025-03-25 01:54:13,477 - training - INFO - Epoch: 83/200000, Batch: 162/163, Loss: 3.1512, Throughput: 24.09 samples/sec
2025-03-25 01:54:13,478 - training - INFO - Epoch 83 completed in 108.25s. Average loss: 3.1573
2025-03-25 01:54:13,482 - training - INFO - Starting epoch 84/200000
2025-03-25 01:54:14,116 - training - INFO - GPU Memory: GPU 0: 3580.1MB/7512.0MB, GPU 1: 3583.1MB/7512.0MB, GPU 2: 3579.2MB/7520.0MB, GPU 3: 3575.0MB/7508.0MB, Total: 14317.4MB allocated, 30052.0MB reserved, Mean: 3579.4MB allocated, 7513.0MB reserved
2025-03-25 01:54:14,116 - training - INFO - Epoch: 84/200000, Batch: 0/163, Loss: 3.8482, Throughput: 25.24 samples/sec
2025-03-25 01:54:49,987 - training - INFO - GPU Memory: GPU 0: 3581.6MB/7512.0MB, GPU 1: 3584.8MB/7508.0MB, GPU 2: 3582.9MB/7512.0MB, GPU 3: 3580.6MB/7514.0MB, Total: 14329.9MB allocated, 30046.0MB reserved, Mean: 3582.5MB allocated, 7511.5MB reserved
2025-03-25 01:54:49,988 - training - INFO - Epoch: 84/200000, Batch: 54/163, Loss: 3.2061, Throughput: 24.11 samples/sec
2025-03-25 01:55:26,221 - training - INFO - GPU Memory: GPU 0: 3579.8MB/7520.0MB, GPU 1: 3579.8MB/7512.0MB, GPU 2: 3580.5MB/7518.0MB, GPU 3: 3578.5MB/7510.0MB, Total: 14318.6MB allocated, 30060.0MB reserved, Mean: 3579.7MB allocated, 7515.0MB reserved
2025-03-25 01:55:26,221 - training - INFO - Epoch: 84/200000, Batch: 108/163, Loss: 3.2115, Throughput: 23.98 samples/sec
2025-03-25 01:56:01,879 - training - INFO - GPU Memory: GPU 0: 3570.0MB/7510.0MB, GPU 1: 3570.2MB/7520.0MB, GPU 2: 3570.8MB/7512.0MB, GPU 3: 3563.9MB/7508.0MB, Total: 14274.9MB allocated, 30050.0MB reserved, Mean: 3568.7MB allocated, 7512.5MB reserved
2025-03-25 01:56:01,879 - training - INFO - Epoch: 84/200000, Batch: 162/163, Loss: 3.2040, Throughput: 24.06 samples/sec
2025-03-25 01:56:01,880 - training - INFO - Epoch 84 completed in 108.40s. Average loss: 3.1603
2025-03-25 01:56:01,885 - training - INFO - Starting epoch 85/200000
2025-03-25 01:56:02,522 - training - INFO - GPU Memory: GPU 0: 3580.7MB/7510.0MB, GPU 1: 3582.1MB/7520.0MB, GPU 2: 3584.2MB/7512.0MB, GPU 3: 3574.7MB/7508.0MB, Total: 14321.8MB allocated, 30050.0MB reserved, Mean: 3580.4MB allocated, 7512.5MB reserved
2025-03-25 01:56:02,522 - training - INFO - Epoch: 85/200000, Batch: 0/163, Loss: 1.9138, Throughput: 25.11 samples/sec
2025-03-25 01:56:38,064 - training - INFO - GPU Memory: GPU 0: 3575.1MB/7500.0MB, GPU 1: 3583.4MB/7512.0MB, GPU 2: 3579.5MB/7502.0MB, GPU 3: 3577.0MB/7512.0MB, Total: 14315.1MB allocated, 30026.0MB reserved, Mean: 3578.8MB allocated, 7506.5MB reserved
2025-03-25 01:56:38,064 - training - INFO - Epoch: 85/200000, Batch: 54/163, Loss: 3.1363, Throughput: 24.32 samples/sec
2025-03-25 01:57:13,801 - training - INFO - GPU Memory: GPU 0: 3575.5MB/7502.0MB, GPU 1: 3581.1MB/7522.0MB, GPU 2: 3576.9MB/7522.0MB, GPU 3: 3576.7MB/7514.0MB, Total: 14310.2MB allocated, 30060.0MB reserved, Mean: 3577.5MB allocated, 7515.0MB reserved
2025-03-25 01:57:13,801 - training - INFO - Epoch: 85/200000, Batch: 108/163, Loss: 3.1845, Throughput: 24.25 samples/sec
2025-03-25 01:57:48,928 - training - INFO - GPU Memory: GPU 0: 3568.2MB/7506.0MB, GPU 1: 3571.6MB/7524.0MB, GPU 2: 3573.3MB/7506.0MB, GPU 3: 3563.4MB/7510.0MB, Total: 14276.6MB allocated, 30046.0MB reserved, Mean: 3569.1MB allocated, 7511.5MB reserved
2025-03-25 01:57:48,928 - training - INFO - Epoch: 85/200000, Batch: 162/163, Loss: 3.1479, Throughput: 24.36 samples/sec
2025-03-25 01:57:48,929 - training - INFO - Epoch 85 completed in 107.04s. Average loss: 3.1470
2025-03-25 01:57:48,936 - training - INFO - Starting epoch 86/200000
2025-03-25 01:57:49,566 - training - INFO - GPU Memory: GPU 0: 3578.4MB/7506.0MB, GPU 1: 3581.3MB/7524.0MB, GPU 2: 3582.6MB/7506.0MB, GPU 3: 3575.1MB/7510.0MB, Total: 14317.4MB allocated, 30046.0MB reserved, Mean: 3579.3MB allocated, 7511.5MB reserved
2025-03-25 01:57:49,567 - training - INFO - Epoch: 86/200000, Batch: 0/163, Loss: 3.9239, Throughput: 25.41 samples/sec
2025-03-25 01:58:25,006 - training - INFO - GPU Memory: GPU 0: 3575.2MB/7508.0MB, GPU 1: 3580.0MB/7512.0MB, GPU 2: 3584.3MB/7512.0MB, GPU 3: 3581.1MB/7516.0MB, Total: 14320.6MB allocated, 30048.0MB reserved, Mean: 3580.1MB allocated, 7512.0MB reserved
2025-03-25 01:58:25,006 - training - INFO - Epoch: 86/200000, Batch: 54/163, Loss: 3.1578, Throughput: 24.40 samples/sec
2025-03-25 01:59:00,694 - training - INFO - GPU Memory: GPU 0: 3584.3MB/7510.0MB, GPU 1: 3586.8MB/7522.0MB, GPU 2: 3584.0MB/7504.0MB, GPU 3: 3575.5MB/7508.0MB, Total: 14330.6MB allocated, 30044.0MB reserved, Mean: 3582.6MB allocated, 7511.0MB reserved
2025-03-25 01:59:00,694 - training - INFO - Epoch: 86/200000, Batch: 108/163, Loss: 3.0841, Throughput: 24.30 samples/sec
2025-03-25 01:59:35,983 - training - INFO - GPU Memory: GPU 0: 3568.4MB/7516.0MB, GPU 1: 3570.9MB/7528.0MB, GPU 2: 3570.1MB/7518.0MB, GPU 3: 3567.4MB/7500.0MB, Total: 14276.8MB allocated, 30062.0MB reserved, Mean: 3569.2MB allocated, 7515.5MB reserved
2025-03-25 01:59:35,983 - training - INFO - Epoch: 86/200000, Batch: 162/163, Loss: 3.0938, Throughput: 24.36 samples/sec
2025-03-25 01:59:35,984 - training - INFO - Epoch 86 completed in 107.05s. Average loss: 3.1398
2025-03-25 01:59:35,990 - training - INFO - Starting epoch 87/200000
2025-03-25 01:59:36,635 - training - INFO - GPU Memory: GPU 0: 3575.4MB/7516.0MB, GPU 1: 3581.6MB/7528.0MB, GPU 2: 3582.3MB/7518.0MB, GPU 3: 3578.6MB/7500.0MB, Total: 14317.8MB allocated, 30062.0MB reserved, Mean: 3579.4MB allocated, 7515.5MB reserved
2025-03-25 01:59:36,635 - training - INFO - Epoch: 87/200000, Batch: 0/163, Loss: 2.4648, Throughput: 24.82 samples/sec
2025-03-25 02:00:12,109 - training - INFO - GPU Memory: GPU 0: 3574.7MB/7506.0MB, GPU 1: 3579.5MB/7516.0MB, GPU 2: 3581.5MB/7504.0MB, GPU 3: 3578.5MB/7514.0MB, Total: 14314.2MB allocated, 30040.0MB reserved, Mean: 3578.5MB allocated, 7510.0MB reserved
2025-03-25 02:00:12,110 - training - INFO - Epoch: 87/200000, Batch: 54/163, Loss: 3.1334, Throughput: 24.36 samples/sec
2025-03-25 02:00:48,024 - training - INFO - GPU Memory: GPU 0: 3577.7MB/7514.0MB, GPU 1: 3581.6MB/7516.0MB, GPU 2: 3583.6MB/7510.0MB, GPU 3: 3576.3MB/7516.0MB, Total: 14319.2MB allocated, 30056.0MB reserved, Mean: 3579.8MB allocated, 7514.0MB reserved
2025-03-25 02:00:48,025 - training - INFO - Epoch: 87/200000, Batch: 108/163, Loss: 3.1761, Throughput: 24.21 samples/sec
2025-03-25 02:01:23,405 - training - INFO - GPU Memory: GPU 0: 3570.8MB/7506.0MB, GPU 1: 3568.4MB/7510.0MB, GPU 2: 3576.7MB/7514.0MB, GPU 3: 3565.5MB/7512.0MB, Total: 14281.4MB allocated, 30042.0MB reserved, Mean: 3570.3MB allocated, 7510.5MB reserved
2025-03-25 02:01:23,405 - training - INFO - Epoch: 87/200000, Batch: 162/163, Loss: 3.1561, Throughput: 24.28 samples/sec
2025-03-25 02:01:23,406 - training - INFO - Epoch 87 completed in 107.42s. Average loss: 3.1137
2025-03-25 02:01:23,415 - training - INFO - Starting epoch 88/200000
2025-03-25 02:01:24,041 - training - INFO - GPU Memory: GPU 0: 3582.2MB/7506.0MB, GPU 1: 3578.9MB/7510.0MB, GPU 2: 3587.3MB/7514.0MB, GPU 3: 3574.5MB/7512.0MB, Total: 14322.9MB allocated, 30042.0MB reserved, Mean: 3580.7MB allocated, 7510.5MB reserved
2025-03-25 02:01:24,041 - training - INFO - Epoch: 88/200000, Batch: 0/163, Loss: 2.4905, Throughput: 25.56 samples/sec
2025-03-25 02:01:59,564 - training - INFO - GPU Memory: GPU 0: 3577.7MB/7520.0MB, GPU 1: 3577.8MB/7518.0MB, GPU 2: 3579.5MB/7514.0MB, GPU 3: 3577.4MB/7512.0MB, Total: 14312.4MB allocated, 30064.0MB reserved, Mean: 3578.1MB allocated, 7516.0MB reserved
2025-03-25 02:01:59,564 - training - INFO - Epoch: 88/200000, Batch: 54/163, Loss: 3.0477, Throughput: 24.34 samples/sec
2025-03-25 02:02:35,407 - training - INFO - GPU Memory: GPU 0: 3577.4MB/7510.0MB, GPU 1: 3580.5MB/7510.0MB, GPU 2: 3582.7MB/7508.0MB, GPU 3: 3576.4MB/7510.0MB, Total: 14316.9MB allocated, 30038.0MB reserved, Mean: 3579.2MB allocated, 7509.5MB reserved
2025-03-25 02:02:35,408 - training - INFO - Epoch: 88/200000, Batch: 108/163, Loss: 3.0888, Throughput: 24.22 samples/sec
2025-03-25 02:03:10,699 - training - INFO - GPU Memory: GPU 0: 3568.1MB/7520.0MB, GPU 1: 3569.1MB/7522.0MB, GPU 2: 3574.8MB/7520.0MB, GPU 3: 3570.1MB/7502.0MB, Total: 14282.1MB allocated, 30064.0MB reserved, Mean: 3570.5MB allocated, 7516.0MB reserved
2025-03-25 02:03:10,699 - training - INFO - Epoch: 88/200000, Batch: 162/163, Loss: 3.1128, Throughput: 24.31 samples/sec
2025-03-25 02:03:10,700 - training - INFO - Epoch 88 completed in 107.28s. Average loss: 3.1101
2025-03-25 02:03:10,712 - training - INFO - Starting epoch 89/200000
2025-03-25 02:03:11,351 - training - INFO - GPU Memory: GPU 0: 3577.7MB/7520.0MB, GPU 1: 3578.2MB/7522.0MB, GPU 2: 3583.9MB/7520.0MB, GPU 3: 3581.0MB/7502.0MB, Total: 14320.9MB allocated, 30064.0MB reserved, Mean: 3580.2MB allocated, 7516.0MB reserved
2025-03-25 02:03:11,352 - training - INFO - Epoch: 89/200000, Batch: 0/163, Loss: 3.1720, Throughput: 25.01 samples/sec
2025-03-25 02:03:46,834 - training - INFO - GPU Memory: GPU 0: 3578.3MB/7510.0MB, GPU 1: 3583.0MB/7520.0MB, GPU 2: 3583.2MB/7502.0MB, GPU 3: 3580.5MB/7520.0MB, Total: 14325.0MB allocated, 30052.0MB reserved, Mean: 3581.2MB allocated, 7513.0MB reserved
2025-03-25 02:03:46,834 - training - INFO - Epoch: 89/200000, Batch: 54/163, Loss: 2.9886, Throughput: 24.36 samples/sec
2025-03-25 02:04:22,726 - training - INFO - GPU Memory: GPU 0: 3579.6MB/7516.0MB, GPU 1: 3582.8MB/7514.0MB, GPU 2: 3576.3MB/7520.0MB, GPU 3: 3578.9MB/7510.0MB, Total: 14317.5MB allocated, 30060.0MB reserved, Mean: 3579.4MB allocated, 7515.0MB reserved
2025-03-25 02:04:22,726 - training - INFO - Epoch: 89/200000, Batch: 108/163, Loss: 3.0198, Throughput: 24.22 samples/sec
2025-03-25 02:04:58,004 - training - INFO - GPU Memory: GPU 0: 3567.3MB/7516.0MB, GPU 1: 3571.7MB/7518.0MB, GPU 2: 3568.3MB/7506.0MB, GPU 3: 3572.8MB/7508.0MB, Total: 14280.0MB allocated, 30048.0MB reserved, Mean: 3570.0MB allocated, 7512.0MB reserved
2025-03-25 02:04:58,004 - training - INFO - Epoch: 89/200000, Batch: 162/163, Loss: 3.0474, Throughput: 24.31 samples/sec
2025-03-25 02:04:58,004 - training - INFO - Epoch 89 completed in 107.29s. Average loss: 3.0840
2025-03-25 02:04:58,009 - training - INFO - Starting epoch 90/200000
2025-03-25 02:04:58,655 - training - INFO - GPU Memory: GPU 0: 3579.2MB/7516.0MB, GPU 1: 3581.3MB/7518.0MB, GPU 2: 3582.6MB/7506.0MB, GPU 3: 3581.7MB/7508.0MB, Total: 14324.8MB allocated, 30048.0MB reserved, Mean: 3581.2MB allocated, 7512.0MB reserved
2025-03-25 02:04:58,655 - training - INFO - Epoch: 90/200000, Batch: 0/163, Loss: 2.4556, Throughput: 24.79 samples/sec
2025-03-25 02:05:34,100 - training - INFO - GPU Memory: GPU 0: 3574.9MB/7516.0MB, GPU 1: 3580.7MB/7512.0MB, GPU 2: 3579.9MB/7516.0MB, GPU 3: 3576.5MB/7512.0MB, Total: 14312.0MB allocated, 30056.0MB reserved, Mean: 3578.0MB allocated, 7514.0MB reserved
2025-03-25 02:05:34,100 - training - INFO - Epoch: 90/200000, Batch: 54/163, Loss: 2.9577, Throughput: 24.38 samples/sec
2025-03-25 02:06:09,965 - training - INFO - GPU Memory: GPU 0: 3578.3MB/7504.0MB, GPU 1: 3576.8MB/7520.0MB, GPU 2: 3581.7MB/7510.0MB, GPU 3: 3576.4MB/7506.0MB, Total: 14313.3MB allocated, 30040.0MB reserved, Mean: 3578.3MB allocated, 7510.0MB reserved
2025-03-25 02:06:09,965 - training - INFO - Epoch: 90/200000, Batch: 108/163, Loss: 3.0284, Throughput: 24.24 samples/sec
2025-03-25 02:06:45,127 - training - INFO - GPU Memory: GPU 0: 3567.1MB/7516.0MB, GPU 1: 3572.5MB/7508.0MB, GPU 2: 3570.5MB/7498.0MB, GPU 3: 3565.3MB/7504.0MB, Total: 14275.3MB allocated, 30026.0MB reserved, Mean: 3568.8MB allocated, 7506.5MB reserved
2025-03-25 02:06:45,127 - training - INFO - Epoch: 90/200000, Batch: 162/163, Loss: 3.0581, Throughput: 24.35 samples/sec
2025-03-25 02:06:45,128 - training - INFO - Epoch 90 completed in 107.12s. Average loss: 3.0898
2025-03-25 02:06:45,134 - training - INFO - Starting epoch 91/200000
2025-03-25 02:06:45,769 - training - INFO - GPU Memory: GPU 0: 3577.2MB/7516.0MB, GPU 1: 3583.2MB/7508.0MB, GPU 2: 3578.9MB/7498.0MB, GPU 3: 3581.2MB/7504.0MB, Total: 14320.6MB allocated, 30026.0MB reserved, Mean: 3580.1MB allocated, 7506.5MB reserved
2025-03-25 02:06:45,770 - training - INFO - Epoch: 91/200000, Batch: 0/163, Loss: 3.6720, Throughput: 25.19 samples/sec
2025-03-25 02:07:21,325 - training - INFO - GPU Memory: GPU 0: 3579.8MB/7512.0MB, GPU 1: 3577.4MB/7518.0MB, GPU 2: 3581.4MB/7524.0MB, GPU 3: 3577.7MB/7528.0MB, Total: 14316.3MB allocated, 30082.0MB reserved, Mean: 3579.1MB allocated, 7520.5MB reserved
2025-03-25 02:07:21,325 - training - INFO - Epoch: 91/200000, Batch: 54/163, Loss: 3.1631, Throughput: 24.32 samples/sec
2025-03-25 02:07:57,121 - training - INFO - GPU Memory: GPU 0: 3577.5MB/7514.0MB, GPU 1: 3576.7MB/7514.0MB, GPU 2: 3583.8MB/7504.0MB, GPU 3: 3579.9MB/7510.0MB, Total: 14317.8MB allocated, 30042.0MB reserved, Mean: 3579.5MB allocated, 7510.5MB reserved
2025-03-25 02:07:57,121 - training - INFO - Epoch: 91/200000, Batch: 108/163, Loss: 3.0669, Throughput: 24.23 samples/sec
2025-03-25 02:08:32,349 - training - INFO - GPU Memory: GPU 0: 3568.9MB/7518.0MB, GPU 1: 3570.4MB/7524.0MB, GPU 2: 3571.4MB/7516.0MB, GPU 3: 3565.0MB/7512.0MB, Total: 14275.8MB allocated, 30070.0MB reserved, Mean: 3568.9MB allocated, 7517.5MB reserved
2025-03-25 02:08:32,349 - training - INFO - Epoch: 91/200000, Batch: 162/163, Loss: 3.0522, Throughput: 24.33 samples/sec
2025-03-25 02:08:32,349 - training - INFO - Epoch 91 completed in 107.22s. Average loss: 3.0665
2025-03-25 02:08:32,355 - training - INFO - Starting epoch 92/200000
2025-03-25 02:08:33,010 - training - INFO - GPU Memory: GPU 0: 3579.6MB/7518.0MB, GPU 1: 3580.8MB/7524.0MB, GPU 2: 3581.3MB/7516.0MB, GPU 3: 3572.7MB/7512.0MB, Total: 14314.5MB allocated, 30070.0MB reserved, Mean: 3578.6MB allocated, 7517.5MB reserved
2025-03-25 02:08:33,136 - training - INFO - Epoch: 92/200000, Batch: 0/163, Loss: 1.9517, Throughput: 24.45 samples/sec
2025-03-25 02:09:08,710 - training - INFO - GPU Memory: GPU 0: 3579.0MB/7516.0MB, GPU 1: 3579.0MB/7524.0MB, GPU 2: 3583.9MB/7502.0MB, GPU 3: 3577.8MB/7510.0MB, Total: 14319.7MB allocated, 30052.0MB reserved, Mean: 3579.9MB allocated, 7513.0MB reserved
2025-03-25 02:09:08,710 - training - INFO - Epoch: 92/200000, Batch: 54/163, Loss: 2.9940, Throughput: 24.21 samples/sec
2025-03-25 02:09:44,704 - training - INFO - GPU Memory: GPU 0: 3579.2MB/7516.0MB, GPU 1: 3581.0MB/7516.0MB, GPU 2: 3586.9MB/7518.0MB, GPU 3: 3583.4MB/7522.0MB, Total: 14330.5MB allocated, 30072.0MB reserved, Mean: 3582.6MB allocated, 7518.0MB reserved
2025-03-25 02:09:44,705 - training - INFO - Epoch: 92/200000, Batch: 108/163, Loss: 2.9638, Throughput: 24.11 samples/sec
2025-03-25 02:10:20,475 - training - INFO - GPU Memory: GPU 0: 3570.6MB/7508.0MB, GPU 1: 3568.3MB/7516.0MB, GPU 2: 3569.4MB/7518.0MB, GPU 3: 3568.4MB/7518.0MB, Total: 14276.7MB allocated, 30060.0MB reserved, Mean: 3569.2MB allocated, 7515.0MB reserved
2025-03-25 02:10:20,475 - training - INFO - Epoch: 92/200000, Batch: 162/163, Loss: 2.9482, Throughput: 24.12 samples/sec
2025-03-25 02:10:20,476 - training - INFO - Epoch 92 completed in 108.12s. Average loss: 3.0638
2025-03-25 02:10:20,487 - training - INFO - Starting epoch 93/200000
2025-03-25 02:10:21,153 - training - INFO - GPU Memory: GPU 0: 3581.2MB/7508.0MB, GPU 1: 3576.9MB/7516.0MB, GPU 2: 3580.5MB/7518.0MB, GPU 3: 3578.6MB/7518.0MB, Total: 14317.3MB allocated, 30060.0MB reserved, Mean: 3579.3MB allocated, 7515.0MB reserved
2025-03-25 02:10:21,154 - training - INFO - Epoch: 93/200000, Batch: 0/163, Loss: 4.0629, Throughput: 24.03 samples/sec
2025-03-25 02:10:57,020 - training - INFO - GPU Memory: GPU 0: 3582.6MB/7510.0MB, GPU 1: 3583.3MB/7520.0MB, GPU 2: 3584.1MB/7510.0MB, GPU 3: 3576.0MB/7506.0MB, Total: 14326.0MB allocated, 30046.0MB reserved, Mean: 3581.5MB allocated, 7511.5MB reserved
2025-03-25 02:10:57,020 - training - INFO - Epoch: 93/200000, Batch: 54/163, Loss: 2.9731, Throughput: 24.09 samples/sec
2025-03-25 02:11:33,189 - training - INFO - GPU Memory: GPU 0: 3577.7MB/7516.0MB, GPU 1: 3575.9MB/7526.0MB, GPU 2: 3588.0MB/7506.0MB, GPU 3: 3580.1MB/7518.0MB, Total: 14321.7MB allocated, 30066.0MB reserved, Mean: 3580.4MB allocated, 7516.5MB reserved
2025-03-25 02:11:33,190 - training - INFO - Epoch: 93/200000, Batch: 108/163, Loss: 3.0001, Throughput: 23.99 samples/sec
2025-03-25 02:12:08,842 - training - INFO - GPU Memory: GPU 0: 3569.7MB/7510.0MB, GPU 1: 3572.3MB/7520.0MB, GPU 2: 3569.3MB/7514.0MB, GPU 3: 3566.1MB/7514.0MB, Total: 14277.3MB allocated, 30058.0MB reserved, Mean: 3569.3MB allocated, 7514.5MB reserved
2025-03-25 02:12:08,842 - training - INFO - Epoch: 93/200000, Batch: 162/163, Loss: 3.0526, Throughput: 24.07 samples/sec
2025-03-25 02:12:08,843 - training - INFO - Epoch 93 completed in 108.36s. Average loss: 3.0498
2025-03-25 02:12:08,852 - training - INFO - Starting epoch 94/200000
2025-03-25 02:12:09,488 - training - INFO - GPU Memory: GPU 0: 3580.6MB/7510.0MB, GPU 1: 3585.4MB/7520.0MB, GPU 2: 3580.2MB/7514.0MB, GPU 3: 3578.0MB/7514.0MB, Total: 14324.1MB allocated, 30058.0MB reserved, Mean: 3581.0MB allocated, 7514.5MB reserved
2025-03-25 02:12:09,488 - training - INFO - Epoch: 94/200000, Batch: 0/163, Loss: 3.7620, Throughput: 25.15 samples/sec
2025-03-25 02:12:44,787 - training - INFO - GPU Memory: GPU 0: 3579.4MB/7514.0MB, GPU 1: 3586.3MB/7524.0MB, GPU 2: 3582.7MB/7504.0MB, GPU 3: 3578.3MB/7518.0MB, Total: 14326.6MB allocated, 30060.0MB reserved, Mean: 3581.7MB allocated, 7515.0MB reserved
2025-03-25 02:12:44,787 - training - INFO - Epoch: 94/200000, Batch: 54/163, Loss: 3.1193, Throughput: 24.49 samples/sec
2025-03-25 02:13:20,403 - training - INFO - GPU Memory: GPU 0: 3574.7MB/7514.0MB, GPU 1: 3584.9MB/7522.0MB, GPU 2: 3583.3MB/7514.0MB, GPU 3: 3579.0MB/7510.0MB, Total: 14321.8MB allocated, 30060.0MB reserved, Mean: 3580.5MB allocated, 7515.0MB reserved
2025-03-25 02:13:20,403 - training - INFO - Epoch: 94/200000, Batch: 108/163, Loss: 3.0577, Throughput: 24.37 samples/sec
2025-03-25 02:13:55,634 - training - INFO - GPU Memory: GPU 0: 3568.4MB/7512.0MB, GPU 1: 3570.6MB/7524.0MB, GPU 2: 3569.8MB/7504.0MB, GPU 3: 3570.6MB/7516.0MB, Total: 14279.4MB allocated, 30056.0MB reserved, Mean: 3569.9MB allocated, 7514.0MB reserved
2025-03-25 02:13:55,634 - training - INFO - Epoch: 94/200000, Batch: 162/163, Loss: 3.0427, Throughput: 24.42 samples/sec
2025-03-25 02:13:55,635 - training - INFO - Epoch 94 completed in 106.78s. Average loss: 3.0520
2025-03-25 02:13:55,643 - training - INFO - Starting epoch 95/200000
2025-03-25 02:13:56,277 - training - INFO - GPU Memory: GPU 0: 3579.1MB/7512.0MB, GPU 1: 3581.3MB/7524.0MB, GPU 2: 3582.5MB/7504.0MB, GPU 3: 3580.2MB/7516.0MB, Total: 14323.1MB allocated, 30056.0MB reserved, Mean: 3580.8MB allocated, 7514.0MB reserved
2025-03-25 02:13:56,278 - training - INFO - Epoch: 95/200000, Batch: 0/163, Loss: 2.1182, Throughput: 25.23 samples/sec
2025-03-25 02:14:31,715 - training - INFO - GPU Memory: GPU 0: 3573.9MB/7504.0MB, GPU 1: 3582.5MB/7528.0MB, GPU 2: 3584.2MB/7508.0MB, GPU 3: 3583.3MB/7506.0MB, Total: 14324.0MB allocated, 30046.0MB reserved, Mean: 3581.0MB allocated, 7511.5MB reserved
2025-03-25 02:14:31,716 - training - INFO - Epoch: 95/200000, Batch: 54/163, Loss: 2.9603, Throughput: 24.40 samples/sec
2025-03-25 02:15:07,502 - training - INFO - GPU Memory: GPU 0: 3577.6MB/7510.0MB, GPU 1: 3581.8MB/7524.0MB, GPU 2: 3582.2MB/7514.0MB, GPU 3: 3577.7MB/7522.0MB, Total: 14319.4MB allocated, 30070.0MB reserved, Mean: 3579.8MB allocated, 7517.5MB reserved
2025-03-25 02:15:07,503 - training - INFO - Epoch: 95/200000, Batch: 108/163, Loss: 3.0099, Throughput: 24.27 samples/sec
2025-03-25 02:15:42,745 - training - INFO - GPU Memory: GPU 0: 3568.3MB/7514.0MB, GPU 1: 3569.7MB/7526.0MB, GPU 2: 3570.0MB/7504.0MB, GPU 3: 3567.8MB/7510.0MB, Total: 14275.8MB allocated, 30054.0MB reserved, Mean: 3568.9MB allocated, 7513.5MB reserved
2025-03-25 02:15:42,745 - training - INFO - Epoch: 95/200000, Batch: 162/163, Loss: 3.0451, Throughput: 24.35 samples/sec
2025-03-25 02:15:42,746 - training - INFO - Epoch 95 completed in 107.10s. Average loss: 3.0274
2025-03-25 02:15:42,756 - training - INFO - Starting epoch 96/200000
2025-03-25 02:15:43,380 - training - INFO - GPU Memory: GPU 0: 3578.0MB/7514.0MB, GPU 1: 3580.8MB/7526.0MB, GPU 2: 3584.4MB/7504.0MB, GPU 3: 3579.8MB/7510.0MB, Total: 14323.0MB allocated, 30054.0MB reserved, Mean: 3580.8MB allocated, 7513.5MB reserved
2025-03-25 02:15:43,380 - training - INFO - Epoch: 96/200000, Batch: 0/163, Loss: 4.0959, Throughput: 25.63 samples/sec
2025-03-25 02:16:19,255 - training - INFO - GPU Memory: GPU 0: 3574.6MB/7508.0MB, GPU 1: 3581.9MB/7520.0MB, GPU 2: 3585.4MB/7518.0MB, GPU 3: 3575.3MB/7506.0MB, Total: 14317.2MB allocated, 30052.0MB reserved, Mean: 3579.3MB allocated, 7513.0MB reserved
2025-03-25 02:16:19,255 - training - INFO - Epoch: 96/200000, Batch: 54/163, Loss: 3.1628, Throughput: 24.11 samples/sec
2025-03-25 02:16:55,378 - training - INFO - GPU Memory: GPU 0: 3575.4MB/7508.0MB, GPU 1: 3584.8MB/7522.0MB, GPU 2: 3587.1MB/7506.0MB, GPU 3: 3584.0MB/7508.0MB, Total: 14331.2MB allocated, 30044.0MB reserved, Mean: 3582.8MB allocated, 7511.0MB reserved
2025-03-25 02:16:55,378 - training - INFO - Epoch: 96/200000, Batch: 108/163, Loss: 3.1530, Throughput: 24.01 samples/sec
2025-03-25 02:17:30,937 - training - INFO - GPU Memory: GPU 0: 3569.0MB/7512.0MB, GPU 1: 3576.7MB/7526.0MB, GPU 2: 3571.5MB/7514.0MB, GPU 3: 3569.4MB/7518.0MB, Total: 14286.6MB allocated, 30070.0MB reserved, Mean: 3571.7MB allocated, 7517.5MB reserved
2025-03-25 02:17:30,937 - training - INFO - Epoch: 96/200000, Batch: 162/163, Loss: 3.0694, Throughput: 24.11 samples/sec
2025-03-25 02:17:30,938 - training - INFO - Epoch 96 completed in 108.18s. Average loss: 3.0146
2025-03-25 02:17:30,944 - training - INFO - Starting epoch 97/200000
2025-03-25 02:17:31,588 - training - INFO - GPU Memory: GPU 0: 3579.9MB/7512.0MB, GPU 1: 3587.4MB/7526.0MB, GPU 2: 3582.4MB/7514.0MB, GPU 3: 3576.6MB/7518.0MB, Total: 14326.2MB allocated, 30070.0MB reserved, Mean: 3581.6MB allocated, 7517.5MB reserved
2025-03-25 02:17:31,588 - training - INFO - Epoch: 97/200000, Batch: 0/163, Loss: 2.6642, Throughput: 24.86 samples/sec
2025-03-25 02:18:07,068 - training - INFO - GPU Memory: GPU 0: 3581.4MB/7506.0MB, GPU 1: 3587.1MB/7514.0MB, GPU 2: 3583.7MB/7502.0MB, GPU 3: 3576.4MB/7510.0MB, Total: 14328.6MB allocated, 30032.0MB reserved, Mean: 3582.2MB allocated, 7508.0MB reserved
2025-03-25 02:18:07,068 - training - INFO - Epoch: 97/200000, Batch: 54/163, Loss: 2.9634, Throughput: 24.36 samples/sec
2025-03-25 02:18:42,741 - training - INFO - GPU Memory: GPU 0: 3581.4MB/7512.0MB, GPU 1: 3584.1MB/7526.0MB, GPU 2: 3584.1MB/7516.0MB, GPU 3: 3583.8MB/7508.0MB, Total: 14333.4MB allocated, 30062.0MB reserved, Mean: 3583.4MB allocated, 7515.5MB reserved
2025-03-25 02:18:42,741 - training - INFO - Epoch: 97/200000, Batch: 108/163, Loss: 2.9310, Throughput: 24.29 samples/sec
2025-03-25 02:19:18,046 - training - INFO - GPU Memory: GPU 0: 3571.1MB/7520.0MB, GPU 1: 3576.0MB/7520.0MB, GPU 2: 3570.7MB/7502.0MB, GPU 3: 3565.1MB/7514.0MB, Total: 14282.9MB allocated, 30056.0MB reserved, Mean: 3570.7MB allocated, 7514.0MB reserved
2025-03-25 02:19:18,047 - training - INFO - Epoch: 97/200000, Batch: 162/163, Loss: 2.9320, Throughput: 24.35 samples/sec
2025-03-25 02:19:18,047 - training - INFO - Epoch 97 completed in 107.10s. Average loss: 3.0040
2025-03-25 02:19:18,054 - training - INFO - Starting epoch 98/200000
2025-03-25 02:19:18,694 - training - INFO - GPU Memory: GPU 0: 3581.7MB/7520.0MB, GPU 1: 3584.2MB/7520.0MB, GPU 2: 3584.5MB/7502.0MB, GPU 3: 3577.1MB/7514.0MB, Total: 14327.5MB allocated, 30056.0MB reserved, Mean: 3581.9MB allocated, 7514.0MB reserved
2025-03-25 02:19:18,694 - training - INFO - Epoch: 98/200000, Batch: 0/163, Loss: 2.7276, Throughput: 24.97 samples/sec
2025-03-25 02:19:54,213 - training - INFO - GPU Memory: GPU 0: 3573.5MB/7508.0MB, GPU 1: 3579.8MB/7530.0MB, GPU 2: 3581.9MB/7512.0MB, GPU 3: 3577.7MB/7508.0MB, Total: 14313.0MB allocated, 30058.0MB reserved, Mean: 3578.2MB allocated, 7514.5MB reserved
2025-03-25 02:19:54,213 - training - INFO - Epoch: 98/200000, Batch: 54/163, Loss: 2.8487, Throughput: 24.34 samples/sec
2025-03-25 02:20:30,017 - training - INFO - GPU Memory: GPU 0: 3575.0MB/7506.0MB, GPU 1: 3579.2MB/7510.0MB, GPU 2: 3585.9MB/7512.0MB, GPU 3: 3577.6MB/7508.0MB, Total: 14317.6MB allocated, 30036.0MB reserved, Mean: 3579.4MB allocated, 7509.0MB reserved
2025-03-25 02:20:30,018 - training - INFO - Epoch: 98/200000, Batch: 108/163, Loss: 2.9662, Throughput: 24.23 samples/sec
2025-03-25 02:21:05,097 - training - INFO - GPU Memory: GPU 0: 3569.8MB/7512.0MB, GPU 1: 3568.4MB/7510.0MB, GPU 2: 3566.6MB/7506.0MB, GPU 3: 3569.1MB/7512.0MB, Total: 14273.9MB allocated, 30040.0MB reserved, Mean: 3568.5MB allocated, 7510.0MB reserved
2025-03-25 02:21:05,098 - training - INFO - Epoch: 98/200000, Batch: 162/163, Loss: 3.0193, Throughput: 24.36 samples/sec
2025-03-25 02:21:05,098 - training - INFO - Epoch 98 completed in 107.04s. Average loss: 2.9906
2025-03-25 02:21:05,104 - training - INFO - Starting epoch 99/200000
2025-03-25 02:21:05,760 - training - INFO - GPU Memory: GPU 0: 3582.6MB/7512.0MB, GPU 1: 3579.0MB/7510.0MB, GPU 2: 3579.5MB/7506.0MB, GPU 3: 3579.0MB/7512.0MB, Total: 14320.1MB allocated, 30040.0MB reserved, Mean: 3580.0MB allocated, 7510.0MB reserved
2025-03-25 02:21:05,760 - training - INFO - Epoch: 99/200000, Batch: 0/163, Loss: 2.2630, Throughput: 24.42 samples/sec
2025-03-25 02:21:41,256 - training - INFO - GPU Memory: GPU 0: 3577.7MB/7508.0MB, GPU 1: 3582.6MB/7510.0MB, GPU 2: 3577.6MB/7516.0MB, GPU 3: 3580.9MB/7516.0MB, Total: 14318.7MB allocated, 30050.0MB reserved, Mean: 3579.7MB allocated, 7512.5MB reserved
2025-03-25 02:21:41,257 - training - INFO - Epoch: 99/200000, Batch: 54/163, Loss: 2.9235, Throughput: 24.34 samples/sec
2025-03-25 02:22:16,995 - training - INFO - GPU Memory: GPU 0: 3578.3MB/7516.0MB, GPU 1: 3583.9MB/7518.0MB, GPU 2: 3579.8MB/7506.0MB, GPU 3: 3575.8MB/7518.0MB, Total: 14317.9MB allocated, 30058.0MB reserved, Mean: 3579.5MB allocated, 7514.5MB reserved
2025-03-25 02:22:16,996 - training - INFO - Epoch: 99/200000, Batch: 108/163, Loss: 3.0281, Throughput: 24.26 samples/sec
2025-03-25 02:22:52,642 - training - INFO - GPU Memory: GPU 0: 3570.0MB/7516.0MB, GPU 1: 3570.3MB/7520.0MB, GPU 2: 3570.7MB/7520.0MB, GPU 3: 3565.4MB/7510.0MB, Total: 14276.4MB allocated, 30066.0MB reserved, Mean: 3569.1MB allocated, 7516.5MB reserved
2025-03-25 02:22:52,642 - training - INFO - Epoch: 99/200000, Batch: 162/163, Loss: 3.0158, Throughput: 24.25 samples/sec
2025-03-25 02:22:52,643 - training - INFO - Epoch 99 completed in 107.54s. Average loss: 2.9923
2025-03-25 02:22:52,650 - training - INFO - Starting epoch 100/200000
2025-03-25 02:22:53,278 - training - INFO - GPU Memory: GPU 0: 3580.7MB/7516.0MB, GPU 1: 3584.0MB/7520.0MB, GPU 2: 3581.6MB/7520.0MB, GPU 3: 3575.5MB/7510.0MB, Total: 14321.8MB allocated, 30066.0MB reserved, Mean: 3580.5MB allocated, 7516.5MB reserved
2025-03-25 02:22:53,278 - training - INFO - Epoch: 100/200000, Batch: 0/163, Loss: 3.9036, Throughput: 25.49 samples/sec
2025-03-25 02:23:28,698 - training - INFO - GPU Memory: GPU 0: 3576.1MB/7508.0MB, GPU 1: 3580.2MB/7520.0MB, GPU 2: 3591.6MB/7510.0MB, GPU 3: 3584.0MB/7508.0MB, Total: 14331.9MB allocated, 30046.0MB reserved, Mean: 3583.0MB allocated, 7511.5MB reserved
2025-03-25 02:23:28,698 - training - INFO - Epoch: 100/200000, Batch: 54/163, Loss: 2.8536, Throughput: 24.41 samples/sec
2025-03-25 02:24:04,460 - training - INFO - GPU Memory: GPU 0: 3576.8MB/7516.0MB, GPU 1: 3587.3MB/7522.0MB, GPU 2: 3582.4MB/7528.0MB, GPU 3: 3579.3MB/7514.0MB, Total: 14325.8MB allocated, 30080.0MB reserved, Mean: 3581.5MB allocated, 7520.0MB reserved
2025-03-25 02:24:04,461 - training - INFO - Epoch: 100/200000, Batch: 108/163, Loss: 2.9210, Throughput: 24.29 samples/sec
2025-03-25 02:24:39,935 - training - INFO - GPU Memory: GPU 0: 3561.4MB/7510.0MB, GPU 1: 3574.0MB/7534.0MB, GPU 2: 3569.9MB/7514.0MB, GPU 3: 3564.9MB/7518.0MB, Total: 14270.3MB allocated, 30076.0MB reserved, Mean: 3567.6MB allocated, 7519.0MB reserved
2025-03-25 02:24:39,936 - training - INFO - Epoch: 100/200000, Batch: 162/163, Loss: 2.9779, Throughput: 24.31 samples/sec
2025-03-25 02:24:39,936 - training - INFO - Epoch 100 completed in 107.29s. Average loss: 2.9677
2025-03-25 02:24:39,947 - training - INFO - Starting epoch 101/200000
2025-03-25 02:24:40,584 - training - INFO - GPU Memory: GPU 0: 3572.1MB/7510.0MB, GPU 1: 3587.2MB/7534.0MB, GPU 2: 3577.1MB/7514.0MB, GPU 3: 3574.6MB/7518.0MB, Total: 14311.1MB allocated, 30076.0MB reserved, Mean: 3577.8MB allocated, 7519.0MB reserved
2025-03-25 02:24:40,584 - training - INFO - Epoch: 101/200000, Batch: 0/163, Loss: 3.5486, Throughput: 25.14 samples/sec
2025-03-25 02:25:16,159 - training - INFO - GPU Memory: GPU 0: 3583.3MB/7520.0MB, GPU 1: 3577.4MB/7528.0MB, GPU 2: 3584.7MB/7518.0MB, GPU 3: 3575.2MB/7518.0MB, Total: 14320.6MB allocated, 30084.0MB reserved, Mean: 3580.2MB allocated, 7521.0MB reserved
2025-03-25 02:25:16,159 - training - INFO - Epoch: 101/200000, Batch: 54/163, Loss: 2.9320, Throughput: 24.30 samples/sec
2025-03-25 02:25:51,929 - training - INFO - GPU Memory: GPU 0: 3578.6MB/7518.0MB, GPU 1: 3578.8MB/7520.0MB, GPU 2: 3583.4MB/7508.0MB, GPU 3: 3581.7MB/7508.0MB, Total: 14322.5MB allocated, 30054.0MB reserved, Mean: 3580.6MB allocated, 7513.5MB reserved
2025-03-25 02:25:51,929 - training - INFO - Epoch: 101/200000, Batch: 108/163, Loss: 2.9966, Throughput: 24.23 samples/sec
2025-03-25 02:26:27,186 - training - INFO - GPU Memory: GPU 0: 3567.7MB/7510.0MB, GPU 1: 3571.6MB/7518.0MB, GPU 2: 3572.5MB/7504.0MB, GPU 3: 3564.3MB/7518.0MB, Total: 14276.0MB allocated, 30050.0MB reserved, Mean: 3569.0MB allocated, 7512.5MB reserved
2025-03-25 02:26:27,187 - training - INFO - Epoch: 101/200000, Batch: 162/163, Loss: 2.9786, Throughput: 24.32 samples/sec
2025-03-25 02:26:27,188 - training - INFO - Epoch 101 completed in 107.24s. Average loss: 2.9695
2025-03-25 02:26:27,197 - training - INFO - Starting epoch 102/200000
2025-03-25 02:26:27,824 - training - INFO - GPU Memory: GPU 0: 3577.4MB/7510.0MB, GPU 1: 3581.7MB/7518.0MB, GPU 2: 3584.7MB/7504.0MB, GPU 3: 3579.9MB/7518.0MB, Total: 14323.7MB allocated, 30050.0MB reserved, Mean: 3580.9MB allocated, 7512.5MB reserved
2025-03-25 02:26:27,825 - training - INFO - Epoch: 102/200000, Batch: 0/163, Loss: 2.7707, Throughput: 25.51 samples/sec
2025-03-25 02:27:03,282 - training - INFO - GPU Memory: GPU 0: 3580.2MB/7514.0MB, GPU 1: 3575.3MB/7522.0MB, GPU 2: 3587.7MB/7512.0MB, GPU 3: 3578.9MB/7512.0MB, Total: 14322.1MB allocated, 30060.0MB reserved, Mean: 3580.5MB allocated, 7515.0MB reserved
2025-03-25 02:27:03,283 - training - INFO - Epoch: 102/200000, Batch: 54/163, Loss: 3.0124, Throughput: 24.39 samples/sec
2025-03-25 02:27:39,040 - training - INFO - GPU Memory: GPU 0: 3583.6MB/7518.0MB, GPU 1: 3582.3MB/7520.0MB, GPU 2: 3582.6MB/7516.0MB, GPU 3: 3575.1MB/7512.0MB, Total: 14323.6MB allocated, 30066.0MB reserved, Mean: 3580.9MB allocated, 7516.5MB reserved
2025-03-25 02:27:39,041 - training - INFO - Epoch: 102/200000, Batch: 108/163, Loss: 2.9811, Throughput: 24.28 samples/sec
2025-03-25 02:28:14,473 - training - INFO - GPU Memory: GPU 0: 3563.9MB/7516.0MB, GPU 1: 3573.5MB/7524.0MB, GPU 2: 3569.6MB/7508.0MB, GPU 3: 3563.4MB/7504.0MB, Total: 14270.3MB allocated, 30052.0MB reserved, Mean: 3567.6MB allocated, 7513.0MB reserved
2025-03-25 02:28:14,473 - training - INFO - Epoch: 102/200000, Batch: 162/163, Loss: 2.9803, Throughput: 24.31 samples/sec
2025-03-25 02:28:14,474 - training - INFO - Epoch 102 completed in 107.28s. Average loss: 2.9465
2025-03-25 02:28:14,481 - training - INFO - Starting epoch 103/200000
2025-03-25 02:28:15,113 - training - INFO - GPU Memory: GPU 0: 3571.8MB/7516.0MB, GPU 1: 3584.1MB/7524.0MB, GPU 2: 3575.9MB/7508.0MB, GPU 3: 3574.2MB/7504.0MB, Total: 14306.1MB allocated, 30052.0MB reserved, Mean: 3576.5MB allocated, 7513.0MB reserved
2025-03-25 02:28:15,113 - training - INFO - Epoch: 103/200000, Batch: 0/163, Loss: 2.9705, Throughput: 25.33 samples/sec
2025-03-25 02:28:50,652 - training - INFO - GPU Memory: GPU 0: 3581.6MB/7516.0MB, GPU 1: 3584.5MB/7524.0MB, GPU 2: 3579.6MB/7504.0MB, GPU 3: 3577.8MB/7506.0MB, Total: 14323.5MB allocated, 30050.0MB reserved, Mean: 3580.9MB allocated, 7512.5MB reserved
2025-03-25 02:28:50,652 - training - INFO - Epoch: 103/200000, Batch: 54/163, Loss: 3.0315, Throughput: 24.33 samples/sec
2025-03-25 02:29:26,671 - training - INFO - GPU Memory: GPU 0: 3573.8MB/7512.0MB, GPU 1: 3580.2MB/7524.0MB, GPU 2: 3585.4MB/7524.0MB, GPU 3: 3575.7MB/7508.0MB, Total: 14315.1MB allocated, 30068.0MB reserved, Mean: 3578.8MB allocated, 7517.0MB reserved
2025-03-25 02:29:26,672 - training - INFO - Epoch: 103/200000, Batch: 108/163, Loss: 3.0569, Throughput: 24.16 samples/sec
2025-03-25 02:30:07,574 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_u4ge1dss.log
2025-03-25 02:30:07,574 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 02:30:07,574 - training - INFO - Device: cuda:0
2025-03-25 02:30:08,301 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 02:30:08,301 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 02:30:08,301 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 02:30:08,303 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 02:30:08,303 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 02:30:11,318 - training - INFO - Starting model initialization...
2025-03-25 02:30:20,550 - training - INFO - Per-GPU batch size: 8 (global batch size: 8)
2025-03-25 02:30:20,556 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-25 02:30:20,559 - training - INFO - Starting epoch 1/200000
2025-03-25 02:30:23,592 - training - ERROR - Critical error in training batch 0: shape '[8, 4096, 96]' is invalid for input of size 6144
2025-03-25 02:30:23,594 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 154, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 239, in forward
    text_with_visual_context = self.feature_attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 75, in forward
    k = k.contiguous().view(src_len, batch_size * self.num_heads, self.head_dim).transpose(0, 1)
RuntimeError: shape '[8, 4096, 96]' is invalid for input of size 6144

2025-03-25 02:30:23,594 - training - ERROR - Fatal error in training loop: shape '[8, 4096, 96]' is invalid for input of size 6144
2025-03-25 02:30:23,594 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 221, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 154, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 239, in forward
    text_with_visual_context = self.feature_attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 75, in forward
    k = k.contiguous().view(src_len, batch_size * self.num_heads, self.head_dim).transpose(0, 1)
RuntimeError: shape '[8, 4096, 96]' is invalid for input of size 6144

2025-03-25 02:30:23,594 - training - ERROR - Fatal error in main function: shape '[8, 4096, 96]' is invalid for input of size 6144
2025-03-25 02:30:23,594 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 675, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 322, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 221, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 154, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 239, in forward
    text_with_visual_context = self.feature_attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 75, in forward
    k = k.contiguous().view(src_len, batch_size * self.num_heads, self.head_dim).transpose(0, 1)
RuntimeError: shape '[8, 4096, 96]' is invalid for input of size 6144

2025-03-25 02:33:34,989 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_exgyz0l3.log
2025-03-25 02:33:34,989 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 02:33:34,989 - training - INFO - Device: cuda:0
2025-03-25 02:33:35,524 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 02:33:35,524 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 02:33:35,524 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 02:33:35,526 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 02:33:35,526 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 02:33:38,494 - training - INFO - Starting model initialization...
2025-03-25 02:33:47,359 - training - INFO - Per-GPU batch size: 8 (global batch size: 8)
2025-03-25 02:33:47,365 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-25 02:33:47,369 - training - INFO - Starting epoch 1/200000
2025-03-25 02:33:50,711 - training - INFO - GPU Memory: GPU 0: 2279.1MB/6660.0MB, Total: 2279.1MB allocated, 6660.0MB reserved, Mean: 2279.1MB allocated, 6660.0MB reserved
2025-03-25 02:33:50,712 - training - INFO - Epoch: 1/200000, Batch: 0/326, Loss: 11.2091, Throughput: 2.39 samples/sec
2025-03-25 02:33:55,118 - training - ERROR - Critical error in training batch 14: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 0; 10.75 GiB total capacity; 9.19 GiB already allocated; 213.81 MiB free; 9.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 02:33:55,120 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 168, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 0; 10.75 GiB total capacity; 9.19 GiB already allocated; 213.81 MiB free; 9.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 02:33:55,120 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 0; 10.75 GiB total capacity; 9.19 GiB already allocated; 213.81 MiB free; 9.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 02:33:55,120 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 221, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 168, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 0; 10.75 GiB total capacity; 9.19 GiB already allocated; 213.81 MiB free; 9.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 02:33:55,120 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 0; 10.75 GiB total capacity; 9.19 GiB already allocated; 213.81 MiB free; 9.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 02:33:55,120 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 675, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 322, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 221, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 168, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 0; 10.75 GiB total capacity; 9.19 GiB already allocated; 213.81 MiB free; 9.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 02:37:05,861 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_f_cb8x7a.log
2025-03-25 02:37:05,861 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 02:37:05,861 - training - INFO - Device: cuda:0
2025-03-25 02:37:06,452 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 02:37:06,452 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 02:37:06,452 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 02:37:06,454 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 02:37:06,454 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 02:37:09,477 - training - INFO - Starting model initialization...
2025-03-25 02:37:18,455 - training - INFO - Per-GPU batch size: 6 (global batch size: 6)
2025-03-25 02:37:18,461 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-25 02:37:18,465 - training - INFO - Starting epoch 1/200000
2025-03-25 02:37:21,470 - training - INFO - GPU Memory: GPU 0: 2232.5MB/5710.0MB, Total: 2232.5MB allocated, 5710.0MB reserved, Mean: 2232.5MB allocated, 5710.0MB reserved
2025-03-25 02:37:21,470 - training - INFO - Epoch: 1/200000, Batch: 0/434, Loss: 10.9127, Throughput: 2.00 samples/sec
2025-03-25 02:37:53,296 - training - INFO - GPU Memory: GPU 0: 3539.7MB/8610.0MB, Total: 3539.7MB allocated, 8610.0MB reserved, Mean: 3539.7MB allocated, 8610.0MB reserved
2025-03-25 02:37:53,296 - training - INFO - Epoch: 1/200000, Batch: 144/434, Loss: 6.9967, Throughput: 24.98 samples/sec
2025-03-25 02:38:25,128 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error__y6vjtb3.log
2025-03-25 02:38:25,129 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 8
2025-03-25 02:38:25,279 - training - INFO - Device: cuda:0
2025-03-25 02:38:26,178 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 02:38:26,178 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 02:38:26,178 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 02:38:26,178 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 02:38:26,178 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 02:38:30,291 - training - INFO - Starting model initialization...
2025-03-25 02:38:41,173 - training - INFO - Per-GPU batch size: 6 (global batch size: 48)
2025-03-25 02:38:41,183 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-25 02:38:41,188 - training - INFO - Starting epoch 1/200000
2025-03-25 02:38:46,022 - training - INFO - GPU Memory: GPU 0: 2234.4MB/5708.0MB, GPU 1: 2233.9MB/5730.0MB, GPU 2: 2233.9MB/5730.0MB, GPU 3: 2234.4MB/5708.0MB, GPU 4: 2233.9MB/5730.0MB, GPU 5: 2233.9MB/5730.0MB, GPU 6: 2233.9MB/5730.0MB, GPU 7: 2233.9MB/5730.0MB, Total: 17872.0MB allocated, 45796.0MB reserved, Mean: 2234.0MB allocated, 5724.5MB reserved
2025-03-25 02:38:46,023 - training - INFO - Epoch: 1/200000, Batch: 0/55, Loss: 10.9663, Throughput: 9.93 samples/sec
2025-03-25 02:38:59,189 - training - INFO - GPU Memory: GPU 0: 3549.5MB/8584.0MB, GPU 1: 3546.5MB/8582.0MB, GPU 2: 3545.4MB/8588.0MB, GPU 3: 3545.3MB/8584.0MB, GPU 4: 3545.4MB/8582.0MB, GPU 5: 3546.5MB/8582.0MB, GPU 6: 3545.0MB/8582.0MB, GPU 7: 3543.5MB/8588.0MB, Total: 28367.2MB allocated, 68672.0MB reserved, Mean: 3545.9MB allocated, 8584.0MB reserved
2025-03-25 02:38:59,189 - training - INFO - Epoch: 1/200000, Batch: 18/55, Loss: 9.8643, Throughput: 50.66 samples/sec
2025-03-25 02:39:12,813 - training - INFO - GPU Memory: GPU 0: 3548.1MB/8584.0MB, GPU 1: 3545.9MB/8582.0MB, GPU 2: 3543.8MB/8588.0MB, GPU 3: 3548.1MB/8584.0MB, GPU 4: 3545.2MB/8582.0MB, GPU 5: 3544.9MB/8582.0MB, GPU 6: 3544.9MB/8582.0MB, GPU 7: 3544.2MB/8588.0MB, Total: 28365.1MB allocated, 68672.0MB reserved, Mean: 3545.6MB allocated, 8584.0MB reserved
2025-03-25 02:39:12,816 - training - INFO - Epoch: 1/200000, Batch: 36/55, Loss: 8.8143, Throughput: 56.16 samples/sec
2025-03-25 02:39:27,117 - training - INFO - GPU Memory: GPU 0: 3502.7MB/6024.0MB, GPU 1: 3501.3MB/6140.0MB, GPU 2: 3497.3MB/5842.0MB, GPU 3: 3499.7MB/5868.0MB, GPU 4: 3503.3MB/6034.0MB, GPU 5: 3499.6MB/5910.0MB, GPU 6: 3500.5MB/5874.0MB, GPU 7: 3496.0MB/5842.0MB, Total: 28000.4MB allocated, 47534.0MB reserved, Mean: 3500.0MB allocated, 5941.8MB reserved
2025-03-25 02:39:27,117 - training - INFO - Epoch: 1/200000, Batch: 54/55, Loss: 8.2002, Throughput: 57.48 samples/sec
2025-03-25 02:39:27,118 - training - INFO - Epoch 1 completed in 45.93s. Average loss: 8.1978
2025-03-25 02:39:27,129 - training - INFO - Starting epoch 2/200000
2025-03-25 02:39:27,908 - training - INFO - GPU Memory: GPU 0: 3544.2MB/8186.0MB, GPU 1: 3542.3MB/8176.0MB, GPU 2: 3542.3MB/8240.0MB, GPU 3: 3542.6MB/8196.0MB, GPU 4: 3542.0MB/8240.0MB, GPU 5: 3541.2MB/8246.0MB, GPU 6: 3539.1MB/8184.0MB, GPU 7: 3541.9MB/8184.0MB, Total: 28335.5MB allocated, 65652.0MB reserved, Mean: 3541.9MB allocated, 8206.5MB reserved
2025-03-25 02:39:27,908 - training - INFO - Epoch: 2/200000, Batch: 0/55, Loss: 6.7012, Throughput: 61.62 samples/sec
2025-03-25 02:39:41,011 - training - INFO - GPU Memory: GPU 0: 3545.3MB/8618.0MB, GPU 1: 3540.9MB/8606.0MB, GPU 2: 3542.0MB/8610.0MB, GPU 3: 3539.9MB/8632.0MB, GPU 4: 3541.0MB/8624.0MB, GPU 5: 3542.8MB/8626.0MB, GPU 6: 3539.3MB/8622.0MB, GPU 7: 3540.8MB/8612.0MB, Total: 28332.0MB allocated, 68950.0MB reserved, Mean: 3541.5MB allocated, 8618.8MB reserved
2025-03-25 02:39:41,011 - training - INFO - Epoch: 2/200000, Batch: 18/55, Loss: 6.5844, Throughput: 65.70 samples/sec
2025-03-25 02:39:54,449 - training - INFO - GPU Memory: GPU 0: 3544.7MB/8618.0MB, GPU 1: 3540.2MB/8606.0MB, GPU 2: 3541.2MB/8610.0MB, GPU 3: 3539.8MB/8632.0MB, GPU 4: 3540.5MB/8624.0MB, GPU 5: 3542.3MB/8626.0MB, GPU 6: 3538.7MB/8622.0MB, GPU 7: 3540.2MB/8612.0MB, Total: 28327.6MB allocated, 68950.0MB reserved, Mean: 3541.0MB allocated, 8618.8MB reserved
2025-03-25 02:39:54,451 - training - INFO - Epoch: 2/200000, Batch: 36/55, Loss: 6.3723, Throughput: 65.01 samples/sec
2025-03-25 02:40:07,500 - training - INFO - GPU Memory: GPU 0: 3504.1MB/8642.0MB, GPU 1: 3499.1MB/8642.0MB, GPU 2: 3499.2MB/8640.0MB, GPU 3: 3500.6MB/8668.0MB, GPU 4: 3500.1MB/8642.0MB, GPU 5: 3499.9MB/8644.0MB, GPU 6: 3502.7MB/8636.0MB, GPU 7: 3498.0MB/8642.0MB, Total: 28003.7MB allocated, 69156.0MB reserved, Mean: 3500.5MB allocated, 8644.5MB reserved
2025-03-25 02:40:07,500 - training - INFO - Epoch: 2/200000, Batch: 54/55, Loss: 6.2007, Throughput: 65.39 samples/sec
2025-03-25 02:40:07,501 - training - INFO - Epoch 2 completed in 40.37s. Average loss: 6.2274
2025-03-25 02:40:07,510 - training - INFO - Starting epoch 3/200000
2025-03-25 02:40:08,273 - training - INFO - GPU Memory: GPU 0: 3545.7MB/8642.0MB, GPU 1: 3540.6MB/8642.0MB, GPU 2: 3540.7MB/8640.0MB, GPU 3: 3543.2MB/8668.0MB, GPU 4: 3541.6MB/8642.0MB, GPU 5: 3540.1MB/8644.0MB, GPU 6: 3544.5MB/8636.0MB, GPU 7: 3539.6MB/8642.0MB, Total: 28336.0MB allocated, 69156.0MB reserved, Mean: 3542.0MB allocated, 8644.5MB reserved
2025-03-25 02:40:08,273 - training - INFO - Epoch: 3/200000, Batch: 0/55, Loss: 5.8567, Throughput: 62.93 samples/sec
2025-03-25 02:40:21,526 - training - INFO - GPU Memory: GPU 0: 3543.0MB/8614.0MB, GPU 1: 3540.1MB/8612.0MB, GPU 2: 3543.9MB/8610.0MB, GPU 3: 3541.4MB/8632.0MB, GPU 4: 3543.5MB/8604.0MB, GPU 5: 3543.0MB/8620.0MB, GPU 6: 3544.3MB/8598.0MB, GPU 7: 3542.9MB/8608.0MB, Total: 28342.0MB allocated, 68898.0MB reserved, Mean: 3542.8MB allocated, 8612.2MB reserved
2025-03-25 02:40:21,527 - training - INFO - Epoch: 3/200000, Batch: 18/55, Loss: 5.7635, Throughput: 65.07 samples/sec
2025-03-25 02:40:35,001 - training - INFO - GPU Memory: GPU 0: 3545.4MB/8614.0MB, GPU 1: 3540.5MB/8612.0MB, GPU 2: 3543.3MB/8610.0MB, GPU 3: 3540.8MB/8632.0MB, GPU 4: 3542.9MB/8604.0MB, GPU 5: 3542.3MB/8620.0MB, GPU 6: 3541.7MB/8598.0MB, GPU 7: 3542.3MB/8608.0MB, Total: 28339.3MB allocated, 68898.0MB reserved, Mean: 3542.4MB allocated, 8612.2MB reserved
2025-03-25 02:40:35,001 - training - INFO - Epoch: 3/200000, Batch: 36/55, Loss: 5.6724, Throughput: 64.60 samples/sec
2025-03-25 02:40:48,048 - training - INFO - GPU Memory: GPU 0: 3503.9MB/8634.0MB, GPU 1: 3502.0MB/8644.0MB, GPU 2: 3499.5MB/8630.0MB, GPU 3: 3500.1MB/8644.0MB, GPU 4: 3502.4MB/8638.0MB, GPU 5: 3504.0MB/8652.0MB, GPU 6: 3500.2MB/8642.0MB, GPU 7: 3499.9MB/8644.0MB, Total: 28012.2MB allocated, 69128.0MB reserved, Mean: 3501.5MB allocated, 8641.0MB reserved
2025-03-25 02:40:48,049 - training - INFO - Epoch: 3/200000, Batch: 54/55, Loss: 5.6271, Throughput: 65.12 samples/sec
2025-03-25 02:40:48,049 - training - INFO - Epoch 3 completed in 40.54s. Average loss: 5.6249
2025-03-25 02:40:48,055 - training - INFO - Starting epoch 4/200000
2025-03-25 02:40:48,823 - training - INFO - GPU Memory: GPU 0: 3546.5MB/8634.0MB, GPU 1: 3543.6MB/8644.0MB, GPU 2: 3541.3MB/8630.0MB, GPU 3: 3542.2MB/8644.0MB, GPU 4: 3544.3MB/8638.0MB, GPU 5: 3544.8MB/8652.0MB, GPU 6: 3542.0MB/8642.0MB, GPU 7: 3542.6MB/8644.0MB, Total: 28347.3MB allocated, 69128.0MB reserved, Mean: 3543.4MB allocated, 8641.0MB reserved
2025-03-25 02:40:48,823 - training - INFO - Epoch: 4/200000, Batch: 0/55, Loss: 5.3412, Throughput: 62.57 samples/sec
2025-03-25 02:41:02,060 - training - INFO - GPU Memory: GPU 0: 3544.0MB/8630.0MB, GPU 1: 3544.3MB/8610.0MB, GPU 2: 3544.8MB/8600.0MB, GPU 3: 3542.3MB/8618.0MB, GPU 4: 3544.9MB/8600.0MB, GPU 5: 3545.8MB/8596.0MB, GPU 6: 3544.8MB/8602.0MB, GPU 7: 3542.3MB/8602.0MB, Total: 28353.3MB allocated, 68858.0MB reserved, Mean: 3544.2MB allocated, 8607.2MB reserved
2025-03-25 02:41:02,061 - training - INFO - Epoch: 4/200000, Batch: 18/55, Loss: 5.4457, Throughput: 65.12 samples/sec
2025-03-25 02:41:15,442 - training - INFO - GPU Memory: GPU 0: 3543.7MB/8630.0MB, GPU 1: 3544.0MB/8610.0MB, GPU 2: 3544.0MB/8600.0MB, GPU 3: 3541.7MB/8618.0MB, GPU 4: 3543.8MB/8600.0MB, GPU 5: 3545.2MB/8596.0MB, GPU 6: 3544.5MB/8602.0MB, GPU 7: 3541.7MB/8602.0MB, Total: 28348.6MB allocated, 68858.0MB reserved, Mean: 3543.6MB allocated, 8607.2MB reserved
2025-03-25 02:41:15,443 - training - INFO - Epoch: 4/200000, Batch: 36/55, Loss: 5.3756, Throughput: 64.85 samples/sec
2025-03-25 02:41:28,407 - training - INFO - GPU Memory: GPU 0: 3503.0MB/8640.0MB, GPU 1: 3503.7MB/8634.0MB, GPU 2: 3502.3MB/8632.0MB, GPU 3: 3504.6MB/8638.0MB, GPU 4: 3501.1MB/8648.0MB, GPU 5: 3502.8MB/8636.0MB, GPU 6: 3507.6MB/8636.0MB, GPU 7: 3499.3MB/8650.0MB, Total: 28024.5MB allocated, 69114.0MB reserved, Mean: 3503.1MB allocated, 8639.2MB reserved
2025-03-25 02:41:28,407 - training - INFO - Epoch: 4/200000, Batch: 54/55, Loss: 5.3336, Throughput: 65.42 samples/sec
2025-03-25 02:41:28,408 - training - INFO - Epoch 4 completed in 40.35s. Average loss: 5.3395
2025-03-25 02:41:28,413 - training - INFO - Starting epoch 5/200000
2025-03-25 02:41:29,173 - training - INFO - GPU Memory: GPU 0: 3544.5MB/8640.0MB, GPU 1: 3545.0MB/8634.0MB, GPU 2: 3544.0MB/8632.0MB, GPU 3: 3546.9MB/8638.0MB, GPU 4: 3542.7MB/8648.0MB, GPU 5: 3544.1MB/8636.0MB, GPU 6: 3548.3MB/8636.0MB, GPU 7: 3540.8MB/8650.0MB, Total: 28356.3MB allocated, 69114.0MB reserved, Mean: 3544.5MB allocated, 8639.2MB reserved
2025-03-25 02:41:29,173 - training - INFO - Epoch: 5/200000, Batch: 0/55, Loss: 4.9966, Throughput: 63.19 samples/sec
2025-03-25 02:41:42,349 - training - INFO - GPU Memory: GPU 0: 3546.8MB/8600.0MB, GPU 1: 3545.2MB/8604.0MB, GPU 2: 3547.6MB/8598.0MB, GPU 3: 3547.0MB/8622.0MB, GPU 4: 3543.6MB/8618.0MB, GPU 5: 3546.1MB/8604.0MB, GPU 6: 3544.0MB/8616.0MB, GPU 7: 3545.3MB/8598.0MB, Total: 28365.4MB allocated, 68860.0MB reserved, Mean: 3545.7MB allocated, 8607.5MB reserved
2025-03-25 02:41:42,349 - training - INFO - Epoch: 5/200000, Batch: 18/55, Loss: 5.2253, Throughput: 65.44 samples/sec
2025-03-25 02:41:55,784 - training - INFO - GPU Memory: GPU 0: 3546.0MB/8600.0MB, GPU 1: 3544.8MB/8604.0MB, GPU 2: 3547.0MB/8598.0MB, GPU 3: 3546.6MB/8622.0MB, GPU 4: 3544.1MB/8618.0MB, GPU 5: 3545.7MB/8604.0MB, GPU 6: 3544.6MB/8616.0MB, GPU 7: 3545.3MB/8598.0MB, Total: 28364.1MB allocated, 68860.0MB reserved, Mean: 3545.5MB allocated, 8607.5MB reserved
2025-03-25 02:41:55,784 - training - INFO - Epoch: 5/200000, Batch: 36/55, Loss: 5.2021, Throughput: 64.89 samples/sec
2025-03-25 02:42:08,813 - training - INFO - GPU Memory: GPU 0: 3504.1MB/8658.0MB, GPU 1: 3501.8MB/8652.0MB, GPU 2: 3507.5MB/8636.0MB, GPU 3: 3506.3MB/8634.0MB, GPU 4: 3505.5MB/8632.0MB, GPU 5: 3503.5MB/8634.0MB, GPU 6: 3505.5MB/8656.0MB, GPU 7: 3501.3MB/8652.0MB, Total: 28035.4MB allocated, 69154.0MB reserved, Mean: 3504.4MB allocated, 8644.2MB reserved
2025-03-25 02:42:08,813 - training - INFO - Epoch: 5/200000, Batch: 54/55, Loss: 5.1940, Throughput: 65.35 samples/sec
2025-03-25 02:42:08,814 - training - INFO - Epoch 5 completed in 40.40s. Average loss: 5.1904
2025-03-25 02:42:08,820 - training - INFO - Starting epoch 6/200000
2025-03-25 02:42:09,556 - training - INFO - GPU Memory: GPU 0: 3546.2MB/8658.0MB, GPU 1: 3547.0MB/8652.0MB, GPU 2: 3548.7MB/8636.0MB, GPU 3: 3547.6MB/8634.0MB, GPU 4: 3547.0MB/8632.0MB, GPU 5: 3545.5MB/8634.0MB, GPU 6: 3546.6MB/8656.0MB, GPU 7: 3542.6MB/8652.0MB, Total: 28371.2MB allocated, 69154.0MB reserved, Mean: 3546.4MB allocated, 8644.2MB reserved
2025-03-25 02:42:09,557 - training - INFO - Epoch: 6/200000, Batch: 0/55, Loss: 4.8791, Throughput: 65.19 samples/sec
2025-03-25 02:42:22,799 - training - INFO - GPU Memory: GPU 0: 3546.8MB/8608.0MB, GPU 1: 3548.8MB/8606.0MB, GPU 2: 3545.7MB/8602.0MB, GPU 3: 3547.1MB/8610.0MB, GPU 4: 3542.2MB/8606.0MB, GPU 5: 3544.6MB/8616.0MB, GPU 6: 3546.4MB/8606.0MB, GPU 7: 3543.4MB/8608.0MB, Total: 28365.0MB allocated, 68862.0MB reserved, Mean: 3545.6MB allocated, 8607.8MB reserved
2025-03-25 02:42:22,800 - training - INFO - Epoch: 6/200000, Batch: 18/55, Loss: 5.1145, Throughput: 65.24 samples/sec
2025-03-25 02:42:36,313 - training - INFO - GPU Memory: GPU 0: 3546.1MB/8608.0MB, GPU 1: 3548.2MB/8606.0MB, GPU 2: 3545.1MB/8602.0MB, GPU 3: 3547.0MB/8610.0MB, GPU 4: 3544.0MB/8606.0MB, GPU 5: 3544.0MB/8616.0MB, GPU 6: 3546.1MB/8606.0MB, GPU 7: 3542.8MB/8608.0MB, Total: 28363.3MB allocated, 68862.0MB reserved, Mean: 3545.4MB allocated, 8607.8MB reserved
2025-03-25 02:42:36,314 - training - INFO - Epoch: 6/200000, Batch: 36/55, Loss: 5.0881, Throughput: 64.60 samples/sec
2025-03-25 02:42:49,252 - training - INFO - GPU Memory: GPU 0: 3504.0MB/8640.0MB, GPU 1: 3505.5MB/8632.0MB, GPU 2: 3501.0MB/8632.0MB, GPU 3: 3504.4MB/8646.0MB, GPU 4: 3502.3MB/8644.0MB, GPU 5: 3506.0MB/8646.0MB, GPU 6: 3506.7MB/8640.0MB, GPU 7: 3503.4MB/8654.0MB, Total: 28033.3MB allocated, 69134.0MB reserved, Mean: 3504.2MB allocated, 8641.8MB reserved
2025-03-25 02:42:49,253 - training - INFO - Epoch: 6/200000, Batch: 54/55, Loss: 5.0844, Throughput: 65.30 samples/sec
2025-03-25 02:42:49,253 - training - INFO - Epoch 6 completed in 40.43s. Average loss: 5.0908
2025-03-25 02:42:49,264 - training - INFO - Starting epoch 7/200000
2025-03-25 02:42:49,999 - training - INFO - GPU Memory: GPU 0: 3545.0MB/8640.0MB, GPU 1: 3547.2MB/8632.0MB, GPU 2: 3542.7MB/8632.0MB, GPU 3: 3547.5MB/8646.0MB, GPU 4: 3548.1MB/8644.0MB, GPU 5: 3546.6MB/8646.0MB, GPU 6: 3547.1MB/8640.0MB, GPU 7: 3543.8MB/8654.0MB, Total: 28368.0MB allocated, 69134.0MB reserved, Mean: 3546.0MB allocated, 8641.8MB reserved
2025-03-25 02:42:50,000 - training - INFO - Epoch: 7/200000, Batch: 0/55, Loss: 4.8365, Throughput: 65.29 samples/sec
2025-03-25 02:43:03,089 - training - INFO - GPU Memory: GPU 0: 3546.9MB/8604.0MB, GPU 1: 3546.5MB/8620.0MB, GPU 2: 3546.8MB/8616.0MB, GPU 3: 3546.9MB/8614.0MB, GPU 4: 3545.0MB/8600.0MB, GPU 5: 3543.4MB/8584.0MB, GPU 6: 3545.7MB/8626.0MB, GPU 7: 3545.9MB/8596.0MB, Total: 28367.0MB allocated, 68860.0MB reserved, Mean: 3545.9MB allocated, 8607.5MB reserved
2025-03-25 02:43:03,089 - training - INFO - Epoch: 7/200000, Batch: 18/55, Loss: 5.0540, Throughput: 65.97 samples/sec
2025-03-25 02:43:16,465 - training - INFO - GPU Memory: GPU 0: 3544.8MB/8604.0MB, GPU 1: 3546.5MB/8620.0MB, GPU 2: 3546.2MB/8616.0MB, GPU 3: 3546.3MB/8614.0MB, GPU 4: 3544.7MB/8600.0MB, GPU 5: 3542.8MB/8584.0MB, GPU 6: 3545.1MB/8626.0MB, GPU 7: 3545.3MB/8596.0MB, Total: 28361.7MB allocated, 68860.0MB reserved, Mean: 3545.2MB allocated, 8607.5MB reserved
2025-03-25 02:43:16,465 - training - INFO - Epoch: 7/200000, Batch: 36/55, Loss: 5.0439, Throughput: 65.29 samples/sec
2025-03-25 02:43:29,413 - training - INFO - GPU Memory: GPU 0: 3503.9MB/8652.0MB, GPU 1: 3505.8MB/8634.0MB, GPU 2: 3503.0MB/8630.0MB, GPU 3: 3504.3MB/8662.0MB, GPU 4: 3500.8MB/8634.0MB, GPU 5: 3503.4MB/8646.0MB, GPU 6: 3507.0MB/8636.0MB, GPU 7: 3502.8MB/8630.0MB, Total: 28031.0MB allocated, 69124.0MB reserved, Mean: 3503.9MB allocated, 8640.5MB reserved
2025-03-25 02:43:29,413 - training - INFO - Epoch: 7/200000, Batch: 54/55, Loss: 5.0009, Throughput: 65.76 samples/sec
2025-03-25 02:43:29,414 - training - INFO - Epoch 7 completed in 40.15s. Average loss: 5.0182
2025-03-25 02:43:29,424 - training - INFO - Starting epoch 8/200000
2025-03-25 02:43:30,173 - training - INFO - GPU Memory: GPU 0: 3547.2MB/8652.0MB, GPU 1: 3546.5MB/8634.0MB, GPU 2: 3544.3MB/8630.0MB, GPU 3: 3546.2MB/8662.0MB, GPU 4: 3542.1MB/8634.0MB, GPU 5: 3544.8MB/8646.0MB, GPU 6: 3547.5MB/8636.0MB, GPU 7: 3544.3MB/8630.0MB, Total: 28362.9MB allocated, 69124.0MB reserved, Mean: 3545.4MB allocated, 8640.5MB reserved
2025-03-25 02:43:30,174 - training - INFO - Epoch: 8/200000, Batch: 0/55, Loss: 4.8707, Throughput: 64.03 samples/sec
2025-03-25 02:43:43,305 - training - INFO - GPU Memory: GPU 0: 3545.9MB/8616.0MB, GPU 1: 3545.3MB/8618.0MB, GPU 2: 3547.1MB/8598.0MB, GPU 3: 3546.4MB/8630.0MB, GPU 4: 3543.3MB/8614.0MB, GPU 5: 3544.4MB/8620.0MB, GPU 6: 3544.1MB/8618.0MB, GPU 7: 3545.4MB/8596.0MB, Total: 28362.0MB allocated, 68910.0MB reserved, Mean: 3545.2MB allocated, 8613.8MB reserved
2025-03-25 02:43:43,305 - training - INFO - Epoch: 8/200000, Batch: 18/55, Loss: 5.0853, Throughput: 65.70 samples/sec
2025-03-25 02:43:56,791 - training - INFO - GPU Memory: GPU 0: 3545.3MB/8616.0MB, GPU 1: 3544.8MB/8618.0MB, GPU 2: 3546.5MB/8598.0MB, GPU 3: 3545.6MB/8630.0MB, GPU 4: 3542.5MB/8614.0MB, GPU 5: 3543.8MB/8620.0MB, GPU 6: 3543.5MB/8618.0MB, GPU 7: 3544.8MB/8596.0MB, Total: 28356.8MB allocated, 68910.0MB reserved, Mean: 3544.6MB allocated, 8613.8MB reserved
2025-03-25 02:43:56,791 - training - INFO - Epoch: 8/200000, Batch: 36/55, Loss: 5.0457, Throughput: 64.90 samples/sec
2025-03-25 02:44:09,774 - training - INFO - GPU Memory: GPU 0: 3502.9MB/8642.0MB, GPU 1: 3505.4MB/8636.0MB, GPU 2: 3502.5MB/8644.0MB, GPU 3: 3504.4MB/8638.0MB, GPU 4: 3503.8MB/8630.0MB, GPU 5: 3503.2MB/8650.0MB, GPU 6: 3505.7MB/8644.0MB, GPU 7: 3505.0MB/8632.0MB, Total: 28032.8MB allocated, 69116.0MB reserved, Mean: 3504.1MB allocated, 8639.5MB reserved
2025-03-25 02:44:09,774 - training - INFO - Epoch: 8/200000, Batch: 54/55, Loss: 5.0045, Throughput: 65.43 samples/sec
2025-03-25 02:44:09,775 - training - INFO - Epoch 8 completed in 40.35s. Average loss: 4.9581
2025-03-25 02:44:09,785 - training - INFO - Starting epoch 9/200000
2025-03-25 02:44:10,543 - training - INFO - GPU Memory: GPU 0: 3545.1MB/8642.0MB, GPU 1: 3547.0MB/8636.0MB, GPU 2: 3544.5MB/8644.0MB, GPU 3: 3547.1MB/8638.0MB, GPU 4: 3545.4MB/8630.0MB, GPU 5: 3543.6MB/8650.0MB, GPU 6: 3547.3MB/8644.0MB, GPU 7: 3546.0MB/8632.0MB, Total: 28366.0MB allocated, 69116.0MB reserved, Mean: 3545.7MB allocated, 8639.5MB reserved
2025-03-25 02:44:10,544 - training - INFO - Epoch: 9/200000, Batch: 0/55, Loss: 4.9781, Throughput: 63.28 samples/sec
2025-03-25 02:44:23,692 - training - INFO - GPU Memory: GPU 0: 3546.8MB/8618.0MB, GPU 1: 3547.5MB/8600.0MB, GPU 2: 3545.7MB/8610.0MB, GPU 3: 3545.0MB/8626.0MB, GPU 4: 3544.1MB/8614.0MB, GPU 5: 3546.8MB/8604.0MB, GPU 6: 3546.5MB/8614.0MB, GPU 7: 3548.0MB/8600.0MB, Total: 28370.4MB allocated, 68886.0MB reserved, Mean: 3546.3MB allocated, 8610.8MB reserved
2025-03-25 02:44:23,692 - training - INFO - Epoch: 9/200000, Batch: 18/55, Loss: 4.9558, Throughput: 65.58 samples/sec
2025-03-25 02:44:37,147 - training - INFO - GPU Memory: GPU 0: 3546.0MB/8618.0MB, GPU 1: 3546.7MB/8600.0MB, GPU 2: 3544.8MB/8610.0MB, GPU 3: 3544.6MB/8626.0MB, GPU 4: 3544.1MB/8614.0MB, GPU 5: 3546.3MB/8604.0MB, GPU 6: 3546.2MB/8614.0MB, GPU 7: 3547.4MB/8600.0MB, Total: 28366.1MB allocated, 68886.0MB reserved, Mean: 3545.8MB allocated, 8610.8MB reserved
2025-03-25 02:44:37,147 - training - INFO - Epoch: 9/200000, Batch: 36/55, Loss: 4.9109, Throughput: 64.91 samples/sec
2025-03-25 02:44:50,185 - training - INFO - GPU Memory: GPU 0: 3505.3MB/8658.0MB, GPU 1: 3505.0MB/8638.0MB, GPU 2: 3502.9MB/8624.0MB, GPU 3: 3504.4MB/8636.0MB, GPU 4: 3503.9MB/8646.0MB, GPU 5: 3504.7MB/8648.0MB, GPU 6: 3503.7MB/8646.0MB, GPU 7: 3506.6MB/8640.0MB, Total: 28036.4MB allocated, 69136.0MB reserved, Mean: 3504.5MB allocated, 8642.0MB reserved
2025-03-25 02:44:50,185 - training - INFO - Epoch: 9/200000, Batch: 54/55, Loss: 4.9033, Throughput: 65.35 samples/sec
2025-03-25 02:44:50,186 - training - INFO - Epoch 9 completed in 40.40s. Average loss: 4.8843
2025-03-25 02:44:50,196 - training - INFO - Starting epoch 10/200000
2025-03-25 02:44:50,946 - training - INFO - GPU Memory: GPU 0: 3545.9MB/8658.0MB, GPU 1: 3546.5MB/8638.0MB, GPU 2: 3544.3MB/8624.0MB, GPU 3: 3544.2MB/8636.0MB, GPU 4: 3545.2MB/8646.0MB, GPU 5: 3546.0MB/8648.0MB, GPU 6: 3545.3MB/8646.0MB, GPU 7: 3547.8MB/8640.0MB, Total: 28365.2MB allocated, 69136.0MB reserved, Mean: 3545.7MB allocated, 8642.0MB reserved
2025-03-25 02:44:50,947 - training - INFO - Epoch: 10/200000, Batch: 0/55, Loss: 4.7855, Throughput: 63.99 samples/sec
2025-03-25 02:45:04,096 - training - INFO - GPU Memory: GPU 0: 3544.6MB/8618.0MB, GPU 1: 3546.7MB/8618.0MB, GPU 2: 3543.8MB/8606.0MB, GPU 3: 3546.1MB/8596.0MB, GPU 4: 3546.8MB/8598.0MB, GPU 5: 3546.4MB/8584.0MB, GPU 6: 3548.1MB/8612.0MB, GPU 7: 3544.1MB/8618.0MB, Total: 28366.5MB allocated, 68850.0MB reserved, Mean: 3545.8MB allocated, 8606.2MB reserved
2025-03-25 02:45:04,096 - training - INFO - Epoch: 10/200000, Batch: 18/55, Loss: 4.8058, Throughput: 65.61 samples/sec
2025-03-25 02:45:17,418 - training - INFO - GPU Memory: GPU 0: 3544.0MB/8618.0MB, GPU 1: 3545.8MB/8618.0MB, GPU 2: 3543.0MB/8606.0MB, GPU 3: 3545.5MB/8596.0MB, GPU 4: 3546.5MB/8598.0MB, GPU 5: 3545.8MB/8584.0MB, GPU 6: 3547.5MB/8612.0MB, GPU 7: 3544.1MB/8618.0MB, Total: 28362.2MB allocated, 68850.0MB reserved, Mean: 3545.3MB allocated, 8606.2MB reserved
2025-03-25 02:45:17,418 - training - INFO - Epoch: 10/200000, Batch: 36/55, Loss: 4.8027, Throughput: 65.24 samples/sec
2025-03-25 02:45:30,299 - training - INFO - GPU Memory: GPU 0: 3504.6MB/8658.0MB, GPU 1: 3503.6MB/8656.0MB, GPU 2: 3504.1MB/8636.0MB, GPU 3: 3503.7MB/8644.0MB, GPU 4: 3504.5MB/8644.0MB, GPU 5: 3503.5MB/8650.0MB, GPU 6: 3508.0MB/8640.0MB, GPU 7: 3503.0MB/8650.0MB, Total: 28035.0MB allocated, 69178.0MB reserved, Mean: 3504.4MB allocated, 8647.2MB reserved
2025-03-25 02:45:30,299 - training - INFO - Epoch: 10/200000, Batch: 54/55, Loss: 4.8159, Throughput: 65.83 samples/sec
2025-03-25 02:45:30,300 - training - INFO - Epoch 10 completed in 40.10s. Average loss: 4.8288
2025-03-25 02:45:30,306 - training - INFO - Starting epoch 11/200000
2025-03-25 02:45:31,075 - training - INFO - GPU Memory: GPU 0: 3546.5MB/8658.0MB, GPU 1: 3544.9MB/8656.0MB, GPU 2: 3545.6MB/8636.0MB, GPU 3: 3546.5MB/8644.0MB, GPU 4: 3545.8MB/8644.0MB, GPU 5: 3545.0MB/8650.0MB, GPU 6: 3549.0MB/8640.0MB, GPU 7: 3543.4MB/8650.0MB, Total: 28366.7MB allocated, 69178.0MB reserved, Mean: 3545.8MB allocated, 8647.2MB reserved
2025-03-25 02:45:31,076 - training - INFO - Epoch: 11/200000, Batch: 0/55, Loss: 4.9507, Throughput: 62.41 samples/sec
2025-03-25 02:45:44,180 - training - INFO - GPU Memory: GPU 0: 3545.9MB/8618.0MB, GPU 1: 3543.7MB/8602.0MB, GPU 2: 3546.7MB/8618.0MB, GPU 3: 3548.2MB/8614.0MB, GPU 4: 3547.1MB/8596.0MB, GPU 5: 3546.6MB/8610.0MB, GPU 6: 3544.8MB/8608.0MB, GPU 7: 3545.7MB/8596.0MB, Total: 28368.7MB allocated, 68862.0MB reserved, Mean: 3546.1MB allocated, 8607.8MB reserved
2025-03-25 02:45:44,180 - training - INFO - Epoch: 11/200000, Batch: 18/55, Loss: 4.7018, Throughput: 65.73 samples/sec
2025-03-25 02:45:57,564 - training - INFO - GPU Memory: GPU 0: 3545.3MB/8618.0MB, GPU 1: 3543.1MB/8602.0MB, GPU 2: 3545.8MB/8618.0MB, GPU 3: 3547.6MB/8614.0MB, GPU 4: 3546.5MB/8596.0MB, GPU 5: 3546.0MB/8610.0MB, GPU 6: 3544.3MB/8608.0MB, GPU 7: 3545.6MB/8596.0MB, Total: 28364.3MB allocated, 68862.0MB reserved, Mean: 3545.5MB allocated, 8607.8MB reserved
2025-03-25 02:45:57,564 - training - INFO - Epoch: 11/200000, Batch: 36/55, Loss: 4.7477, Throughput: 65.16 samples/sec
2025-03-25 02:46:10,465 - training - INFO - GPU Memory: GPU 0: 3502.2MB/8644.0MB, GPU 1: 3502.7MB/8632.0MB, GPU 2: 3502.8MB/8638.0MB, GPU 3: 3502.4MB/8664.0MB, GPU 4: 3504.7MB/8628.0MB, GPU 5: 3504.4MB/8660.0MB, GPU 6: 3501.6MB/8640.0MB, GPU 7: 3502.6MB/8648.0MB, Total: 28023.5MB allocated, 69154.0MB reserved, Mean: 3502.9MB allocated, 8644.2MB reserved
2025-03-25 02:46:10,465 - training - INFO - Epoch: 11/200000, Batch: 54/55, Loss: 4.7565, Throughput: 65.74 samples/sec
2025-03-25 02:46:10,466 - training - INFO - Epoch 11 completed in 40.16s. Average loss: 4.7743
2025-03-25 02:46:10,477 - training - INFO - Starting epoch 12/200000
2025-03-25 02:46:11,224 - training - INFO - GPU Memory: GPU 0: 3544.4MB/8644.0MB, GPU 1: 3544.0MB/8632.0MB, GPU 2: 3544.0MB/8638.0MB, GPU 3: 3544.7MB/8664.0MB, GPU 4: 3546.3MB/8628.0MB, GPU 5: 3546.6MB/8660.0MB, GPU 6: 3545.0MB/8640.0MB, GPU 7: 3544.3MB/8648.0MB, Total: 28359.3MB allocated, 69154.0MB reserved, Mean: 3544.9MB allocated, 8644.2MB reserved
2025-03-25 02:46:11,225 - training - INFO - Epoch: 12/200000, Batch: 0/55, Loss: 5.1111, Throughput: 64.25 samples/sec
2025-03-25 02:46:24,502 - training - INFO - GPU Memory: GPU 0: 3545.7MB/8602.0MB, GPU 1: 3548.0MB/8612.0MB, GPU 2: 3544.6MB/8618.0MB, GPU 3: 3547.0MB/8594.0MB, GPU 4: 3543.2MB/8608.0MB, GPU 5: 3546.0MB/8622.0MB, GPU 6: 3547.3MB/8598.0MB, GPU 7: 3547.8MB/8618.0MB, Total: 28369.6MB allocated, 68872.0MB reserved, Mean: 3546.2MB allocated, 8609.0MB reserved
2025-03-25 02:46:24,503 - training - INFO - Epoch: 12/200000, Batch: 18/55, Loss: 4.8004, Throughput: 65.03 samples/sec
2025-03-25 02:46:37,970 - training - INFO - GPU Memory: GPU 0: 3544.9MB/8602.0MB, GPU 1: 3547.2MB/8612.0MB, GPU 2: 3544.6MB/8618.0MB, GPU 3: 3545.9MB/8594.0MB, GPU 4: 3542.6MB/8608.0MB, GPU 5: 3545.4MB/8622.0MB, GPU 6: 3546.7MB/8598.0MB, GPU 7: 3547.3MB/8618.0MB, Total: 28364.5MB allocated, 68872.0MB reserved, Mean: 3545.6MB allocated, 8609.0MB reserved
2025-03-25 02:46:37,971 - training - INFO - Epoch: 12/200000, Batch: 36/55, Loss: 4.8449, Throughput: 64.60 samples/sec
2025-03-25 02:46:51,039 - training - INFO - GPU Memory: GPU 0: 3505.5MB/8654.0MB, GPU 1: 3504.2MB/8646.0MB, GPU 2: 3505.6MB/8628.0MB, GPU 3: 3502.5MB/8642.0MB, GPU 4: 3505.4MB/8630.0MB, GPU 5: 3503.6MB/8648.0MB, GPU 6: 3505.0MB/8658.0MB, GPU 7: 3505.6MB/8624.0MB, Total: 28037.5MB allocated, 69130.0MB reserved, Mean: 3504.7MB allocated, 8641.2MB reserved
2025-03-25 02:46:51,039 - training - INFO - Epoch: 12/200000, Batch: 54/55, Loss: 4.8220, Throughput: 65.09 samples/sec
2025-03-25 02:46:51,040 - training - INFO - Epoch 12 completed in 40.56s. Average loss: 4.7352
2025-03-25 02:46:51,047 - training - INFO - Starting epoch 13/200000
2025-03-25 02:46:51,818 - training - INFO - GPU Memory: GPU 0: 3547.8MB/8654.0MB, GPU 1: 3545.8MB/8646.0MB, GPU 2: 3547.3MB/8628.0MB, GPU 3: 3545.8MB/8642.0MB, GPU 4: 3546.2MB/8630.0MB, GPU 5: 3544.9MB/8648.0MB, GPU 6: 3550.3MB/8658.0MB, GPU 7: 3546.3MB/8624.0MB, Total: 28374.4MB allocated, 69130.0MB reserved, Mean: 3546.8MB allocated, 8641.2MB reserved
2025-03-25 02:46:51,819 - training - INFO - Epoch: 13/200000, Batch: 0/55, Loss: 4.5346, Throughput: 62.19 samples/sec
2025-03-25 02:47:04,971 - training - INFO - GPU Memory: GPU 0: 3543.9MB/8632.0MB, GPU 1: 3544.9MB/8594.0MB, GPU 2: 3545.8MB/8616.0MB, GPU 3: 3548.4MB/8602.0MB, GPU 4: 3545.8MB/8612.0MB, GPU 5: 3547.3MB/8596.0MB, GPU 6: 3547.1MB/8620.0MB, GPU 7: 3546.6MB/8594.0MB, Total: 28369.9MB allocated, 68866.0MB reserved, Mean: 3546.2MB allocated, 8608.2MB reserved
2025-03-25 02:47:04,971 - training - INFO - Epoch: 13/200000, Batch: 18/55, Loss: 4.7043, Throughput: 65.50 samples/sec
2025-03-25 02:47:18,320 - training - INFO - GPU Memory: GPU 0: 3543.3MB/8632.0MB, GPU 1: 3544.1MB/8594.0MB, GPU 2: 3545.2MB/8616.0MB, GPU 3: 3547.8MB/8602.0MB, GPU 4: 3545.3MB/8612.0MB, GPU 5: 3546.8MB/8596.0MB, GPU 6: 3546.5MB/8620.0MB, GPU 7: 3546.0MB/8594.0MB, Total: 28365.0MB allocated, 68866.0MB reserved, Mean: 3545.6MB allocated, 8608.2MB reserved
2025-03-25 02:47:18,320 - training - INFO - Epoch: 13/200000, Batch: 36/55, Loss: 4.7120, Throughput: 65.12 samples/sec
2025-03-25 02:47:31,334 - training - INFO - GPU Memory: GPU 0: 3503.4MB/8636.0MB, GPU 1: 3506.6MB/8638.0MB, GPU 2: 3505.0MB/8630.0MB, GPU 3: 3502.7MB/8650.0MB, GPU 4: 3504.1MB/8636.0MB, GPU 5: 3507.1MB/8644.0MB, GPU 6: 3501.8MB/8636.0MB, GPU 7: 3505.6MB/8648.0MB, Total: 28036.4MB allocated, 69118.0MB reserved, Mean: 3504.6MB allocated, 8639.8MB reserved
2025-03-25 02:47:31,335 - training - INFO - Epoch: 13/200000, Batch: 54/55, Loss: 4.7218, Throughput: 65.53 samples/sec
2025-03-25 02:47:31,336 - training - INFO - Epoch 13 completed in 40.29s. Average loss: 4.6964
2025-03-25 02:47:31,346 - training - INFO - Starting epoch 14/200000
2025-03-25 02:47:32,099 - training - INFO - GPU Memory: GPU 0: 3545.9MB/8636.0MB, GPU 1: 3547.1MB/8638.0MB, GPU 2: 3546.5MB/8630.0MB, GPU 3: 3544.6MB/8650.0MB, GPU 4: 3545.6MB/8636.0MB, GPU 5: 3548.1MB/8644.0MB, GPU 6: 3543.3MB/8636.0MB, GPU 7: 3546.3MB/8648.0MB, Total: 28367.5MB allocated, 69118.0MB reserved, Mean: 3545.9MB allocated, 8639.8MB reserved
2025-03-25 02:47:32,099 - training - INFO - Epoch: 14/200000, Batch: 0/55, Loss: 4.5198, Throughput: 63.78 samples/sec
2025-03-25 02:47:45,235 - training - INFO - GPU Memory: GPU 0: 3546.8MB/8618.0MB, GPU 1: 3547.4MB/8608.0MB, GPU 2: 3545.2MB/8612.0MB, GPU 3: 3547.3MB/8620.0MB, GPU 4: 3545.4MB/8598.0MB, GPU 5: 3546.2MB/8590.0MB, GPU 6: 3543.2MB/8610.0MB, GPU 7: 3546.4MB/8598.0MB, Total: 28367.9MB allocated, 68854.0MB reserved, Mean: 3546.0MB allocated, 8606.8MB reserved
2025-03-25 02:47:45,235 - training - INFO - Epoch: 14/200000, Batch: 18/55, Loss: 4.6450, Throughput: 65.67 samples/sec
2025-03-25 02:47:58,589 - training - INFO - GPU Memory: GPU 0: 3546.0MB/8618.0MB, GPU 1: 3547.1MB/8608.0MB, GPU 2: 3544.6MB/8612.0MB, GPU 3: 3546.7MB/8620.0MB, GPU 4: 3545.1MB/8598.0MB, GPU 5: 3545.6MB/8590.0MB, GPU 6: 3542.6MB/8610.0MB, GPU 7: 3546.1MB/8598.0MB, Total: 28363.7MB allocated, 68854.0MB reserved, Mean: 3545.5MB allocated, 8606.8MB reserved
2025-03-25 02:47:58,590 - training - INFO - Epoch: 14/200000, Batch: 36/55, Loss: 4.6471, Throughput: 65.19 samples/sec
2025-03-25 02:48:11,537 - training - INFO - GPU Memory: GPU 0: 3504.0MB/8652.0MB, GPU 1: 3504.9MB/8626.0MB, GPU 2: 3503.4MB/8648.0MB, GPU 3: 3503.8MB/8660.0MB, GPU 4: 3502.8MB/8632.0MB, GPU 5: 3503.7MB/8636.0MB, GPU 6: 3503.5MB/8628.0MB, GPU 7: 3502.2MB/8648.0MB, Total: 28028.3MB allocated, 69130.0MB reserved, Mean: 3503.5MB allocated, 8641.2MB reserved
2025-03-25 02:48:11,537 - training - INFO - Epoch: 14/200000, Batch: 54/55, Loss: 4.6386, Throughput: 65.69 samples/sec
2025-03-25 02:48:11,538 - training - INFO - Epoch 14 completed in 40.19s. Average loss: 4.6641
2025-03-25 02:48:11,542 - training - INFO - Starting epoch 15/200000
2025-03-25 02:48:12,311 - training - INFO - GPU Memory: GPU 0: 3545.7MB/8652.0MB, GPU 1: 3546.3MB/8626.0MB, GPU 2: 3544.8MB/8648.0MB, GPU 3: 3545.7MB/8660.0MB, GPU 4: 3544.8MB/8632.0MB, GPU 5: 3544.8MB/8636.0MB, GPU 6: 3544.1MB/8628.0MB, GPU 7: 3543.5MB/8648.0MB, Total: 28359.7MB allocated, 69130.0MB reserved, Mean: 3545.0MB allocated, 8641.2MB reserved
2025-03-25 02:48:12,311 - training - INFO - Epoch: 15/200000, Batch: 0/55, Loss: 4.7662, Throughput: 62.46 samples/sec
2025-03-25 02:48:25,437 - training - INFO - GPU Memory: GPU 0: 3547.7MB/8596.0MB, GPU 1: 3545.4MB/8608.0MB, GPU 2: 3546.1MB/8600.0MB, GPU 3: 3545.4MB/8594.0MB, GPU 4: 3543.2MB/8600.0MB, GPU 5: 3546.8MB/8600.0MB, GPU 6: 3546.4MB/8588.0MB, GPU 7: 3547.6MB/8598.0MB, Total: 28368.6MB allocated, 68784.0MB reserved, Mean: 3546.1MB allocated, 8598.0MB reserved
2025-03-25 02:48:25,437 - training - INFO - Epoch: 15/200000, Batch: 18/55, Loss: 4.7162, Throughput: 65.64 samples/sec
2025-03-25 02:48:38,971 - training - INFO - GPU Memory: GPU 0: 3546.8MB/8596.0MB, GPU 1: 3544.8MB/8608.0MB, GPU 2: 3545.3MB/8600.0MB, GPU 3: 3547.1MB/8594.0MB, GPU 4: 3542.6MB/8600.0MB, GPU 5: 3547.3MB/8600.0MB, GPU 6: 3546.1MB/8588.0MB, GPU 7: 3544.8MB/8598.0MB, Total: 28364.8MB allocated, 68784.0MB reserved, Mean: 3545.6MB allocated, 8598.0MB reserved
2025-03-25 02:48:38,971 - training - INFO - Epoch: 15/200000, Batch: 36/55, Loss: 4.6909, Throughput: 64.75 samples/sec
2025-03-25 02:48:52,023 - training - INFO - GPU Memory: GPU 0: 3505.9MB/8656.0MB, GPU 1: 3505.3MB/8630.0MB, GPU 2: 3504.0MB/8642.0MB, GPU 3: 3503.9MB/8642.0MB, GPU 4: 3501.8MB/8630.0MB, GPU 5: 3502.0MB/8640.0MB, GPU 6: 3504.1MB/8636.0MB, GPU 7: 3502.0MB/8642.0MB, Total: 28029.1MB allocated, 69118.0MB reserved, Mean: 3503.6MB allocated, 8639.8MB reserved
2025-03-25 02:48:52,024 - training - INFO - Epoch: 15/200000, Batch: 54/55, Loss: 4.6674, Throughput: 65.22 samples/sec
2025-03-25 02:48:52,024 - training - INFO - Epoch 15 completed in 40.48s. Average loss: 4.6428
2025-03-25 02:48:52,029 - training - INFO - Starting epoch 16/200000
2025-03-25 02:48:52,794 - training - INFO - GPU Memory: GPU 0: 3548.1MB/8656.0MB, GPU 1: 3546.3MB/8630.0MB, GPU 2: 3545.6MB/8642.0MB, GPU 3: 3545.9MB/8642.0MB, GPU 4: 3544.3MB/8630.0MB, GPU 5: 3543.6MB/8640.0MB, GPU 6: 3547.0MB/8636.0MB, GPU 7: 3544.3MB/8642.0MB, Total: 28365.0MB allocated, 69118.0MB reserved, Mean: 3545.6MB allocated, 8639.8MB reserved
2025-03-25 02:48:52,795 - training - INFO - Epoch: 16/200000, Batch: 0/55, Loss: 4.6897, Throughput: 62.71 samples/sec
2025-03-25 02:49:05,908 - training - INFO - GPU Memory: GPU 0: 3547.4MB/8602.0MB, GPU 1: 3547.1MB/8600.0MB, GPU 2: 3548.1MB/8606.0MB, GPU 3: 3546.3MB/8602.0MB, GPU 4: 3545.1MB/8586.0MB, GPU 5: 3547.1MB/8614.0MB, GPU 6: 3544.6MB/8616.0MB, GPU 7: 3547.3MB/8618.0MB, Total: 28372.9MB allocated, 68844.0MB reserved, Mean: 3546.6MB allocated, 8605.5MB reserved
2025-03-25 02:49:05,908 - training - INFO - Epoch: 16/200000, Batch: 18/55, Loss: 4.6270, Throughput: 65.71 samples/sec
2025-03-25 02:49:19,290 - training - INFO - GPU Memory: GPU 0: 3546.8MB/8602.0MB, GPU 1: 3546.0MB/8600.0MB, GPU 2: 3547.8MB/8606.0MB, GPU 3: 3545.7MB/8602.0MB, GPU 4: 3544.5MB/8586.0MB, GPU 5: 3546.5MB/8614.0MB, GPU 6: 3544.0MB/8616.0MB, GPU 7: 3546.5MB/8620.0MB, Total: 28367.8MB allocated, 68846.0MB reserved, Mean: 3546.0MB allocated, 8605.8MB reserved
2025-03-25 02:49:19,291 - training - INFO - Epoch: 16/200000, Batch: 36/55, Loss: 4.6395, Throughput: 65.15 samples/sec
2025-03-25 02:49:32,219 - training - INFO - GPU Memory: GPU 0: 3502.5MB/8648.0MB, GPU 1: 3505.7MB/8658.0MB, GPU 2: 3505.5MB/8656.0MB, GPU 3: 3507.2MB/8660.0MB, GPU 4: 3501.5MB/8640.0MB, GPU 5: 3505.8MB/8634.0MB, GPU 6: 3504.7MB/8638.0MB, GPU 7: 3505.0MB/8656.0MB, Total: 28037.8MB allocated, 69190.0MB reserved, Mean: 3504.7MB allocated, 8648.8MB reserved
2025-03-25 02:49:32,219 - training - INFO - Epoch: 16/200000, Batch: 54/55, Loss: 4.6302, Throughput: 65.69 samples/sec
2025-03-25 02:49:32,220 - training - INFO - Epoch 16 completed in 40.19s. Average loss: 4.6244
2025-03-25 02:49:32,232 - training - INFO - Starting epoch 17/200000
2025-03-25 02:49:32,990 - training - INFO - GPU Memory: GPU 0: 3546.2MB/8648.0MB, GPU 1: 3547.0MB/8658.0MB, GPU 2: 3546.8MB/8656.0MB, GPU 3: 3549.0MB/8660.0MB, GPU 4: 3543.8MB/8640.0MB, GPU 5: 3547.3MB/8634.0MB, GPU 6: 3546.0MB/8638.0MB, GPU 7: 3546.0MB/8656.0MB, Total: 28372.1MB allocated, 69190.0MB reserved, Mean: 3546.5MB allocated, 8648.8MB reserved
2025-03-25 02:49:32,990 - training - INFO - Epoch: 17/200000, Batch: 0/55, Loss: 4.4918, Throughput: 63.27 samples/sec
2025-03-25 02:49:46,175 - training - INFO - GPU Memory: GPU 0: 3548.0MB/8606.0MB, GPU 1: 3545.1MB/8586.0MB, GPU 2: 3542.1MB/8594.0MB, GPU 3: 3546.1MB/8622.0MB, GPU 4: 3547.7MB/8600.0MB, GPU 5: 3546.6MB/8604.0MB, GPU 6: 3544.6MB/8614.0MB, GPU 7: 3544.8MB/8592.0MB, Total: 28365.1MB allocated, 68818.0MB reserved, Mean: 3545.6MB allocated, 8602.2MB reserved
2025-03-25 02:49:46,176 - training - INFO - Epoch: 17/200000, Batch: 18/55, Loss: 4.6309, Throughput: 65.41 samples/sec
2025-03-25 02:49:59,630 - training - INFO - GPU Memory: GPU 0: 3549.7MB/8606.0MB, GPU 1: 3544.3MB/8586.0MB, GPU 2: 3542.1MB/8594.0MB, GPU 3: 3545.8MB/8622.0MB, GPU 4: 3547.1MB/8600.0MB, GPU 5: 3546.0MB/8604.0MB, GPU 6: 3543.8MB/8614.0MB, GPU 7: 3544.3MB/8592.0MB, Total: 28363.0MB allocated, 68818.0MB reserved, Mean: 3545.4MB allocated, 8602.2MB reserved
2025-03-25 02:49:59,630 - training - INFO - Epoch: 17/200000, Batch: 36/55, Loss: 4.5729, Throughput: 64.82 samples/sec
2025-03-25 02:50:12,592 - training - INFO - GPU Memory: GPU 0: 3505.8MB/8638.0MB, GPU 1: 3505.2MB/8646.0MB, GPU 2: 3504.1MB/8640.0MB, GPU 3: 3505.0MB/8648.0MB, GPU 4: 3504.5MB/8644.0MB, GPU 5: 3505.0MB/8642.0MB, GPU 6: 3505.7MB/8638.0MB, GPU 7: 3503.4MB/8630.0MB, Total: 28038.6MB allocated, 69126.0MB reserved, Mean: 3504.8MB allocated, 8640.8MB reserved
2025-03-25 02:50:12,593 - training - INFO - Epoch: 17/200000, Batch: 54/55, Loss: 4.5762, Throughput: 65.41 samples/sec
2025-03-25 02:50:12,593 - training - INFO - Epoch 17 completed in 40.36s. Average loss: 4.6046
2025-03-25 02:50:12,600 - training - INFO - Starting epoch 18/200000
2025-03-25 02:50:13,377 - training - INFO - GPU Memory: GPU 0: 3548.1MB/8638.0MB, GPU 1: 3546.5MB/8646.0MB, GPU 2: 3544.6MB/8640.0MB, GPU 3: 3545.8MB/8648.0MB, GPU 4: 3545.8MB/8644.0MB, GPU 5: 3546.0MB/8642.0MB, GPU 6: 3547.0MB/8638.0MB, GPU 7: 3544.6MB/8630.0MB, Total: 28368.4MB allocated, 69126.0MB reserved, Mean: 3546.0MB allocated, 8640.8MB reserved
2025-03-25 02:50:13,378 - training - INFO - Epoch: 18/200000, Batch: 0/55, Loss: 4.4706, Throughput: 61.74 samples/sec
2025-03-25 02:50:26,531 - training - INFO - GPU Memory: GPU 0: 3547.5MB/8598.0MB, GPU 1: 3547.6MB/8598.0MB, GPU 2: 3544.7MB/8594.0MB, GPU 3: 3546.6MB/8620.0MB, GPU 4: 3546.7MB/8594.0MB, GPU 5: 3546.8MB/8608.0MB, GPU 6: 3545.3MB/8614.0MB, GPU 7: 3546.1MB/8616.0MB, Total: 28371.2MB allocated, 68842.0MB reserved, Mean: 3546.4MB allocated, 8605.2MB reserved
2025-03-25 02:50:26,532 - training - INFO - Epoch: 18/200000, Batch: 18/55, Loss: 4.6024, Throughput: 65.46 samples/sec
2025-03-25 02:50:39,933 - training - INFO - GPU Memory: GPU 0: 3546.9MB/8598.0MB, GPU 1: 3546.8MB/8598.0MB, GPU 2: 3544.3MB/8594.0MB, GPU 3: 3545.9MB/8620.0MB, GPU 4: 3546.1MB/8594.0MB, GPU 5: 3546.3MB/8608.0MB, GPU 6: 3544.8MB/8614.0MB, GPU 7: 3545.2MB/8616.0MB, Total: 28366.2MB allocated, 68842.0MB reserved, Mean: 3545.8MB allocated, 8605.2MB reserved
2025-03-25 02:50:39,933 - training - INFO - Epoch: 18/200000, Batch: 36/55, Loss: 4.5858, Throughput: 64.98 samples/sec
2025-03-25 02:50:52,909 - training - INFO - GPU Memory: GPU 0: 3507.9MB/8654.0MB, GPU 1: 3503.3MB/8640.0MB, GPU 2: 3503.8MB/8634.0MB, GPU 3: 3505.3MB/8646.0MB, GPU 4: 3506.2MB/8648.0MB, GPU 5: 3504.5MB/8644.0MB, GPU 6: 3503.8MB/8648.0MB, GPU 7: 3506.1MB/8626.0MB, Total: 28040.9MB allocated, 69140.0MB reserved, Mean: 3505.1MB allocated, 8642.5MB reserved
2025-03-25 02:50:52,909 - training - INFO - Epoch: 18/200000, Batch: 54/55, Loss: 4.5669, Throughput: 65.49 samples/sec
2025-03-25 02:50:52,910 - training - INFO - Epoch 18 completed in 40.31s. Average loss: 4.5864
2025-03-25 02:50:52,920 - training - INFO - Starting epoch 19/200000
2025-03-25 02:50:53,661 - training - INFO - GPU Memory: GPU 0: 3548.4MB/8654.0MB, GPU 1: 3544.8MB/8640.0MB, GPU 2: 3545.8MB/8634.0MB, GPU 3: 3547.1MB/8646.0MB, GPU 4: 3547.7MB/8648.0MB, GPU 5: 3546.3MB/8644.0MB, GPU 6: 3545.3MB/8648.0MB, GPU 7: 3546.3MB/8626.0MB, Total: 28371.6MB allocated, 69140.0MB reserved, Mean: 3546.4MB allocated, 8642.5MB reserved
2025-03-25 02:50:53,661 - training - INFO - Epoch: 19/200000, Batch: 0/55, Loss: 4.4633, Throughput: 64.74 samples/sec
2025-03-25 02:51:06,864 - training - INFO - GPU Memory: GPU 0: 3546.9MB/8596.0MB, GPU 1: 3545.4MB/8586.0MB, GPU 2: 3544.6MB/8614.0MB, GPU 3: 3545.7MB/8596.0MB, GPU 4: 3546.9MB/8588.0MB, GPU 5: 3543.9MB/8620.0MB, GPU 6: 3546.3MB/8612.0MB, GPU 7: 3543.1MB/8606.0MB, Total: 28362.9MB allocated, 68818.0MB reserved, Mean: 3545.4MB allocated, 8602.2MB reserved
2025-03-25 02:51:06,864 - training - INFO - Epoch: 19/200000, Batch: 18/55, Loss: 4.6514, Throughput: 65.40 samples/sec
2025-03-25 02:51:20,307 - training - INFO - GPU Memory: GPU 0: 3547.1MB/8596.0MB, GPU 1: 3544.6MB/8586.0MB, GPU 2: 3544.2MB/8614.0MB, GPU 3: 3545.1MB/8596.0MB, GPU 4: 3546.3MB/8588.0MB, GPU 5: 3543.6MB/8620.0MB, GPU 6: 3546.0MB/8612.0MB, GPU 7: 3542.8MB/8606.0MB, Total: 28359.8MB allocated, 68818.0MB reserved, Mean: 3545.0MB allocated, 8602.2MB reserved
2025-03-25 02:51:20,307 - training - INFO - Epoch: 19/200000, Batch: 36/55, Loss: 4.5766, Throughput: 64.85 samples/sec
2025-03-25 02:51:33,287 - training - INFO - GPU Memory: GPU 0: 3507.0MB/8666.0MB, GPU 1: 3502.7MB/8632.0MB, GPU 2: 3505.5MB/8634.0MB, GPU 3: 3501.4MB/8652.0MB, GPU 4: 3504.0MB/8646.0MB, GPU 5: 3504.6MB/8650.0MB, GPU 6: 3506.5MB/8646.0MB, GPU 7: 3503.9MB/8632.0MB, Total: 28035.7MB allocated, 69158.0MB reserved, Mean: 3504.5MB allocated, 8644.8MB reserved
2025-03-25 02:51:33,287 - training - INFO - Epoch: 19/200000, Batch: 54/55, Loss: 4.5464, Throughput: 65.40 samples/sec
2025-03-25 02:51:33,288 - training - INFO - Epoch 19 completed in 40.37s. Average loss: 4.5708
2025-03-25 02:51:33,292 - training - INFO - Starting epoch 20/200000
2025-03-25 02:51:34,048 - training - INFO - GPU Memory: GPU 0: 3548.8MB/8666.0MB, GPU 1: 3544.0MB/8632.0MB, GPU 2: 3546.6MB/8634.0MB, GPU 3: 3545.2MB/8652.0MB, GPU 4: 3545.2MB/8646.0MB, GPU 5: 3543.0MB/8650.0MB, GPU 6: 3547.5MB/8646.0MB, GPU 7: 3545.3MB/8632.0MB, Total: 28365.6MB allocated, 69158.0MB reserved, Mean: 3545.7MB allocated, 8644.8MB reserved
2025-03-25 02:51:34,049 - training - INFO - Epoch: 20/200000, Batch: 0/55, Loss: 4.3190, Throughput: 63.47 samples/sec
2025-03-25 02:51:47,091 - training - INFO - GPU Memory: GPU 0: 3549.0MB/8620.0MB, GPU 1: 3547.8MB/8612.0MB, GPU 2: 3542.3MB/8618.0MB, GPU 3: 3546.5MB/8614.0MB, GPU 4: 3546.5MB/8618.0MB, GPU 5: 3546.9MB/8610.0MB, GPU 6: 3543.1MB/8620.0MB, GPU 7: 3545.5MB/8588.0MB, Total: 28367.5MB allocated, 68900.0MB reserved, Mean: 3545.9MB allocated, 8612.5MB reserved
2025-03-25 02:51:47,091 - training - INFO - Epoch: 20/200000, Batch: 18/55, Loss: 4.5534, Throughput: 66.09 samples/sec
2025-03-25 02:52:00,479 - training - INFO - GPU Memory: GPU 0: 3548.6MB/8620.0MB, GPU 1: 3547.2MB/8612.0MB, GPU 2: 3542.3MB/8618.0MB, GPU 3: 3545.6MB/8614.0MB, GPU 4: 3546.2MB/8618.0MB, GPU 5: 3546.6MB/8610.0MB, GPU 6: 3543.1MB/8620.0MB, GPU 7: 3544.9MB/8588.0MB, Total: 28364.5MB allocated, 68900.0MB reserved, Mean: 3545.6MB allocated, 8612.5MB reserved
2025-03-25 02:52:00,479 - training - INFO - Epoch: 20/200000, Batch: 36/55, Loss: 4.5186, Throughput: 65.33 samples/sec
2025-03-25 02:52:13,447 - training - INFO - GPU Memory: GPU 0: 3504.5MB/8642.0MB, GPU 1: 3505.0MB/8642.0MB, GPU 2: 3502.4MB/8640.0MB, GPU 3: 3501.8MB/8648.0MB, GPU 4: 3502.4MB/8648.0MB, GPU 5: 3502.2MB/8630.0MB, GPU 6: 3503.4MB/8648.0MB, GPU 7: 3504.9MB/8652.0MB, Total: 28026.5MB allocated, 69150.0MB reserved, Mean: 3503.3MB allocated, 8643.8MB reserved
2025-03-25 02:52:13,447 - training - INFO - Epoch: 20/200000, Batch: 54/55, Loss: 4.5281, Throughput: 65.75 samples/sec
2025-03-25 02:52:13,448 - training - INFO - Epoch 20 completed in 40.16s. Average loss: 4.5518
2025-03-25 02:52:13,456 - training - INFO - Starting epoch 21/200000
2025-03-25 02:52:14,222 - training - INFO - GPU Memory: GPU 0: 3546.3MB/8642.0MB, GPU 1: 3545.8MB/8642.0MB, GPU 2: 3544.8MB/8640.0MB, GPU 3: 3543.9MB/8648.0MB, GPU 4: 3543.8MB/8648.0MB, GPU 5: 3542.3MB/8630.0MB, GPU 6: 3545.3MB/8648.0MB, GPU 7: 3544.3MB/8652.0MB, Total: 28356.6MB allocated, 69150.0MB reserved, Mean: 3544.6MB allocated, 8643.8MB reserved
2025-03-25 02:52:14,223 - training - INFO - Epoch: 21/200000, Batch: 0/55, Loss: 4.5348, Throughput: 62.63 samples/sec
2025-03-25 02:52:27,460 - training - INFO - GPU Memory: GPU 0: 3545.3MB/8630.0MB, GPU 1: 3544.5MB/8610.0MB, GPU 2: 3543.6MB/8584.0MB, GPU 3: 3547.0MB/8602.0MB, GPU 4: 3546.7MB/8600.0MB, GPU 5: 3545.4MB/8614.0MB, GPU 6: 3543.2MB/8604.0MB, GPU 7: 3545.6MB/8584.0MB, Total: 28361.2MB allocated, 68828.0MB reserved, Mean: 3545.2MB allocated, 8603.5MB reserved
2025-03-25 02:52:27,461 - training - INFO - Epoch: 21/200000, Batch: 18/55, Loss: 4.4814, Throughput: 65.12 samples/sec
2025-03-25 02:52:40,917 - training - INFO - GPU Memory: GPU 0: 3544.4MB/8630.0MB, GPU 1: 3543.9MB/8610.0MB, GPU 2: 3543.8MB/8584.0MB, GPU 3: 3546.6MB/8602.0MB, GPU 4: 3547.0MB/8600.0MB, GPU 5: 3547.1MB/8614.0MB, GPU 6: 3543.2MB/8604.0MB, GPU 7: 3545.0MB/8584.0MB, Total: 28360.9MB allocated, 68828.0MB reserved, Mean: 3545.1MB allocated, 8603.5MB reserved
2025-03-25 02:52:40,917 - training - INFO - Epoch: 21/200000, Batch: 36/55, Loss: 4.4549, Throughput: 64.67 samples/sec
2025-03-25 02:52:53,912 - training - INFO - GPU Memory: GPU 0: 3505.7MB/8644.0MB, GPU 1: 3502.4MB/8650.0MB, GPU 2: 3506.4MB/8622.0MB, GPU 3: 3504.9MB/8654.0MB, GPU 4: 3504.8MB/8640.0MB, GPU 5: 3508.0MB/8636.0MB, GPU 6: 3502.7MB/8632.0MB, GPU 7: 3504.5MB/8650.0MB, Total: 28039.3MB allocated, 69128.0MB reserved, Mean: 3504.9MB allocated, 8641.0MB reserved
2025-03-25 02:52:53,912 - training - INFO - Epoch: 21/200000, Batch: 54/55, Loss: 4.4604, Throughput: 65.26 samples/sec
2025-03-25 02:52:53,912 - training - INFO - Epoch 21 completed in 40.46s. Average loss: 4.5345
2025-03-25 02:52:53,922 - training - INFO - Starting epoch 22/200000
2025-03-25 02:52:54,674 - training - INFO - GPU Memory: GPU 0: 3547.3MB/8644.0MB, GPU 1: 3543.9MB/8650.0MB, GPU 2: 3547.8MB/8622.0MB, GPU 3: 3547.0MB/8654.0MB, GPU 4: 3546.3MB/8640.0MB, GPU 5: 3549.5MB/8636.0MB, GPU 6: 3544.3MB/8632.0MB, GPU 7: 3543.8MB/8650.0MB, Total: 28369.7MB allocated, 69128.0MB reserved, Mean: 3546.2MB allocated, 8641.0MB reserved
2025-03-25 02:52:54,675 - training - INFO - Epoch: 22/200000, Batch: 0/55, Loss: 4.3065, Throughput: 63.77 samples/sec
2025-03-25 02:53:07,953 - training - INFO - GPU Memory: GPU 0: 3546.0MB/8604.0MB, GPU 1: 3546.9MB/8616.0MB, GPU 2: 3543.2MB/8614.0MB, GPU 3: 3549.9MB/8624.0MB, GPU 4: 3545.1MB/8602.0MB, GPU 5: 3550.0MB/8622.0MB, GPU 6: 3547.8MB/8612.0MB, GPU 7: 3544.7MB/8604.0MB, Total: 28373.6MB allocated, 68898.0MB reserved, Mean: 3546.7MB allocated, 8612.2MB reserved
2025-03-25 02:53:07,954 - training - INFO - Epoch: 22/200000, Batch: 18/55, Loss: 4.5291, Throughput: 65.00 samples/sec
2025-03-25 02:53:21,297 - training - INFO - GPU Memory: GPU 0: 3544.6MB/8604.0MB, GPU 1: 3546.3MB/8616.0MB, GPU 2: 3542.9MB/8614.0MB, GPU 3: 3549.3MB/8624.0MB, GPU 4: 3545.0MB/8602.0MB, GPU 5: 3549.2MB/8622.0MB, GPU 6: 3547.4MB/8612.0MB, GPU 7: 3544.3MB/8604.0MB, Total: 28369.1MB allocated, 68898.0MB reserved, Mean: 3546.1MB allocated, 8612.2MB reserved
2025-03-25 02:53:21,297 - training - INFO - Epoch: 22/200000, Batch: 36/55, Loss: 4.5277, Throughput: 64.88 samples/sec
2025-03-25 02:53:34,304 - training - INFO - GPU Memory: GPU 0: 3505.0MB/8632.0MB, GPU 1: 3506.3MB/8642.0MB, GPU 2: 3505.5MB/8648.0MB, GPU 3: 3504.1MB/8644.0MB, GPU 4: 3504.0MB/8640.0MB, GPU 5: 3506.0MB/8638.0MB, GPU 6: 3503.7MB/8650.0MB, GPU 7: 3504.6MB/8648.0MB, Total: 28039.3MB allocated, 69142.0MB reserved, Mean: 3504.9MB allocated, 8642.8MB reserved
2025-03-25 02:53:34,305 - training - INFO - Epoch: 22/200000, Batch: 54/55, Loss: 4.5485, Throughput: 65.37 samples/sec
2025-03-25 02:53:34,305 - training - INFO - Epoch 22 completed in 40.38s. Average loss: 4.5206
2025-03-25 02:53:34,310 - training - INFO - Starting epoch 23/200000
2025-03-25 02:53:35,077 - training - INFO - GPU Memory: GPU 0: 3546.5MB/8632.0MB, GPU 1: 3547.3MB/8642.0MB, GPU 2: 3546.8MB/8648.0MB, GPU 3: 3546.3MB/8644.0MB, GPU 4: 3545.3MB/8640.0MB, GPU 5: 3547.5MB/8638.0MB, GPU 6: 3544.5MB/8650.0MB, GPU 7: 3545.6MB/8648.0MB, Total: 28369.9MB allocated, 69142.0MB reserved, Mean: 3546.2MB allocated, 8642.8MB reserved
2025-03-25 02:53:35,077 - training - INFO - Epoch: 23/200000, Batch: 0/55, Loss: 4.4348, Throughput: 62.63 samples/sec
2025-03-25 02:53:48,165 - training - INFO - GPU Memory: GPU 0: 3547.4MB/8590.0MB, GPU 1: 3546.4MB/8604.0MB, GPU 2: 3545.3MB/8616.0MB, GPU 3: 3546.6MB/8622.0MB, GPU 4: 3545.9MB/8594.0MB, GPU 5: 3543.7MB/8610.0MB, GPU 6: 3545.3MB/8620.0MB, GPU 7: 3546.6MB/8614.0MB, Total: 28367.3MB allocated, 68870.0MB reserved, Mean: 3545.9MB allocated, 8608.8MB reserved
2025-03-25 02:53:48,166 - training - INFO - Epoch: 23/200000, Batch: 18/55, Loss: 4.4176, Throughput: 65.82 samples/sec
2025-03-25 02:54:01,636 - training - INFO - GPU Memory: GPU 0: 3546.8MB/8590.0MB, GPU 1: 3546.1MB/8604.0MB, GPU 2: 3544.5MB/8616.0MB, GPU 3: 3545.9MB/8622.0MB, GPU 4: 3545.6MB/8594.0MB, GPU 5: 3543.1MB/8610.0MB, GPU 6: 3544.5MB/8620.0MB, GPU 7: 3546.0MB/8614.0MB, Total: 28362.5MB allocated, 68870.0MB reserved, Mean: 3545.3MB allocated, 8608.8MB reserved
2025-03-25 02:54:01,637 - training - INFO - Epoch: 23/200000, Batch: 36/55, Loss: 4.4956, Throughput: 64.99 samples/sec
2025-03-25 02:54:14,601 - training - INFO - GPU Memory: GPU 0: 3501.2MB/8630.0MB, GPU 1: 3506.3MB/8624.0MB, GPU 2: 3503.6MB/8640.0MB, GPU 3: 3502.4MB/8654.0MB, GPU 4: 3503.0MB/8622.0MB, GPU 5: 3502.7MB/8644.0MB, GPU 6: 3501.6MB/8646.0MB, GPU 7: 3504.5MB/8620.0MB, Total: 28025.3MB allocated, 69080.0MB reserved, Mean: 3503.2MB allocated, 8635.0MB reserved
2025-03-25 02:54:14,601 - training - INFO - Epoch: 23/200000, Batch: 54/55, Loss: 4.4684, Throughput: 65.52 samples/sec
2025-03-25 02:54:14,602 - training - INFO - Epoch 23 completed in 40.29s. Average loss: 4.5105
2025-03-25 02:54:14,608 - training - INFO - Starting epoch 24/200000
2025-03-25 02:54:15,370 - training - INFO - GPU Memory: GPU 0: 3543.9MB/8630.0MB, GPU 1: 3548.0MB/8624.0MB, GPU 2: 3544.6MB/8640.0MB, GPU 3: 3545.6MB/8654.0MB, GPU 4: 3544.6MB/8622.0MB, GPU 5: 3544.3MB/8644.0MB, GPU 6: 3542.9MB/8646.0MB, GPU 7: 3546.0MB/8620.0MB, Total: 28359.9MB allocated, 69080.0MB reserved, Mean: 3545.0MB allocated, 8635.0MB reserved
2025-03-25 02:54:15,371 - training - INFO - Epoch: 24/200000, Batch: 0/55, Loss: 4.6909, Throughput: 62.97 samples/sec
2025-03-25 02:54:28,406 - training - INFO - GPU Memory: GPU 0: 3547.2MB/8596.0MB, GPU 1: 3546.8MB/8600.0MB, GPU 2: 3544.8MB/8588.0MB, GPU 3: 3546.5MB/8612.0MB, GPU 4: 3543.8MB/8616.0MB, GPU 5: 3547.7MB/8602.0MB, GPU 6: 3544.8MB/8612.0MB, GPU 7: 3544.3MB/8608.0MB, Total: 28366.0MB allocated, 68834.0MB reserved, Mean: 3545.8MB allocated, 8604.2MB reserved
2025-03-25 02:54:28,407 - training - INFO - Epoch: 24/200000, Batch: 18/55, Loss: 4.5843, Throughput: 66.10 samples/sec
2025-03-25 02:54:41,868 - training - INFO - GPU Memory: GPU 0: 3546.6MB/8596.0MB, GPU 1: 3546.5MB/8600.0MB, GPU 2: 3544.3MB/8588.0MB, GPU 3: 3545.9MB/8612.0MB, GPU 4: 3543.0MB/8616.0MB, GPU 5: 3547.1MB/8602.0MB, GPU 6: 3544.3MB/8612.0MB, GPU 7: 3543.3MB/8608.0MB, Total: 28360.9MB allocated, 68834.0MB reserved, Mean: 3545.1MB allocated, 8604.2MB reserved
2025-03-25 02:54:41,868 - training - INFO - Epoch: 24/200000, Batch: 36/55, Loss: 4.5071, Throughput: 65.15 samples/sec
2025-03-25 02:54:54,806 - training - INFO - GPU Memory: GPU 0: 3503.9MB/8650.0MB, GPU 1: 3507.8MB/8648.0MB, GPU 2: 3504.0MB/8616.0MB, GPU 3: 3503.7MB/8648.0MB, GPU 4: 3504.3MB/8646.0MB, GPU 5: 3501.6MB/8648.0MB, GPU 6: 3503.6MB/8636.0MB, GPU 7: 3502.3MB/8634.0MB, Total: 28031.3MB allocated, 69126.0MB reserved, Mean: 3503.9MB allocated, 8640.8MB reserved
2025-03-25 02:54:54,806 - training - INFO - Epoch: 24/200000, Batch: 54/55, Loss: 4.5071, Throughput: 65.68 samples/sec
2025-03-25 02:54:54,807 - training - INFO - Epoch 24 completed in 40.20s. Average loss: 4.4981
2025-03-25 02:54:54,816 - training - INFO - Starting epoch 25/200000
2025-03-25 02:54:55,570 - training - INFO - GPU Memory: GPU 0: 3545.7MB/8650.0MB, GPU 1: 3549.1MB/8648.0MB, GPU 2: 3545.5MB/8616.0MB, GPU 3: 3545.7MB/8648.0MB, GPU 4: 3545.8MB/8646.0MB, GPU 5: 3543.1MB/8648.0MB, GPU 6: 3544.3MB/8636.0MB, GPU 7: 3544.3MB/8634.0MB, Total: 28363.5MB allocated, 69126.0MB reserved, Mean: 3545.4MB allocated, 8640.8MB reserved
2025-03-25 02:54:55,570 - training - INFO - Epoch: 25/200000, Batch: 0/55, Loss: 4.3230, Throughput: 63.69 samples/sec
2025-03-25 02:55:08,817 - training - INFO - GPU Memory: GPU 0: 3547.0MB/8608.0MB, GPU 1: 3549.3MB/8600.0MB, GPU 2: 3546.9MB/8602.0MB, GPU 3: 3545.3MB/8606.0MB, GPU 4: 3545.8MB/8606.0MB, GPU 5: 3545.7MB/8594.0MB, GPU 6: 3549.2MB/8614.0MB, GPU 7: 3543.4MB/8608.0MB, Total: 28372.7MB allocated, 68838.0MB reserved, Mean: 3546.6MB allocated, 8604.8MB reserved
2025-03-25 02:55:08,817 - training - INFO - Epoch: 25/200000, Batch: 18/55, Loss: 4.5327, Throughput: 65.14 samples/sec
2025-03-25 02:55:22,350 - training - INFO - GPU Memory: GPU 0: 3546.5MB/8608.0MB, GPU 1: 3548.8MB/8600.0MB, GPU 2: 3546.3MB/8602.0MB, GPU 3: 3544.9MB/8606.0MB, GPU 4: 3545.2MB/8606.0MB, GPU 5: 3545.1MB/8594.0MB, GPU 6: 3548.6MB/8614.0MB, GPU 7: 3542.8MB/8608.0MB, Total: 28368.2MB allocated, 68838.0MB reserved, Mean: 3546.0MB allocated, 8604.8MB reserved
2025-03-25 02:55:22,351 - training - INFO - Epoch: 25/200000, Batch: 36/55, Loss: 4.4965, Throughput: 64.50 samples/sec
2025-03-25 02:55:35,360 - training - INFO - GPU Memory: GPU 0: 3505.5MB/8620.0MB, GPU 1: 3501.8MB/8648.0MB, GPU 2: 3504.1MB/8656.0MB, GPU 3: 3503.3MB/8646.0MB, GPU 4: 3500.6MB/8656.0MB, GPU 5: 3503.1MB/8644.0MB, GPU 6: 3505.5MB/8656.0MB, GPU 7: 3502.6MB/8646.0MB, Total: 28026.5MB allocated, 69172.0MB reserved, Mean: 3503.3MB allocated, 8646.5MB reserved
2025-03-25 02:55:35,360 - training - INFO - Epoch: 25/200000, Batch: 54/55, Loss: 4.5036, Throughput: 65.11 samples/sec
2025-03-25 02:55:35,361 - training - INFO - Epoch 25 completed in 40.54s. Average loss: 4.4922
2025-03-25 02:55:35,372 - training - INFO - Starting epoch 26/200000
2025-03-25 02:55:36,125 - training - INFO - GPU Memory: GPU 0: 3547.6MB/8620.0MB, GPU 1: 3543.4MB/8648.0MB, GPU 2: 3545.1MB/8656.0MB, GPU 3: 3545.4MB/8646.0MB, GPU 4: 3543.8MB/8656.0MB, GPU 5: 3544.4MB/8644.0MB, GPU 6: 3546.8MB/8656.0MB, GPU 7: 3543.6MB/8646.0MB, Total: 28360.1MB allocated, 69172.0MB reserved, Mean: 3545.0MB allocated, 8646.5MB reserved
2025-03-25 02:55:36,125 - training - INFO - Epoch: 26/200000, Batch: 0/55, Loss: 4.3021, Throughput: 63.75 samples/sec
2025-03-25 02:55:49,270 - training - INFO - GPU Memory: GPU 0: 3547.1MB/8620.0MB, GPU 1: 3544.8MB/8608.0MB, GPU 2: 3546.9MB/8598.0MB, GPU 3: 3550.2MB/8624.0MB, GPU 4: 3544.4MB/8612.0MB, GPU 5: 3547.9MB/8600.0MB, GPU 6: 3547.1MB/8616.0MB, GPU 7: 3547.3MB/8602.0MB, Total: 28375.7MB allocated, 68880.0MB reserved, Mean: 3547.0MB allocated, 8610.0MB reserved
2025-03-25 02:55:49,270 - training - INFO - Epoch: 26/200000, Batch: 18/55, Loss: 4.4790, Throughput: 65.62 samples/sec
2025-03-25 02:56:02,700 - training - INFO - GPU Memory: GPU 0: 3549.6MB/8620.0MB, GPU 1: 3544.3MB/8608.0MB, GPU 2: 3546.8MB/8598.0MB, GPU 3: 3549.8MB/8624.0MB, GPU 4: 3544.1MB/8612.0MB, GPU 5: 3547.6MB/8600.0MB, GPU 6: 3546.5MB/8616.0MB, GPU 7: 3546.8MB/8602.0MB, Total: 28375.5MB allocated, 68880.0MB reserved, Mean: 3546.9MB allocated, 8610.0MB reserved
2025-03-25 02:56:02,700 - training - INFO - Epoch: 26/200000, Batch: 36/55, Loss: 4.4723, Throughput: 64.99 samples/sec
2025-03-25 02:56:15,749 - training - INFO - GPU Memory: GPU 0: 3503.8MB/8662.0MB, GPU 1: 3503.5MB/8650.0MB, GPU 2: 3505.1MB/8660.0MB, GPU 3: 3504.1MB/8666.0MB, GPU 4: 3502.3MB/8624.0MB, GPU 5: 3506.5MB/8654.0MB, GPU 6: 3502.5MB/8652.0MB, GPU 7: 3503.3MB/8638.0MB, Total: 28031.1MB allocated, 69206.0MB reserved, Mean: 3503.9MB allocated, 8650.8MB reserved
2025-03-25 02:56:15,749 - training - INFO - Epoch: 26/200000, Batch: 54/55, Loss: 4.4741, Throughput: 65.38 samples/sec
2025-03-25 02:56:15,750 - training - INFO - Epoch 26 completed in 40.38s. Average loss: 4.4800
2025-03-25 02:56:15,760 - training - INFO - Starting epoch 27/200000
2025-03-25 02:56:16,529 - training - INFO - GPU Memory: GPU 0: 3546.9MB/8662.0MB, GPU 1: 3545.0MB/8650.0MB, GPU 2: 3546.5MB/8660.0MB, GPU 3: 3547.2MB/8666.0MB, GPU 4: 3546.6MB/8624.0MB, GPU 5: 3546.6MB/8654.0MB, GPU 6: 3543.8MB/8652.0MB, GPU 7: 3544.8MB/8638.0MB, Total: 28367.4MB allocated, 69206.0MB reserved, Mean: 3545.9MB allocated, 8650.8MB reserved
2025-03-25 02:56:16,529 - training - INFO - Epoch: 27/200000, Batch: 0/55, Loss: 4.5218, Throughput: 62.39 samples/sec
2025-03-25 02:56:29,640 - training - INFO - GPU Memory: GPU 0: 3544.7MB/8616.0MB, GPU 1: 3544.7MB/8596.0MB, GPU 2: 3545.0MB/8616.0MB, GPU 3: 3546.8MB/8624.0MB, GPU 4: 3547.2MB/8598.0MB, GPU 5: 3545.6MB/8606.0MB, GPU 6: 3546.5MB/8594.0MB, GPU 7: 3546.2MB/8596.0MB, Total: 28366.6MB allocated, 68846.0MB reserved, Mean: 3545.8MB allocated, 8605.8MB reserved
2025-03-25 02:56:29,640 - training - INFO - Epoch: 27/200000, Batch: 18/55, Loss: 4.4922, Throughput: 65.71 samples/sec
2025-03-25 02:56:43,056 - training - INFO - GPU Memory: GPU 0: 3544.1MB/8616.0MB, GPU 1: 3544.1MB/8596.0MB, GPU 2: 3544.4MB/8616.0MB, GPU 3: 3546.2MB/8624.0MB, GPU 4: 3546.3MB/8598.0MB, GPU 5: 3545.0MB/8606.0MB, GPU 6: 3546.0MB/8594.0MB, GPU 7: 3545.3MB/8596.0MB, Total: 28361.5MB allocated, 68846.0MB reserved, Mean: 3545.2MB allocated, 8605.8MB reserved
2025-03-25 02:56:43,057 - training - INFO - Epoch: 27/200000, Batch: 36/55, Loss: 4.5127, Throughput: 65.06 samples/sec
2025-03-25 02:56:56,076 - training - INFO - GPU Memory: GPU 0: 3504.5MB/8660.0MB, GPU 1: 3505.2MB/8640.0MB, GPU 2: 3506.1MB/8636.0MB, GPU 3: 3503.4MB/8650.0MB, GPU 4: 3503.8MB/8652.0MB, GPU 5: 3504.5MB/8652.0MB, GPU 6: 3502.2MB/8640.0MB, GPU 7: 3503.6MB/8628.0MB, Total: 28033.3MB allocated, 69158.0MB reserved, Mean: 3504.2MB allocated, 8644.8MB reserved
2025-03-25 02:56:56,076 - training - INFO - Epoch: 27/200000, Batch: 54/55, Loss: 4.4910, Throughput: 65.48 samples/sec
2025-03-25 02:56:56,077 - training - INFO - Epoch 27 completed in 40.32s. Average loss: 4.4732
2025-03-25 02:56:56,087 - training - INFO - Starting epoch 28/200000
2025-03-25 02:56:56,850 - training - INFO - GPU Memory: GPU 0: 3545.9MB/8660.0MB, GPU 1: 3546.8MB/8640.0MB, GPU 2: 3547.3MB/8636.0MB, GPU 3: 3545.6MB/8650.0MB, GPU 4: 3545.1MB/8652.0MB, GPU 5: 3545.9MB/8652.0MB, GPU 6: 3543.5MB/8640.0MB, GPU 7: 3545.5MB/8628.0MB, Total: 28365.6MB allocated, 69158.0MB reserved, Mean: 3545.7MB allocated, 8644.8MB reserved
2025-03-25 02:56:56,851 - training - INFO - Epoch: 28/200000, Batch: 0/55, Loss: 4.5335, Throughput: 62.86 samples/sec
2025-03-25 02:57:09,951 - training - INFO - GPU Memory: GPU 0: 3548.3MB/8618.0MB, GPU 1: 3546.3MB/8602.0MB, GPU 2: 3545.9MB/8586.0MB, GPU 3: 3549.4MB/8624.0MB, GPU 4: 3545.8MB/8610.0MB, GPU 5: 3546.3MB/8622.0MB, GPU 6: 3547.7MB/8600.0MB, GPU 7: 3545.8MB/8596.0MB, Total: 28375.6MB allocated, 68858.0MB reserved, Mean: 3547.0MB allocated, 8607.2MB reserved
2025-03-25 02:57:09,951 - training - INFO - Epoch: 28/200000, Batch: 18/55, Loss: 4.4891, Throughput: 65.78 samples/sec
2025-03-25 02:57:23,401 - training - INFO - GPU Memory: GPU 0: 3548.0MB/8618.0MB, GPU 1: 3545.8MB/8602.0MB, GPU 2: 3545.6MB/8586.0MB, GPU 3: 3548.8MB/8624.0MB, GPU 4: 3545.0MB/8610.0MB, GPU 5: 3546.3MB/8622.0MB, GPU 6: 3547.1MB/8600.0MB, GPU 7: 3545.3MB/8596.0MB, Total: 28371.8MB allocated, 68858.0MB reserved, Mean: 3546.5MB allocated, 8607.2MB reserved
2025-03-25 02:57:23,401 - training - INFO - Epoch: 28/200000, Batch: 36/55, Loss: 4.4990, Throughput: 65.02 samples/sec
2025-03-25 02:57:36,209 - training - INFO - GPU Memory: GPU 0: 3504.7MB/8664.0MB, GPU 1: 3505.2MB/8616.0MB, GPU 2: 3501.3MB/8636.0MB, GPU 3: 3504.5MB/8668.0MB, GPU 4: 3503.3MB/8646.0MB, GPU 5: 3501.2MB/8648.0MB, GPU 6: 3502.5MB/8652.0MB, GPU 7: 3503.2MB/8656.0MB, Total: 28025.9MB allocated, 69186.0MB reserved, Mean: 3503.2MB allocated, 8648.2MB reserved
2025-03-25 02:57:37,364 - training - INFO - Epoch: 28/200000, Batch: 54/55, Loss: 4.4710, Throughput: 65.80 samples/sec
2025-03-25 02:57:37,364 - training - INFO - Epoch 28 completed in 41.28s. Average loss: 4.4652
2025-03-25 02:57:37,376 - training - INFO - Starting epoch 29/200000
2025-03-25 02:57:38,120 - training - INFO - GPU Memory: GPU 0: 3546.2MB/8664.0MB, GPU 1: 3546.9MB/8616.0MB, GPU 2: 3542.1MB/8636.0MB, GPU 3: 3546.6MB/8668.0MB, GPU 4: 3544.6MB/8646.0MB, GPU 5: 3542.9MB/8648.0MB, GPU 6: 3544.0MB/8652.0MB, GPU 7: 3543.9MB/8656.0MB, Total: 28357.1MB allocated, 69186.0MB reserved, Mean: 3544.6MB allocated, 8648.2MB reserved
2025-03-25 02:57:38,121 - training - INFO - Epoch: 29/200000, Batch: 0/55, Loss: 4.6567, Throughput: 64.50 samples/sec
2025-03-25 02:57:51,186 - training - INFO - GPU Memory: GPU 0: 3548.6MB/8620.0MB, GPU 1: 3544.1MB/8616.0MB, GPU 2: 3544.3MB/8584.0MB, GPU 3: 3548.5MB/8628.0MB, GPU 4: 3544.8MB/8598.0MB, GPU 5: 3546.1MB/8602.0MB, GPU 6: 3544.8MB/8606.0MB, GPU 7: 3546.8MB/8576.0MB, Total: 28368.0MB allocated, 68830.0MB reserved, Mean: 3546.0MB allocated, 8603.8MB reserved
2025-03-25 02:57:51,187 - training - INFO - Epoch: 29/200000, Batch: 18/55, Loss: 4.4933, Throughput: 66.04 samples/sec
2025-03-25 02:58:04,603 - training - INFO - GPU Memory: GPU 0: 3546.5MB/8620.0MB, GPU 1: 3544.4MB/8616.0MB, GPU 2: 3544.3MB/8584.0MB, GPU 3: 3548.4MB/8628.0MB, GPU 4: 3547.5MB/8598.0MB, GPU 5: 3545.5MB/8602.0MB, GPU 6: 3544.5MB/8606.0MB, GPU 7: 3543.6MB/8576.0MB, Total: 28364.7MB allocated, 68830.0MB reserved, Mean: 3545.6MB allocated, 8603.8MB reserved
2025-03-25 02:58:04,603 - training - INFO - Epoch: 29/200000, Batch: 36/55, Loss: 4.4735, Throughput: 65.23 samples/sec
2025-03-25 02:58:17,603 - training - INFO - GPU Memory: GPU 0: 3506.0MB/8636.0MB, GPU 1: 3502.5MB/8616.0MB, GPU 2: 3504.0MB/8636.0MB, GPU 3: 3503.6MB/8646.0MB, GPU 4: 3502.7MB/8632.0MB, GPU 5: 3503.2MB/8632.0MB, GPU 6: 3505.7MB/8636.0MB, GPU 7: 3504.1MB/8614.0MB, Total: 28031.7MB allocated, 69048.0MB reserved, Mean: 3504.0MB allocated, 8631.0MB reserved
2025-03-25 02:58:17,603 - training - INFO - Epoch: 29/200000, Batch: 54/55, Loss: 4.4595, Throughput: 65.63 samples/sec
2025-03-25 02:58:17,604 - training - INFO - Epoch 29 completed in 40.23s. Average loss: 4.4608
2025-03-25 02:58:17,612 - training - INFO - Starting epoch 30/200000
2025-03-25 02:58:18,372 - training - INFO - GPU Memory: GPU 0: 3547.9MB/8636.0MB, GPU 1: 3543.5MB/8616.0MB, GPU 2: 3545.3MB/8636.0MB, GPU 3: 3544.7MB/8646.0MB, GPU 4: 3544.4MB/8632.0MB, GPU 5: 3544.8MB/8632.0MB, GPU 6: 3547.2MB/8636.0MB, GPU 7: 3545.1MB/8614.0MB, Total: 28362.8MB allocated, 69048.0MB reserved, Mean: 3545.4MB allocated, 8631.0MB reserved
2025-03-25 02:58:18,372 - training - INFO - Epoch: 30/200000, Batch: 0/55, Loss: 4.4296, Throughput: 63.20 samples/sec
2025-03-25 02:58:31,583 - training - INFO - GPU Memory: GPU 0: 3545.4MB/8614.0MB, GPU 1: 3545.7MB/8600.0MB, GPU 2: 3547.3MB/8610.0MB, GPU 3: 3546.8MB/8614.0MB, GPU 4: 3547.5MB/8590.0MB, GPU 5: 3546.6MB/8612.0MB, GPU 6: 3544.6MB/8614.0MB, GPU 7: 3546.6MB/8614.0MB, Total: 28370.4MB allocated, 68868.0MB reserved, Mean: 3546.3MB allocated, 8608.5MB reserved
2025-03-25 02:58:31,583 - training - INFO - Epoch: 30/200000, Batch: 18/55, Loss: 4.5019, Throughput: 65.28 samples/sec
2025-03-25 02:58:44,957 - training - INFO - GPU Memory: GPU 0: 3545.3MB/8614.0MB, GPU 1: 3545.1MB/8600.0MB, GPU 2: 3546.7MB/8610.0MB, GPU 3: 3546.5MB/8614.0MB, GPU 4: 3546.8MB/8590.0MB, GPU 5: 3545.8MB/8612.0MB, GPU 6: 3544.0MB/8614.0MB, GPU 7: 3545.7MB/8614.0MB, Total: 28366.0MB allocated, 68868.0MB reserved, Mean: 3545.7MB allocated, 8608.5MB reserved
2025-03-25 02:58:44,957 - training - INFO - Epoch: 30/200000, Batch: 36/55, Loss: 4.4934, Throughput: 64.95 samples/sec
2025-03-25 02:58:58,024 - training - INFO - GPU Memory: GPU 0: 3506.2MB/8652.0MB, GPU 1: 3505.8MB/8634.0MB, GPU 2: 3506.3MB/8628.0MB, GPU 3: 3503.3MB/8642.0MB, GPU 4: 3503.8MB/8652.0MB, GPU 5: 3504.5MB/8638.0MB, GPU 6: 3505.1MB/8634.0MB, GPU 7: 3503.5MB/8658.0MB, Total: 28038.5MB allocated, 69138.0MB reserved, Mean: 3504.8MB allocated, 8642.2MB reserved
2025-03-25 02:58:58,025 - training - INFO - Epoch: 30/200000, Batch: 54/55, Loss: 4.4857, Throughput: 65.33 samples/sec
2025-03-25 02:58:58,026 - training - INFO - Epoch 30 completed in 40.41s. Average loss: 4.4547
2025-03-25 02:58:58,032 - training - INFO - Starting epoch 31/200000
2025-03-25 02:58:58,787 - training - INFO - GPU Memory: GPU 0: 3548.3MB/8652.0MB, GPU 1: 3547.2MB/8634.0MB, GPU 2: 3547.8MB/8628.0MB, GPU 3: 3545.8MB/8642.0MB, GPU 4: 3544.3MB/8652.0MB, GPU 5: 3546.0MB/8638.0MB, GPU 6: 3546.6MB/8634.0MB, GPU 7: 3544.5MB/8658.0MB, Total: 28370.5MB allocated, 69138.0MB reserved, Mean: 3546.3MB allocated, 8642.2MB reserved
2025-03-25 02:58:58,787 - training - INFO - Epoch: 31/200000, Batch: 0/55, Loss: 4.8092, Throughput: 63.54 samples/sec
2025-03-25 02:59:11,947 - training - INFO - GPU Memory: GPU 0: 3547.7MB/8600.0MB, GPU 1: 3545.6MB/8620.0MB, GPU 2: 3548.6MB/8620.0MB, GPU 3: 3545.4MB/8616.0MB, GPU 4: 3545.6MB/8612.0MB, GPU 5: 3545.3MB/8596.0MB, GPU 6: 3545.1MB/8622.0MB, GPU 7: 3545.9MB/8600.0MB, Total: 28369.0MB allocated, 68886.0MB reserved, Mean: 3546.1MB allocated, 8610.8MB reserved
2025-03-25 02:59:11,947 - training - INFO - Epoch: 31/200000, Batch: 18/55, Loss: 4.5084, Throughput: 65.54 samples/sec
2025-03-25 02:59:25,388 - training - INFO - GPU Memory: GPU 0: 3547.3MB/8600.0MB, GPU 1: 3545.6MB/8620.0MB, GPU 2: 3548.0MB/8620.0MB, GPU 3: 3545.1MB/8616.0MB, GPU 4: 3545.0MB/8612.0MB, GPU 5: 3544.5MB/8596.0MB, GPU 6: 3544.5MB/8622.0MB, GPU 7: 3546.1MB/8600.0MB, Total: 28366.0MB allocated, 68886.0MB reserved, Mean: 3545.7MB allocated, 8610.8MB reserved
2025-03-25 02:59:25,388 - training - INFO - Epoch: 31/200000, Batch: 36/55, Loss: 4.4793, Throughput: 64.92 samples/sec
2025-03-25 02:59:38,364 - training - INFO - GPU Memory: GPU 0: 3502.7MB/8638.0MB, GPU 1: 3506.0MB/8644.0MB, GPU 2: 3505.0MB/8660.0MB, GPU 3: 3505.2MB/8638.0MB, GPU 4: 3502.8MB/8618.0MB, GPU 5: 3501.9MB/8654.0MB, GPU 6: 3503.4MB/8644.0MB, GPU 7: 3504.4MB/8640.0MB, Total: 28031.3MB allocated, 69136.0MB reserved, Mean: 3503.9MB allocated, 8642.0MB reserved
2025-03-25 02:59:38,364 - training - INFO - Epoch: 31/200000, Batch: 54/55, Loss: 4.4473, Throughput: 65.46 samples/sec
2025-03-25 02:59:38,365 - training - INFO - Epoch 31 completed in 40.33s. Average loss: 4.4413
2025-03-25 02:59:38,369 - training - INFO - Starting epoch 32/200000
2025-03-25 02:59:39,133 - training - INFO - GPU Memory: GPU 0: 3545.8MB/8638.0MB, GPU 1: 3546.7MB/8644.0MB, GPU 2: 3546.3MB/8660.0MB, GPU 3: 3547.2MB/8638.0MB, GPU 4: 3544.3MB/8618.0MB, GPU 5: 3543.3MB/8654.0MB, GPU 6: 3544.6MB/8644.0MB, GPU 7: 3546.1MB/8640.0MB, Total: 28364.3MB allocated, 69136.0MB reserved, Mean: 3545.5MB allocated, 8642.0MB reserved
2025-03-25 02:59:39,133 - training - INFO - Epoch: 32/200000, Batch: 0/55, Loss: 4.3964, Throughput: 62.87 samples/sec
2025-03-25 02:59:52,286 - training - INFO - GPU Memory: GPU 0: 3549.8MB/8620.0MB, GPU 1: 3547.1MB/8618.0MB, GPU 2: 3547.6MB/8600.0MB, GPU 3: 3545.0MB/8606.0MB, GPU 4: 3549.8MB/8598.0MB, GPU 5: 3547.1MB/8602.0MB, GPU 6: 3545.1MB/8580.0MB, GPU 7: 3544.1MB/8610.0MB, Total: 28375.6MB allocated, 68834.0MB reserved, Mean: 3547.0MB allocated, 8604.2MB reserved
2025-03-25 02:59:52,286 - training - INFO - Epoch: 32/200000, Batch: 18/55, Loss: 4.4500, Throughput: 65.53 samples/sec
2025-03-25 03:00:05,780 - training - INFO - GPU Memory: GPU 0: 3549.1MB/8620.0MB, GPU 1: 3546.5MB/8618.0MB, GPU 2: 3547.5MB/8600.0MB, GPU 3: 3544.7MB/8606.0MB, GPU 4: 3549.3MB/8598.0MB, GPU 5: 3546.6MB/8604.0MB, GPU 6: 3544.6MB/8580.0MB, GPU 7: 3542.4MB/8610.0MB, Total: 28370.6MB allocated, 68836.0MB reserved, Mean: 3546.3MB allocated, 8604.5MB reserved
2025-03-25 03:00:05,780 - training - INFO - Epoch: 32/200000, Batch: 36/55, Loss: 4.3976, Throughput: 64.79 samples/sec
2025-03-25 03:00:18,883 - training - INFO - GPU Memory: GPU 0: 3506.6MB/8632.0MB, GPU 1: 3504.1MB/8654.0MB, GPU 2: 3502.2MB/8642.0MB, GPU 3: 3503.9MB/8664.0MB, GPU 4: 3502.5MB/8640.0MB, GPU 5: 3502.0MB/8650.0MB, GPU 6: 3503.5MB/8634.0MB, GPU 7: 3500.6MB/8634.0MB, Total: 28025.7MB allocated, 69150.0MB reserved, Mean: 3503.2MB allocated, 8643.8MB reserved
2025-03-25 03:00:18,883 - training - INFO - Epoch: 32/200000, Batch: 54/55, Loss: 4.3906, Throughput: 65.16 samples/sec
2025-03-25 03:00:18,884 - training - INFO - Epoch 32 completed in 40.52s. Average loss: 4.4390
2025-03-25 03:00:18,890 - training - INFO - Starting epoch 33/200000
2025-03-25 03:00:19,654 - training - INFO - GPU Memory: GPU 0: 3548.0MB/8632.0MB, GPU 1: 3545.8MB/8654.0MB, GPU 2: 3543.3MB/8642.0MB, GPU 3: 3546.1MB/8664.0MB, GPU 4: 3544.5MB/8640.0MB, GPU 5: 3543.3MB/8650.0MB, GPU 6: 3544.8MB/8634.0MB, GPU 7: 3541.6MB/8634.0MB, Total: 28357.5MB allocated, 69150.0MB reserved, Mean: 3544.7MB allocated, 8643.8MB reserved
2025-03-25 03:00:19,655 - training - INFO - Epoch: 33/200000, Batch: 0/55, Loss: 4.3638, Throughput: 62.81 samples/sec
2025-03-25 03:00:32,805 - training - INFO - GPU Memory: GPU 0: 3546.5MB/8612.0MB, GPU 1: 3544.0MB/8612.0MB, GPU 2: 3547.9MB/8596.0MB, GPU 3: 3544.2MB/8620.0MB, GPU 4: 3546.7MB/8600.0MB, GPU 5: 3547.2MB/8592.0MB, GPU 6: 3544.7MB/8600.0MB, GPU 7: 3544.0MB/8580.0MB, Total: 28365.1MB allocated, 68812.0MB reserved, Mean: 3545.6MB allocated, 8601.5MB reserved
2025-03-25 03:00:32,805 - training - INFO - Epoch: 33/200000, Batch: 18/55, Loss: 4.4572, Throughput: 65.54 samples/sec
2025-03-25 03:00:46,170 - training - INFO - GPU Memory: GPU 0: 3545.7MB/8612.0MB, GPU 1: 3543.4MB/8612.0MB, GPU 2: 3547.3MB/8596.0MB, GPU 3: 3544.1MB/8620.0MB, GPU 4: 3545.8MB/8600.0MB, GPU 5: 3546.6MB/8594.0MB, GPU 6: 3544.1MB/8600.0MB, GPU 7: 3543.8MB/8582.0MB, Total: 28360.8MB allocated, 68816.0MB reserved, Mean: 3545.1MB allocated, 8602.0MB reserved
2025-03-25 03:00:46,170 - training - INFO - Epoch: 33/200000, Batch: 36/55, Loss: 4.4887, Throughput: 65.10 samples/sec
2025-03-25 03:00:59,093 - training - INFO - GPU Memory: GPU 0: 3504.4MB/8648.0MB, GPU 1: 3507.2MB/8650.0MB, GPU 2: 3503.0MB/8650.0MB, GPU 3: 3505.6MB/8652.0MB, GPU 4: 3502.1MB/8652.0MB, GPU 5: 3503.6MB/8634.0MB, GPU 6: 3504.8MB/8636.0MB, GPU 7: 3501.9MB/8650.0MB, Total: 28032.6MB allocated, 69172.0MB reserved, Mean: 3504.1MB allocated, 8646.5MB reserved
2025-03-25 03:00:59,093 - training - INFO - Epoch: 33/200000, Batch: 54/55, Loss: 4.4592, Throughput: 65.67 samples/sec
2025-03-25 03:00:59,094 - training - INFO - Epoch 33 completed in 40.20s. Average loss: 4.4345
2025-03-25 03:00:59,100 - training - INFO - Starting epoch 34/200000
2025-03-25 03:00:59,850 - training - INFO - GPU Memory: GPU 0: 3546.6MB/8648.0MB, GPU 1: 3548.9MB/8650.0MB, GPU 2: 3544.6MB/8650.0MB, GPU 3: 3548.8MB/8652.0MB, GPU 4: 3544.6MB/8652.0MB, GPU 5: 3545.1MB/8634.0MB, GPU 6: 3546.1MB/8636.0MB, GPU 7: 3543.6MB/8650.0MB, Total: 28368.4MB allocated, 69172.0MB reserved, Mean: 3546.0MB allocated, 8646.5MB reserved
2025-03-25 03:00:59,850 - training - INFO - Epoch: 34/200000, Batch: 0/55, Loss: 4.4837, Throughput: 63.95 samples/sec
2025-03-25 03:01:13,087 - training - INFO - GPU Memory: GPU 0: 3548.7MB/8598.0MB, GPU 1: 3545.1MB/8624.0MB, GPU 2: 3543.2MB/8612.0MB, GPU 3: 3550.0MB/8630.0MB, GPU 4: 3547.2MB/8612.0MB, GPU 5: 3545.1MB/8618.0MB, GPU 6: 3549.1MB/8602.0MB, GPU 7: 3543.4MB/8606.0MB, Total: 28371.8MB allocated, 68902.0MB reserved, Mean: 3546.5MB allocated, 8612.8MB reserved
2025-03-25 03:01:13,087 - training - INFO - Epoch: 34/200000, Batch: 18/55, Loss: 4.4379, Throughput: 65.20 samples/sec
2025-03-25 03:01:26,463 - training - INFO - GPU Memory: GPU 0: 3548.1MB/8598.0MB, GPU 1: 3544.5MB/8624.0MB, GPU 2: 3542.8MB/8612.0MB, GPU 3: 3549.4MB/8630.0MB, GPU 4: 3546.6MB/8612.0MB, GPU 5: 3544.8MB/8620.0MB, GPU 6: 3548.3MB/8602.0MB, GPU 7: 3542.6MB/8606.0MB, Total: 28367.2MB allocated, 68904.0MB reserved, Mean: 3545.9MB allocated, 8613.0MB reserved
2025-03-25 03:01:26,464 - training - INFO - Epoch: 34/200000, Batch: 36/55, Loss: 4.4294, Throughput: 64.90 samples/sec
2025-03-25 03:01:39,444 - training - INFO - GPU Memory: GPU 0: 3506.3MB/8668.0MB, GPU 1: 3505.1MB/8648.0MB, GPU 2: 3503.8MB/8630.0MB, GPU 3: 3505.2MB/8632.0MB, GPU 4: 3498.3MB/8620.0MB, GPU 5: 3504.9MB/8634.0MB, GPU 6: 3504.5MB/8642.0MB, GPU 7: 3502.6MB/8626.0MB, Total: 28030.8MB allocated, 69100.0MB reserved, Mean: 3503.9MB allocated, 8637.5MB reserved
2025-03-25 03:01:39,444 - training - INFO - Epoch: 34/200000, Batch: 54/55, Loss: 4.4157, Throughput: 65.44 samples/sec
2025-03-25 03:01:39,445 - training - INFO - Epoch 34 completed in 40.35s. Average loss: 4.4291
2025-03-25 03:01:39,453 - training - INFO - Starting epoch 35/200000
2025-03-25 03:01:40,199 - training - INFO - GPU Memory: GPU 0: 3547.3MB/8668.0MB, GPU 1: 3546.8MB/8648.0MB, GPU 2: 3545.3MB/8630.0MB, GPU 3: 3547.1MB/8632.0MB, GPU 4: 3539.6MB/8620.0MB, GPU 5: 3546.5MB/8634.0MB, GPU 6: 3547.7MB/8642.0MB, GPU 7: 3544.1MB/8626.0MB, Total: 28364.4MB allocated, 69100.0MB reserved, Mean: 3545.6MB allocated, 8637.5MB reserved
2025-03-25 03:01:40,199 - training - INFO - Epoch: 35/200000, Batch: 0/55, Loss: 4.2364, Throughput: 64.35 samples/sec
2025-03-25 03:01:53,326 - training - INFO - GPU Memory: GPU 0: 3546.8MB/8622.0MB, GPU 1: 3547.4MB/8606.0MB, GPU 2: 3546.9MB/8582.0MB, GPU 3: 3548.7MB/8618.0MB, GPU 4: 3544.6MB/8586.0MB, GPU 5: 3545.6MB/8620.0MB, GPU 6: 3544.1MB/8614.0MB, GPU 7: 3545.7MB/8584.0MB, Total: 28369.8MB allocated, 68832.0MB reserved, Mean: 3546.2MB allocated, 8604.0MB reserved
2025-03-25 03:01:53,326 - training - INFO - Epoch: 35/200000, Batch: 18/55, Loss: 4.3920, Throughput: 65.74 samples/sec
2025-03-25 03:02:06,896 - training - INFO - GPU Memory: GPU 0: 3546.0MB/8622.0MB, GPU 1: 3546.8MB/8606.0MB, GPU 2: 3546.1MB/8582.0MB, GPU 3: 3548.1MB/8618.0MB, GPU 4: 3544.0MB/8586.0MB, GPU 5: 3545.3MB/8620.0MB, GPU 6: 3543.3MB/8614.0MB, GPU 7: 3545.3MB/8584.0MB, Total: 28364.9MB allocated, 68832.0MB reserved, Mean: 3545.6MB allocated, 8604.0MB reserved
2025-03-25 03:02:06,896 - training - INFO - Epoch: 35/200000, Batch: 36/55, Loss: 4.4215, Throughput: 64.72 samples/sec
2025-03-25 03:02:19,931 - training - INFO - GPU Memory: GPU 0: 3505.3MB/8642.0MB, GPU 1: 3505.9MB/8642.0MB, GPU 2: 3503.5MB/8648.0MB, GPU 3: 3503.1MB/8654.0MB, GPU 4: 3503.0MB/8626.0MB, GPU 5: 3504.5MB/8634.0MB, GPU 6: 3503.0MB/8634.0MB, GPU 7: 3503.8MB/8638.0MB, Total: 28032.0MB allocated, 69118.0MB reserved, Mean: 3504.0MB allocated, 8639.8MB reserved
2025-03-25 03:02:19,932 - training - INFO - Epoch: 35/200000, Batch: 54/55, Loss: 4.4167, Throughput: 65.22 samples/sec
2025-03-25 03:02:19,935 - training - INFO - Epoch 35 completed in 40.48s. Average loss: 4.4238
2025-03-25 03:02:19,946 - training - INFO - Starting epoch 36/200000
2025-03-25 03:02:20,702 - training - INFO - GPU Memory: GPU 0: 3543.9MB/8642.0MB, GPU 1: 3547.3MB/8642.0MB, GPU 2: 3544.2MB/8648.0MB, GPU 3: 3546.3MB/8654.0MB, GPU 4: 3543.8MB/8626.0MB, GPU 5: 3544.8MB/8634.0MB, GPU 6: 3545.3MB/8634.0MB, GPU 7: 3545.8MB/8638.0MB, Total: 28361.7MB allocated, 69118.0MB reserved, Mean: 3545.2MB allocated, 8639.8MB reserved
2025-03-25 03:02:20,702 - training - INFO - Epoch: 36/200000, Batch: 0/55, Loss: 4.6136, Throughput: 63.49 samples/sec
2025-03-25 03:02:33,846 - training - INFO - GPU Memory: GPU 0: 3549.5MB/8624.0MB, GPU 1: 3547.0MB/8622.0MB, GPU 2: 3546.2MB/8580.0MB, GPU 3: 3546.5MB/8622.0MB, GPU 4: 3546.6MB/8612.0MB, GPU 5: 3546.1MB/8598.0MB, GPU 6: 3543.3MB/8612.0MB, GPU 7: 3544.6MB/8598.0MB, Total: 28369.9MB allocated, 68868.0MB reserved, Mean: 3546.2MB allocated, 8608.5MB reserved
2025-03-25 03:02:33,846 - training - INFO - Epoch: 36/200000, Batch: 18/55, Loss: 4.4255, Throughput: 65.61 samples/sec
2025-03-25 03:02:47,302 - training - INFO - GPU Memory: GPU 0: 3548.3MB/8624.0MB, GPU 1: 3546.5MB/8622.0MB, GPU 2: 3545.6MB/8580.0MB, GPU 3: 3546.2MB/8622.0MB, GPU 4: 3546.0MB/8612.0MB, GPU 5: 3545.6MB/8598.0MB, GPU 6: 3542.8MB/8612.0MB, GPU 7: 3543.5MB/8598.0MB, Total: 28364.4MB allocated, 68868.0MB reserved, Mean: 3545.5MB allocated, 8608.5MB reserved
2025-03-25 03:02:47,302 - training - INFO - Epoch: 36/200000, Batch: 36/55, Loss: 4.4471, Throughput: 64.92 samples/sec
2025-03-25 03:03:00,358 - training - INFO - GPU Memory: GPU 0: 3504.1MB/8648.0MB, GPU 1: 3504.6MB/8652.0MB, GPU 2: 3503.8MB/8650.0MB, GPU 3: 3503.8MB/8648.0MB, GPU 4: 3505.0MB/8648.0MB, GPU 5: 3502.7MB/8642.0MB, GPU 6: 3502.8MB/8632.0MB, GPU 7: 3503.4MB/8646.0MB, Total: 28030.3MB allocated, 69166.0MB reserved, Mean: 3503.8MB allocated, 8645.8MB reserved
2025-03-25 03:03:00,359 - training - INFO - Epoch: 36/200000, Batch: 54/55, Loss: 4.4303, Throughput: 65.33 samples/sec
2025-03-25 03:03:00,360 - training - INFO - Epoch 36 completed in 40.41s. Average loss: 4.4218
2025-03-25 03:03:00,370 - training - INFO - Starting epoch 37/200000
2025-03-25 03:03:01,116 - training - INFO - GPU Memory: GPU 0: 3544.8MB/8648.0MB, GPU 1: 3546.1MB/8652.0MB, GPU 2: 3545.2MB/8650.0MB, GPU 3: 3543.8MB/8648.0MB, GPU 4: 3546.2MB/8648.0MB, GPU 5: 3544.3MB/8642.0MB, GPU 6: 3544.1MB/8632.0MB, GPU 7: 3544.6MB/8646.0MB, Total: 28359.1MB allocated, 69166.0MB reserved, Mean: 3544.9MB allocated, 8645.8MB reserved
2025-03-25 03:03:01,116 - training - INFO - Epoch: 37/200000, Batch: 0/55, Loss: 4.2277, Throughput: 64.42 samples/sec
2025-03-25 03:03:14,229 - training - INFO - GPU Memory: GPU 0: 3545.7MB/8608.0MB, GPU 1: 3545.1MB/8610.0MB, GPU 2: 3544.8MB/8618.0MB, GPU 3: 3544.9MB/8610.0MB, GPU 4: 3541.8MB/8618.0MB, GPU 5: 3546.8MB/8594.0MB, GPU 6: 3544.7MB/8590.0MB, GPU 7: 3546.8MB/8596.0MB, Total: 28360.6MB allocated, 68844.0MB reserved, Mean: 3545.1MB allocated, 8605.5MB reserved
2025-03-25 03:03:14,229 - training - INFO - Epoch: 37/200000, Batch: 18/55, Loss: 4.3432, Throughput: 65.81 samples/sec
2025-03-25 03:03:27,651 - training - INFO - GPU Memory: GPU 0: 3545.4MB/8608.0MB, GPU 1: 3544.5MB/8610.0MB, GPU 2: 3544.8MB/8618.0MB, GPU 3: 3544.1MB/8610.0MB, GPU 4: 3541.8MB/8618.0MB, GPU 5: 3546.0MB/8594.0MB, GPU 6: 3544.1MB/8590.0MB, GPU 7: 3546.0MB/8596.0MB, Total: 28356.7MB allocated, 68844.0MB reserved, Mean: 3544.6MB allocated, 8605.5MB reserved
2025-03-25 03:03:27,651 - training - INFO - Epoch: 37/200000, Batch: 36/55, Loss: 4.3517, Throughput: 65.10 samples/sec
2025-03-25 03:03:40,682 - training - INFO - GPU Memory: GPU 0: 3502.7MB/8638.0MB, GPU 1: 3504.0MB/8630.0MB, GPU 2: 3503.5MB/8636.0MB, GPU 3: 3503.0MB/8640.0MB, GPU 4: 3502.3MB/8614.0MB, GPU 5: 3505.7MB/8658.0MB, GPU 6: 3504.9MB/8626.0MB, GPU 7: 3503.6MB/8652.0MB, Total: 28029.6MB allocated, 69094.0MB reserved, Mean: 3503.7MB allocated, 8636.8MB reserved
2025-03-25 03:03:40,682 - training - INFO - Epoch: 37/200000, Batch: 54/55, Loss: 4.4028, Throughput: 65.49 samples/sec
2025-03-25 03:03:40,683 - training - INFO - Epoch 37 completed in 40.31s. Average loss: 4.4138
2025-03-25 03:03:40,695 - training - INFO - Starting epoch 38/200000
2025-03-25 03:03:41,454 - training - INFO - GPU Memory: GPU 0: 3545.1MB/8638.0MB, GPU 1: 3545.3MB/8630.0MB, GPU 2: 3545.0MB/8636.0MB, GPU 3: 3542.8MB/8640.0MB, GPU 4: 3543.5MB/8614.0MB, GPU 5: 3546.2MB/8658.0MB, GPU 6: 3546.3MB/8626.0MB, GPU 7: 3544.9MB/8652.0MB, Total: 28359.1MB allocated, 69094.0MB reserved, Mean: 3544.9MB allocated, 8636.8MB reserved
2025-03-25 03:03:41,454 - training - INFO - Epoch: 38/200000, Batch: 0/55, Loss: 4.8068, Throughput: 63.27 samples/sec
2025-03-25 03:03:54,642 - training - INFO - GPU Memory: GPU 0: 3546.1MB/8602.0MB, GPU 1: 3544.6MB/8594.0MB, GPU 2: 3545.0MB/8616.0MB, GPU 3: 3547.5MB/8626.0MB, GPU 4: 3546.3MB/8614.0MB, GPU 5: 3548.6MB/8598.0MB, GPU 6: 3546.6MB/8614.0MB, GPU 7: 3545.2MB/8600.0MB, Total: 28369.8MB allocated, 68864.0MB reserved, Mean: 3546.2MB allocated, 8608.0MB reserved
2025-03-25 03:03:54,642 - training - INFO - Epoch: 38/200000, Batch: 18/55, Loss: 4.4003, Throughput: 65.39 samples/sec
2025-03-25 03:04:08,082 - training - INFO - GPU Memory: GPU 0: 3545.2MB/8602.0MB, GPU 1: 3544.3MB/8594.0MB, GPU 2: 3544.6MB/8616.0MB, GPU 3: 3546.2MB/8626.0MB, GPU 4: 3545.8MB/8614.0MB, GPU 5: 3547.8MB/8598.0MB, GPU 6: 3546.0MB/8614.0MB, GPU 7: 3544.6MB/8600.0MB, Total: 28364.4MB allocated, 68864.0MB reserved, Mean: 3545.6MB allocated, 8608.0MB reserved
2025-03-25 03:04:08,082 - training - INFO - Epoch: 38/200000, Batch: 36/55, Loss: 4.3981, Throughput: 64.85 samples/sec
2025-03-25 03:04:21,115 - training - INFO - GPU Memory: GPU 0: 3502.0MB/8660.0MB, GPU 1: 3506.7MB/8630.0MB, GPU 2: 3504.1MB/8650.0MB, GPU 3: 3502.4MB/8638.0MB, GPU 4: 3502.6MB/8626.0MB, GPU 5: 3505.8MB/8640.0MB, GPU 6: 3506.5MB/8658.0MB, GPU 7: 3502.5MB/8620.0MB, Total: 28032.6MB allocated, 69122.0MB reserved, Mean: 3504.1MB allocated, 8640.2MB reserved
2025-03-25 03:04:21,115 - training - INFO - Epoch: 38/200000, Batch: 54/55, Loss: 4.4109, Throughput: 65.31 samples/sec
2025-03-25 03:04:21,116 - training - INFO - Epoch 38 completed in 40.42s. Average loss: 4.4191
2025-03-25 03:04:21,121 - training - INFO - Starting epoch 39/200000
2025-03-25 03:04:21,872 - training - INFO - GPU Memory: GPU 0: 3545.7MB/8660.0MB, GPU 1: 3548.0MB/8630.0MB, GPU 2: 3544.9MB/8650.0MB, GPU 3: 3544.6MB/8638.0MB, GPU 4: 3543.9MB/8626.0MB, GPU 5: 3547.1MB/8640.0MB, GPU 6: 3547.6MB/8658.0MB, GPU 7: 3543.5MB/8620.0MB, Total: 28365.2MB allocated, 69122.0MB reserved, Mean: 3545.7MB allocated, 8640.2MB reserved
2025-03-25 03:04:21,873 - training - INFO - Epoch: 39/200000, Batch: 0/55, Loss: 4.7347, Throughput: 63.88 samples/sec
2025-03-25 03:04:34,962 - training - INFO - GPU Memory: GPU 0: 3546.9MB/8610.0MB, GPU 1: 3546.2MB/8602.0MB, GPU 2: 3545.3MB/8606.0MB, GPU 3: 3550.3MB/8616.0MB, GPU 4: 3542.3MB/8616.0MB, GPU 5: 3544.8MB/8612.0MB, GPU 6: 3549.1MB/8598.0MB, GPU 7: 3545.7MB/8616.0MB, Total: 28370.8MB allocated, 68876.0MB reserved, Mean: 3546.3MB allocated, 8609.5MB reserved
2025-03-25 03:04:34,962 - training - INFO - Epoch: 39/200000, Batch: 18/55, Loss: 4.4233, Throughput: 65.89 samples/sec
2025-03-25 03:04:48,375 - training - INFO - GPU Memory: GPU 0: 3546.3MB/8610.0MB, GPU 1: 3545.6MB/8602.0MB, GPU 2: 3545.0MB/8606.0MB, GPU 3: 3549.8MB/8616.0MB, GPU 4: 3541.8MB/8616.0MB, GPU 5: 3544.3MB/8612.0MB, GPU 6: 3548.6MB/8598.0MB, GPU 7: 3545.1MB/8616.0MB, Total: 28366.4MB allocated, 68876.0MB reserved, Mean: 3545.8MB allocated, 8609.5MB reserved
2025-03-25 03:04:48,376 - training - INFO - Epoch: 39/200000, Batch: 36/55, Loss: 4.3603, Throughput: 65.16 samples/sec
2025-03-25 03:05:01,301 - training - INFO - GPU Memory: GPU 0: 3502.7MB/8638.0MB, GPU 1: 3507.3MB/8642.0MB, GPU 2: 3505.0MB/8648.0MB, GPU 3: 3503.7MB/8660.0MB, GPU 4: 3502.8MB/8640.0MB, GPU 5: 3504.2MB/8622.0MB, GPU 6: 3503.5MB/8654.0MB, GPU 7: 3505.5MB/8636.0MB, Total: 28034.5MB allocated, 69140.0MB reserved, Mean: 3504.3MB allocated, 8642.5MB reserved
2025-03-25 03:05:01,301 - training - INFO - Epoch: 39/200000, Batch: 54/55, Loss: 4.3731, Throughput: 65.70 samples/sec
2025-03-25 03:05:01,302 - training - INFO - Epoch 39 completed in 40.18s. Average loss: 4.4069
2025-03-25 03:05:01,311 - training - INFO - Starting epoch 40/200000
2025-03-25 03:05:02,046 - training - INFO - GPU Memory: GPU 0: 3544.6MB/8638.0MB, GPU 1: 3548.3MB/8642.0MB, GPU 2: 3546.3MB/8648.0MB, GPU 3: 3546.0MB/8660.0MB, GPU 4: 3543.6MB/8640.0MB, GPU 5: 3545.5MB/8622.0MB, GPU 6: 3545.0MB/8654.0MB, GPU 7: 3546.8MB/8636.0MB, Total: 28366.0MB allocated, 69140.0MB reserved, Mean: 3545.8MB allocated, 8642.5MB reserved
2025-03-25 03:05:02,046 - training - INFO - Epoch: 40/200000, Batch: 0/55, Loss: 4.3930, Throughput: 65.33 samples/sec
2025-03-25 03:05:15,161 - training - INFO - GPU Memory: GPU 0: 3545.0MB/8622.0MB, GPU 1: 3545.9MB/8592.0MB, GPU 2: 3545.1MB/8582.0MB, GPU 3: 3546.9MB/8602.0MB, GPU 4: 3543.7MB/8618.0MB, GPU 5: 3544.8MB/8606.0MB, GPU 6: 3548.7MB/8600.0MB, GPU 7: 3547.0MB/8604.0MB, Total: 28367.2MB allocated, 68826.0MB reserved, Mean: 3545.9MB allocated, 8603.2MB reserved
2025-03-25 03:05:15,162 - training - INFO - Epoch: 40/200000, Batch: 18/55, Loss: 4.4179, Throughput: 65.85 samples/sec
2025-03-25 03:05:28,600 - training - INFO - GPU Memory: GPU 0: 3544.4MB/8622.0MB, GPU 1: 3546.1MB/8592.0MB, GPU 2: 3544.6MB/8582.0MB, GPU 3: 3546.8MB/8602.0MB, GPU 4: 3543.1MB/8618.0MB, GPU 5: 3544.3MB/8608.0MB, GPU 6: 3547.9MB/8600.0MB, GPU 7: 3546.5MB/8604.0MB, Total: 28363.6MB allocated, 68828.0MB reserved, Mean: 3545.4MB allocated, 8603.5MB reserved
2025-03-25 03:05:28,600 - training - INFO - Epoch: 40/200000, Batch: 36/55, Loss: 4.4141, Throughput: 65.08 samples/sec
2025-03-25 03:05:41,586 - training - INFO - GPU Memory: GPU 0: 3503.6MB/8636.0MB, GPU 1: 3501.8MB/8646.0MB, GPU 2: 3502.6MB/8652.0MB, GPU 3: 3504.0MB/8642.0MB, GPU 4: 3502.2MB/8650.0MB, GPU 5: 3501.6MB/8642.0MB, GPU 6: 3507.8MB/8654.0MB, GPU 7: 3504.0MB/8642.0MB, Total: 28027.7MB allocated, 69164.0MB reserved, Mean: 3503.5MB allocated, 8645.5MB reserved
2025-03-25 03:05:41,587 - training - INFO - Epoch: 40/200000, Batch: 54/55, Loss: 4.3702, Throughput: 65.55 samples/sec
2025-03-25 03:05:41,588 - training - INFO - Epoch 40 completed in 40.28s. Average loss: 4.4076
2025-03-25 03:05:41,598 - training - INFO - Starting epoch 41/200000
2025-03-25 03:05:42,348 - training - INFO - GPU Memory: GPU 0: 3545.7MB/8636.0MB, GPU 1: 3543.4MB/8646.0MB, GPU 2: 3544.1MB/8652.0MB, GPU 3: 3546.2MB/8642.0MB, GPU 4: 3543.7MB/8650.0MB, GPU 5: 3545.0MB/8642.0MB, GPU 6: 3548.9MB/8654.0MB, GPU 7: 3545.1MB/8642.0MB, Total: 28362.0MB allocated, 69164.0MB reserved, Mean: 3545.3MB allocated, 8645.5MB reserved
2025-03-25 03:05:42,348 - training - INFO - Epoch: 41/200000, Batch: 0/55, Loss: 4.5420, Throughput: 64.04 samples/sec
2025-03-25 03:05:55,491 - training - INFO - GPU Memory: GPU 0: 3545.7MB/8630.0MB, GPU 1: 3544.9MB/8594.0MB, GPU 2: 3542.1MB/8600.0MB, GPU 3: 3550.1MB/8628.0MB, GPU 4: 3545.4MB/8598.0MB, GPU 5: 3547.1MB/8598.0MB, GPU 6: 3545.9MB/8606.0MB, GPU 7: 3543.8MB/8582.0MB, Total: 28365.0MB allocated, 68836.0MB reserved, Mean: 3545.6MB allocated, 8604.5MB reserved
2025-03-25 03:05:55,491 - training - INFO - Epoch: 41/200000, Batch: 18/55, Loss: 4.4711, Throughput: 65.65 samples/sec
2025-03-25 03:06:08,917 - training - INFO - GPU Memory: GPU 0: 3543.8MB/8630.0MB, GPU 1: 3545.6MB/8594.0MB, GPU 2: 3541.7MB/8600.0MB, GPU 3: 3549.6MB/8628.0MB, GPU 4: 3546.3MB/8598.0MB, GPU 5: 3546.7MB/8598.0MB, GPU 6: 3545.3MB/8606.0MB, GPU 7: 3544.6MB/8582.0MB, Total: 28363.7MB allocated, 68836.0MB reserved, Mean: 3545.5MB allocated, 8604.5MB reserved
2025-03-25 03:06:08,917 - training - INFO - Epoch: 41/200000, Batch: 36/55, Loss: 4.4299, Throughput: 65.01 samples/sec
2025-03-25 03:06:21,915 - training - INFO - GPU Memory: GPU 0: 3503.5MB/8650.0MB, GPU 1: 3504.4MB/8640.0MB, GPU 2: 3501.4MB/8630.0MB, GPU 3: 3502.9MB/8648.0MB, GPU 4: 3503.8MB/8644.0MB, GPU 5: 3508.6MB/8660.0MB, GPU 6: 3508.8MB/8634.0MB, GPU 7: 3503.7MB/8650.0MB, Total: 28037.1MB allocated, 69156.0MB reserved, Mean: 3504.6MB allocated, 8644.5MB reserved
2025-03-25 03:06:21,915 - training - INFO - Epoch: 41/200000, Batch: 54/55, Loss: 4.4486, Throughput: 65.48 samples/sec
2025-03-25 03:06:21,916 - training - INFO - Epoch 41 completed in 40.32s. Average loss: 4.3996
2025-03-25 03:06:21,923 - training - INFO - Starting epoch 42/200000
2025-03-25 03:06:22,697 - training - INFO - GPU Memory: GPU 0: 3543.7MB/8650.0MB, GPU 1: 3545.3MB/8640.0MB, GPU 2: 3546.4MB/8630.0MB, GPU 3: 3544.8MB/8648.0MB, GPU 4: 3544.8MB/8644.0MB, GPU 5: 3550.6MB/8660.0MB, GPU 6: 3549.6MB/8634.0MB, GPU 7: 3544.8MB/8650.0MB, Total: 28369.9MB allocated, 69156.0MB reserved, Mean: 3546.2MB allocated, 8644.5MB reserved
2025-03-25 03:06:22,697 - training - INFO - Epoch: 42/200000, Batch: 0/55, Loss: 4.6173, Throughput: 62.03 samples/sec
2025-03-25 03:06:35,808 - training - INFO - GPU Memory: GPU 0: 3548.8MB/8594.0MB, GPU 1: 3545.9MB/8600.0MB, GPU 2: 3544.0MB/8598.0MB, GPU 3: 3544.4MB/8622.0MB, GPU 4: 3543.8MB/8584.0MB, GPU 5: 3545.6MB/8618.0MB, GPU 6: 3547.6MB/8614.0MB, GPU 7: 3545.9MB/8598.0MB, Total: 28366.0MB allocated, 68828.0MB reserved, Mean: 3545.8MB allocated, 8603.5MB reserved
2025-03-25 03:06:35,808 - training - INFO - Epoch: 42/200000, Batch: 18/55, Loss: 4.3514, Throughput: 65.69 samples/sec
2025-03-25 03:06:49,323 - training - INFO - GPU Memory: GPU 0: 3548.8MB/8594.0MB, GPU 1: 3545.3MB/8600.0MB, GPU 2: 3543.1MB/8598.0MB, GPU 3: 3546.5MB/8622.0MB, GPU 4: 3543.0MB/8584.0MB, GPU 5: 3547.0MB/8618.0MB, GPU 6: 3544.6MB/8614.0MB, GPU 7: 3545.1MB/8598.0MB, Total: 28363.4MB allocated, 68828.0MB reserved, Mean: 3545.4MB allocated, 8603.5MB reserved
2025-03-25 03:06:49,324 - training - INFO - Epoch: 42/200000, Batch: 36/55, Loss: 4.4309, Throughput: 64.82 samples/sec
2025-03-25 03:07:02,333 - training - INFO - GPU Memory: GPU 0: 3505.2MB/8642.0MB, GPU 1: 3508.3MB/8654.0MB, GPU 2: 3503.0MB/8632.0MB, GPU 3: 3506.5MB/8660.0MB, GPU 4: 3505.7MB/8628.0MB, GPU 5: 3503.3MB/8620.0MB, GPU 6: 3506.3MB/8656.0MB, GPU 7: 3505.3MB/8654.0MB, Total: 28043.6MB allocated, 69146.0MB reserved, Mean: 3505.4MB allocated, 8643.2MB reserved
2025-03-25 03:07:02,334 - training - INFO - Epoch: 42/200000, Batch: 54/55, Loss: 4.4166, Throughput: 65.33 samples/sec
2025-03-25 03:07:02,334 - training - INFO - Epoch 42 completed in 40.41s. Average loss: 4.3992
2025-03-25 03:07:02,342 - training - INFO - Starting epoch 43/200000
2025-03-25 03:07:03,109 - training - INFO - GPU Memory: GPU 0: 3546.8MB/8642.0MB, GPU 1: 3549.3MB/8654.0MB, GPU 2: 3543.8MB/8632.0MB, GPU 3: 3548.8MB/8660.0MB, GPU 4: 3547.0MB/8628.0MB, GPU 5: 3545.0MB/8620.0MB, GPU 6: 3548.1MB/8656.0MB, GPU 7: 3546.8MB/8654.0MB, Total: 28375.7MB allocated, 69146.0MB reserved, Mean: 3547.0MB allocated, 8643.2MB reserved
2025-03-25 03:07:03,109 - training - INFO - Epoch: 43/200000, Batch: 0/55, Loss: 4.5599, Throughput: 62.57 samples/sec
2025-03-25 03:07:16,189 - training - INFO - GPU Memory: GPU 0: 3547.3MB/8616.0MB, GPU 1: 3545.6MB/8602.0MB, GPU 2: 3545.1MB/8598.0MB, GPU 3: 3548.3MB/8618.0MB, GPU 4: 3545.8MB/8616.0MB, GPU 5: 3545.3MB/8602.0MB, GPU 6: 3545.6MB/8616.0MB, GPU 7: 3544.3MB/8610.0MB, Total: 28367.4MB allocated, 68878.0MB reserved, Mean: 3545.9MB allocated, 8609.8MB reserved
2025-03-25 03:07:16,190 - training - INFO - Epoch: 43/200000, Batch: 18/55, Loss: 4.2979, Throughput: 65.86 samples/sec
2025-03-25 03:07:29,465 - training - INFO - GPU Memory: GPU 0: 3547.1MB/8616.0MB, GPU 1: 3545.0MB/8602.0MB, GPU 2: 3544.3MB/8598.0MB, GPU 3: 3547.4MB/8618.0MB, GPU 4: 3544.7MB/8616.0MB, GPU 5: 3544.8MB/8602.0MB, GPU 6: 3544.5MB/8616.0MB, GPU 7: 3543.5MB/8610.0MB, Total: 28361.2MB allocated, 68878.0MB reserved, Mean: 3545.2MB allocated, 8609.8MB reserved
2025-03-25 03:07:29,466 - training - INFO - Epoch: 43/200000, Batch: 36/55, Loss: 4.3547, Throughput: 65.48 samples/sec
2025-03-25 03:07:42,438 - training - INFO - GPU Memory: GPU 0: 3504.2MB/8648.0MB, GPU 1: 3502.5MB/8644.0MB, GPU 2: 3502.2MB/8618.0MB, GPU 3: 3506.5MB/8652.0MB, GPU 4: 3505.3MB/8634.0MB, GPU 5: 3506.2MB/8648.0MB, GPU 6: 3505.2MB/8636.0MB, GPU 7: 3500.7MB/8624.0MB, Total: 28032.7MB allocated, 69104.0MB reserved, Mean: 3504.1MB allocated, 8638.0MB reserved
2025-03-25 03:07:42,439 - training - INFO - Epoch: 43/200000, Batch: 54/55, Loss: 4.3834, Throughput: 65.84 samples/sec
2025-03-25 03:07:42,439 - training - INFO - Epoch 43 completed in 40.10s. Average loss: 4.3927
2025-03-25 03:07:42,444 - training - INFO - Starting epoch 44/200000
2025-03-25 03:07:43,224 - training - INFO - GPU Memory: GPU 0: 3546.5MB/8648.0MB, GPU 1: 3543.8MB/8644.0MB, GPU 2: 3543.3MB/8618.0MB, GPU 3: 3547.5MB/8652.0MB, GPU 4: 3546.8MB/8634.0MB, GPU 5: 3547.7MB/8648.0MB, GPU 6: 3546.5MB/8636.0MB, GPU 7: 3542.9MB/8624.0MB, Total: 28365.0MB allocated, 69104.0MB reserved, Mean: 3545.6MB allocated, 8638.0MB reserved
2025-03-25 03:07:43,225 - training - INFO - Epoch: 44/200000, Batch: 0/55, Loss: 4.3253, Throughput: 61.50 samples/sec
2025-03-25 03:07:56,350 - training - INFO - GPU Memory: GPU 0: 3544.8MB/8612.0MB, GPU 1: 3548.8MB/8612.0MB, GPU 2: 3546.9MB/8586.0MB, GPU 3: 3547.5MB/8612.0MB, GPU 4: 3545.8MB/8602.0MB, GPU 5: 3547.7MB/8618.0MB, GPU 6: 3546.9MB/8624.0MB, GPU 7: 3545.5MB/8590.0MB, Total: 28373.9MB allocated, 68856.0MB reserved, Mean: 3546.7MB allocated, 8607.0MB reserved
2025-03-25 03:07:56,350 - training - INFO - Epoch: 44/200000, Batch: 18/55, Loss: 4.3768, Throughput: 65.58 samples/sec
2025-03-25 03:08:09,705 - training - INFO - GPU Memory: GPU 0: 3545.4MB/8612.0MB, GPU 1: 3548.2MB/8612.0MB, GPU 2: 3546.6MB/8586.0MB, GPU 3: 3546.6MB/8612.0MB, GPU 4: 3545.2MB/8602.0MB, GPU 5: 3547.1MB/8618.0MB, GPU 6: 3546.3MB/8624.0MB, GPU 7: 3545.0MB/8590.0MB, Total: 28370.5MB allocated, 68856.0MB reserved, Mean: 3546.3MB allocated, 8607.0MB reserved
2025-03-25 03:08:09,705 - training - INFO - Epoch: 44/200000, Batch: 36/55, Loss: 4.3570, Throughput: 65.15 samples/sec
2025-03-25 03:08:22,617 - training - INFO - GPU Memory: GPU 0: 3502.9MB/8656.0MB, GPU 1: 3503.8MB/8634.0MB, GPU 2: 3501.4MB/8620.0MB, GPU 3: 3503.7MB/8660.0MB, GPU 4: 3505.7MB/8634.0MB, GPU 5: 3505.7MB/8656.0MB, GPU 6: 3504.7MB/8644.0MB, GPU 7: 3501.9MB/8632.0MB, Total: 28029.7MB allocated, 69136.0MB reserved, Mean: 3503.7MB allocated, 8642.0MB reserved
2025-03-25 03:08:22,618 - training - INFO - Epoch: 44/200000, Batch: 54/55, Loss: 4.3881, Throughput: 65.72 samples/sec
2025-03-25 03:08:22,618 - training - INFO - Epoch 44 completed in 40.17s. Average loss: 4.3911
2025-03-25 03:08:22,622 - training - INFO - Starting epoch 45/200000
2025-03-25 03:08:23,395 - training - INFO - GPU Memory: GPU 0: 3544.6MB/8656.0MB, GPU 1: 3545.5MB/8634.0MB, GPU 2: 3542.1MB/8620.0MB, GPU 3: 3546.7MB/8660.0MB, GPU 4: 3546.8MB/8634.0MB, GPU 5: 3547.1MB/8656.0MB, GPU 6: 3546.0MB/8644.0MB, GPU 7: 3543.6MB/8632.0MB, Total: 28362.3MB allocated, 69136.0MB reserved, Mean: 3545.3MB allocated, 8642.0MB reserved
2025-03-25 03:08:23,395 - training - INFO - Epoch: 45/200000, Batch: 0/55, Loss: 4.1017, Throughput: 62.15 samples/sec
2025-03-25 03:08:36,427 - training - INFO - GPU Memory: GPU 0: 3546.1MB/8598.0MB, GPU 1: 3545.6MB/8604.0MB, GPU 2: 3543.2MB/8604.0MB, GPU 3: 3545.2MB/8626.0MB, GPU 4: 3544.2MB/8612.0MB, GPU 5: 3547.3MB/8616.0MB, GPU 6: 3545.8MB/8596.0MB, GPU 7: 3545.5MB/8606.0MB, Total: 28362.9MB allocated, 68862.0MB reserved, Mean: 3545.4MB allocated, 8607.8MB reserved
2025-03-25 03:08:36,427 - training - INFO - Epoch: 45/200000, Batch: 18/55, Loss: 4.3815, Throughput: 66.06 samples/sec
2025-03-25 03:08:49,833 - training - INFO - GPU Memory: GPU 0: 3546.0MB/8598.0MB, GPU 1: 3545.3MB/8604.0MB, GPU 2: 3542.6MB/8604.0MB, GPU 3: 3545.4MB/8626.0MB, GPU 4: 3544.4MB/8612.0MB, GPU 5: 3546.5MB/8616.0MB, GPU 6: 3548.8MB/8596.0MB, GPU 7: 3544.6MB/8606.0MB, Total: 28363.5MB allocated, 68862.0MB reserved, Mean: 3545.4MB allocated, 8607.8MB reserved
2025-03-25 03:08:49,833 - training - INFO - Epoch: 45/200000, Batch: 36/55, Loss: 4.3958, Throughput: 65.27 samples/sec
2025-03-25 03:09:02,883 - training - INFO - GPU Memory: GPU 0: 3504.6MB/8650.0MB, GPU 1: 3505.9MB/8636.0MB, GPU 2: 3503.5MB/8646.0MB, GPU 3: 3501.3MB/8660.0MB, GPU 4: 3503.6MB/8622.0MB, GPU 5: 3504.5MB/8650.0MB, GPU 6: 3505.3MB/8640.0MB, GPU 7: 3503.5MB/8654.0MB, Total: 28032.3MB allocated, 69158.0MB reserved, Mean: 3504.0MB allocated, 8644.8MB reserved
2025-03-25 03:09:02,883 - training - INFO - Epoch: 45/200000, Batch: 54/55, Loss: 4.4023, Throughput: 65.57 samples/sec
2025-03-25 03:09:02,884 - training - INFO - Epoch 45 completed in 40.26s. Average loss: 4.3886
2025-03-25 03:09:02,888 - training - INFO - Starting epoch 46/200000
2025-03-25 03:09:03,639 - training - INFO - GPU Memory: GPU 0: 3547.1MB/8650.0MB, GPU 1: 3547.3MB/8636.0MB, GPU 2: 3544.3MB/8646.0MB, GPU 3: 3543.3MB/8660.0MB, GPU 4: 3544.9MB/8622.0MB, GPU 5: 3544.8MB/8650.0MB, GPU 6: 3546.6MB/8640.0MB, GPU 7: 3545.1MB/8654.0MB, Total: 28363.4MB allocated, 69158.0MB reserved, Mean: 3545.4MB allocated, 8644.8MB reserved
2025-03-25 03:09:03,640 - training - INFO - Epoch: 46/200000, Batch: 0/55, Loss: 4.3319, Throughput: 63.90 samples/sec
2025-03-25 03:09:16,724 - training - INFO - GPU Memory: GPU 0: 3547.0MB/8624.0MB, GPU 1: 3546.8MB/8600.0MB, GPU 2: 3548.1MB/8600.0MB, GPU 3: 3545.4MB/8618.0MB, GPU 4: 3545.7MB/8618.0MB, GPU 5: 3544.5MB/8610.0MB, GPU 6: 3546.9MB/8604.0MB, GPU 7: 3545.6MB/8596.0MB, Total: 28370.0MB allocated, 68870.0MB reserved, Mean: 3546.2MB allocated, 8608.8MB reserved
2025-03-25 03:09:16,724 - training - INFO - Epoch: 46/200000, Batch: 18/55, Loss: 4.3957, Throughput: 65.92 samples/sec
2025-03-25 03:09:30,117 - training - INFO - GPU Memory: GPU 0: 3546.5MB/8624.0MB, GPU 1: 3546.5MB/8600.0MB, GPU 2: 3547.5MB/8600.0MB, GPU 3: 3544.8MB/8618.0MB, GPU 4: 3545.1MB/8618.0MB, GPU 5: 3543.9MB/8610.0MB, GPU 6: 3546.6MB/8604.0MB, GPU 7: 3545.0MB/8596.0MB, Total: 28365.8MB allocated, 68870.0MB reserved, Mean: 3545.7MB allocated, 8608.8MB reserved
2025-03-25 03:09:30,118 - training - INFO - Epoch: 46/200000, Batch: 36/55, Loss: 4.4354, Throughput: 65.22 samples/sec
2025-03-25 03:09:43,108 - training - INFO - GPU Memory: GPU 0: 3502.6MB/8636.0MB, GPU 1: 3505.7MB/8656.0MB, GPU 2: 3503.2MB/8644.0MB, GPU 3: 3502.5MB/8650.0MB, GPU 4: 3504.3MB/8646.0MB, GPU 5: 3503.3MB/8648.0MB, GPU 6: 3503.8MB/8660.0MB, GPU 7: 3503.7MB/8650.0MB, Total: 28029.0MB allocated, 69190.0MB reserved, Mean: 3503.6MB allocated, 8648.8MB reserved
2025-03-25 03:09:43,109 - training - INFO - Epoch: 46/200000, Batch: 54/55, Loss: 4.4399, Throughput: 65.64 samples/sec
2025-03-25 03:09:43,109 - training - INFO - Epoch 46 completed in 40.22s. Average loss: 4.3847
2025-03-25 03:09:43,115 - training - INFO - Starting epoch 47/200000
2025-03-25 03:09:43,872 - training - INFO - GPU Memory: GPU 0: 3546.3MB/8636.0MB, GPU 1: 3547.0MB/8656.0MB, GPU 2: 3544.0MB/8644.0MB, GPU 3: 3544.3MB/8650.0MB, GPU 4: 3545.5MB/8646.0MB, GPU 5: 3543.9MB/8648.0MB, GPU 6: 3545.5MB/8660.0MB, GPU 7: 3545.0MB/8650.0MB, Total: 28361.4MB allocated, 69190.0MB reserved, Mean: 3545.2MB allocated, 8648.8MB reserved
2025-03-25 03:09:43,872 - training - INFO - Epoch: 47/200000, Batch: 0/55, Loss: 4.4640, Throughput: 63.47 samples/sec
2025-03-25 03:09:57,022 - training - INFO - GPU Memory: GPU 0: 3547.4MB/8600.0MB, GPU 1: 3549.4MB/8602.0MB, GPU 2: 3547.9MB/8594.0MB, GPU 3: 3544.9MB/8600.0MB, GPU 4: 3546.5MB/8600.0MB, GPU 5: 3543.4MB/8610.0MB, GPU 6: 3549.0MB/8610.0MB, GPU 7: 3547.7MB/8600.0MB, Total: 28376.1MB allocated, 68816.0MB reserved, Mean: 3547.0MB allocated, 8602.0MB reserved
2025-03-25 03:09:57,023 - training - INFO - Epoch: 47/200000, Batch: 18/55, Loss: 4.4050, Throughput: 65.58 samples/sec
2025-03-25 03:10:10,470 - training - INFO - GPU Memory: GPU 0: 3546.8MB/8600.0MB, GPU 1: 3548.3MB/8602.0MB, GPU 2: 3547.6MB/8594.0MB, GPU 3: 3546.3MB/8600.0MB, GPU 4: 3548.1MB/8600.0MB, GPU 5: 3543.6MB/8610.0MB, GPU 6: 3548.4MB/8610.0MB, GPU 7: 3547.8MB/8600.0MB, Total: 28376.8MB allocated, 68816.0MB reserved, Mean: 3547.1MB allocated, 8602.0MB reserved
2025-03-25 03:10:10,471 - training - INFO - Epoch: 47/200000, Batch: 36/55, Loss: 4.3362, Throughput: 64.92 samples/sec
2025-03-25 03:10:23,481 - training - INFO - GPU Memory: GPU 0: 3504.7MB/8634.0MB, GPU 1: 3504.0MB/8650.0MB, GPU 2: 3502.7MB/8656.0MB, GPU 3: 3506.3MB/8652.0MB, GPU 4: 3504.5MB/8638.0MB, GPU 5: 3504.9MB/8632.0MB, GPU 6: 3505.2MB/8642.0MB, GPU 7: 3504.5MB/8646.0MB, Total: 28036.9MB allocated, 69150.0MB reserved, Mean: 3504.6MB allocated, 8643.8MB reserved
2025-03-25 03:10:23,482 - training - INFO - Epoch: 47/200000, Batch: 54/55, Loss: 4.3425, Throughput: 65.40 samples/sec
2025-03-25 03:10:23,482 - training - INFO - Epoch 47 completed in 40.37s. Average loss: 4.3819
2025-03-25 03:10:23,493 - training - INFO - Starting epoch 48/200000
2025-03-25 03:10:24,264 - training - INFO - GPU Memory: GPU 0: 3547.2MB/8634.0MB, GPU 1: 3545.7MB/8650.0MB, GPU 2: 3543.5MB/8656.0MB, GPU 3: 3548.6MB/8652.0MB, GPU 4: 3546.3MB/8638.0MB, GPU 5: 3546.3MB/8632.0MB, GPU 6: 3544.0MB/8642.0MB, GPU 7: 3546.0MB/8646.0MB, Total: 28367.7MB allocated, 69150.0MB reserved, Mean: 3546.0MB allocated, 8643.8MB reserved
2025-03-25 03:10:24,264 - training - INFO - Epoch: 48/200000, Batch: 0/55, Loss: 4.4114, Throughput: 62.27 samples/sec
2025-03-25 03:10:37,407 - training - INFO - GPU Memory: GPU 0: 3546.0MB/8600.0MB, GPU 1: 3548.1MB/8598.0MB, GPU 2: 3547.6MB/8598.0MB, GPU 3: 3548.6MB/8630.0MB, GPU 4: 3543.8MB/8616.0MB, GPU 5: 3546.9MB/8622.0MB, GPU 6: 3547.3MB/8598.0MB, GPU 7: 3544.8MB/8614.0MB, Total: 28373.1MB allocated, 68876.0MB reserved, Mean: 3546.6MB allocated, 8609.5MB reserved
2025-03-25 03:10:37,407 - training - INFO - Epoch: 48/200000, Batch: 18/55, Loss: 4.3383, Throughput: 65.55 samples/sec
2025-03-25 03:10:50,814 - training - INFO - GPU Memory: GPU 0: 3545.4MB/8600.0MB, GPU 1: 3547.3MB/8598.0MB, GPU 2: 3544.8MB/8598.0MB, GPU 3: 3547.4MB/8630.0MB, GPU 4: 3543.0MB/8616.0MB, GPU 5: 3545.2MB/8622.0MB, GPU 6: 3547.3MB/8598.0MB, GPU 7: 3544.2MB/8614.0MB, Total: 28364.7MB allocated, 68876.0MB reserved, Mean: 3545.6MB allocated, 8609.5MB reserved
2025-03-25 03:10:50,815 - training - INFO - Epoch: 48/200000, Batch: 36/55, Loss: 4.3634, Throughput: 65.00 samples/sec
2025-03-25 03:11:03,750 - training - INFO - GPU Memory: GPU 0: 3504.3MB/8662.0MB, GPU 1: 3504.8MB/8658.0MB, GPU 2: 3502.6MB/8642.0MB, GPU 3: 3500.8MB/8644.0MB, GPU 4: 3502.4MB/8644.0MB, GPU 5: 3505.9MB/8654.0MB, GPU 6: 3504.1MB/8656.0MB, GPU 7: 3504.7MB/8652.0MB, Total: 28029.6MB allocated, 69212.0MB reserved, Mean: 3503.7MB allocated, 8651.5MB reserved
2025-03-25 03:11:03,750 - training - INFO - Epoch: 48/200000, Batch: 54/55, Loss: 4.3511, Throughput: 65.58 samples/sec
2025-03-25 03:11:03,751 - training - INFO - Epoch 48 completed in 40.26s. Average loss: 4.3767
2025-03-25 03:11:03,761 - training - INFO - Starting epoch 49/200000
2025-03-25 03:11:04,531 - training - INFO - GPU Memory: GPU 0: 3545.1MB/8662.0MB, GPU 1: 3546.5MB/8658.0MB, GPU 2: 3544.3MB/8642.0MB, GPU 3: 3544.5MB/8644.0MB, GPU 4: 3543.7MB/8644.0MB, GPU 5: 3547.3MB/8654.0MB, GPU 6: 3545.3MB/8656.0MB, GPU 7: 3545.8MB/8652.0MB, Total: 28362.5MB allocated, 69212.0MB reserved, Mean: 3545.3MB allocated, 8651.5MB reserved
2025-03-25 03:11:04,531 - training - INFO - Epoch: 49/200000, Batch: 0/55, Loss: 4.1300, Throughput: 62.36 samples/sec
2025-03-25 03:11:17,726 - training - INFO - GPU Memory: GPU 0: 3546.0MB/8614.0MB, GPU 1: 3545.8MB/8614.0MB, GPU 2: 3545.2MB/8596.0MB, GPU 3: 3546.6MB/8612.0MB, GPU 4: 3545.9MB/8596.0MB, GPU 5: 3547.3MB/8620.0MB, GPU 6: 3546.0MB/8596.0MB, GPU 7: 3544.5MB/8618.0MB, Total: 28367.2MB allocated, 68866.0MB reserved, Mean: 3545.9MB allocated, 8608.2MB reserved
2025-03-25 03:11:17,726 - training - INFO - Epoch: 49/200000, Batch: 18/55, Loss: 4.3783, Throughput: 65.31 samples/sec
2025-03-25 03:11:31,119 - training - INFO - GPU Memory: GPU 0: 3545.6MB/8614.0MB, GPU 1: 3545.3MB/8614.0MB, GPU 2: 3544.6MB/8596.0MB, GPU 3: 3546.3MB/8612.0MB, GPU 4: 3545.3MB/8596.0MB, GPU 5: 3547.3MB/8620.0MB, GPU 6: 3545.4MB/8596.0MB, GPU 7: 3544.7MB/8618.0MB, Total: 28364.5MB allocated, 68866.0MB reserved, Mean: 3545.6MB allocated, 8608.2MB reserved
2025-03-25 03:11:31,120 - training - INFO - Epoch: 49/200000, Batch: 36/55, Loss: 4.3894, Throughput: 64.92 samples/sec
2025-03-25 03:11:44,099 - training - INFO - GPU Memory: GPU 0: 3504.2MB/8654.0MB, GPU 1: 3506.0MB/8636.0MB, GPU 2: 3500.4MB/8644.0MB, GPU 3: 3503.0MB/8656.0MB, GPU 4: 3503.3MB/8632.0MB, GPU 5: 3504.0MB/8654.0MB, GPU 6: 3506.4MB/8646.0MB, GPU 7: 3501.9MB/8636.0MB, Total: 28029.2MB allocated, 69158.0MB reserved, Mean: 3503.7MB allocated, 8644.8MB reserved
2025-03-25 03:11:44,099 - training - INFO - Epoch: 49/200000, Batch: 54/55, Loss: 4.3856, Throughput: 65.45 samples/sec
2025-03-25 03:11:44,099 - training - INFO - Epoch 49 completed in 40.34s. Average loss: 4.3860
2025-03-25 03:11:44,107 - training - INFO - Starting epoch 50/200000
2025-03-25 03:11:44,862 - training - INFO - GPU Memory: GPU 0: 3547.5MB/8654.0MB, GPU 1: 3547.5MB/8636.0MB, GPU 2: 3542.1MB/8644.0MB, GPU 3: 3544.3MB/8656.0MB, GPU 4: 3544.8MB/8632.0MB, GPU 5: 3545.5MB/8654.0MB, GPU 6: 3547.1MB/8646.0MB, GPU 7: 3545.0MB/8636.0MB, Total: 28363.8MB allocated, 69158.0MB reserved, Mean: 3545.5MB allocated, 8644.8MB reserved
2025-03-25 03:11:44,863 - training - INFO - Epoch: 50/200000, Batch: 0/55, Loss: 4.5432, Throughput: 63.51 samples/sec
2025-03-25 03:11:58,015 - training - INFO - GPU Memory: GPU 0: 3547.3MB/8624.0MB, GPU 1: 3548.3MB/8600.0MB, GPU 2: 3543.3MB/8594.0MB, GPU 3: 3548.2MB/8610.0MB, GPU 4: 3544.0MB/8618.0MB, GPU 5: 3547.1MB/8608.0MB, GPU 6: 3543.7MB/8612.0MB, GPU 7: 3545.3MB/8614.0MB, Total: 28367.3MB allocated, 68880.0MB reserved, Mean: 3545.9MB allocated, 8610.0MB reserved
2025-03-25 03:11:58,015 - training - INFO - Epoch: 50/200000, Batch: 18/55, Loss: 4.3810, Throughput: 65.57 samples/sec
2025-03-25 03:12:11,301 - training - INFO - GPU Memory: GPU 0: 3548.0MB/8624.0MB, GPU 1: 3547.5MB/8600.0MB, GPU 2: 3543.0MB/8594.0MB, GPU 3: 3547.6MB/8610.0MB, GPU 4: 3544.0MB/8618.0MB, GPU 5: 3546.5MB/8608.0MB, GPU 6: 3543.4MB/8612.0MB, GPU 7: 3544.8MB/8614.0MB, Total: 28364.8MB allocated, 68880.0MB reserved, Mean: 3545.6MB allocated, 8610.0MB reserved
2025-03-25 03:12:11,302 - training - INFO - Epoch: 50/200000, Batch: 36/55, Loss: 4.3999, Throughput: 65.31 samples/sec
2025-03-25 03:12:24,258 - training - INFO - GPU Memory: GPU 0: 3503.2MB/8644.0MB, GPU 1: 3504.3MB/8636.0MB, GPU 2: 3501.4MB/8624.0MB, GPU 3: 3503.6MB/8660.0MB, GPU 4: 3505.5MB/8620.0MB, GPU 5: 3504.1MB/8658.0MB, GPU 6: 3507.2MB/8648.0MB, GPU 7: 3505.5MB/8654.0MB, Total: 28034.9MB allocated, 69144.0MB reserved, Mean: 3504.4MB allocated, 8643.0MB reserved
2025-03-25 03:12:24,259 - training - INFO - Epoch: 50/200000, Batch: 54/55, Loss: 4.4031, Throughput: 65.75 samples/sec
2025-03-25 03:12:24,260 - training - INFO - Epoch 50 completed in 40.15s. Average loss: 4.3760
2025-03-25 03:12:24,269 - training - INFO - Starting epoch 51/200000
2025-03-25 03:12:25,017 - training - INFO - GPU Memory: GPU 0: 3545.5MB/8644.0MB, GPU 1: 3545.8MB/8636.0MB, GPU 2: 3542.9MB/8624.0MB, GPU 3: 3545.4MB/8660.0MB, GPU 4: 3546.6MB/8620.0MB, GPU 5: 3545.6MB/8658.0MB, GPU 6: 3548.6MB/8648.0MB, GPU 7: 3546.8MB/8654.0MB, Total: 28367.3MB allocated, 69144.0MB reserved, Mean: 3545.9MB allocated, 8643.0MB reserved
2025-03-25 03:12:25,017 - training - INFO - Epoch: 51/200000, Batch: 0/55, Loss: 4.4811, Throughput: 64.20 samples/sec
2025-03-25 03:13:00,255 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_6nqa0ojt.log
2025-03-25 03:13:00,256 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 8
2025-03-25 03:13:00,256 - training - INFO - Device: cuda:0
2025-03-25 03:13:00,828 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 03:13:00,828 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 03:13:00,828 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 03:13:00,830 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 03:13:00,830 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 03:13:05,455 - training - INFO - Starting model initialization...
2025-03-25 03:13:14,893 - training - INFO - Per-GPU batch size: 6 (global batch size: 48)
2025-03-25 03:13:14,900 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-25 03:13:14,904 - training - INFO - Starting epoch 1/200000
2025-03-25 03:13:26,431 - training - INFO - GPU Memory: GPU 0: 2233.9MB/5730.0MB, GPU 1: 2233.9MB/5730.0MB, GPU 2: 2234.4MB/5708.0MB, GPU 3: 2233.9MB/5730.0MB, GPU 4: 2233.9MB/5730.0MB, GPU 5: 2233.9MB/5730.0MB, GPU 6: 2234.4MB/5708.0MB, GPU 7: 2233.9MB/5730.0MB, Total: 17872.0MB allocated, 45796.0MB reserved, Mean: 2234.0MB allocated, 5724.5MB reserved
2025-03-25 03:13:26,431 - training - INFO - Epoch: 1/200000, Batch: 0/55, Loss: 10.4674, Throughput: 4.16 samples/sec
2025-03-25 03:13:39,484 - training - INFO - GPU Memory: GPU 0: 3546.8MB/8600.0MB, GPU 1: 3547.0MB/8578.0MB, GPU 2: 3549.5MB/8584.0MB, GPU 3: 3546.0MB/8582.0MB, GPU 4: 3545.3MB/8582.0MB, GPU 5: 3545.1MB/8588.0MB, GPU 6: 3544.0MB/8590.0MB, GPU 7: 3543.9MB/8600.0MB, Total: 28367.6MB allocated, 68704.0MB reserved, Mean: 3545.9MB allocated, 8588.0MB reserved
2025-03-25 03:13:39,485 - training - INFO - Epoch: 1/200000, Batch: 18/55, Loss: 9.2445, Throughput: 37.10 samples/sec
2025-03-25 03:13:52,910 - training - INFO - GPU Memory: GPU 0: 3545.8MB/8600.0MB, GPU 1: 3546.6MB/8578.0MB, GPU 2: 3548.9MB/8584.0MB, GPU 3: 3544.8MB/8582.0MB, GPU 4: 3544.2MB/8582.0MB, GPU 5: 3543.0MB/8588.0MB, GPU 6: 3548.2MB/8590.0MB, GPU 7: 3545.2MB/8600.0MB, Total: 28366.6MB allocated, 68704.0MB reserved, Mean: 3545.8MB allocated, 8588.0MB reserved
2025-03-25 03:13:52,910 - training - INFO - Epoch: 1/200000, Batch: 36/55, Loss: 8.2745, Throughput: 46.73 samples/sec
2025-03-25 03:14:07,120 - training - INFO - GPU Memory: GPU 0: 3499.5MB/5972.0MB, GPU 1: 3508.2MB/5936.0MB, GPU 2: 3500.1MB/5932.0MB, GPU 3: 3499.6MB/5958.0MB, GPU 4: 3497.9MB/5886.0MB, GPU 5: 3502.2MB/5922.0MB, GPU 6: 3502.7MB/6188.0MB, GPU 7: 3504.1MB/6002.0MB, Total: 28014.2MB allocated, 47796.0MB reserved, Mean: 3501.8MB allocated, 5974.5MB reserved
2025-03-25 03:14:07,120 - training - INFO - Epoch: 1/200000, Batch: 54/55, Loss: 7.7360, Throughput: 50.56 samples/sec
2025-03-25 03:14:07,121 - training - INFO - Epoch 1 completed in 52.22s. Average loss: 7.7348
2025-03-25 03:14:07,131 - training - INFO - Starting validation...
2025-03-25 03:14:08,455 - training - INFO - Validation Loss: 6.6021
2025-03-25 03:14:08,455 - training - INFO - Validation loss improved from inf to 6.6021
2025-03-25 03:19:15,209 - training - INFO - New best model saved at epoch 1 (val_loss: 6.6021)
2025-03-25 03:24:22,369 - training - INFO - Checkpoint saved at epoch 1
2025-03-25 03:24:22,429 - training - INFO - Starting epoch 2/200000
2025-03-25 03:24:23,155 - training - INFO - GPU Memory: GPU 0: 3544.5MB/7910.0MB, GPU 1: 3549.9MB/7912.0MB, GPU 2: 3546.6MB/7918.0MB, GPU 3: 3540.2MB/7926.0MB, GPU 4: 3541.1MB/7914.0MB, GPU 5: 3541.8MB/7910.0MB, GPU 6: 3546.5MB/7926.0MB, GPU 7: 3544.3MB/7908.0MB, Total: 28355.0MB allocated, 63324.0MB reserved, Mean: 3544.4MB allocated, 7915.5MB reserved
2025-03-25 03:24:23,155 - training - INFO - Epoch: 2/200000, Batch: 0/55, Loss: 6.5265, Throughput: 66.12 samples/sec
2025-03-25 03:24:36,245 - training - INFO - GPU Memory: GPU 0: 3540.6MB/8622.0MB, GPU 1: 3552.6MB/8614.0MB, GPU 2: 3545.2MB/8614.0MB, GPU 3: 3542.1MB/8626.0MB, GPU 4: 3540.8MB/8606.0MB, GPU 5: 3542.5MB/8624.0MB, GPU 6: 3542.0MB/8614.0MB, GPU 7: 3544.7MB/8622.0MB, Total: 28350.6MB allocated, 68942.0MB reserved, Mean: 3543.8MB allocated, 8617.8MB reserved
2025-03-25 03:24:36,245 - training - INFO - Epoch: 2/200000, Batch: 18/55, Loss: 6.4123, Throughput: 66.01 samples/sec
2025-03-25 03:24:49,698 - training - INFO - GPU Memory: GPU 0: 3540.0MB/8622.0MB, GPU 1: 3552.3MB/8614.0MB, GPU 2: 3545.1MB/8614.0MB, GPU 3: 3543.1MB/8626.0MB, GPU 4: 3541.3MB/8606.0MB, GPU 5: 3541.9MB/8624.0MB, GPU 6: 3543.0MB/8614.0MB, GPU 7: 3545.1MB/8622.0MB, Total: 28351.7MB allocated, 68942.0MB reserved, Mean: 3544.0MB allocated, 8617.8MB reserved
2025-03-25 03:24:49,699 - training - INFO - Epoch: 2/200000, Batch: 36/55, Loss: 6.2296, Throughput: 65.13 samples/sec
2025-03-25 03:25:02,593 - training - INFO - GPU Memory: GPU 0: 3498.6MB/8636.0MB, GPU 1: 3511.4MB/8650.0MB, GPU 2: 3500.6MB/8642.0MB, GPU 3: 3501.3MB/8652.0MB, GPU 4: 3499.3MB/8654.0MB, GPU 5: 3497.9MB/8630.0MB, GPU 6: 3495.6MB/8666.0MB, GPU 7: 3500.0MB/8636.0MB, Total: 28004.7MB allocated, 69166.0MB reserved, Mean: 3500.6MB allocated, 8645.8MB reserved
2025-03-25 03:25:02,593 - training - INFO - Epoch: 2/200000, Batch: 54/55, Loss: 6.0836, Throughput: 65.73 samples/sec
2025-03-25 03:25:02,593 - training - INFO - Epoch 2 completed in 40.16s. Average loss: 6.0991
2025-03-25 03:25:02,599 - training - INFO - Starting validation...
2025-03-25 03:25:02,952 - training - INFO - Validation Loss: 5.6557
2025-03-25 03:25:02,952 - training - INFO - Validation loss improved from 6.6021 to 5.6557
2025-03-25 03:25:03,875 - training - INFO - Removed previous best model from epoch 1
2025-03-25 03:30:05,115 - training - INFO - New best model saved at epoch 2 (val_loss: 5.6557)
2025-03-25 03:35:12,756 - training - INFO - Checkpoint saved at epoch 2
2025-03-25 03:35:12,886 - training - INFO - Starting epoch 3/200000
2025-03-25 03:35:13,574 - training - INFO - GPU Memory: GPU 0: 3543.7MB/7916.0MB, GPU 1: 3555.7MB/7894.0MB, GPU 2: 3545.7MB/7924.0MB, GPU 3: 3542.7MB/7912.0MB, GPU 4: 3544.6MB/7894.0MB, GPU 5: 3542.9MB/7910.0MB, GPU 6: 3542.8MB/7904.0MB, GPU 7: 3544.3MB/7906.0MB, Total: 28362.4MB allocated, 63260.0MB reserved, Mean: 3545.3MB allocated, 7907.5MB reserved
2025-03-25 03:35:13,574 - training - INFO - Epoch: 3/200000, Batch: 0/55, Loss: 5.7226, Throughput: 69.79 samples/sec
2025-03-25 03:35:26,675 - training - INFO - GPU Memory: GPU 0: 3544.8MB/8614.0MB, GPU 1: 3556.3MB/8610.0MB, GPU 2: 3545.5MB/8618.0MB, GPU 3: 3548.6MB/8614.0MB, GPU 4: 3546.3MB/8600.0MB, GPU 5: 3542.1MB/8618.0MB, GPU 6: 3542.8MB/8608.0MB, GPU 7: 3544.6MB/8612.0MB, Total: 28371.0MB allocated, 68894.0MB reserved, Mean: 3546.4MB allocated, 8611.8MB reserved
2025-03-25 03:35:26,676 - training - INFO - Epoch: 3/200000, Batch: 18/55, Loss: 5.7045, Throughput: 66.14 samples/sec
2025-03-25 04:04:41,831 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_guep4ry0.log
2025-03-25 04:04:41,831 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:04:41,831 - training - INFO - Device: cuda:0
2025-03-25 04:04:41,941 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_s3ogrlue.log
2025-03-25 04:04:41,941 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_jwcf8ics.log
2025-03-25 04:04:41,941 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_qepprvoa.log
2025-03-25 04:04:41,941 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_8vd39h3r.log
2025-03-25 04:04:41,941 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:04:41,941 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:04:41,941 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_gd179l5l.log
2025-03-25 04:04:41,941 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_souc18yc.log
2025-03-25 04:04:41,941 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:04:41,941 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_4z32kqw9.log
2025-03-25 04:04:41,941 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_br6jfxyz.log
2025-03-25 04:04:41,941 - training - INFO - Device: cuda:0
2025-03-25 04:04:41,941 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:04:41,941 - training - INFO - Device: cuda:0
2025-03-25 04:04:41,941 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:04:41,941 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:04:41,942 - training - INFO - Device: cuda:0
2025-03-25 04:04:41,942 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:04:41,942 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:04:41,942 - training - INFO - Device: cuda:0
2025-03-25 04:04:41,942 - training - INFO - Device: cuda:0
2025-03-25 04:04:41,942 - training - INFO - Device: cuda:0
2025-03-25 04:04:41,942 - training - INFO - Device: cuda:0
2025-03-25 04:04:41,942 - training - INFO - Device: cuda:0
2025-03-25 04:04:42,706 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:04:42,706 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:04:42,706 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:04:42,708 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:04:42,708 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:04:42,708 - training - INFO - Starting model initialization...
2025-03-25 04:04:42,716 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:04:42,716 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:04:42,716 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:04:42,716 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:04:42,716 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:04:42,716 - training - INFO - Starting model initialization...
2025-03-25 04:04:42,718 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:04:42,718 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:04:42,718 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:04:42,718 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:04:42,718 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:04:42,718 - training - INFO - Starting model initialization...
2025-03-25 04:04:42,723 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:04:42,723 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:04:42,723 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:04:42,723 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:04:42,723 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:04:42,723 - training - INFO - Starting model initialization...
2025-03-25 04:04:42,726 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:04:42,727 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:04:42,727 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:04:42,727 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:04:42,727 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:04:42,727 - training - INFO - Starting model initialization...
2025-03-25 04:04:42,733 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:04:42,733 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:04:42,733 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:04:42,733 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:04:42,733 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:04:42,733 - training - INFO - Starting model initialization...
2025-03-25 04:04:42,738 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:04:42,739 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:04:42,739 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:04:42,739 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:04:42,739 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:04:42,739 - training - INFO - Starting model initialization...
2025-03-25 04:04:42,745 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:04:42,745 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:04:42,746 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:04:42,746 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:04:42,746 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:04:42,746 - training - INFO - Starting model initialization...
2025-03-25 04:04:42,756 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:04:42,756 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:04:42,756 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:04:42,757 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:04:42,757 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:04:42,757 - training - INFO - Starting model initialization...
2025-03-25 04:05:06,473 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 380.43 MiB already allocated; 19.81 MiB free; 412.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:05:06,475 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 380.43 MiB already allocated; 19.81 MiB free; 412.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:05:06,474 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 344.38 MiB already allocated; 19.81 MiB free; 372.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:05:06,475 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 344.38 MiB already allocated; 19.81 MiB free; 372.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:05:06,475 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 326.35 MiB already allocated; 17.81 MiB free; 352.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:05:06,475 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 326.35 MiB already allocated; 17.81 MiB free; 352.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:05:06,474 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 416.48 MiB already allocated; 19.81 MiB free; 452.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:05:06,476 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 416.48 MiB already allocated; 19.81 MiB free; 452.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:05:06,483 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 272.28 MiB already allocated; 13.81 MiB free; 292.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:05:06,484 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 272.28 MiB already allocated; 13.81 MiB free; 292.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:05:06,488 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 110.05 MiB already allocated; 13.81 MiB free; 112.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:05:06,488 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 110.05 MiB already allocated; 13.81 MiB free; 112.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:05:06,495 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 453.91 MiB already allocated; 11.81 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:05:06,495 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 453.91 MiB already allocated; 11.81 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:05:06,500 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 90.00 MiB already allocated; 11.81 MiB free; 90.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:05:06,500 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 90.00 MiB already allocated; 11.81 MiB free; 90.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:05:06,530 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 10.75 GiB total capacity; 0 bytes already allocated; 11.81 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:05:06,530 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 10.75 GiB total capacity; 0 bytes already allocated; 11.81 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:06:11,642 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_exq_2fim.log
2025-03-25 04:06:11,642 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:06:11,642 - training - INFO - Device: cuda:0
2025-03-25 04:06:11,809 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_0dfdyht5.log
2025-03-25 04:06:11,809 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:06:11,809 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_ppeowh_q.log
2025-03-25 04:06:11,809 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_iaaqipi_.log
2025-03-25 04:06:11,809 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_iy3fdquy.log
2025-03-25 04:06:11,809 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_ycea0qbt.log
2025-03-25 04:06:11,809 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_ae5_ks38.log
2025-03-25 04:06:11,809 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_579i0wx2.log
2025-03-25 04:06:11,809 - training - INFO - Device: cuda:0
2025-03-25 04:06:11,809 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_5pxozonv.log
2025-03-25 04:06:11,809 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:06:11,809 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:06:11,809 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:06:11,809 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:06:11,809 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:06:11,810 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:06:11,810 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:06:11,810 - training - INFO - Device: cuda:0
2025-03-25 04:06:11,810 - training - INFO - Device: cuda:0
2025-03-25 04:06:11,810 - training - INFO - Device: cuda:0
2025-03-25 04:06:11,810 - training - INFO - Device: cuda:0
2025-03-25 04:06:11,810 - training - INFO - Device: cuda:0
2025-03-25 04:06:11,810 - training - INFO - Device: cuda:0
2025-03-25 04:06:11,810 - training - INFO - Device: cuda:0
2025-03-25 04:06:12,189 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:06:12,189 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:06:12,189 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:06:12,191 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:06:12,191 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:06:12,191 - training - INFO - Starting model initialization...
2025-03-25 04:06:12,301 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:06:12,301 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:06:12,301 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:06:12,301 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:06:12,301 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:06:12,302 - training - INFO - Starting model initialization...
2025-03-25 04:06:12,333 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:06:12,333 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:06:12,333 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:06:12,333 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:06:12,333 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:06:12,333 - training - INFO - Starting model initialization...
2025-03-25 04:06:12,344 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:06:12,344 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:06:12,344 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:06:12,344 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:06:12,344 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:06:12,344 - training - INFO - Starting model initialization...
2025-03-25 04:06:12,356 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:06:12,356 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:06:12,356 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:06:12,356 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:06:12,356 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:06:12,357 - training - INFO - Starting model initialization...
2025-03-25 04:06:12,464 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:06:12,464 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:06:12,464 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:06:12,464 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:06:12,464 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:06:12,464 - training - INFO - Starting model initialization...
2025-03-25 04:06:12,526 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:06:12,526 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:06:12,526 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:06:12,526 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:06:12,526 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:06:12,526 - training - INFO - Starting model initialization...
2025-03-25 04:06:12,807 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:06:12,807 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:06:12,807 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:06:12,807 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:06:12,807 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:06:12,807 - training - INFO - Starting model initialization...
2025-03-25 04:06:14,394 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:06:14,394 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:06:14,394 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:06:14,394 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:06:14,394 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:06:14,394 - training - INFO - Starting model initialization...
2025-03-25 04:06:20,357 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 758.96 MiB already allocated; 17.81 MiB free; 786.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:06:20,357 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 326.35 MiB already allocated; 17.81 MiB free; 352.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:06:20,359 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 758.96 MiB already allocated; 17.81 MiB free; 786.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:06:20,360 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 326.35 MiB already allocated; 17.81 MiB free; 352.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:06:20,361 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 416.48 MiB already allocated; 17.81 MiB free; 452.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:06:20,362 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 416.48 MiB already allocated; 17.81 MiB free; 452.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:06:20,362 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 128.08 MiB already allocated; 15.81 MiB free; 132.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:06:20,362 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 128.08 MiB already allocated; 15.81 MiB free; 132.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:06:20,364 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 686.85 MiB already allocated; 15.81 MiB free; 706.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:06:20,364 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 686.85 MiB already allocated; 15.81 MiB free; 706.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:06:20,371 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 453.91 MiB already allocated; 9.81 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:06:20,372 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 453.91 MiB already allocated; 9.81 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:06:20,406 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-25 04:06:20,406 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-25 04:06:20,449 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-25 04:06:20,496 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 90.00 MiB already allocated; 1.81 MiB free; 90.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:06:20,551 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-25 04:06:20,551 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 90.00 MiB already allocated; 1.81 MiB free; 90.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:06:58,200 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_65ey5nri.log
2025-03-25 04:06:58,201 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:06:58,201 - training - INFO - Device: cuda:0
2025-03-25 04:06:58,337 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_k_sl_5vj.log
2025-03-25 04:06:58,337 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_1x9ah3l1.log
2025-03-25 04:06:58,338 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:06:58,337 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_iwc6vepf.log
2025-03-25 04:06:58,338 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:06:58,338 - training - INFO - Device: cuda:0
2025-03-25 04:06:58,338 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:06:58,338 - training - INFO - Device: cuda:0
2025-03-25 04:06:58,338 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_oxnalg5k.log
2025-03-25 04:06:58,338 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_1b31nioa.log
2025-03-25 04:06:58,338 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_rhr1fx6j.log
2025-03-25 04:06:58,338 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_uvueh6om.log
2025-03-25 04:06:58,338 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_8h98h19v.log
2025-03-25 04:06:58,338 - training - INFO - Device: cuda:0
2025-03-25 04:06:58,338 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:06:58,338 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:06:58,338 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:06:58,338 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:06:58,338 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:06:58,338 - training - INFO - Device: cuda:0
2025-03-25 04:06:58,338 - training - INFO - Device: cuda:0
2025-03-25 04:06:58,338 - training - INFO - Device: cuda:0
2025-03-25 04:06:58,338 - training - INFO - Device: cuda:0
2025-03-25 04:06:58,339 - training - INFO - Device: cuda:0
2025-03-25 04:06:58,678 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:06:58,678 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:06:58,678 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:06:58,678 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:06:58,678 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:06:58,678 - training - INFO - Starting model initialization...
2025-03-25 04:06:58,833 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:06:58,833 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:06:58,833 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:06:58,834 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:06:58,834 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:06:58,834 - training - INFO - Starting model initialization...
2025-03-25 04:06:58,858 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:06:58,858 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:06:58,858 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:06:58,858 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:06:58,858 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:06:58,858 - training - INFO - Starting model initialization...
2025-03-25 04:06:58,860 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:06:58,860 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:06:58,860 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:06:58,860 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:06:58,860 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:06:58,860 - training - INFO - Starting model initialization...
2025-03-25 04:06:58,902 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:06:58,902 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:06:58,902 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:06:58,902 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:06:58,902 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:06:58,902 - training - INFO - Starting model initialization...
2025-03-25 04:06:58,927 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:06:58,927 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:06:58,927 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:06:58,927 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:06:58,927 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:06:58,927 - training - INFO - Starting model initialization...
2025-03-25 04:07:01,972 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:07:01,974 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:07:01,974 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:07:01,974 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:07:01,975 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:07:01,975 - training - INFO - Starting model initialization...
2025-03-25 04:07:02,029 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:07:02,029 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:07:02,029 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:07:02,029 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:07:02,029 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:07:02,030 - training - INFO - Starting model initialization...
2025-03-25 04:07:02,320 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:07:02,320 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:07:02,320 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:07:02,320 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:07:02,320 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:07:02,320 - training - INFO - Starting model initialization...
2025-03-25 04:07:06,940 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.75 GiB total capacity; 593.34 MiB already allocated; 7.81 MiB free; 610.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:07:06,940 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 624, in _apply
    self._buffers[key] = fn(buf)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.75 GiB total capacity; 593.34 MiB already allocated; 7.81 MiB free; 610.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:07:06,958 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 110.05 MiB already allocated; 3.81 MiB free; 112.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:07:06,958 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 110.05 MiB already allocated; 3.81 MiB free; 112.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:07:06,961 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-25 04:07:06,961 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-25 04:07:06,961 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-25 04:07:06,962 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-25 04:07:06,963 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-25 04:07:06,963 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-25 04:09:24,711 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_qhhlil8o.log
2025-03-25 04:09:24,711 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:09:24,711 - training - INFO - Device: cuda:0
2025-03-25 04:09:24,842 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_ocm555zx.log
2025-03-25 04:09:24,842 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_6p8ww3b2.log
2025-03-25 04:09:24,842 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_debjnlq2.log
2025-03-25 04:09:24,843 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_yvkvvrx5.log
2025-03-25 04:09:24,843 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:09:24,843 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:09:24,843 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:09:24,843 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_whmc5t6l.log
2025-03-25 04:09:24,843 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:09:24,843 - training - INFO - Device: cuda:0
2025-03-25 04:09:24,843 - training - INFO - Device: cuda:0
2025-03-25 04:09:24,843 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_52g2_uuc.log
2025-03-25 04:09:24,843 - training - INFO - Device: cuda:0
2025-03-25 04:09:24,843 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_ub2w5ste.log
2025-03-25 04:09:24,843 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:09:24,843 - training - INFO - Device: cuda:0
2025-03-25 04:09:24,843 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:09:24,843 - training - INFO - Device: cuda:0
2025-03-25 04:09:24,843 - training - INFO - Device: cuda:0
2025-03-25 04:09:24,843 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:09:24,844 - training - INFO - Device: cuda:0
2025-03-25 04:09:25,311 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:09:25,311 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:09:25,311 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:09:25,313 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:09:25,313 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:09:25,313 - training - INFO - Starting model initialization...
2025-03-25 04:09:25,342 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:09:25,342 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:09:25,342 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:09:25,342 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:09:25,342 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:09:25,342 - training - INFO - Starting model initialization...
2025-03-25 04:09:25,344 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:09:25,344 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:09:25,344 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:09:25,344 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:09:25,344 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:09:25,344 - training - INFO - Starting model initialization...
2025-03-25 04:09:25,375 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:09:25,376 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:09:25,376 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:09:25,376 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:09:25,376 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:09:25,376 - training - INFO - Starting model initialization...
2025-03-25 04:09:25,394 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:09:25,394 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:09:25,394 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:09:25,394 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:09:25,394 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:09:25,394 - training - INFO - Starting model initialization...
2025-03-25 04:09:25,404 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:09:25,404 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:09:25,404 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:09:25,404 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:09:25,404 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:09:25,404 - training - INFO - Starting model initialization...
2025-03-25 04:09:25,464 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:09:25,464 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:09:25,464 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:09:25,465 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:09:25,465 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:09:25,465 - training - INFO - Starting model initialization...
2025-03-25 04:09:25,541 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:09:25,541 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:09:25,541 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:09:25,541 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:09:25,542 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:09:25,542 - training - INFO - Starting model initialization...
2025-03-25 04:09:33,556 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 398.46 MiB already allocated; 15.81 MiB free; 432.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:09:33,558 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 398.46 MiB already allocated; 15.81 MiB free; 432.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:09:33,557 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 164.12 MiB already allocated; 15.81 MiB free; 172.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:09:33,559 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 164.12 MiB already allocated; 15.81 MiB free; 172.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:09:33,559 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 236.23 MiB already allocated; 15.81 MiB free; 252.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:09:33,559 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 236.23 MiB already allocated; 15.81 MiB free; 252.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:09:33,560 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 362.40 MiB already allocated; 15.81 MiB free; 392.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:09:33,560 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 4 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 362.40 MiB already allocated; 15.81 MiB free; 392.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:09:33,562 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 416.48 MiB already allocated; 15.81 MiB free; 452.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:09:33,562 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 416.48 MiB already allocated; 15.81 MiB free; 452.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:09:33,567 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 453.91 MiB already allocated; 13.81 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:09:33,567 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 453.91 MiB already allocated; 13.81 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:09:33,568 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 398.46 MiB already allocated; 13.81 MiB free; 432.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:09:33,568 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 398.46 MiB already allocated; 13.81 MiB free; 432.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:11:20,656 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_ui827o2l.log
2025-03-25 04:11:20,656 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:11:20,657 - training - INFO - Device: cuda:0
2025-03-25 04:11:20,754 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_i_g92kix.log
2025-03-25 04:11:20,754 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error__n1d2zn5.log
2025-03-25 04:11:20,755 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:11:20,755 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:11:20,755 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_0vc8y9y3.log
2025-03-25 04:11:20,755 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_hmqzhfio.log
2025-03-25 04:11:20,755 - training - INFO - Device: cuda:0
2025-03-25 04:11:20,755 - training - INFO - Device: cuda:0
2025-03-25 04:11:20,755 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_dtpdz3so.log
2025-03-25 04:11:20,755 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_r525mpxv.log
2025-03-25 04:11:20,755 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_z7e4edh7.log
2025-03-25 04:11:20,755 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:11:20,755 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:11:20,755 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:11:20,755 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:11:20,755 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:11:20,755 - training - INFO - Device: cuda:0
2025-03-25 04:11:20,755 - training - INFO - Device: cuda:0
2025-03-25 04:11:20,755 - training - INFO - Device: cuda:0
2025-03-25 04:11:20,755 - training - INFO - Device: cuda:0
2025-03-25 04:11:20,755 - training - INFO - Device: cuda:0
2025-03-25 04:11:21,199 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:11:21,199 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:11:21,199 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:11:21,199 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:11:21,200 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:11:21,200 - training - INFO - Starting model initialization...
2025-03-25 04:11:21,251 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:11:21,251 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:11:21,251 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:11:21,252 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:11:21,252 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:11:21,252 - training - INFO - Starting model initialization...
2025-03-25 04:11:21,281 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:11:21,281 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:11:21,281 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:11:21,281 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:11:21,281 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:11:21,281 - training - INFO - Starting model initialization...
2025-03-25 04:11:21,292 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:11:21,292 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:11:21,292 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:11:21,292 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:11:21,292 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:11:21,292 - training - INFO - Starting model initialization...
2025-03-25 04:11:21,300 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:11:21,300 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:11:21,300 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:11:21,300 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:11:21,300 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:11:21,300 - training - INFO - Starting model initialization...
2025-03-25 04:11:21,312 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:11:21,312 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:11:21,312 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:11:21,312 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:11:21,312 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:11:21,312 - training - INFO - Starting model initialization...
2025-03-25 04:11:21,365 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:11:21,365 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:11:21,365 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:11:21,365 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:11:21,366 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:11:21,366 - training - INFO - Starting model initialization...
2025-03-25 04:11:21,444 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:11:21,444 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:11:21,444 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:11:21,444 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:11:21,444 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:11:21,444 - training - INFO - Starting model initialization...
2025-03-25 04:11:29,505 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 10.75 GiB total capacity; 543.55 MiB already allocated; 7.81 MiB free; 572.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:11:29,507 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 10.75 GiB total capacity; 543.55 MiB already allocated; 7.81 MiB free; 572.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:11:29,507 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 10.75 GiB total capacity; 543.55 MiB already allocated; 7.81 MiB free; 572.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:11:29,507 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 10.75 GiB total capacity; 543.55 MiB already allocated; 7.81 MiB free; 572.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:11:29,510 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 398.46 MiB already allocated; 7.81 MiB free; 432.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:11:29,510 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 398.46 MiB already allocated; 7.81 MiB free; 432.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:11:29,511 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 344.38 MiB already allocated; 7.81 MiB free; 372.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:11:29,512 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 344.38 MiB already allocated; 7.81 MiB free; 372.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:11:29,514 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 10.75 GiB total capacity; 503.50 MiB already allocated; 7.81 MiB free; 536.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:11:29,514 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 10.75 GiB total capacity; 503.50 MiB already allocated; 7.81 MiB free; 536.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:11:29,515 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 344.38 MiB already allocated; 7.81 MiB free; 372.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:11:29,515 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 344.38 MiB already allocated; 7.81 MiB free; 372.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:11:29,520 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 236.23 MiB already allocated; 7.81 MiB free; 252.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:11:29,520 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 236.23 MiB already allocated; 7.81 MiB free; 252.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:11:29,522 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 362.40 MiB already allocated; 7.81 MiB free; 392.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:11:29,522 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 4 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 362.40 MiB already allocated; 7.81 MiB free; 392.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:13:18,893 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_a00ap7nj.log
2025-03-25 04:13:18,893 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:13:18,893 - training - INFO - Device: cuda:0
2025-03-25 04:13:19,020 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_r8cxp7wa.log
2025-03-25 04:13:19,021 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:13:19,021 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_9qw_3a75.log
2025-03-25 04:13:19,021 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_rhwcw9wg.log
2025-03-25 04:13:19,021 - training - INFO - Device: cuda:0
2025-03-25 04:13:19,021 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_1f67_1gj.log
2025-03-25 04:13:19,021 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_cxtyaa1s.log
2025-03-25 04:13:19,021 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:13:19,021 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:13:19,021 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:13:19,021 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:13:19,021 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_7910qgaj.log
2025-03-25 04:13:19,021 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_knfn73cj.log
2025-03-25 04:13:19,021 - training - INFO - Device: cuda:0
2025-03-25 04:13:19,021 - training - INFO - Device: cuda:0
2025-03-25 04:13:19,021 - training - INFO - Device: cuda:0
2025-03-25 04:13:19,021 - training - INFO - Device: cuda:0
2025-03-25 04:13:19,021 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:13:19,021 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:13:19,022 - training - INFO - Device: cuda:0
2025-03-25 04:13:19,022 - training - INFO - Device: cuda:0
2025-03-25 04:13:19,610 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:13:19,610 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:13:19,610 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:13:19,612 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:13:19,612 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:13:19,612 - training - INFO - Starting model initialization...
2025-03-25 04:13:19,617 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:13:19,617 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:13:19,617 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:13:19,618 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:13:19,618 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:13:19,618 - training - INFO - Starting model initialization...
2025-03-25 04:13:19,619 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:13:19,619 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:13:19,619 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:13:19,619 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:13:19,619 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:13:19,619 - training - INFO - Starting model initialization...
2025-03-25 04:13:19,624 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:13:19,624 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:13:19,624 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:13:19,624 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:13:19,624 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:13:19,624 - training - INFO - Starting model initialization...
2025-03-25 04:13:19,633 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:13:19,634 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:13:19,634 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:13:19,634 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:13:19,634 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:13:19,634 - training - INFO - Starting model initialization...
2025-03-25 04:13:19,648 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:13:19,648 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:13:19,648 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:13:19,648 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:13:19,648 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:13:19,648 - training - INFO - Starting model initialization...
2025-03-25 04:13:19,651 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:13:19,652 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:13:19,652 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:13:19,652 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:13:19,652 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:13:19,652 - training - INFO - Starting model initialization...
2025-03-25 04:13:19,779 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:13:19,780 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:13:19,780 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:13:19,780 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:13:19,780 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:13:19,780 - training - INFO - Starting model initialization...
2025-03-25 04:13:27,624 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 10.75 GiB total capacity; 503.50 MiB already allocated; 9.81 MiB free; 536.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:13:27,626 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 10.75 GiB total capacity; 503.50 MiB already allocated; 9.81 MiB free; 536.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:13:27,626 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 758.96 MiB already allocated; 9.81 MiB free; 786.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:13:27,627 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 758.96 MiB already allocated; 9.81 MiB free; 786.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:13:27,629 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 453.91 MiB already allocated; 7.81 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:13:27,629 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 453.91 MiB already allocated; 7.81 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:13:27,629 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 10.75 GiB total capacity; 523.53 MiB already allocated; 7.81 MiB free; 554.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:13:27,629 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 10.75 GiB total capacity; 523.53 MiB already allocated; 7.81 MiB free; 554.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:13:27,630 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 453.91 MiB already allocated; 5.81 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:13:27,631 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 453.91 MiB already allocated; 5.81 MiB free; 476.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:13:27,632 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 308.32 MiB already allocated; 5.81 MiB free; 332.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:13:27,632 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 4 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 308.32 MiB already allocated; 5.81 MiB free; 332.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:13:27,641 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 236.23 MiB already allocated; 5.81 MiB free; 252.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:13:27,641 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 236.23 MiB already allocated; 5.81 MiB free; 252.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:13:27,697 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 90.00 MiB already allocated; 5.81 MiB free; 90.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:13:27,697 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 600, in main
    model.to(device)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 90.00 MiB already allocated; 5.81 MiB free; 90.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:14:26,054 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_82q0ezgi.log
2025-03-25 04:14:26,054 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:14:26,054 - training - INFO - Device: cuda:0
2025-03-25 04:14:26,195 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_79e8lvs7.log
2025-03-25 04:14:26,195 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_y0uufkw1.log
2025-03-25 04:14:26,195 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_khuq2fvv.log
2025-03-25 04:14:26,195 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:14:26,195 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:14:26,195 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:14:26,195 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_topfhpqi.log
2025-03-25 04:14:26,195 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_96lu_ujz.log
2025-03-25 04:14:26,196 - training - INFO - Device: cuda:0
2025-03-25 04:14:26,196 - training - INFO - Device: cuda:0
2025-03-25 04:14:26,196 - training - INFO - Device: cuda:0
2025-03-25 04:14:26,196 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:14:26,196 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 04:14:26,196 - training - INFO - Device: cuda:0
2025-03-25 04:14:26,196 - training - INFO - Device: cuda:0
2025-03-25 04:14:26,634 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:14:26,634 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:14:26,634 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:14:26,636 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:14:26,637 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:14:26,637 - training - INFO - Starting model initialization...
2025-03-25 04:14:26,716 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:14:26,716 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:14:26,716 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:14:26,716 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:14:26,716 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:14:26,716 - training - INFO - Starting model initialization...
2025-03-25 04:14:26,747 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:14:26,747 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:14:26,747 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:14:26,747 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:14:26,747 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:14:26,747 - training - INFO - Starting model initialization...
2025-03-25 04:14:26,981 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:14:26,981 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:14:26,981 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:14:26,981 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:14:26,981 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:14:26,981 - training - INFO - Starting model initialization...
2025-03-25 04:14:27,466 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:14:27,466 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:14:27,466 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:14:27,466 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:14:27,466 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:14:27,466 - training - INFO - Starting model initialization...
2025-03-25 04:14:32,463 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 04:14:32,465 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:14:32,465 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:14:32,465 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:14:32,465 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:14:32,465 - training - INFO - Starting model initialization...
2025-03-25 04:14:40,283 - training - INFO - Per-GPU batch size: 24
2025-03-25 04:14:40,288 - training - INFO - Starting epoch 1/200000
2025-03-25 04:14:40,825 - training - INFO - Per-GPU batch size: 24
2025-03-25 04:14:40,829 - training - INFO - Starting epoch 1/200000
2025-03-25 04:14:40,974 - training - INFO - Per-GPU batch size: 24
2025-03-25 04:14:40,978 - training - INFO - Starting epoch 1/200000
2025-03-25 04:14:41,243 - training - INFO - Per-GPU batch size: 24
2025-03-25 04:14:41,248 - training - INFO - Starting epoch 1/200000
2025-03-25 04:14:41,287 - training - ERROR - Critical error in training batch 0: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 897.80 MiB already allocated; 9.81 MiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:14:41,304 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 162, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 214, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1010, in forward
    embedding_output = self.embeddings(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 243, in forward
    embeddings = self.dropout(embeddings)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 897.80 MiB already allocated; 9.81 MiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:14:41,304 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 897.80 MiB already allocated; 9.81 MiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:14:41,304 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 229, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 162, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 214, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1010, in forward
    embedding_output = self.embeddings(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 243, in forward
    embeddings = self.dropout(embeddings)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 897.80 MiB already allocated; 9.81 MiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:14:41,305 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 897.80 MiB already allocated; 9.81 MiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:14:41,305 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 700, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 346, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 229, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 162, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 214, in forward
    text_outputs = self.bert(**text_input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1010, in forward
    embedding_output = self.embeddings(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 243, in forward
    embeddings = self.dropout(embeddings)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 897.80 MiB already allocated; 9.81 MiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:14:41,966 - training - ERROR - Critical error in training batch 0: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:14:41,967 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 157, in train_model
    previous_views = batch['previous_views_image'].to(device, non_blocking=True)
RuntimeError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:14:41,967 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:14:41,967 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 229, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 157, in train_model
    previous_views = batch['previous_views_image'].to(device, non_blocking=True)
RuntimeError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:14:41,967 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:14:41,968 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 700, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 346, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 229, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 157, in train_model
    previous_views = batch['previous_views_image'].to(device, non_blocking=True)
RuntimeError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:14:42,099 - training - ERROR - Critical error in training batch 0: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:14:42,100 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 157, in train_model
    previous_views = batch['previous_views_image'].to(device, non_blocking=True)
RuntimeError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:14:42,100 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:14:42,100 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 229, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 157, in train_model
    previous_views = batch['previous_views_image'].to(device, non_blocking=True)
RuntimeError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:14:42,101 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:14:42,101 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 700, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 346, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 229, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 157, in train_model
    previous_views = batch['previous_views_image'].to(device, non_blocking=True)
RuntimeError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:14:42,146 - training - INFO - Per-GPU batch size: 24
2025-03-25 04:14:42,150 - training - INFO - Starting epoch 1/200000
2025-03-25 04:14:42,362 - training - ERROR - Critical error in training batch 0: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:14:42,363 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 157, in train_model
    previous_views = batch['previous_views_image'].to(device, non_blocking=True)
RuntimeError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:14:42,363 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:14:42,363 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 229, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 157, in train_model
    previous_views = batch['previous_views_image'].to(device, non_blocking=True)
RuntimeError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:14:42,364 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:14:42,364 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 700, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 346, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 229, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 157, in train_model
    previous_views = batch['previous_views_image'].to(device, non_blocking=True)
RuntimeError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:14:43,073 - training - ERROR - Critical error in training batch 0: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:14:43,074 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 157, in train_model
    previous_views = batch['previous_views_image'].to(device, non_blocking=True)
RuntimeError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:14:43,074 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:14:43,075 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 229, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 157, in train_model
    previous_views = batch['previous_views_image'].to(device, non_blocking=True)
RuntimeError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:14:43,075 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:14:43,075 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 700, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 346, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 229, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 157, in train_model
    previous_views = batch['previous_views_image'].to(device, non_blocking=True)
RuntimeError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.75 GiB total capacity; 843.61 MiB already allocated; 9.81 MiB free; 886.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:18:36,161 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_l9t7wnif.log
2025-03-25 04:18:36,161 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 9
2025-03-25 04:18:36,162 - training - INFO - Device: cuda:0
2025-03-25 04:18:36,797 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 04:18:36,797 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:18:36,797 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:18:36,799 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:18:36,799 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:18:42,029 - training - INFO - Starting model initialization...
2025-03-25 04:18:52,036 - training - INFO - Per-GPU batch size: 16 (global batch size: 144, with gradient_accumulation_steps=2)
2025-03-25 04:18:52,045 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-25 04:18:52,049 - training - INFO - Starting epoch 1/200000
2025-03-25 04:18:58,806 - training - INFO - GPU Memory: GPU 0: 2282.5MB/6560.0MB, GPU 1: 2267.9MB/6686.0MB, GPU 2: 2280.4MB/6544.0MB, GPU 3: 2267.9MB/6686.0MB, GPU 4: 2280.4MB/6544.0MB, GPU 5: 2282.5MB/6560.0MB, GPU 6: 2268.6MB/6668.0MB, GPU 7: 2280.4MB/6544.0MB, GPU 8: 2280.4MB/6544.0MB, Total: 20491.0MB allocated, 59336.0MB reserved, Mean: 2276.8MB allocated, 6592.9MB reserved
2025-03-25 04:18:58,807 - training - INFO - Epoch: 1/200000, Batch: 0/37, Loss: 11.0420, Throughput: 10.66 samples/sec
2025-03-25 04:19:13,301 - training - INFO - GPU Memory: GPU 0: 3588.2MB/9668.0MB, GPU 1: 3586.1MB/9704.0MB, GPU 2: 3588.9MB/9634.0MB, GPU 3: 3591.5MB/9694.0MB, GPU 4: 3591.5MB/9626.0MB, GPU 5: 3587.8MB/9640.0MB, GPU 6: 3591.9MB/9646.0MB, GPU 7: 3591.5MB/9640.0MB, GPU 8: 3591.8MB/9822.0MB, Total: 32309.1MB allocated, 87074.0MB reserved, Mean: 3589.9MB allocated, 9674.9MB reserved
2025-03-25 04:19:13,301 - training - INFO - Epoch: 1/200000, Batch: 12/37, Loss: 9.4679, Throughput: 44.04 samples/sec
2025-03-25 04:19:13,718 - training - ERROR - Critical error in training batch 13: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 7; 10.75 GiB total capacity; 9.19 GiB already allocated; 227.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:19:13,721 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 176, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 7; 10.75 GiB total capacity; 9.19 GiB already allocated; 227.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:19:13,721 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 7; 10.75 GiB total capacity; 9.19 GiB already allocated; 227.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:19:13,721 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 229, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 176, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 7; 10.75 GiB total capacity; 9.19 GiB already allocated; 227.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:19:13,721 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 7; 10.75 GiB total capacity; 9.19 GiB already allocated; 227.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:19:13,721 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 700, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 346, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 229, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 176, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 7; 10.75 GiB total capacity; 9.19 GiB already allocated; 227.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:19:13,732 - training - ERROR - Critical error in training batch 13: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 5; 10.75 GiB total capacity; 9.18 GiB already allocated; 227.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:19:13,733 - training - ERROR - Critical error in training batch 13: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 6; 10.75 GiB total capacity; 9.19 GiB already allocated; 227.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:19:13,734 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 176, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 5; 10.75 GiB total capacity; 9.18 GiB already allocated; 227.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:19:13,734 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 5; 10.75 GiB total capacity; 9.18 GiB already allocated; 227.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:19:13,734 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 229, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 176, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 5; 10.75 GiB total capacity; 9.18 GiB already allocated; 227.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:19:13,734 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 5; 10.75 GiB total capacity; 9.18 GiB already allocated; 227.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:19:13,734 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 700, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 346, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 229, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 176, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 5; 10.75 GiB total capacity; 9.18 GiB already allocated; 227.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:19:13,735 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 176, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 6; 10.75 GiB total capacity; 9.19 GiB already allocated; 227.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:19:13,736 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 6; 10.75 GiB total capacity; 9.19 GiB already allocated; 227.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:19:13,736 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 229, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 176, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 6; 10.75 GiB total capacity; 9.19 GiB already allocated; 227.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:19:13,737 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 6; 10.75 GiB total capacity; 9.19 GiB already allocated; 227.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:19:13,737 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 700, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 346, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 229, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 176, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 6; 10.75 GiB total capacity; 9.19 GiB already allocated; 227.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:19:13,748 - training - ERROR - Critical error in training batch 13: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 3; 10.75 GiB total capacity; 9.19 GiB already allocated; 181.81 MiB free; 9.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:19:13,751 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 176, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 3; 10.75 GiB total capacity; 9.19 GiB already allocated; 181.81 MiB free; 9.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:19:13,751 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 3; 10.75 GiB total capacity; 9.19 GiB already allocated; 181.81 MiB free; 9.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:19:13,752 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 229, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 176, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 3; 10.75 GiB total capacity; 9.19 GiB already allocated; 181.81 MiB free; 9.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:19:13,752 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 3; 10.75 GiB total capacity; 9.19 GiB already allocated; 181.81 MiB free; 9.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 04:19:13,752 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 700, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 346, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 229, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 176, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 3; 10.75 GiB total capacity; 9.19 GiB already allocated; 181.81 MiB free; 9.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 04:20:58,652 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_fx4d2vwh.log
2025-03-25 04:20:58,652 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 9
2025-03-25 04:20:58,652 - training - INFO - Device: cuda:0
2025-03-25 04:20:59,173 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 04:20:59,173 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:20:59,173 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:20:59,173 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:20:59,173 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:21:04,281 - training - INFO - Starting model initialization...
2025-03-25 04:21:15,620 - training - INFO - Per-GPU batch size: 14 (global batch size: 126, with gradient_accumulation_steps=2)
2025-03-25 04:21:15,628 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-25 04:21:15,631 - training - INFO - Starting epoch 1/200000
2025-03-25 04:21:21,403 - training - INFO - GPU Memory: GPU 0: 2261.3MB/6108.0MB, GPU 1: 2261.3MB/6108.0MB, GPU 2: 2257.4MB/6274.0MB, GPU 3: 2261.3MB/6128.0MB, GPU 4: 2261.3MB/6108.0MB, GPU 5: 2261.3MB/6108.0MB, GPU 6: 2261.3MB/6108.0MB, GPU 7: 2261.3MB/6108.0MB, GPU 8: 2261.3MB/6108.0MB, Total: 20347.8MB allocated, 55158.0MB reserved, Mean: 2260.9MB allocated, 6128.7MB reserved
2025-03-25 04:21:21,404 - training - INFO - Epoch: 1/200000, Batch: 0/42, Loss: 10.6062, Throughput: 10.91 samples/sec
2025-03-25 04:21:36,053 - training - INFO - GPU Memory: GPU 0: 3566.9MB/9574.0MB, GPU 1: 3569.4MB/9592.0MB, GPU 2: 3572.3MB/9600.0MB, GPU 3: 3561.5MB/9578.0MB, GPU 4: 3569.7MB/9598.0MB, GPU 5: 3571.9MB/9588.0MB, GPU 6: 3570.6MB/9596.0MB, GPU 7: 3570.1MB/9596.0MB, GPU 8: 3568.8MB/9598.0MB, Total: 32121.1MB allocated, 86320.0MB reserved, Mean: 3569.0MB allocated, 9591.1MB reserved
2025-03-25 04:21:36,054 - training - INFO - Epoch: 1/200000, Batch: 14/42, Loss: 8.8864, Throughput: 46.27 samples/sec
2025-03-25 04:21:50,604 - training - INFO - GPU Memory: GPU 0: 3567.3MB/9574.0MB, GPU 1: 3567.7MB/9594.0MB, GPU 2: 3571.5MB/9600.0MB, GPU 3: 3562.7MB/9578.0MB, GPU 4: 3568.4MB/9598.0MB, GPU 5: 3572.2MB/9588.0MB, GPU 6: 3569.1MB/9596.0MB, GPU 7: 3569.1MB/9596.0MB, GPU 8: 3573.8MB/9598.0MB, Total: 32121.7MB allocated, 86322.0MB reserved, Mean: 3569.1MB allocated, 9591.3MB reserved
2025-03-25 04:21:50,604 - training - INFO - Epoch: 1/200000, Batch: 28/42, Loss: 7.9665, Throughput: 52.24 samples/sec
2025-03-25 04:22:05,692 - training - INFO - Epoch 1 completed in 50.06s. Average loss: 7.4571
2025-03-25 04:22:05,696 - training - INFO - Starting epoch 2/200000
2025-03-25 04:22:06,660 - training - INFO - GPU Memory: GPU 0: 3577.1MB/9120.0MB, GPU 1: 3577.1MB/9126.0MB, GPU 2: 3571.8MB/9126.0MB, GPU 3: 3567.3MB/9104.0MB, GPU 4: 3575.9MB/9212.0MB, GPU 5: 3579.8MB/9118.0MB, GPU 6: 3573.2MB/9124.0MB, GPU 7: 3576.9MB/9122.0MB, GPU 8: 3574.7MB/9124.0MB, Total: 32174.0MB allocated, 82176.0MB reserved, Mean: 3574.9MB allocated, 9130.7MB reserved
2025-03-25 04:22:06,660 - training - INFO - Epoch: 2/200000, Batch: 0/42, Loss: 5.7773, Throughput: 65.34 samples/sec
2025-03-25 04:22:21,358 - training - INFO - GPU Memory: GPU 0: 3576.0MB/9598.0MB, GPU 1: 3578.6MB/9590.0MB, GPU 2: 3573.0MB/9600.0MB, GPU 3: 3566.3MB/9578.0MB, GPU 4: 3577.7MB/9600.0MB, GPU 5: 3575.7MB/9588.0MB, GPU 6: 3573.5MB/9596.0MB, GPU 7: 3574.2MB/9588.0MB, GPU 8: 3575.9MB/9582.0MB, Total: 32170.8MB allocated, 86320.0MB reserved, Mean: 3574.5MB allocated, 9591.1MB reserved
2025-03-25 04:22:21,358 - training - INFO - Epoch: 2/200000, Batch: 14/42, Loss: 5.9167, Throughput: 60.34 samples/sec
2025-03-25 04:22:35,849 - training - INFO - GPU Memory: GPU 0: 3575.4MB/9598.0MB, GPU 1: 3578.6MB/9592.0MB, GPU 2: 3573.0MB/9600.0MB, GPU 3: 3565.5MB/9578.0MB, GPU 4: 3577.7MB/9600.0MB, GPU 5: 3576.3MB/9588.0MB, GPU 6: 3576.7MB/9596.0MB, GPU 7: 3573.9MB/9588.0MB, GPU 8: 3575.8MB/9582.0MB, Total: 32172.9MB allocated, 86322.0MB reserved, Mean: 3574.8MB allocated, 9591.3MB reserved
2025-03-25 04:22:35,849 - training - INFO - Epoch: 2/200000, Batch: 28/42, Loss: 5.8075, Throughput: 60.59 samples/sec
2025-03-25 04:22:49,520 - training - INFO - Epoch 2 completed in 43.82s. Average loss: 5.6861
2025-03-25 04:22:49,524 - training - INFO - Starting epoch 3/200000
2025-03-25 04:22:50,437 - training - INFO - GPU Memory: GPU 0: 3575.0MB/9602.0MB, GPU 1: 3575.1MB/9594.0MB, GPU 2: 3573.0MB/9606.0MB, GPU 3: 3565.8MB/9582.0MB, GPU 4: 3576.2MB/9602.0MB, GPU 5: 3576.2MB/9592.0MB, GPU 6: 3573.5MB/9600.0MB, GPU 7: 3574.0MB/9592.0MB, GPU 8: 3575.4MB/9586.0MB, Total: 32164.1MB allocated, 86356.0MB reserved, Mean: 3573.8MB allocated, 9595.1MB reserved
2025-03-25 04:22:50,438 - training - INFO - Epoch: 3/200000, Batch: 0/42, Loss: 5.5696, Throughput: 68.99 samples/sec
2025-03-25 04:23:05,182 - training - INFO - GPU Memory: GPU 0: 3576.1MB/9676.0MB, GPU 1: 3579.0MB/9682.0MB, GPU 2: 3572.6MB/9582.0MB, GPU 3: 3564.3MB/9694.0MB, GPU 4: 3576.0MB/9676.0MB, GPU 5: 3574.4MB/9688.0MB, GPU 6: 3573.5MB/9678.0MB, GPU 7: 3575.3MB/9676.0MB, GPU 8: 3575.5MB/9698.0MB, Total: 32166.6MB allocated, 87050.0MB reserved, Mean: 3574.1MB allocated, 9672.2MB reserved
2025-03-25 04:23:05,183 - training - INFO - Epoch: 3/200000, Batch: 14/42, Loss: 5.3875, Throughput: 60.35 samples/sec
2025-03-25 04:23:19,712 - training - INFO - GPU Memory: GPU 0: 3574.5MB/9676.0MB, GPU 1: 3575.4MB/9682.0MB, GPU 2: 3574.0MB/9582.0MB, GPU 3: 3565.0MB/9694.0MB, GPU 4: 3576.0MB/9676.0MB, GPU 5: 3574.0MB/9690.0MB, GPU 6: 3573.7MB/9678.0MB, GPU 7: 3575.5MB/9676.0MB, GPU 8: 3573.4MB/9698.0MB, Total: 32161.5MB allocated, 87052.0MB reserved, Mean: 3573.5MB allocated, 9672.4MB reserved
2025-03-25 04:23:19,712 - training - INFO - Epoch: 3/200000, Batch: 28/42, Loss: 5.3324, Throughput: 60.52 samples/sec
2025-03-25 04:23:33,375 - training - INFO - Epoch 3 completed in 43.85s. Average loss: 5.2834
2025-03-25 04:23:33,381 - training - INFO - Starting epoch 4/200000
2025-03-25 04:23:34,293 - training - INFO - GPU Memory: GPU 0: 3576.1MB/9680.0MB, GPU 1: 3575.4MB/9684.0MB, GPU 2: 3574.0MB/9588.0MB, GPU 3: 3566.6MB/9696.0MB, GPU 4: 3574.3MB/9680.0MB, GPU 5: 3574.6MB/9692.0MB, GPU 6: 3572.8MB/9682.0MB, GPU 7: 3578.0MB/9680.0MB, GPU 8: 3574.4MB/9702.0MB, Total: 32166.3MB allocated, 87084.0MB reserved, Mean: 3574.0MB allocated, 9676.0MB reserved
2025-03-25 04:23:34,293 - training - INFO - Epoch: 4/200000, Batch: 0/42, Loss: 5.1632, Throughput: 69.12 samples/sec
2025-03-25 04:23:49,004 - training - INFO - GPU Memory: GPU 0: 3573.6MB/9556.0MB, GPU 1: 3573.8MB/9580.0MB, GPU 2: 3570.3MB/9580.0MB, GPU 3: 3563.8MB/9594.0MB, GPU 4: 3572.0MB/9582.0MB, GPU 5: 3573.8MB/9568.0MB, GPU 6: 3575.7MB/9578.0MB, GPU 7: 3573.8MB/9570.0MB, GPU 8: 3572.3MB/9568.0MB, Total: 32148.9MB allocated, 86176.0MB reserved, Mean: 3572.1MB allocated, 9575.1MB reserved
2025-03-25 04:23:49,004 - training - INFO - Epoch: 4/200000, Batch: 14/42, Loss: 5.1932, Throughput: 60.49 samples/sec
2025-03-25 04:24:03,533 - training - INFO - GPU Memory: GPU 0: 3575.8MB/9556.0MB, GPU 1: 3573.8MB/9580.0MB, GPU 2: 3569.3MB/9580.0MB, GPU 3: 3563.3MB/9596.0MB, GPU 4: 3571.7MB/9582.0MB, GPU 5: 3572.9MB/9674.0MB, GPU 6: 3576.7MB/9580.0MB, GPU 7: 3577.9MB/9570.0MB, GPU 8: 3574.3MB/9568.0MB, Total: 32155.5MB allocated, 86286.0MB reserved, Mean: 3572.8MB allocated, 9587.3MB reserved
2025-03-25 04:24:03,533 - training - INFO - Epoch: 4/200000, Batch: 28/42, Loss: 5.1539, Throughput: 60.59 samples/sec
2025-03-25 04:24:17,192 - training - INFO - Epoch 4 completed in 43.81s. Average loss: 5.1107
2025-03-25 04:24:17,196 - training - INFO - Starting epoch 5/200000
2025-03-25 04:24:18,100 - training - INFO - GPU Memory: GPU 0: 3575.8MB/9560.0MB, GPU 1: 3573.8MB/9582.0MB, GPU 2: 3569.5MB/9586.0MB, GPU 3: 3565.8MB/9598.0MB, GPU 4: 3571.7MB/9586.0MB, GPU 5: 3572.9MB/9678.0MB, GPU 6: 3573.5MB/9582.0MB, GPU 7: 3577.9MB/9574.0MB, GPU 8: 3578.0MB/9572.0MB, Total: 32158.8MB allocated, 86318.0MB reserved, Mean: 3573.2MB allocated, 9590.9MB reserved
2025-03-25 04:24:18,100 - training - INFO - Epoch: 5/200000, Batch: 0/42, Loss: 4.7267, Throughput: 69.70 samples/sec
2025-03-25 04:24:32,845 - training - INFO - GPU Memory: GPU 0: 3572.6MB/9592.0MB, GPU 1: 3573.9MB/9576.0MB, GPU 2: 3570.0MB/9570.0MB, GPU 3: 3567.1MB/9568.0MB, GPU 4: 3569.7MB/9576.0MB, GPU 5: 3573.6MB/9580.0MB, GPU 6: 3570.5MB/9578.0MB, GPU 7: 3574.8MB/9580.0MB, GPU 8: 3573.4MB/9576.0MB, Total: 32145.6MB allocated, 86196.0MB reserved, Mean: 3571.7MB allocated, 9577.3MB reserved
2025-03-25 04:24:32,845 - training - INFO - Epoch: 5/200000, Batch: 14/42, Loss: 4.9725, Throughput: 60.39 samples/sec
2025-03-25 04:24:47,295 - training - INFO - GPU Memory: GPU 0: 3573.0MB/9594.0MB, GPU 1: 3573.9MB/9578.0MB, GPU 2: 3570.0MB/9570.0MB, GPU 3: 3567.1MB/9674.0MB, GPU 4: 3570.1MB/9576.0MB, GPU 5: 3574.0MB/9580.0MB, GPU 6: 3571.4MB/9580.0MB, GPU 7: 3576.3MB/9580.0MB, GPU 8: 3572.1MB/9576.0MB, Total: 32147.8MB allocated, 86308.0MB reserved, Mean: 3572.0MB allocated, 9589.8MB reserved
2025-03-25 04:24:47,296 - training - INFO - Epoch: 5/200000, Batch: 28/42, Loss: 5.0304, Throughput: 60.70 samples/sec
2025-03-25 04:25:00,955 - training - INFO - Epoch 5 completed in 43.76s. Average loss: 5.0104
2025-03-25 04:25:00,959 - training - INFO - Starting epoch 6/200000
2025-03-25 04:25:01,855 - training - INFO - GPU Memory: GPU 0: 3572.2MB/9596.0MB, GPU 1: 3571.6MB/9580.0MB, GPU 2: 3569.5MB/9576.0MB, GPU 3: 3567.1MB/9676.0MB, GPU 4: 3571.1MB/9580.0MB, GPU 5: 3573.5MB/9582.0MB, GPU 6: 3570.6MB/9582.0MB, GPU 7: 3576.3MB/9584.0MB, GPU 8: 3574.5MB/9580.0MB, Total: 32146.4MB allocated, 86336.0MB reserved, Mean: 3571.8MB allocated, 9592.9MB reserved
2025-03-25 04:25:01,855 - training - INFO - Epoch: 6/200000, Batch: 0/42, Loss: 4.6977, Throughput: 70.29 samples/sec
2025-03-25 04:25:16,597 - training - INFO - GPU Memory: GPU 0: 3573.5MB/9572.0MB, GPU 1: 3569.9MB/9578.0MB, GPU 2: 3570.9MB/9578.0MB, GPU 3: 3564.1MB/9668.0MB, GPU 4: 3572.3MB/9582.0MB, GPU 5: 3571.3MB/9576.0MB, GPU 6: 3573.3MB/9674.0MB, GPU 7: 3572.8MB/9572.0MB, GPU 8: 3571.8MB/9582.0MB, Total: 32140.0MB allocated, 86382.0MB reserved, Mean: 3571.1MB allocated, 9598.0MB reserved
2025-03-25 04:25:16,598 - training - INFO - Epoch: 6/200000, Batch: 14/42, Loss: 4.9299, Throughput: 60.43 samples/sec
2025-03-25 04:25:31,118 - training - INFO - GPU Memory: GPU 0: 3572.8MB/9574.0MB, GPU 1: 3570.9MB/9578.0MB, GPU 2: 3569.6MB/9578.0MB, GPU 3: 3565.9MB/9668.0MB, GPU 4: 3571.5MB/9582.0MB, GPU 5: 3572.7MB/9576.0MB, GPU 6: 3571.7MB/9674.0MB, GPU 7: 3572.8MB/9572.0MB, GPU 8: 3571.8MB/9582.0MB, Total: 32139.6MB allocated, 86384.0MB reserved, Mean: 3571.1MB allocated, 9598.2MB reserved
2025-03-25 04:25:31,118 - training - INFO - Epoch: 6/200000, Batch: 28/42, Loss: 4.9343, Throughput: 60.58 samples/sec
2025-03-25 04:25:44,787 - training - INFO - Epoch 6 completed in 43.83s. Average loss: 4.9209
2025-03-25 04:25:44,791 - training - INFO - Starting epoch 7/200000
2025-03-25 04:25:45,690 - training - INFO - GPU Memory: GPU 0: 3572.8MB/9578.0MB, GPU 1: 3573.2MB/9582.0MB, GPU 2: 3568.2MB/9582.0MB, GPU 3: 3568.4MB/9670.0MB, GPU 4: 3570.5MB/9586.0MB, GPU 5: 3572.2MB/9580.0MB, GPU 6: 3577.3MB/9678.0MB, GPU 7: 3572.8MB/9576.0MB, GPU 8: 3572.0MB/9584.0MB, Total: 32147.3MB allocated, 86416.0MB reserved, Mean: 3571.9MB allocated, 9601.8MB reserved
2025-03-25 04:25:45,690 - training - INFO - Epoch: 7/200000, Batch: 0/42, Loss: 5.2052, Throughput: 70.08 samples/sec
2025-03-25 04:26:00,459 - training - INFO - GPU Memory: GPU 0: 3572.9MB/9564.0MB, GPU 1: 3574.1MB/9582.0MB, GPU 2: 3570.3MB/9580.0MB, GPU 3: 3563.9MB/9678.0MB, GPU 4: 3571.6MB/9682.0MB, GPU 5: 3575.4MB/9584.0MB, GPU 6: 3570.6MB/9568.0MB, GPU 7: 3571.3MB/9576.0MB, GPU 8: 3573.3MB/9576.0MB, Total: 32143.3MB allocated, 86390.0MB reserved, Mean: 3571.5MB allocated, 9598.9MB reserved
2025-03-25 04:26:00,459 - training - INFO - Epoch: 7/200000, Batch: 14/42, Loss: 4.7967, Throughput: 60.31 samples/sec
2025-03-25 04:26:14,892 - training - INFO - GPU Memory: GPU 0: 3572.9MB/9564.0MB, GPU 1: 3574.1MB/9582.0MB, GPU 2: 3572.0MB/9580.0MB, GPU 3: 3563.1MB/9678.0MB, GPU 4: 3572.1MB/9682.0MB, GPU 5: 3575.9MB/9584.0MB, GPU 6: 3574.3MB/9568.0MB, GPU 7: 3571.0MB/9576.0MB, GPU 8: 3575.1MB/9576.0MB, Total: 32150.5MB allocated, 86390.0MB reserved, Mean: 3572.3MB allocated, 9598.9MB reserved
2025-03-25 04:26:14,893 - training - INFO - Epoch: 7/200000, Batch: 28/42, Loss: 4.8452, Throughput: 60.70 samples/sec
2025-03-25 04:26:28,591 - training - INFO - Epoch 7 completed in 43.80s. Average loss: 4.8319
2025-03-25 04:26:28,595 - training - INFO - Starting epoch 8/200000
2025-03-25 04:26:29,493 - training - INFO - GPU Memory: GPU 0: 3572.9MB/9568.0MB, GPU 1: 3572.3MB/9584.0MB, GPU 2: 3570.3MB/9584.0MB, GPU 3: 3566.5MB/9682.0MB, GPU 4: 3570.2MB/9686.0MB, GPU 5: 3572.0MB/9586.0MB, GPU 6: 3574.0MB/9572.0MB, GPU 7: 3570.5MB/9580.0MB, GPU 8: 3574.5MB/9580.0MB, Total: 32143.1MB allocated, 86422.0MB reserved, Mean: 3571.5MB allocated, 9602.4MB reserved
2025-03-25 04:26:29,494 - training - INFO - Epoch: 8/200000, Batch: 0/42, Loss: 5.0145, Throughput: 70.11 samples/sec
2025-03-25 04:26:44,186 - training - INFO - GPU Memory: GPU 0: 3570.8MB/9570.0MB, GPU 1: 3573.1MB/9568.0MB, GPU 2: 3571.0MB/9580.0MB, GPU 3: 3562.3MB/9684.0MB, GPU 4: 3574.3MB/9578.0MB, GPU 5: 3573.4MB/9574.0MB, GPU 6: 3574.8MB/9674.0MB, GPU 7: 3572.2MB/9582.0MB, GPU 8: 3572.0MB/9576.0MB, Total: 32143.8MB allocated, 86386.0MB reserved, Mean: 3571.5MB allocated, 9598.4MB reserved
2025-03-25 04:26:44,187 - training - INFO - Epoch: 8/200000, Batch: 14/42, Loss: 4.8162, Throughput: 60.61 samples/sec
2025-03-25 04:26:58,653 - training - INFO - GPU Memory: GPU 0: 3570.8MB/9572.0MB, GPU 1: 3573.1MB/9568.0MB, GPU 2: 3573.0MB/9580.0MB, GPU 3: 3566.8MB/9686.0MB, GPU 4: 3573.8MB/9578.0MB, GPU 5: 3574.0MB/9574.0MB, GPU 6: 3576.5MB/9676.0MB, GPU 7: 3573.6MB/9582.0MB, GPU 8: 3570.9MB/9578.0MB, Total: 32152.5MB allocated, 86394.0MB reserved, Mean: 3572.5MB allocated, 9599.3MB reserved
2025-03-25 04:26:58,653 - training - INFO - Epoch: 8/200000, Batch: 28/42, Loss: 4.7834, Throughput: 60.78 samples/sec
2025-03-25 04:27:12,290 - training - INFO - Epoch 8 completed in 43.70s. Average loss: 4.7629
2025-03-25 04:27:12,294 - training - INFO - Starting epoch 9/200000
2025-03-25 04:27:13,206 - training - INFO - GPU Memory: GPU 0: 3571.9MB/9574.0MB, GPU 1: 3573.1MB/9572.0MB, GPU 2: 3572.7MB/9586.0MB, GPU 3: 3566.8MB/9688.0MB, GPU 4: 3575.8MB/9582.0MB, GPU 5: 3574.0MB/9578.0MB, GPU 6: 3576.5MB/9678.0MB, GPU 7: 3573.1MB/9586.0MB, GPU 8: 3571.9MB/9580.0MB, Total: 32155.8MB allocated, 86424.0MB reserved, Mean: 3572.9MB allocated, 9602.7MB reserved
2025-03-25 04:27:13,206 - training - INFO - Epoch: 9/200000, Batch: 0/42, Loss: 4.5183, Throughput: 69.05 samples/sec
2025-03-25 04:27:27,940 - training - INFO - GPU Memory: GPU 0: 3573.4MB/9560.0MB, GPU 1: 3574.9MB/9584.0MB, GPU 2: 3568.5MB/9672.0MB, GPU 3: 3564.1MB/9692.0MB, GPU 4: 3572.9MB/9586.0MB, GPU 5: 3575.3MB/9576.0MB, GPU 6: 3576.9MB/9574.0MB, GPU 7: 3574.3MB/9576.0MB, GPU 8: 3572.9MB/9676.0MB, Total: 32153.3MB allocated, 86496.0MB reserved, Mean: 3572.6MB allocated, 9610.7MB reserved
2025-03-25 04:27:27,940 - training - INFO - Epoch: 9/200000, Batch: 14/42, Loss: 4.7187, Throughput: 60.40 samples/sec
2025-03-25 04:27:42,412 - training - INFO - GPU Memory: GPU 0: 3573.4MB/9560.0MB, GPU 1: 3574.9MB/9584.0MB, GPU 2: 3568.5MB/9672.0MB, GPU 3: 3563.9MB/9692.0MB, GPU 4: 3571.1MB/9586.0MB, GPU 5: 3572.0MB/9576.0MB, GPU 6: 3573.6MB/9574.0MB, GPU 7: 3573.1MB/9576.0MB, GPU 8: 3572.9MB/9676.0MB, Total: 32143.5MB allocated, 86496.0MB reserved, Mean: 3571.5MB allocated, 9610.7MB reserved
2025-03-25 04:27:42,412 - training - INFO - Epoch: 9/200000, Batch: 28/42, Loss: 4.7058, Throughput: 60.66 samples/sec
2025-03-25 04:27:56,059 - training - INFO - Epoch 9 completed in 43.77s. Average loss: 4.7123
2025-03-25 04:27:56,063 - training - INFO - Starting epoch 10/200000
2025-03-25 04:27:56,975 - training - INFO - GPU Memory: GPU 0: 3572.8MB/9564.0MB, GPU 1: 3571.4MB/9586.0MB, GPU 2: 3570.3MB/9678.0MB, GPU 3: 3565.4MB/9696.0MB, GPU 4: 3572.6MB/9588.0MB, GPU 5: 3572.8MB/9578.0MB, GPU 6: 3576.3MB/9578.0MB, GPU 7: 3574.9MB/9580.0MB, GPU 8: 3574.5MB/9678.0MB, Total: 32150.8MB allocated, 86526.0MB reserved, Mean: 3572.3MB allocated, 9614.0MB reserved
2025-03-25 04:27:56,975 - training - INFO - Epoch: 10/200000, Batch: 0/42, Loss: 4.7636, Throughput: 69.08 samples/sec
2025-03-25 04:28:11,674 - training - INFO - GPU Memory: GPU 0: 3570.4MB/9658.0MB, GPU 1: 3575.5MB/9572.0MB, GPU 2: 3571.8MB/9578.0MB, GPU 3: 3567.4MB/9578.0MB, GPU 4: 3572.9MB/9578.0MB, GPU 5: 3570.6MB/9582.0MB, GPU 6: 3574.8MB/9574.0MB, GPU 7: 3575.5MB/9578.0MB, GPU 8: 3571.8MB/9580.0MB, Total: 32150.6MB allocated, 86278.0MB reserved, Mean: 3572.3MB allocated, 9586.4MB reserved
2025-03-25 04:28:11,675 - training - INFO - Epoch: 10/200000, Batch: 14/42, Loss: 4.6282, Throughput: 60.53 samples/sec
2025-03-25 04:28:26,150 - training - INFO - GPU Memory: GPU 0: 3571.0MB/9658.0MB, GPU 1: 3573.5MB/9572.0MB, GPU 2: 3569.3MB/9578.0MB, GPU 3: 3564.1MB/9684.0MB, GPU 4: 3570.9MB/9578.0MB, GPU 5: 3573.8MB/9582.0MB, GPU 6: 3577.4MB/9574.0MB, GPU 7: 3575.5MB/9578.0MB, GPU 8: 3571.1MB/9580.0MB, Total: 32146.5MB allocated, 86384.0MB reserved, Mean: 3571.8MB allocated, 9598.2MB reserved
2025-03-25 04:28:26,151 - training - INFO - Epoch: 10/200000, Batch: 28/42, Loss: 4.6966, Throughput: 60.72 samples/sec
2025-03-25 04:28:39,835 - training - INFO - Epoch 10 completed in 43.77s. Average loss: 4.6709
2025-03-25 04:28:39,839 - training - INFO - Starting epoch 11/200000
2025-03-25 04:28:40,742 - training - INFO - GPU Memory: GPU 0: 3570.4MB/9660.0MB, GPU 1: 3573.4MB/9574.0MB, GPU 2: 3568.8MB/9584.0MB, GPU 3: 3565.9MB/9686.0MB, GPU 4: 3573.4MB/9582.0MB, GPU 5: 3573.4MB/9586.0MB, GPU 6: 3577.0MB/9576.0MB, GPU 7: 3574.7MB/9582.0MB, GPU 8: 3572.1MB/9582.0MB, Total: 32149.0MB allocated, 86412.0MB reserved, Mean: 3572.1MB allocated, 9601.3MB reserved
2025-03-25 04:28:40,742 - training - INFO - Epoch: 11/200000, Batch: 0/42, Loss: 4.9650, Throughput: 69.76 samples/sec
2025-03-25 04:28:55,471 - training - INFO - GPU Memory: GPU 0: 3571.7MB/9662.0MB, GPU 1: 3570.8MB/9678.0MB, GPU 2: 3572.3MB/9582.0MB, GPU 3: 3568.8MB/9590.0MB, GPU 4: 3576.4MB/9584.0MB, GPU 5: 3575.9MB/9578.0MB, GPU 6: 3574.2MB/9580.0MB, GPU 7: 3572.5MB/9578.0MB, GPU 8: 3570.8MB/9576.0MB, Total: 32153.5MB allocated, 86408.0MB reserved, Mean: 3572.6MB allocated, 9600.9MB reserved
2025-03-25 04:28:55,471 - training - INFO - Epoch: 11/200000, Batch: 14/42, Loss: 4.6706, Throughput: 60.45 samples/sec
2025-03-25 04:29:09,965 - training - INFO - GPU Memory: GPU 0: 3571.7MB/9662.0MB, GPU 1: 3570.8MB/9678.0MB, GPU 2: 3572.6MB/9582.0MB, GPU 3: 3568.1MB/9590.0MB, GPU 4: 3577.8MB/9584.0MB, GPU 5: 3575.4MB/9578.0MB, GPU 6: 3572.1MB/9580.0MB, GPU 7: 3573.6MB/9578.0MB, GPU 8: 3570.9MB/9576.0MB, Total: 32152.9MB allocated, 86408.0MB reserved, Mean: 3572.5MB allocated, 9600.9MB reserved
2025-03-25 04:29:09,965 - training - INFO - Epoch: 11/200000, Batch: 28/42, Loss: 4.6537, Throughput: 60.65 samples/sec
2025-03-25 04:29:23,614 - training - INFO - Epoch 11 completed in 43.78s. Average loss: 4.6337
2025-03-25 04:29:23,618 - training - INFO - Starting epoch 12/200000
2025-03-25 04:29:24,526 - training - INFO - GPU Memory: GPU 0: 3571.7MB/9668.0MB, GPU 1: 3573.2MB/9682.0MB, GPU 2: 3572.6MB/9588.0MB, GPU 3: 3569.6MB/9592.0MB, GPU 4: 3576.3MB/9588.0MB, GPU 5: 3575.6MB/9582.0MB, GPU 6: 3573.1MB/9584.0MB, GPU 7: 3573.0MB/9580.0MB, GPU 8: 3572.5MB/9580.0MB, Total: 32157.6MB allocated, 86444.0MB reserved, Mean: 3573.1MB allocated, 9604.9MB reserved
2025-03-25 04:29:24,526 - training - INFO - Epoch: 12/200000, Batch: 0/42, Loss: 4.6816, Throughput: 69.39 samples/sec
2025-03-25 04:29:39,287 - training - INFO - GPU Memory: GPU 0: 3572.4MB/9562.0MB, GPU 1: 3568.9MB/9580.0MB, GPU 2: 3570.3MB/9564.0MB, GPU 3: 3566.4MB/9584.0MB, GPU 4: 3572.8MB/9580.0MB, GPU 5: 3575.6MB/9678.0MB, GPU 6: 3574.8MB/9678.0MB, GPU 7: 3575.6MB/9670.0MB, GPU 8: 3575.8MB/9578.0MB, Total: 32152.4MB allocated, 86474.0MB reserved, Mean: 3572.5MB allocated, 9608.2MB reserved
2025-03-25 04:29:39,288 - training - INFO - Epoch: 12/200000, Batch: 14/42, Loss: 4.5790, Throughput: 60.31 samples/sec
2025-03-25 04:29:53,846 - training - INFO - GPU Memory: GPU 0: 3572.4MB/9564.0MB, GPU 1: 3568.9MB/9580.0MB, GPU 2: 3568.8MB/9564.0MB, GPU 3: 3568.0MB/9586.0MB, GPU 4: 3572.5MB/9580.0MB, GPU 5: 3573.4MB/9678.0MB, GPU 6: 3572.0MB/9678.0MB, GPU 7: 3574.2MB/9670.0MB, GPU 8: 3572.0MB/9578.0MB, Total: 32142.1MB allocated, 86478.0MB reserved, Mean: 3571.3MB allocated, 9608.7MB reserved
2025-03-25 04:29:53,847 - training - INFO - Epoch: 12/200000, Batch: 28/42, Loss: 4.5728, Throughput: 60.44 samples/sec
2025-03-25 04:30:07,542 - training - INFO - Epoch 12 completed in 43.92s. Average loss: 4.5942
2025-03-25 04:30:07,546 - training - INFO - Starting epoch 13/200000
2025-03-25 04:30:08,441 - training - INFO - GPU Memory: GPU 0: 3570.6MB/9568.0MB, GPU 1: 3568.9MB/9582.0MB, GPU 2: 3570.8MB/9570.0MB, GPU 3: 3567.2MB/9588.0MB, GPU 4: 3575.0MB/9584.0MB, GPU 5: 3572.1MB/9682.0MB, GPU 6: 3572.0MB/9682.0MB, GPU 7: 3575.6MB/9674.0MB, GPU 8: 3576.7MB/9580.0MB, Total: 32148.9MB allocated, 86510.0MB reserved, Mean: 3572.1MB allocated, 9612.2MB reserved
2025-03-25 04:30:08,441 - training - INFO - Epoch: 13/200000, Batch: 0/42, Loss: 4.2764, Throughput: 70.40 samples/sec
2025-03-25 04:30:23,190 - training - INFO - GPU Memory: GPU 0: 3572.6MB/9664.0MB, GPU 1: 3573.3MB/9566.0MB, GPU 2: 3571.5MB/9570.0MB, GPU 3: 3566.6MB/9592.0MB, GPU 4: 3570.5MB/9580.0MB, GPU 5: 3572.3MB/9678.0MB, GPU 6: 3571.0MB/9584.0MB, GPU 7: 3575.5MB/9572.0MB, GPU 8: 3571.8MB/9566.0MB, Total: 32145.1MB allocated, 86372.0MB reserved, Mean: 3571.7MB allocated, 9596.9MB reserved
2025-03-25 04:30:23,191 - training - INFO - Epoch: 13/200000, Batch: 14/42, Loss: 4.5559, Throughput: 60.41 samples/sec
2025-03-25 04:30:37,735 - training - INFO - GPU Memory: GPU 0: 3572.6MB/9664.0MB, GPU 1: 3573.3MB/9566.0MB, GPU 2: 3571.8MB/9570.0MB, GPU 3: 3570.8MB/9592.0MB, GPU 4: 3573.0MB/9580.0MB, GPU 5: 3573.1MB/9678.0MB, GPU 6: 3573.3MB/9584.0MB, GPU 7: 3577.5MB/9572.0MB, GPU 8: 3571.5MB/9566.0MB, Total: 32156.8MB allocated, 86372.0MB reserved, Mean: 3573.0MB allocated, 9596.9MB reserved
2025-03-25 04:30:37,735 - training - INFO - Epoch: 13/200000, Batch: 28/42, Loss: 4.5384, Throughput: 60.52 samples/sec
2025-03-25 04:30:51,402 - training - INFO - Epoch 13 completed in 43.86s. Average loss: 4.5722
2025-03-25 04:30:51,406 - training - INFO - Starting epoch 14/200000
2025-03-25 04:30:52,320 - training - INFO - GPU Memory: GPU 0: 3572.6MB/9668.0MB, GPU 1: 3573.0MB/9570.0MB, GPU 2: 3571.5MB/9576.0MB, GPU 3: 3566.6MB/9594.0MB, GPU 4: 3572.6MB/9584.0MB, GPU 5: 3574.3MB/9682.0MB, GPU 6: 3570.4MB/9586.0MB, GPU 7: 3574.8MB/9576.0MB, GPU 8: 3571.8MB/9568.0MB, Total: 32147.6MB allocated, 86404.0MB reserved, Mean: 3572.0MB allocated, 9600.4MB reserved
2025-03-25 04:30:52,321 - training - INFO - Epoch: 14/200000, Batch: 0/42, Loss: 4.3490, Throughput: 68.90 samples/sec
2025-03-25 04:31:07,013 - training - INFO - GPU Memory: GPU 0: 3572.5MB/9572.0MB, GPU 1: 3572.2MB/9572.0MB, GPU 2: 3568.8MB/9576.0MB, GPU 3: 3564.1MB/9582.0MB, GPU 4: 3573.5MB/9574.0MB, GPU 5: 3570.8MB/9580.0MB, GPU 6: 3572.5MB/9676.0MB, GPU 7: 3573.8MB/9582.0MB, GPU 8: 3573.5MB/9582.0MB, Total: 32141.9MB allocated, 86296.0MB reserved, Mean: 3571.3MB allocated, 9588.4MB reserved
2025-03-25 04:31:07,013 - training - INFO - Epoch: 14/200000, Batch: 14/42, Loss: 4.5127, Throughput: 60.55 samples/sec
2025-03-25 04:32:21,208 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_r_rdeeee.log
2025-03-25 04:32:21,208 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 9
2025-03-25 04:32:21,208 - training - INFO - Device: cuda:0
2025-03-25 04:32:21,812 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 04:32:21,812 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:32:21,812 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:32:21,813 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:32:21,814 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:32:27,109 - training - INFO - Starting model initialization...
2025-03-25 04:32:37,705 - training - INFO - Per-GPU batch size: 28 (global batch size: 252, with gradient_accumulation_steps=4)
2025-03-25 04:32:37,715 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-25 04:32:37,719 - training - INFO - Starting epoch 1/200000
2025-03-25 04:32:44,236 - training - INFO - GPU Memory: GPU 0: 2261.3MB/6108.0MB, GPU 1: 2257.4MB/6274.0MB, GPU 2: 2261.3MB/6108.0MB, GPU 3: 2261.3MB/6108.0MB, GPU 4: 2257.4MB/6274.0MB, GPU 5: 2261.3MB/6128.0MB, GPU 6: 2257.4MB/6274.0MB, GPU 7: 2261.3MB/6108.0MB, GPU 8: 2261.3MB/6108.0MB, Total: 20340.1MB allocated, 55490.0MB reserved, Mean: 2260.0MB allocated, 6165.6MB reserved
2025-03-25 04:32:44,237 - training - INFO - Epoch: 1/200000, Batch: 0/42, Loss: 10.3956, Throughput: 9.67 samples/sec
2025-03-25 04:32:56,514 - training - INFO - GPU Memory: GPU 0: 3571.6MB/9584.0MB, GPU 1: 3564.7MB/9582.0MB, GPU 2: 3569.6MB/9566.0MB, GPU 3: 3569.5MB/9578.0MB, GPU 4: 3571.8MB/9496.0MB, GPU 5: 3563.3MB/9574.0MB, GPU 6: 3570.6MB/9594.0MB, GPU 7: 3570.5MB/9568.0MB, GPU 8: 3573.3MB/9568.0MB, Total: 32124.9MB allocated, 86110.0MB reserved, Mean: 3569.4MB allocated, 9567.8MB reserved
2025-03-25 04:32:56,514 - training - INFO - Epoch: 1/200000, Batch: 14/42, Loss: 9.5350, Throughput: 50.28 samples/sec
2025-03-25 04:33:09,266 - training - INFO - GPU Memory: GPU 0: 3571.1MB/9586.0MB, GPU 1: 3564.7MB/9582.0MB, GPU 2: 3570.3MB/9566.0MB, GPU 3: 3569.5MB/9578.0MB, GPU 4: 3570.1MB/9602.0MB, GPU 5: 3563.3MB/9574.0MB, GPU 6: 3571.2MB/9594.0MB, GPU 7: 3570.0MB/9570.0MB, GPU 8: 3571.9MB/9568.0MB, Total: 32122.1MB allocated, 86220.0MB reserved, Mean: 3569.1MB allocated, 9580.0MB reserved
2025-03-25 04:33:09,269 - training - INFO - Epoch: 1/200000, Batch: 28/42, Loss: 8.7162, Throughput: 57.91 samples/sec
2025-03-25 04:33:22,182 - training - INFO - Epoch 1 completed in 44.46s. Average loss: 8.2196
2025-03-25 04:33:22,198 - training - INFO - Starting epoch 2/200000
2025-03-25 04:33:23,190 - training - INFO - GPU Memory: GPU 0: 3572.9MB/9210.0MB, GPU 1: 3568.0MB/9174.0MB, GPU 2: 3577.7MB/9218.0MB, GPU 3: 3576.5MB/9122.0MB, GPU 4: 3574.8MB/9168.0MB, GPU 5: 3567.1MB/9090.0MB, GPU 6: 3578.0MB/9108.0MB, GPU 7: 3578.4MB/9222.0MB, GPU 8: 3576.1MB/9124.0MB, Total: 32169.4MB allocated, 82436.0MB reserved, Mean: 3574.4MB allocated, 9159.6MB reserved
2025-03-25 04:33:23,190 - training - INFO - Epoch: 2/200000, Batch: 0/42, Loss: 6.5200, Throughput: 63.53 samples/sec
2025-03-25 04:33:35,533 - training - INFO - GPU Memory: GPU 0: 3577.2MB/9576.0MB, GPU 1: 3567.8MB/9584.0MB, GPU 2: 3575.8MB/9568.0MB, GPU 3: 3579.0MB/9576.0MB, GPU 4: 3573.0MB/9680.0MB, GPU 5: 3567.6MB/9576.0MB, GPU 6: 3576.4MB/9580.0MB, GPU 7: 3579.2MB/9582.0MB, GPU 8: 3575.7MB/9686.0MB, Total: 32171.7MB allocated, 86408.0MB reserved, Mean: 3574.6MB allocated, 9600.9MB reserved
2025-03-25 04:33:35,533 - training - INFO - Epoch: 2/200000, Batch: 14/42, Loss: 6.7337, Throughput: 70.87 samples/sec
2025-03-25 04:33:48,282 - training - INFO - GPU Memory: GPU 0: 3577.2MB/9576.0MB, GPU 1: 3567.8MB/9586.0MB, GPU 2: 3575.8MB/9568.0MB, GPU 3: 3579.0MB/9576.0MB, GPU 4: 3574.6MB/9680.0MB, GPU 5: 3569.6MB/9576.0MB, GPU 6: 3577.4MB/9582.0MB, GPU 7: 3577.3MB/9582.0MB, GPU 8: 3577.0MB/9686.0MB, Total: 32175.8MB allocated, 86412.0MB reserved, Mean: 3575.1MB allocated, 9601.3MB reserved
2025-03-25 04:33:48,285 - training - INFO - Epoch: 2/200000, Batch: 28/42, Loss: 6.5941, Throughput: 70.04 samples/sec
2025-03-25 04:33:59,512 - training - INFO - Epoch 2 completed in 37.31s. Average loss: 6.4231
2025-03-25 04:33:59,525 - training - INFO - Starting epoch 3/200000
2025-03-25 04:34:00,462 - training - INFO - GPU Memory: GPU 0: 3577.2MB/9580.0MB, GPU 1: 3571.7MB/9588.0MB, GPU 2: 3574.4MB/9572.0MB, GPU 3: 3575.1MB/9578.0MB, GPU 4: 3574.6MB/9684.0MB, GPU 5: 3566.4MB/9580.0MB, GPU 6: 3575.5MB/9584.0MB, GPU 7: 3579.9MB/9586.0MB, GPU 8: 3575.7MB/9690.0MB, Total: 32170.6MB allocated, 86442.0MB reserved, Mean: 3574.5MB allocated, 9604.7MB reserved
2025-03-25 04:34:00,462 - training - INFO - Epoch: 3/200000, Batch: 0/42, Loss: 6.0797, Throughput: 67.27 samples/sec
2025-03-25 04:34:12,823 - training - INFO - GPU Memory: GPU 0: 3573.4MB/9684.0MB, GPU 1: 3574.0MB/9686.0MB, GPU 2: 3573.6MB/9680.0MB, GPU 3: 3573.0MB/9682.0MB, GPU 4: 3577.0MB/9690.0MB, GPU 5: 3566.2MB/9572.0MB, GPU 6: 3575.7MB/9692.0MB, GPU 7: 3578.7MB/9674.0MB, GPU 8: 3576.0MB/9686.0MB, Total: 32167.6MB allocated, 87046.0MB reserved, Mean: 3574.2MB allocated, 9671.8MB reserved
2025-03-25 04:34:12,824 - training - INFO - Epoch: 3/200000, Batch: 14/42, Loss: 5.9915, Throughput: 71.06 samples/sec
2025-03-25 04:34:25,588 - training - INFO - GPU Memory: GPU 0: 3572.9MB/9684.0MB, GPU 1: 3570.5MB/9686.0MB, GPU 2: 3575.8MB/9680.0MB, GPU 3: 3573.0MB/9682.0MB, GPU 4: 3572.3MB/9690.0MB, GPU 5: 3566.9MB/9572.0MB, GPU 6: 3575.8MB/9694.0MB, GPU 7: 3578.1MB/9674.0MB, GPU 8: 3571.8MB/9686.0MB, Total: 32157.1MB allocated, 87048.0MB reserved, Mean: 3573.0MB allocated, 9672.0MB reserved
2025-03-25 04:34:25,588 - training - INFO - Epoch: 3/200000, Batch: 28/42, Loss: 5.9043, Throughput: 70.10 samples/sec
2025-03-25 04:34:36,859 - training - INFO - Epoch 3 completed in 37.33s. Average loss: 5.8301
2025-03-25 04:34:36,871 - training - INFO - Starting epoch 4/200000
2025-03-25 04:34:37,809 - training - INFO - GPU Memory: GPU 0: 3572.9MB/9686.0MB, GPU 1: 3572.2MB/9690.0MB, GPU 2: 3573.1MB/9682.0MB, GPU 3: 3573.0MB/9686.0MB, GPU 4: 3576.0MB/9692.0MB, GPU 5: 3566.7MB/9576.0MB, GPU 6: 3574.0MB/9698.0MB, GPU 7: 3574.2MB/9678.0MB, GPU 8: 3572.8MB/9690.0MB, Total: 32155.0MB allocated, 87078.0MB reserved, Mean: 3572.8MB allocated, 9675.3MB reserved
2025-03-25 04:34:37,810 - training - INFO - Epoch: 4/200000, Batch: 0/42, Loss: 5.5748, Throughput: 67.16 samples/sec
2025-03-25 04:34:50,219 - training - INFO - GPU Memory: GPU 0: 3576.2MB/9578.0MB, GPU 1: 3569.7MB/9582.0MB, GPU 2: 3573.0MB/9572.0MB, GPU 3: 3571.3MB/9570.0MB, GPU 4: 3575.9MB/9578.0MB, GPU 5: 3564.6MB/9674.0MB, GPU 6: 3578.2MB/9568.0MB, GPU 7: 3576.6MB/9578.0MB, GPU 8: 3572.0MB/9574.0MB, Total: 32157.6MB allocated, 86274.0MB reserved, Mean: 3573.1MB allocated, 9586.0MB reserved
2025-03-25 04:34:50,219 - training - INFO - Epoch: 4/200000, Batch: 14/42, Loss: 5.6139, Throughput: 70.80 samples/sec
2025-03-25 04:35:02,972 - training - INFO - GPU Memory: GPU 0: 3575.7MB/9578.0MB, GPU 1: 3571.3MB/9582.0MB, GPU 2: 3572.1MB/9574.0MB, GPU 3: 3571.3MB/9570.0MB, GPU 4: 3576.4MB/9578.0MB, GPU 5: 3565.0MB/9674.0MB, GPU 6: 3577.7MB/9674.0MB, GPU 7: 3574.8MB/9578.0MB, GPU 8: 3574.7MB/9574.0MB, Total: 32159.1MB allocated, 86382.0MB reserved, Mean: 3573.2MB allocated, 9598.0MB reserved
2025-03-25 04:35:02,972 - training - INFO - Epoch: 4/200000, Batch: 28/42, Loss: 5.5629, Throughput: 70.00 samples/sec
2025-03-25 04:35:14,216 - training - INFO - Epoch 4 completed in 37.34s. Average loss: 5.5031
2025-03-25 04:35:14,226 - training - INFO - Starting epoch 5/200000
2025-03-25 04:35:15,186 - training - INFO - GPU Memory: GPU 0: 3575.7MB/9582.0MB, GPU 1: 3569.2MB/9586.0MB, GPU 2: 3572.1MB/9576.0MB, GPU 3: 3571.6MB/9572.0MB, GPU 4: 3575.7MB/9580.0MB, GPU 5: 3566.9MB/9678.0MB, GPU 6: 3579.2MB/9676.0MB, GPU 7: 3572.1MB/9582.0MB, GPU 8: 3573.2MB/9578.0MB, Total: 32155.8MB allocated, 86410.0MB reserved, Mean: 3572.9MB allocated, 9601.1MB reserved
2025-03-25 04:35:15,186 - training - INFO - Epoch: 5/200000, Batch: 0/42, Loss: 5.0948, Throughput: 65.62 samples/sec
2025-03-25 04:35:27,591 - training - INFO - GPU Memory: GPU 0: 3573.6MB/9582.0MB, GPU 1: 3569.0MB/9580.0MB, GPU 2: 3571.4MB/9572.0MB, GPU 3: 3572.8MB/9570.0MB, GPU 4: 3573.6MB/9566.0MB, GPU 5: 3563.1MB/9674.0MB, GPU 6: 3574.7MB/9576.0MB, GPU 7: 3571.0MB/9582.0MB, GPU 8: 3573.7MB/9576.0MB, Total: 32142.9MB allocated, 86278.0MB reserved, Mean: 3571.4MB allocated, 9586.4MB reserved
2025-03-25 04:35:27,591 - training - INFO - Epoch: 5/200000, Batch: 14/42, Loss: 5.3320, Throughput: 70.71 samples/sec
2025-03-25 04:35:40,321 - training - INFO - GPU Memory: GPU 0: 3573.1MB/9582.0MB, GPU 1: 3570.0MB/9580.0MB, GPU 2: 3570.9MB/9572.0MB, GPU 3: 3572.0MB/9572.0MB, GPU 4: 3573.6MB/9568.0MB, GPU 5: 3562.9MB/9674.0MB, GPU 6: 3575.6MB/9576.0MB, GPU 7: 3570.5MB/9582.0MB, GPU 8: 3571.3MB/9576.0MB, Total: 32139.8MB allocated, 86282.0MB reserved, Mean: 3571.1MB allocated, 9586.9MB reserved
2025-03-25 04:35:40,322 - training - INFO - Epoch: 5/200000, Batch: 28/42, Loss: 5.3679, Throughput: 70.01 samples/sec
2025-03-25 04:35:51,593 - training - INFO - Epoch 5 completed in 37.37s. Average loss: 5.3297
2025-03-25 04:35:51,608 - training - INFO - Starting epoch 6/200000
2025-03-25 04:35:52,533 - training - INFO - GPU Memory: GPU 0: 3573.1MB/9586.0MB, GPU 1: 3571.8MB/9584.0MB, GPU 2: 3571.4MB/9576.0MB, GPU 3: 3573.3MB/9574.0MB, GPU 4: 3572.3MB/9570.0MB, GPU 5: 3563.2MB/9678.0MB, GPU 6: 3574.2MB/9578.0MB, GPU 7: 3575.5MB/9586.0MB, GPU 8: 3571.8MB/9580.0MB, Total: 32146.7MB allocated, 86312.0MB reserved, Mean: 3571.9MB allocated, 9590.2MB reserved
2025-03-25 04:35:52,534 - training - INFO - Epoch: 6/200000, Batch: 0/42, Loss: 4.8759, Throughput: 68.12 samples/sec
2025-03-25 04:36:04,873 - training - INFO - GPU Memory: GPU 0: 3571.8MB/9570.0MB, GPU 1: 3569.5MB/9582.0MB, GPU 2: 3572.7MB/9574.0MB, GPU 3: 3573.2MB/9656.0MB, GPU 4: 3568.0MB/9572.0MB, GPU 5: 3566.4MB/9578.0MB, GPU 6: 3572.7MB/9560.0MB, GPU 7: 3569.7MB/9568.0MB, GPU 8: 3572.1MB/9566.0MB, Total: 32136.1MB allocated, 86226.0MB reserved, Mean: 3570.7MB allocated, 9580.7MB reserved
2025-03-25 04:36:04,874 - training - INFO - Epoch: 6/200000, Batch: 14/42, Loss: 5.2030, Throughput: 71.24 samples/sec
2025-03-25 04:36:17,600 - training - INFO - GPU Memory: GPU 0: 3571.3MB/9570.0MB, GPU 1: 3569.0MB/9582.0MB, GPU 2: 3570.7MB/9574.0MB, GPU 3: 3573.2MB/9656.0MB, GPU 4: 3573.2MB/9574.0MB, GPU 5: 3566.1MB/9578.0MB, GPU 6: 3575.3MB/9666.0MB, GPU 7: 3570.7MB/9568.0MB, GPU 8: 3575.1MB/9566.0MB, Total: 32144.6MB allocated, 86334.0MB reserved, Mean: 3571.6MB allocated, 9592.7MB reserved
2025-03-25 04:36:17,600 - training - INFO - Epoch: 6/200000, Batch: 28/42, Loss: 5.2075, Throughput: 70.29 samples/sec
2025-03-25 04:36:28,855 - training - INFO - Epoch 6 completed in 37.25s. Average loss: 5.2035
2025-03-25 04:36:28,863 - training - INFO - Starting epoch 7/200000
2025-03-25 04:36:29,802 - training - INFO - GPU Memory: GPU 0: 3571.3MB/9574.0MB, GPU 1: 3567.7MB/9588.0MB, GPU 2: 3570.6MB/9576.0MB, GPU 3: 3571.4MB/9660.0MB, GPU 4: 3570.5MB/9576.0MB, GPU 5: 3563.4MB/9582.0MB, GPU 6: 3575.3MB/9668.0MB, GPU 7: 3568.5MB/9572.0MB, GPU 8: 3573.2MB/9570.0MB, Total: 32131.9MB allocated, 86366.0MB reserved, Mean: 3570.2MB allocated, 9596.2MB reserved
2025-03-25 04:36:29,803 - training - INFO - Epoch: 7/200000, Batch: 0/42, Loss: 5.3833, Throughput: 67.06 samples/sec
2025-03-25 04:36:42,121 - training - INFO - GPU Memory: GPU 0: 3573.1MB/9582.0MB, GPU 1: 3567.5MB/9588.0MB, GPU 2: 3570.7MB/9566.0MB, GPU 3: 3572.2MB/9566.0MB, GPU 4: 3573.7MB/9680.0MB, GPU 5: 3571.0MB/9578.0MB, GPU 6: 3573.8MB/9578.0MB, GPU 7: 3569.6MB/9572.0MB, GPU 8: 3573.1MB/9574.0MB, Total: 32144.5MB allocated, 86284.0MB reserved, Mean: 3571.6MB allocated, 9587.1MB reserved
2025-03-25 04:36:42,122 - training - INFO - Epoch: 7/200000, Batch: 14/42, Loss: 5.0822, Throughput: 71.28 samples/sec
2025-03-25 04:36:54,830 - training - INFO - GPU Memory: GPU 0: 3572.6MB/9582.0MB, GPU 1: 3569.0MB/9590.0MB, GPU 2: 3570.2MB/9568.0MB, GPU 3: 3571.3MB/9566.0MB, GPU 4: 3573.2MB/9680.0MB, GPU 5: 3568.5MB/9578.0MB, GPU 6: 3574.3MB/9578.0MB, GPU 7: 3573.0MB/9572.0MB, GPU 8: 3572.7MB/9576.0MB, Total: 32144.7MB allocated, 86290.0MB reserved, Mean: 3571.6MB allocated, 9587.8MB reserved
2025-03-25 04:36:54,830 - training - INFO - Epoch: 7/200000, Batch: 28/42, Loss: 5.1297, Throughput: 70.36 samples/sec
2025-03-25 04:37:06,137 - training - INFO - Epoch 7 completed in 37.27s. Average loss: 5.1251
2025-03-25 04:37:06,150 - training - INFO - Starting epoch 8/200000
2025-03-25 04:37:07,116 - training - INFO - GPU Memory: GPU 0: 3572.6MB/9586.0MB, GPU 1: 3570.0MB/9592.0MB, GPU 2: 3570.2MB/9570.0MB, GPU 3: 3573.8MB/9568.0MB, GPU 4: 3572.4MB/9684.0MB, GPU 5: 3569.4MB/9584.0MB, GPU 6: 3572.7MB/9582.0MB, GPU 7: 3569.6MB/9576.0MB, GPU 8: 3572.2MB/9578.0MB, Total: 32142.7MB allocated, 86320.0MB reserved, Mean: 3571.4MB allocated, 9591.1MB reserved
2025-03-25 04:37:07,116 - training - INFO - Epoch: 8/200000, Batch: 0/42, Loss: 5.3622, Throughput: 65.24 samples/sec
2025-03-25 04:37:19,513 - training - INFO - GPU Memory: GPU 0: 3569.2MB/9580.0MB, GPU 1: 3570.1MB/9586.0MB, GPU 2: 3570.2MB/9570.0MB, GPU 3: 3574.6MB/9572.0MB, GPU 4: 3573.4MB/9568.0MB, GPU 5: 3565.1MB/9672.0MB, GPU 6: 3572.8MB/9580.0MB, GPU 7: 3572.6MB/9570.0MB, GPU 8: 3574.4MB/9576.0MB, Total: 32142.4MB allocated, 86274.0MB reserved, Mean: 3571.4MB allocated, 9586.0MB reserved
2025-03-25 04:37:19,514 - training - INFO - Epoch: 8/200000, Batch: 14/42, Loss: 5.1089, Throughput: 70.72 samples/sec
2025-03-25 04:37:32,210 - training - INFO - GPU Memory: GPU 0: 3568.7MB/9582.0MB, GPU 1: 3565.7MB/9586.0MB, GPU 2: 3570.8MB/9676.0MB, GPU 3: 3572.8MB/9574.0MB, GPU 4: 3573.1MB/9568.0MB, GPU 5: 3565.1MB/9672.0MB, GPU 6: 3574.6MB/9582.0MB, GPU 7: 3572.1MB/9570.0MB, GPU 8: 3572.8MB/9576.0MB, Total: 32135.5MB allocated, 86386.0MB reserved, Mean: 3570.6MB allocated, 9598.4MB reserved
2025-03-25 04:37:32,211 - training - INFO - Epoch: 8/200000, Batch: 28/42, Loss: 5.0958, Throughput: 70.11 samples/sec
2025-03-25 04:37:43,528 - training - INFO - Epoch 8 completed in 37.38s. Average loss: 5.0595
2025-03-25 04:37:43,534 - training - INFO - Starting epoch 9/200000
2025-03-25 04:37:44,471 - training - INFO - GPU Memory: GPU 0: 3568.7MB/9584.0MB, GPU 1: 3568.2MB/9590.0MB, GPU 2: 3569.9MB/9678.0MB, GPU 3: 3573.0MB/9576.0MB, GPU 4: 3572.9MB/9572.0MB, GPU 5: 3565.6MB/9678.0MB, GPU 6: 3575.0MB/9584.0MB, GPU 7: 3570.6MB/9574.0MB, GPU 8: 3572.7MB/9580.0MB, Total: 32136.5MB allocated, 86416.0MB reserved, Mean: 3570.7MB allocated, 9601.8MB reserved
2025-03-25 04:37:44,471 - training - INFO - Epoch: 9/200000, Batch: 0/42, Loss: 4.9304, Throughput: 67.22 samples/sec
2025-03-25 04:37:56,827 - training - INFO - GPU Memory: GPU 0: 3571.3MB/9576.0MB, GPU 1: 3570.5MB/9676.0MB, GPU 2: 3571.0MB/9582.0MB, GPU 3: 3571.8MB/9566.0MB, GPU 4: 3573.4MB/9572.0MB, GPU 5: 3568.2MB/9686.0MB, GPU 6: 3572.1MB/9672.0MB, GPU 7: 3570.6MB/9574.0MB, GPU 8: 3570.6MB/9672.0MB, Total: 32139.3MB allocated, 86576.0MB reserved, Mean: 3571.0MB allocated, 9619.6MB reserved
2025-03-25 04:37:56,828 - training - INFO - Epoch: 9/200000, Batch: 14/42, Loss: 5.0087, Throughput: 71.09 samples/sec
2025-03-25 04:38:09,597 - training - INFO - GPU Memory: GPU 0: 3572.1MB/9576.0MB, GPU 1: 3570.0MB/9676.0MB, GPU 2: 3570.5MB/9582.0MB, GPU 3: 3572.8MB/9566.0MB, GPU 4: 3573.8MB/9572.0MB, GPU 5: 3568.8MB/9686.0MB, GPU 6: 3569.8MB/9672.0MB, GPU 7: 3572.0MB/9574.0MB, GPU 8: 3569.8MB/9672.0MB, Total: 32139.5MB allocated, 86576.0MB reserved, Mean: 3571.1MB allocated, 9619.6MB reserved
2025-03-25 04:38:09,597 - training - INFO - Epoch: 9/200000, Batch: 28/42, Loss: 4.9978, Throughput: 70.10 samples/sec
2025-03-25 04:38:20,866 - training - INFO - Epoch 9 completed in 37.33s. Average loss: 5.0026
2025-03-25 04:38:20,878 - training - INFO - Starting epoch 10/200000
2025-03-25 04:38:21,838 - training - INFO - GPU Memory: GPU 0: 3570.6MB/9580.0MB, GPU 1: 3566.3MB/9680.0MB, GPU 2: 3569.7MB/9584.0MB, GPU 3: 3571.9MB/9568.0MB, GPU 4: 3572.3MB/9576.0MB, GPU 5: 3568.8MB/9690.0MB, GPU 6: 3571.6MB/9674.0MB, GPU 7: 3570.7MB/9578.0MB, GPU 8: 3573.2MB/9676.0MB, Total: 32135.0MB allocated, 86606.0MB reserved, Mean: 3570.6MB allocated, 9622.9MB reserved
2025-03-25 04:38:21,838 - training - INFO - Epoch: 10/200000, Batch: 0/42, Loss: 5.1362, Throughput: 65.65 samples/sec
2025-03-25 04:38:34,271 - training - INFO - GPU Memory: GPU 0: 3572.2MB/9588.0MB, GPU 1: 3569.0MB/9576.0MB, GPU 2: 3568.3MB/9584.0MB, GPU 3: 3576.9MB/9572.0MB, GPU 4: 3571.7MB/9580.0MB, GPU 5: 3569.7MB/9574.0MB, GPU 6: 3575.1MB/9582.0MB, GPU 7: 3571.0MB/9578.0MB, GPU 8: 3570.2MB/9586.0MB, Total: 32144.2MB allocated, 86220.0MB reserved, Mean: 3571.6MB allocated, 9580.0MB reserved
2025-03-25 04:38:34,271 - training - INFO - Epoch: 10/200000, Batch: 14/42, Loss: 4.9013, Throughput: 70.56 samples/sec
2025-03-25 04:38:46,991 - training - INFO - GPU Memory: GPU 0: 3571.7MB/9588.0MB, GPU 1: 3568.0MB/9576.0MB, GPU 2: 3571.1MB/9584.0MB, GPU 3: 3577.4MB/9574.0MB, GPU 4: 3571.2MB/9580.0MB, GPU 5: 3564.1MB/9680.0MB, GPU 6: 3572.1MB/9582.0MB, GPU 7: 3577.3MB/9578.0MB, GPU 8: 3569.4MB/9586.0MB, Total: 32142.3MB allocated, 86328.0MB reserved, Mean: 3571.4MB allocated, 9592.0MB reserved
2025-03-25 04:38:46,991 - training - INFO - Epoch: 10/200000, Batch: 28/42, Loss: 4.9673, Throughput: 69.97 samples/sec
2025-03-25 04:38:58,228 - training - INFO - Epoch 10 completed in 37.35s. Average loss: 4.9517
2025-03-25 04:38:58,232 - training - INFO - Starting epoch 11/200000
2025-03-25 04:38:59,185 - training - INFO - GPU Memory: GPU 0: 3570.7MB/9590.0MB, GPU 1: 3567.5MB/9580.0MB, GPU 2: 3567.8MB/9588.0MB, GPU 3: 3571.4MB/9576.0MB, GPU 4: 3572.5MB/9584.0MB, GPU 5: 3566.1MB/9684.0MB, GPU 6: 3572.0MB/9584.0MB, GPU 7: 3572.5MB/9582.0MB, GPU 8: 3569.9MB/9588.0MB, Total: 32130.4MB allocated, 86356.0MB reserved, Mean: 3570.0MB allocated, 9595.1MB reserved
2025-03-25 04:38:59,186 - training - INFO - Epoch: 11/200000, Batch: 0/42, Loss: 5.3171, Throughput: 66.11 samples/sec
2025-03-25 04:39:11,575 - training - INFO - GPU Memory: GPU 0: 3573.1MB/9570.0MB, GPU 1: 3567.9MB/9588.0MB, GPU 2: 3571.8MB/9574.0MB, GPU 3: 3576.5MB/9560.0MB, GPU 4: 3573.2MB/9578.0MB, GPU 5: 3565.4MB/9672.0MB, GPU 6: 3573.4MB/9572.0MB, GPU 7: 3571.9MB/9670.0MB, GPU 8: 3569.4MB/9580.0MB, Total: 32142.7MB allocated, 86364.0MB reserved, Mean: 3571.4MB allocated, 9596.0MB reserved
2025-03-25 04:39:11,576 - training - INFO - Epoch: 11/200000, Batch: 14/42, Loss: 4.9335, Throughput: 70.82 samples/sec
2025-03-25 04:39:24,365 - training - INFO - GPU Memory: GPU 0: 3572.6MB/9570.0MB, GPU 1: 3567.4MB/9588.0MB, GPU 2: 3570.1MB/9576.0MB, GPU 3: 3576.5MB/9560.0MB, GPU 4: 3573.8MB/9578.0MB, GPU 5: 3567.8MB/9672.0MB, GPU 6: 3571.9MB/9572.0MB, GPU 7: 3571.4MB/9670.0MB, GPU 8: 3567.8MB/9580.0MB, Total: 32139.4MB allocated, 86366.0MB reserved, Mean: 3571.0MB allocated, 9596.2MB reserved
2025-03-25 04:39:24,366 - training - INFO - Epoch: 11/200000, Batch: 28/42, Loss: 4.9082, Throughput: 69.91 samples/sec
2025-03-25 04:43:59,511 - training - INFO - Overriding batch size with command-line value: 7
2025-03-25 04:43:59,511 - training - INFO - Overriding gradient accumulation steps with command-line value: 2
2025-03-25 04:43:59,511 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_k3l3v33m.log
2025-03-25 04:43:59,511 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 9
2025-03-25 04:43:59,511 - training - INFO - Device: cuda:0
2025-03-25 04:43:59,994 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 04:43:59,994 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:43:59,994 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:43:59,996 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:43:59,996 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:44:06,098 - training - INFO - Starting model initialization...
2025-03-25 04:44:15,824 - training - INFO - Per-GPU batch size: 14 (global batch size: 126, with gradient_accumulation_steps=2)
2025-03-25 04:44:15,833 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-25 04:44:15,837 - training - INFO - Starting epoch 1/200000
2025-03-25 04:44:22,035 - training - INFO - GPU Memory: GPU 0: 2257.4MB/6274.0MB, GPU 1: 2261.3MB/6108.0MB, GPU 2: 2257.4MB/6274.0MB, GPU 3: 2261.3MB/6128.0MB, GPU 4: 2257.4MB/6274.0MB, GPU 5: 2261.3MB/6108.0MB, GPU 6: 2257.4MB/6274.0MB, GPU 7: 2261.3MB/6108.0MB, GPU 8: 2261.3MB/6108.0MB, Total: 20336.2MB allocated, 55656.0MB reserved, Mean: 2259.6MB allocated, 6184.0MB reserved
2025-03-25 04:44:22,036 - training - INFO - Epoch: 1/200000, Batch: 0/42, Loss: 10.4916, Throughput: 10.16 samples/sec
2025-03-25 04:44:36,668 - training - INFO - GPU Memory: GPU 0: 3567.5MB/9492.0MB, GPU 1: 3572.2MB/9596.0MB, GPU 2: 3570.2MB/9590.0MB, GPU 3: 3563.3MB/9580.0MB, GPU 4: 3570.9MB/9590.0MB, GPU 5: 3572.7MB/9596.0MB, GPU 6: 3567.9MB/9588.0MB, GPU 7: 3573.1MB/9586.0MB, GPU 8: 3569.1MB/9596.0MB, Total: 32126.7MB allocated, 86214.0MB reserved, Mean: 3569.6MB allocated, 9579.3MB reserved
2025-03-25 04:44:36,668 - training - INFO - Epoch: 1/200000, Batch: 14/42, Loss: 8.9322, Throughput: 45.37 samples/sec
2025-03-25 04:44:51,133 - training - INFO - GPU Memory: GPU 0: 3567.9MB/9492.0MB, GPU 1: 3572.2MB/9596.0MB, GPU 2: 3569.7MB/9590.0MB, GPU 3: 3564.8MB/9580.0MB, GPU 4: 3570.1MB/9590.0MB, GPU 5: 3568.9MB/9596.0MB, GPU 6: 3567.9MB/9588.0MB, GPU 7: 3570.8MB/9586.0MB, GPU 8: 3570.8MB/9596.0MB, Total: 32123.1MB allocated, 86214.0MB reserved, Mean: 3569.2MB allocated, 9579.3MB reserved
2025-03-25 04:44:51,135 - training - INFO - Epoch: 1/200000, Batch: 28/42, Loss: 7.9945, Throughput: 51.76 samples/sec
2025-03-25 04:45:06,452 - training - INFO - Epoch 1 completed in 50.62s. Average loss: 7.4569
2025-03-25 04:45:06,457 - training - INFO - Starting epoch 2/200000
2025-03-25 04:45:07,395 - training - INFO - GPU Memory: GPU 0: 3572.8MB/9172.0MB, GPU 1: 3574.2MB/9114.0MB, GPU 2: 3573.4MB/9104.0MB, GPU 3: 3564.6MB/9098.0MB, GPU 4: 3576.7MB/9122.0MB, GPU 5: 3574.9MB/9124.0MB, GPU 6: 3578.0MB/9120.0MB, GPU 7: 3578.8MB/9122.0MB, GPU 8: 3575.7MB/9118.0MB, Total: 32169.1MB allocated, 82094.0MB reserved, Mean: 3574.3MB allocated, 9121.6MB reserved
2025-03-25 04:45:07,395 - training - INFO - Epoch: 2/200000, Batch: 0/42, Loss: 5.7646, Throughput: 67.15 samples/sec
2025-03-25 04:45:22,092 - training - INFO - GPU Memory: GPU 0: 3574.1MB/9686.0MB, GPU 1: 3576.2MB/9594.0MB, GPU 2: 3579.6MB/9596.0MB, GPU 3: 3562.7MB/9582.0MB, GPU 4: 3574.8MB/9592.0MB, GPU 5: 3577.5MB/9598.0MB, GPU 6: 3575.9MB/9594.0MB, GPU 7: 3573.4MB/9608.0MB, GPU 8: 3577.3MB/9586.0MB, Total: 32171.4MB allocated, 86436.0MB reserved, Mean: 3574.6MB allocated, 9604.0MB reserved
2025-03-25 04:45:22,093 - training - INFO - Epoch: 2/200000, Batch: 14/42, Loss: 5.9346, Throughput: 60.44 samples/sec
2025-03-25 04:45:36,565 - training - INFO - GPU Memory: GPU 0: 3574.1MB/9686.0MB, GPU 1: 3574.7MB/9596.0MB, GPU 2: 3576.1MB/9598.0MB, GPU 3: 3564.2MB/9582.0MB, GPU 4: 3574.8MB/9592.0MB, GPU 5: 3574.7MB/9598.0MB, GPU 6: 3577.3MB/9594.0MB, GPU 7: 3576.5MB/9608.0MB, GPU 8: 3575.6MB/9586.0MB, Total: 32167.9MB allocated, 86440.0MB reserved, Mean: 3574.2MB allocated, 9604.4MB reserved
2025-03-25 04:45:36,566 - training - INFO - Epoch: 2/200000, Batch: 28/42, Loss: 5.8424, Throughput: 60.68 samples/sec
2025-03-25 04:45:50,177 - training - INFO - Epoch 2 completed in 43.72s. Average loss: 5.7156
2025-03-25 04:45:50,182 - training - INFO - Starting epoch 3/200000
2025-03-25 04:45:51,094 - training - INFO - GPU Memory: GPU 0: 3574.1MB/9690.0MB, GPU 1: 3577.7MB/9598.0MB, GPU 2: 3578.1MB/9600.0MB, GPU 3: 3566.3MB/9588.0MB, GPU 4: 3574.8MB/9594.0MB, GPU 5: 3579.1MB/9602.0MB, GPU 6: 3574.2MB/9598.0MB, GPU 7: 3579.0MB/9610.0MB, GPU 8: 3574.6MB/9588.0MB, Total: 32177.9MB allocated, 86468.0MB reserved, Mean: 3575.3MB allocated, 9607.6MB reserved
2025-03-25 04:45:51,094 - training - INFO - Epoch: 3/200000, Batch: 0/42, Loss: 5.4085, Throughput: 69.07 samples/sec
2025-03-25 04:46:05,758 - training - INFO - GPU Memory: GPU 0: 3573.8MB/9580.0MB, GPU 1: 3573.2MB/9698.0MB, GPU 2: 3574.5MB/9564.0MB, GPU 3: 3567.4MB/9598.0MB, GPU 4: 3577.1MB/9682.0MB, GPU 5: 3574.4MB/9688.0MB, GPU 6: 3576.4MB/9580.0MB, GPU 7: 3575.9MB/9574.0MB, GPU 8: 3575.8MB/9696.0MB, Total: 32168.5MB allocated, 86660.0MB reserved, Mean: 3574.3MB allocated, 9628.9MB reserved
2025-03-25 04:46:05,761 - training - INFO - Epoch: 3/200000, Batch: 14/42, Loss: 5.4000, Throughput: 60.67 samples/sec
2025-03-25 04:46:20,225 - training - INFO - GPU Memory: GPU 0: 3573.8MB/9582.0MB, GPU 1: 3573.2MB/9698.0MB, GPU 2: 3574.3MB/9564.0MB, GPU 3: 3566.2MB/9598.0MB, GPU 4: 3576.1MB/9682.0MB, GPU 5: 3575.6MB/9688.0MB, GPU 6: 3576.4MB/9580.0MB, GPU 7: 3575.3MB/9574.0MB, GPU 8: 3577.1MB/9696.0MB, Total: 32168.0MB allocated, 86662.0MB reserved, Mean: 3574.2MB allocated, 9629.1MB reserved
2025-03-25 04:46:20,226 - training - INFO - Epoch: 3/200000, Batch: 28/42, Loss: 5.3399, Throughput: 60.81 samples/sec
2025-03-25 04:46:33,813 - training - INFO - Epoch 3 completed in 43.63s. Average loss: 5.2855
2025-03-25 04:46:33,817 - training - INFO - Starting epoch 4/200000
2025-03-25 04:46:34,739 - training - INFO - GPU Memory: GPU 0: 3573.8MB/9584.0MB, GPU 1: 3576.6MB/9700.0MB, GPU 2: 3574.5MB/9568.0MB, GPU 3: 3566.2MB/9602.0MB, GPU 4: 3572.6MB/9686.0MB, GPU 5: 3572.6MB/9692.0MB, GPU 6: 3576.3MB/9584.0MB, GPU 7: 3577.7MB/9578.0MB, GPU 8: 3573.6MB/9700.0MB, Total: 32164.0MB allocated, 86694.0MB reserved, Mean: 3573.8MB allocated, 9632.7MB reserved
2025-03-25 04:46:34,739 - training - INFO - Epoch: 4/200000, Batch: 0/42, Loss: 5.1495, Throughput: 68.36 samples/sec
2025-03-25 04:46:49,461 - training - INFO - GPU Memory: GPU 0: 3570.8MB/9572.0MB, GPU 1: 3573.6MB/9584.0MB, GPU 2: 3576.0MB/9562.0MB, GPU 3: 3564.3MB/9694.0MB, GPU 4: 3576.7MB/9578.0MB, GPU 5: 3575.6MB/9574.0MB, GPU 6: 3575.6MB/9676.0MB, GPU 7: 3574.8MB/9580.0MB, GPU 8: 3570.3MB/9578.0MB, Total: 32157.6MB allocated, 86398.0MB reserved, Mean: 3573.1MB allocated, 9599.8MB reserved
2025-03-25 04:46:49,461 - training - INFO - Epoch: 4/200000, Batch: 14/42, Loss: 5.1861, Throughput: 60.41 samples/sec
2025-03-25 04:47:03,965 - training - INFO - GPU Memory: GPU 0: 3571.2MB/9572.0MB, GPU 1: 3573.6MB/9584.0MB, GPU 2: 3576.9MB/9562.0MB, GPU 3: 3567.5MB/9694.0MB, GPU 4: 3574.6MB/9578.0MB, GPU 5: 3578.0MB/9576.0MB, GPU 6: 3574.1MB/9676.0MB, GPU 7: 3576.3MB/9580.0MB, GPU 8: 3572.1MB/9580.0MB, Total: 32164.3MB allocated, 86402.0MB reserved, Mean: 3573.8MB allocated, 9600.2MB reserved
2025-03-25 04:47:03,966 - training - INFO - Epoch: 4/200000, Batch: 28/42, Loss: 5.1500, Throughput: 60.60 samples/sec
2025-03-25 04:47:17,609 - training - INFO - Epoch 4 completed in 43.79s. Average loss: 5.1076
2025-03-25 04:47:17,613 - training - INFO - Starting epoch 5/200000
2025-03-25 04:47:18,539 - training - INFO - GPU Memory: GPU 0: 3570.2MB/9576.0MB, GPU 1: 3572.6MB/9586.0MB, GPU 2: 3573.3MB/9566.0MB, GPU 3: 3567.5MB/9698.0MB, GPU 4: 3575.1MB/9582.0MB, GPU 5: 3575.6MB/9580.0MB, GPU 6: 3570.1MB/9680.0MB, GPU 7: 3574.8MB/9582.0MB, GPU 8: 3570.3MB/9582.0MB, Total: 32149.5MB allocated, 86432.0MB reserved, Mean: 3572.2MB allocated, 9603.6MB reserved
2025-03-25 04:47:18,539 - training - INFO - Epoch: 5/200000, Batch: 0/42, Loss: 4.7107, Throughput: 68.07 samples/sec
2025-03-25 04:47:33,225 - training - INFO - GPU Memory: GPU 0: 3572.3MB/9674.0MB, GPU 1: 3571.0MB/9680.0MB, GPU 2: 3572.0MB/9660.0MB, GPU 3: 3565.4MB/9594.0MB, GPU 4: 3571.3MB/9666.0MB, GPU 5: 3570.8MB/9578.0MB, GPU 6: 3570.8MB/9574.0MB, GPU 7: 3574.3MB/9572.0MB, GPU 8: 3573.9MB/9582.0MB, Total: 32141.6MB allocated, 86580.0MB reserved, Mean: 3571.3MB allocated, 9620.0MB reserved
2025-03-25 04:47:33,226 - training - INFO - Epoch: 5/200000, Batch: 14/42, Loss: 4.9629, Throughput: 60.53 samples/sec
2025-03-25 04:47:47,754 - training - INFO - GPU Memory: GPU 0: 3573.3MB/9674.0MB, GPU 1: 3567.9MB/9680.0MB, GPU 2: 3572.6MB/9660.0MB, GPU 3: 3568.6MB/9594.0MB, GPU 4: 3572.8MB/9668.0MB, GPU 5: 3573.2MB/9578.0MB, GPU 6: 3572.3MB/9574.0MB, GPU 7: 3574.3MB/9572.0MB, GPU 8: 3573.9MB/9582.0MB, Total: 32148.7MB allocated, 86582.0MB reserved, Mean: 3572.1MB allocated, 9620.2MB reserved
2025-03-25 04:47:47,754 - training - INFO - Epoch: 5/200000, Batch: 28/42, Loss: 5.0312, Throughput: 60.62 samples/sec
2025-03-25 04:48:01,392 - training - INFO - Epoch 5 completed in 43.78s. Average loss: 5.0064
2025-03-25 04:48:01,395 - training - INFO - Starting epoch 6/200000
2025-03-25 04:48:02,303 - training - INFO - GPU Memory: GPU 0: 3572.3MB/9678.0MB, GPU 1: 3571.0MB/9682.0MB, GPU 2: 3572.8MB/9664.0MB, GPU 3: 3563.8MB/9598.0MB, GPU 4: 3571.0MB/9670.0MB, GPU 5: 3572.5MB/9582.0MB, GPU 6: 3572.3MB/9580.0MB, GPU 7: 3575.5MB/9574.0MB, GPU 8: 3573.9MB/9586.0MB, Total: 32145.2MB allocated, 86614.0MB reserved, Mean: 3571.7MB allocated, 9623.8MB reserved
2025-03-25 04:48:02,303 - training - INFO - Epoch: 6/200000, Batch: 0/42, Loss: 4.5599, Throughput: 69.42 samples/sec
2025-03-25 04:48:17,027 - training - INFO - GPU Memory: GPU 0: 3569.1MB/9586.0MB, GPU 1: 3572.8MB/9598.0MB, GPU 2: 3574.3MB/9578.0MB, GPU 3: 3564.7MB/9678.0MB, GPU 4: 3571.6MB/9578.0MB, GPU 5: 3573.5MB/9580.0MB, GPU 6: 3573.8MB/9580.0MB, GPU 7: 3574.5MB/9672.0MB, GPU 8: 3571.5MB/9578.0MB, Total: 32145.8MB allocated, 86428.0MB reserved, Mean: 3571.8MB allocated, 9603.1MB reserved
2025-03-25 04:48:17,027 - training - INFO - Epoch: 6/200000, Batch: 14/42, Loss: 4.8891, Throughput: 60.46 samples/sec
2025-03-25 04:48:31,477 - training - INFO - GPU Memory: GPU 0: 3569.1MB/9586.0MB, GPU 1: 3572.8MB/9600.0MB, GPU 2: 3574.6MB/9580.0MB, GPU 3: 3563.7MB/9678.0MB, GPU 4: 3574.1MB/9578.0MB, GPU 5: 3573.5MB/9580.0MB, GPU 6: 3570.3MB/9580.0MB, GPU 7: 3574.5MB/9672.0MB, GPU 8: 3572.4MB/9578.0MB, Total: 32144.9MB allocated, 86432.0MB reserved, Mean: 3571.7MB allocated, 9603.6MB reserved
2025-03-25 04:48:31,477 - training - INFO - Epoch: 6/200000, Batch: 28/42, Loss: 4.9111, Throughput: 60.74 samples/sec
2025-03-25 04:48:45,104 - training - INFO - Epoch 6 completed in 43.71s. Average loss: 4.9088
2025-03-25 04:48:45,108 - training - INFO - Starting epoch 7/200000
2025-03-25 04:49:31,459 - training - INFO - Overriding batch size with command-line value: 6
2025-03-25 04:49:31,460 - training - INFO - Overriding gradient accumulation steps with command-line value: 3
2025-03-25 04:49:31,460 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_lre0d17a.log
2025-03-25 04:49:31,460 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 9
2025-03-25 04:49:31,460 - training - INFO - Device: cuda:0
2025-03-25 04:49:32,011 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 04:49:32,011 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 04:49:32,011 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 04:49:32,013 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 04:49:32,013 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 04:49:37,116 - training - INFO - Starting model initialization...
2025-03-25 04:49:47,857 - training - INFO - Per-GPU batch size: 18 (global batch size: 162, with gradient_accumulation_steps=3)
2025-03-25 04:49:47,866 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-25 04:49:47,870 - training - INFO - Starting epoch 1/200000
2025-03-25 04:49:52,958 - training - INFO - GPU Memory: GPU 0: 2233.9MB/5730.0MB, GPU 1: 2234.4MB/5708.0MB, GPU 2: 2233.9MB/5730.0MB, GPU 3: 2233.9MB/5730.0MB, GPU 4: 2233.9MB/5730.0MB, GPU 5: 2233.9MB/5730.0MB, GPU 6: 2233.9MB/5730.0MB, GPU 7: 2233.9MB/5730.0MB, GPU 8: 2233.9MB/5730.0MB, Total: 20105.3MB allocated, 51548.0MB reserved, Mean: 2233.9MB allocated, 5727.6MB reserved
2025-03-25 04:49:52,959 - training - INFO - Epoch: 1/200000, Batch: 0/49, Loss: 11.6665, Throughput: 10.61 samples/sec
2025-03-25 04:50:07,697 - training - INFO - GPU Memory: GPU 0: 3548.0MB/8576.0MB, GPU 1: 3546.9MB/8584.0MB, GPU 2: 3544.3MB/8588.0MB, GPU 3: 3546.0MB/8580.0MB, GPU 4: 3545.0MB/8588.0MB, GPU 5: 3544.6MB/8588.0MB, GPU 6: 3547.4MB/8582.0MB, GPU 7: 3547.4MB/8582.0MB, GPU 8: 3543.1MB/8582.0MB, Total: 31912.7MB allocated, 77250.0MB reserved, Mean: 3545.9MB allocated, 8583.3MB reserved
2025-03-25 04:50:07,697 - training - INFO - Epoch: 1/200000, Batch: 16/49, Loss: 8.0353, Throughput: 46.30 samples/sec
2025-03-25 04:50:22,884 - training - INFO - GPU Memory: GPU 0: 2884.6MB/8576.0MB, GPU 1: 2882.8MB/8584.0MB, GPU 2: 2882.6MB/8588.0MB, GPU 3: 2883.4MB/8580.0MB, GPU 4: 2882.6MB/8588.0MB, GPU 5: 2882.6MB/8588.0MB, GPU 6: 2882.6MB/8582.0MB, GPU 7: 2882.6MB/8582.0MB, GPU 8: 2882.6MB/8582.0MB, Total: 25946.3MB allocated, 77250.0MB reserved, Mean: 2882.9MB allocated, 8583.3MB reserved
2025-03-25 04:50:22,886 - training - INFO - Epoch: 1/200000, Batch: 32/49, Loss: 7.0152, Throughput: 50.89 samples/sec
2025-03-25 04:50:38,728 - training - INFO - GPU Memory: GPU 0: 3530.6MB/4964.0MB, GPU 1: 3520.6MB/5028.0MB, GPU 2: 3520.0MB/4970.0MB, GPU 3: 3525.9MB/5012.0MB, GPU 4: 3528.4MB/4982.0MB, GPU 5: 3521.1MB/4970.0MB, GPU 6: 3529.2MB/4978.0MB, GPU 7: 3529.2MB/4980.0MB, GPU 8: 3525.1MB/4978.0MB, Total: 31730.0MB allocated, 44862.0MB reserved, Mean: 3525.6MB allocated, 4984.7MB reserved
2025-03-25 04:50:38,728 - training - INFO - Epoch: 1/200000, Batch: 48/49, Loss: 6.5045, Throughput: 52.03 samples/sec
2025-03-25 04:50:38,729 - training - INFO - Epoch 1 completed in 50.86s. Average loss: 6.4709
2025-03-25 04:50:38,733 - training - INFO - Starting epoch 2/200000
2025-03-25 04:50:39,651 - training - INFO - GPU Memory: GPU 0: 3544.6MB/8222.0MB, GPU 1: 3539.2MB/8168.0MB, GPU 2: 3540.1MB/8176.0MB, GPU 3: 3538.8MB/8184.0MB, GPU 4: 3536.8MB/8238.0MB, GPU 5: 3538.5MB/8176.0MB, GPU 6: 3539.1MB/8238.0MB, GPU 7: 3539.3MB/8232.0MB, GPU 8: 3538.3MB/8230.0MB, Total: 31854.6MB allocated, 73864.0MB reserved, Mean: 3539.4MB allocated, 8207.1MB reserved
2025-03-25 04:50:39,652 - training - INFO - Epoch: 2/200000, Batch: 0/49, Loss: 4.8518, Throughput: 58.80 samples/sec
2025-03-25 04:50:54,509 - training - INFO - GPU Memory: GPU 0: 3543.6MB/8612.0MB, GPU 1: 3541.2MB/8622.0MB, GPU 2: 3541.2MB/8622.0MB, GPU 3: 3542.4MB/8610.0MB, GPU 4: 3541.8MB/8608.0MB, GPU 5: 3540.6MB/8608.0MB, GPU 6: 3538.5MB/8620.0MB, GPU 7: 3540.9MB/8604.0MB, GPU 8: 3539.0MB/8606.0MB, Total: 31869.1MB allocated, 77512.0MB reserved, Mean: 3541.0MB allocated, 8612.4MB reserved
2025-03-25 04:50:54,510 - training - INFO - Epoch: 2/200000, Batch: 16/49, Loss: 5.2091, Throughput: 58.19 samples/sec
2025-03-25 04:51:09,716 - training - INFO - GPU Memory: GPU 0: 2884.6MB/8612.0MB, GPU 1: 2882.8MB/8622.0MB, GPU 2: 2882.6MB/8622.0MB, GPU 3: 2882.8MB/8610.0MB, GPU 4: 2882.6MB/8608.0MB, GPU 5: 2882.6MB/8608.0MB, GPU 6: 2882.6MB/8620.0MB, GPU 7: 2882.6MB/8604.0MB, GPU 8: 2882.6MB/8606.0MB, Total: 25945.7MB allocated, 77512.0MB reserved, Mean: 2882.9MB allocated, 8612.4MB reserved
2025-03-25 04:51:09,716 - training - INFO - Epoch: 2/200000, Batch: 32/49, Loss: 5.1444, Throughput: 57.52 samples/sec
2025-03-25 04:51:24,208 - training - INFO - GPU Memory: GPU 0: 3525.0MB/8644.0MB, GPU 1: 3519.1MB/8656.0MB, GPU 2: 3523.5MB/8656.0MB, GPU 3: 3520.8MB/8642.0MB, GPU 4: 3512.7MB/8640.0MB, GPU 5: 3513.9MB/8642.0MB, GPU 6: 3523.0MB/8652.0MB, GPU 7: 3518.7MB/8638.0MB, GPU 8: 3520.8MB/8640.0MB, Total: 31677.5MB allocated, 77810.0MB reserved, Mean: 3519.7MB allocated, 8645.6MB reserved
2025-03-25 04:51:24,209 - training - INFO - Epoch: 2/200000, Batch: 48/49, Loss: 5.1145, Throughput: 58.19 samples/sec
2025-03-25 04:51:24,209 - training - INFO - Epoch 2 completed in 45.48s. Average loss: 5.0772
2025-03-25 04:51:24,219 - training - INFO - Starting epoch 3/200000
2025-03-25 04:51:25,089 - training - INFO - GPU Memory: GPU 0: 3543.0MB/8644.0MB, GPU 1: 3540.1MB/8656.0MB, GPU 2: 3539.3MB/8656.0MB, GPU 3: 3540.7MB/8642.0MB, GPU 4: 3540.8MB/8640.0MB, GPU 5: 3539.7MB/8642.0MB, GPU 6: 3541.2MB/8652.0MB, GPU 7: 3538.6MB/8638.0MB, GPU 8: 3537.7MB/8640.0MB, Total: 31861.2MB allocated, 77810.0MB reserved, Mean: 3540.1MB allocated, 8645.6MB reserved
2025-03-25 04:51:25,089 - training - INFO - Epoch: 3/200000, Batch: 0/49, Loss: 5.0122, Throughput: 62.09 samples/sec
2025-03-25 04:51:39,902 - training - INFO - GPU Memory: GPU 0: 3547.8MB/8602.0MB, GPU 1: 3541.8MB/8628.0MB, GPU 2: 3541.9MB/8622.0MB, GPU 3: 3542.0MB/8602.0MB, GPU 4: 3539.3MB/8620.0MB, GPU 5: 3539.0MB/8596.0MB, GPU 6: 3540.8MB/8606.0MB, GPU 7: 3542.5MB/8596.0MB, GPU 8: 3540.9MB/8602.0MB, Total: 31875.9MB allocated, 77474.0MB reserved, Mean: 3541.8MB allocated, 8608.2MB reserved
2025-03-25 04:51:39,902 - training - INFO - Epoch: 3/200000, Batch: 16/49, Loss: 4.9497, Throughput: 58.54 samples/sec
2025-03-25 04:51:55,010 - training - INFO - GPU Memory: GPU 0: 2884.6MB/8602.0MB, GPU 1: 2882.8MB/8628.0MB, GPU 2: 2882.6MB/8622.0MB, GPU 3: 2883.4MB/8602.0MB, GPU 4: 2882.6MB/8620.0MB, GPU 5: 2882.6MB/8596.0MB, GPU 6: 2882.6MB/8606.0MB, GPU 7: 2882.6MB/8596.0MB, GPU 8: 2882.6MB/8602.0MB, Total: 25946.3MB allocated, 77474.0MB reserved, Mean: 2882.9MB allocated, 8608.2MB reserved
2025-03-25 04:51:55,010 - training - INFO - Epoch: 3/200000, Batch: 32/49, Loss: 4.9006, Throughput: 57.87 samples/sec
2025-03-25 04:52:09,444 - training - INFO - GPU Memory: GPU 0: 3532.4MB/8634.0MB, GPU 1: 3516.4MB/8660.0MB, GPU 2: 3518.5MB/8654.0MB, GPU 3: 3522.0MB/8634.0MB, GPU 4: 3518.9MB/8654.0MB, GPU 5: 3531.5MB/8628.0MB, GPU 6: 3525.3MB/8638.0MB, GPU 7: 3518.4MB/8630.0MB, GPU 8: 3521.6MB/8636.0MB, Total: 31704.8MB allocated, 77768.0MB reserved, Mean: 3522.8MB allocated, 8640.9MB reserved
2025-03-25 04:52:09,444 - training - INFO - Epoch: 3/200000, Batch: 48/49, Loss: 4.8898, Throughput: 58.51 samples/sec
2025-03-25 04:52:09,445 - training - INFO - Epoch 3 completed in 45.23s. Average loss: 4.8688
2025-03-25 04:52:09,450 - training - INFO - Starting epoch 4/200000
2025-03-25 04:52:10,335 - training - INFO - GPU Memory: GPU 0: 3547.2MB/8634.0MB, GPU 1: 3541.2MB/8660.0MB, GPU 2: 3541.3MB/8654.0MB, GPU 3: 3543.8MB/8634.0MB, GPU 4: 3538.8MB/8654.0MB, GPU 5: 3539.6MB/8628.0MB, GPU 6: 3540.3MB/8638.0MB, GPU 7: 3542.6MB/8630.0MB, GPU 8: 3542.6MB/8636.0MB, Total: 31877.3MB allocated, 77768.0MB reserved, Mean: 3541.9MB allocated, 8640.9MB reserved
2025-03-25 04:52:10,336 - training - INFO - Epoch: 4/200000, Batch: 0/49, Loss: 4.6743, Throughput: 61.01 samples/sec
2025-03-25 04:52:25,185 - training - INFO - GPU Memory: GPU 0: 3551.8MB/8600.0MB, GPU 1: 3544.2MB/8628.0MB, GPU 2: 3542.5MB/8610.0MB, GPU 3: 3543.1MB/8614.0MB, GPU 4: 3540.2MB/8592.0MB, GPU 5: 3545.0MB/8590.0MB, GPU 6: 3542.2MB/8606.0MB, GPU 7: 3545.4MB/8622.0MB, GPU 8: 3541.8MB/8624.0MB, Total: 31896.1MB allocated, 77486.0MB reserved, Mean: 3544.0MB allocated, 8609.6MB reserved
2025-03-25 04:52:25,185 - training - INFO - Epoch: 4/200000, Batch: 16/49, Loss: 4.8241, Throughput: 58.34 samples/sec
2025-03-25 04:52:40,375 - training - INFO - GPU Memory: GPU 0: 2884.6MB/8600.0MB, GPU 1: 2882.8MB/8628.0MB, GPU 2: 2882.6MB/8610.0MB, GPU 3: 2883.4MB/8614.0MB, GPU 4: 2882.6MB/8592.0MB, GPU 5: 2882.6MB/8590.0MB, GPU 6: 2882.6MB/8606.0MB, GPU 7: 2882.6MB/8622.0MB, GPU 8: 2882.6MB/8624.0MB, Total: 25946.3MB allocated, 77486.0MB reserved, Mean: 2882.9MB allocated, 8609.6MB reserved
2025-03-25 04:52:40,375 - training - INFO - Epoch: 4/200000, Batch: 32/49, Loss: 4.8134, Throughput: 57.62 samples/sec
2025-03-25 04:52:54,790 - training - INFO - GPU Memory: GPU 0: 3530.8MB/8632.0MB, GPU 1: 3519.4MB/8662.0MB, GPU 2: 3525.0MB/8642.0MB, GPU 3: 3520.7MB/8646.0MB, GPU 4: 3519.2MB/8624.0MB, GPU 5: 3524.0MB/8624.0MB, GPU 6: 3524.7MB/8638.0MB, GPU 7: 3519.8MB/8656.0MB, GPU 8: 3527.6MB/8656.0MB, Total: 31711.3MB allocated, 77780.0MB reserved, Mean: 3523.5MB allocated, 8642.2MB reserved
2025-03-25 04:52:54,790 - training - INFO - Epoch: 4/200000, Batch: 48/49, Loss: 4.7750, Throughput: 58.36 samples/sec
2025-03-25 04:52:54,791 - training - INFO - Epoch 4 completed in 45.34s. Average loss: 4.7701
2025-03-25 04:52:54,795 - training - INFO - Starting epoch 5/200000
2025-03-25 04:52:55,682 - training - INFO - GPU Memory: GPU 0: 3551.0MB/8632.0MB, GPU 1: 3542.7MB/8662.0MB, GPU 2: 3541.9MB/8642.0MB, GPU 3: 3542.5MB/8646.0MB, GPU 4: 3539.3MB/8624.0MB, GPU 5: 3544.4MB/8624.0MB, GPU 6: 3541.6MB/8638.0MB, GPU 7: 3545.1MB/8656.0MB, GPU 8: 3541.0MB/8656.0MB, Total: 31889.4MB allocated, 77780.0MB reserved, Mean: 3543.3MB allocated, 8642.2MB reserved
2025-03-25 04:52:55,682 - training - INFO - Epoch: 5/200000, Batch: 0/49, Loss: 4.3355, Throughput: 60.88 samples/sec
2025-03-25 04:53:10,457 - training - INFO - GPU Memory: GPU 0: 3551.0MB/8608.0MB, GPU 1: 3542.6MB/8624.0MB, GPU 2: 3544.8MB/8610.0MB, GPU 3: 3546.2MB/8622.0MB, GPU 4: 3540.2MB/8622.0MB, GPU 5: 3544.7MB/8612.0MB, GPU 6: 3545.5MB/8596.0MB, GPU 7: 3542.6MB/8620.0MB, GPU 8: 3540.6MB/8620.0MB, Total: 31898.2MB allocated, 77534.0MB reserved, Mean: 3544.2MB allocated, 8614.9MB reserved
2025-03-25 04:53:10,458 - training - INFO - Epoch: 5/200000, Batch: 16/49, Loss: 4.6366, Throughput: 58.61 samples/sec
2025-03-25 04:53:25,664 - training - INFO - GPU Memory: GPU 0: 2884.6MB/8608.0MB, GPU 1: 2882.8MB/8624.0MB, GPU 2: 2882.6MB/8610.0MB, GPU 3: 2883.4MB/8622.0MB, GPU 4: 2882.6MB/8622.0MB, GPU 5: 2882.6MB/8612.0MB, GPU 6: 2882.6MB/8598.0MB, GPU 7: 2882.0MB/8620.0MB, GPU 8: 2882.6MB/8620.0MB, Total: 25945.7MB allocated, 77536.0MB reserved, Mean: 2882.9MB allocated, 8615.1MB reserved
2025-03-25 04:53:25,664 - training - INFO - Epoch: 5/200000, Batch: 32/49, Loss: 4.7148, Throughput: 57.73 samples/sec
2025-03-25 04:53:40,245 - training - INFO - GPU Memory: GPU 0: 3525.7MB/8642.0MB, GPU 1: 3516.5MB/8656.0MB, GPU 2: 3516.6MB/8642.0MB, GPU 3: 3517.2MB/8654.0MB, GPU 4: 3523.5MB/8654.0MB, GPU 5: 3520.8MB/8644.0MB, GPU 6: 3520.4MB/8628.0MB, GPU 7: 3523.3MB/8654.0MB, GPU 8: 3522.4MB/8652.0MB, Total: 31686.2MB allocated, 77826.0MB reserved, Mean: 3520.7MB allocated, 8647.3MB reserved
2025-03-25 04:53:40,245 - training - INFO - Epoch: 5/200000, Batch: 48/49, Loss: 4.7134, Throughput: 58.22 samples/sec
2025-03-25 04:53:40,246 - training - INFO - Epoch 5 completed in 45.45s. Average loss: 4.6845
2025-03-25 04:53:40,257 - training - INFO - Starting epoch 6/200000
2025-03-25 04:53:41,142 - training - INFO - GPU Memory: GPU 0: 3550.4MB/8642.0MB, GPU 1: 3542.0MB/8656.0MB, GPU 2: 3544.5MB/8642.0MB, GPU 3: 3545.6MB/8654.0MB, GPU 4: 3539.6MB/8654.0MB, GPU 5: 3544.1MB/8644.0MB, GPU 6: 3544.9MB/8628.0MB, GPU 7: 3544.6MB/8654.0MB, GPU 8: 3540.1MB/8652.0MB, Total: 31895.8MB allocated, 77826.0MB reserved, Mean: 3544.0MB allocated, 8647.3MB reserved
2025-03-25 04:53:41,142 - training - INFO - Epoch: 6/200000, Batch: 0/49, Loss: 4.3485, Throughput: 61.04 samples/sec
2025-03-25 04:53:55,999 - training - INFO - GPU Memory: GPU 0: 3552.6MB/8592.0MB, GPU 1: 3545.1MB/8620.0MB, GPU 2: 3545.5MB/8598.0MB, GPU 3: 3544.1MB/8600.0MB, GPU 4: 3544.6MB/8598.0MB, GPU 5: 3544.2MB/8606.0MB, GPU 6: 3542.3MB/8604.0MB, GPU 7: 3546.1MB/8610.0MB, GPU 8: 3543.8MB/8592.0MB, Total: 31908.5MB allocated, 77420.0MB reserved, Mean: 3545.4MB allocated, 8602.2MB reserved
2025-03-25 04:53:55,999 - training - INFO - Epoch: 6/200000, Batch: 16/49, Loss: 4.5958, Throughput: 58.31 samples/sec
2025-03-25 04:54:11,206 - training - INFO - GPU Memory: GPU 0: 2884.6MB/8592.0MB, GPU 1: 2882.8MB/8620.0MB, GPU 2: 2882.6MB/8598.0MB, GPU 3: 2883.4MB/8600.0MB, GPU 4: 2882.6MB/8598.0MB, GPU 5: 2882.6MB/8606.0MB, GPU 6: 2882.6MB/8604.0MB, GPU 7: 2882.6MB/8610.0MB, GPU 8: 2882.6MB/8592.0MB, Total: 25946.3MB allocated, 77420.0MB reserved, Mean: 2882.9MB allocated, 8602.2MB reserved
2025-03-25 04:54:11,206 - training - INFO - Epoch: 6/200000, Batch: 32/49, Loss: 4.6183, Throughput: 57.58 samples/sec
2025-03-25 04:54:25,707 - training - INFO - GPU Memory: GPU 0: 3529.2MB/8624.0MB, GPU 1: 3518.9MB/8652.0MB, GPU 2: 3526.1MB/8630.0MB, GPU 3: 3518.0MB/8632.0MB, GPU 4: 3520.2MB/8630.0MB, GPU 5: 3521.2MB/8640.0MB, GPU 6: 3518.0MB/8636.0MB, GPU 7: 3526.4MB/8644.0MB, GPU 8: 3516.4MB/8624.0MB, Total: 31694.4MB allocated, 77712.0MB reserved, Mean: 3521.6MB allocated, 8634.7MB reserved
2025-03-25 04:54:25,707 - training - INFO - Epoch: 6/200000, Batch: 48/49, Loss: 4.6118, Throughput: 58.22 samples/sec
2025-03-25 04:54:25,708 - training - INFO - Epoch 6 completed in 45.45s. Average loss: 4.5938
2025-03-25 04:54:25,719 - training - INFO - Starting epoch 7/200000
2025-03-25 04:54:26,589 - training - INFO - GPU Memory: GPU 0: 3552.0MB/8624.0MB, GPU 1: 3544.3MB/8652.0MB, GPU 2: 3546.3MB/8630.0MB, GPU 3: 3541.7MB/8632.0MB, GPU 4: 3545.2MB/8630.0MB, GPU 5: 3544.0MB/8640.0MB, GPU 6: 3541.8MB/8636.0MB, GPU 7: 3545.3MB/8644.0MB, GPU 8: 3543.3MB/8624.0MB, Total: 31903.8MB allocated, 77712.0MB reserved, Mean: 3544.9MB allocated, 8634.7MB reserved
2025-03-25 04:54:26,589 - training - INFO - Epoch: 7/200000, Batch: 0/49, Loss: 4.8250, Throughput: 62.08 samples/sec
2025-03-25 04:54:41,437 - training - INFO - GPU Memory: GPU 0: 3555.2MB/8604.0MB, GPU 1: 3547.7MB/8616.0MB, GPU 2: 3545.3MB/8620.0MB, GPU 3: 3544.9MB/8590.0MB, GPU 4: 3544.8MB/8612.0MB, GPU 5: 3546.1MB/8596.0MB, GPU 6: 3546.6MB/8596.0MB, GPU 7: 3546.6MB/8620.0MB, GPU 8: 3545.3MB/8578.0MB, Total: 31922.4MB allocated, 77432.0MB reserved, Mean: 3546.9MB allocated, 8603.6MB reserved
2025-03-25 04:54:41,438 - training - INFO - Epoch: 7/200000, Batch: 16/49, Loss: 4.5191, Throughput: 58.41 samples/sec
2025-03-25 04:54:56,614 - training - INFO - GPU Memory: GPU 0: 2884.4MB/8604.0MB, GPU 1: 2882.8MB/8616.0MB, GPU 2: 2882.4MB/8620.0MB, GPU 3: 2883.4MB/8590.0MB, GPU 4: 2882.6MB/8612.0MB, GPU 5: 2882.6MB/8596.0MB, GPU 6: 2882.6MB/8596.0MB, GPU 7: 2882.6MB/8620.0MB, GPU 8: 2882.0MB/8578.0MB, Total: 25945.3MB allocated, 77432.0MB reserved, Mean: 2882.8MB allocated, 8603.6MB reserved
2025-03-25 04:54:56,614 - training - INFO - Epoch: 7/200000, Batch: 32/49, Loss: 4.5446, Throughput: 57.68 samples/sec
2025-03-25 04:55:11,106 - training - INFO - GPU Memory: GPU 0: 3523.0MB/8636.0MB, GPU 1: 3521.8MB/8650.0MB, GPU 2: 3516.6MB/8652.0MB, GPU 3: 3520.0MB/8624.0MB, GPU 4: 3514.5MB/8644.0MB, GPU 5: 3516.9MB/8628.0MB, GPU 6: 3515.6MB/8628.0MB, GPU 7: 3517.1MB/8654.0MB, GPU 8: 3521.8MB/8610.0MB, Total: 31667.2MB allocated, 77726.0MB reserved, Mean: 3518.6MB allocated, 8636.2MB reserved
2025-03-25 04:55:11,107 - training - INFO - Epoch: 7/200000, Batch: 48/49, Loss: 4.5309, Throughput: 58.30 samples/sec
2025-03-25 04:55:11,107 - training - INFO - Epoch 7 completed in 45.39s. Average loss: 4.5401
2025-03-25 04:55:11,116 - training - INFO - Starting epoch 8/200000
2025-03-25 04:55:11,980 - training - INFO - GPU Memory: GPU 0: 3554.8MB/8636.0MB, GPU 1: 3546.9MB/8650.0MB, GPU 2: 3546.0MB/8652.0MB, GPU 3: 3544.3MB/8624.0MB, GPU 4: 3545.7MB/8644.0MB, GPU 5: 3545.3MB/8628.0MB, GPU 6: 3546.0MB/8628.0MB, GPU 7: 3546.0MB/8654.0MB, GPU 8: 3547.8MB/8610.0MB, Total: 31922.9MB allocated, 77726.0MB reserved, Mean: 3547.0MB allocated, 8636.2MB reserved
2025-03-25 04:55:11,980 - training - INFO - Epoch: 8/200000, Batch: 0/49, Loss: 4.3525, Throughput: 62.50 samples/sec
2025-03-25 04:55:26,819 - training - INFO - GPU Memory: GPU 0: 3557.8MB/8606.0MB, GPU 1: 3547.0MB/8610.0MB, GPU 2: 3545.9MB/8602.0MB, GPU 3: 3544.8MB/8616.0MB, GPU 4: 3547.1MB/8612.0MB, GPU 5: 3547.7MB/8614.0MB, GPU 6: 3547.1MB/8608.0MB, GPU 7: 3545.3MB/8620.0MB, GPU 8: 3544.6MB/8612.0MB, Total: 31927.3MB allocated, 77500.0MB reserved, Mean: 3547.5MB allocated, 8611.1MB reserved
2025-03-25 04:55:26,819 - training - INFO - Epoch: 8/200000, Batch: 16/49, Loss: 4.5446, Throughput: 58.46 samples/sec
2025-03-25 04:55:41,973 - training - INFO - GPU Memory: GPU 0: 2884.6MB/8606.0MB, GPU 1: 2882.8MB/8610.0MB, GPU 2: 2882.6MB/8602.0MB, GPU 3: 2883.4MB/8616.0MB, GPU 4: 2882.6MB/8612.0MB, GPU 5: 2882.6MB/8614.0MB, GPU 6: 2882.6MB/8608.0MB, GPU 7: 2882.6MB/8620.0MB, GPU 8: 2882.4MB/8612.0MB, Total: 25946.1MB allocated, 77500.0MB reserved, Mean: 2882.9MB allocated, 8611.1MB reserved
2025-03-25 04:55:41,973 - training - INFO - Epoch: 8/200000, Batch: 32/49, Loss: 4.5076, Throughput: 57.75 samples/sec
2025-03-25 04:55:56,436 - training - INFO - GPU Memory: GPU 0: 3522.2MB/8640.0MB, GPU 1: 3516.9MB/8644.0MB, GPU 2: 3526.6MB/8634.0MB, GPU 3: 3518.6MB/8648.0MB, GPU 4: 3521.6MB/8646.0MB, GPU 5: 3520.5MB/8646.0MB, GPU 6: 3516.7MB/8640.0MB, GPU 7: 3517.4MB/8654.0MB, GPU 8: 3521.5MB/8644.0MB, Total: 31681.9MB allocated, 77796.0MB reserved, Mean: 3520.2MB allocated, 8644.0MB reserved
2025-03-25 04:55:56,436 - training - INFO - Epoch: 8/200000, Batch: 48/49, Loss: 4.5022, Throughput: 58.39 samples/sec
2025-03-25 04:55:56,437 - training - INFO - Epoch 8 completed in 45.32s. Average loss: 4.4899
2025-03-25 04:55:56,445 - training - INFO - Starting epoch 9/200000
2025-03-25 04:55:57,324 - training - INFO - GPU Memory: GPU 0: 3557.3MB/8640.0MB, GPU 1: 3546.2MB/8644.0MB, GPU 2: 3545.6MB/8634.0MB, GPU 3: 3542.7MB/8648.0MB, GPU 4: 3546.7MB/8646.0MB, GPU 5: 3546.1MB/8646.0MB, GPU 6: 3546.7MB/8640.0MB, GPU 7: 3544.8MB/8654.0MB, GPU 8: 3544.5MB/8644.0MB, Total: 31920.6MB allocated, 77796.0MB reserved, Mean: 3546.7MB allocated, 8644.0MB reserved
2025-03-25 04:55:57,324 - training - INFO - Epoch: 9/200000, Batch: 0/49, Loss: 4.3713, Throughput: 61.44 samples/sec
2025-03-25 04:56:12,177 - training - INFO - GPU Memory: GPU 0: 3563.3MB/8620.0MB, GPU 1: 3546.4MB/8630.0MB, GPU 2: 3545.7MB/8596.0MB, GPU 3: 3545.0MB/8608.0MB, GPU 4: 3546.8MB/8622.0MB, GPU 5: 3549.1MB/8604.0MB, GPU 6: 3545.9MB/8600.0MB, GPU 7: 3546.1MB/8622.0MB, GPU 8: 3549.1MB/8596.0MB, Total: 31937.4MB allocated, 77498.0MB reserved, Mean: 3548.6MB allocated, 8610.9MB reserved
2025-03-25 04:56:12,177 - training - INFO - Epoch: 9/200000, Batch: 16/49, Loss: 4.4627, Throughput: 58.35 samples/sec
2025-03-25 04:56:27,405 - training - INFO - GPU Memory: GPU 0: 2884.6MB/8620.0MB, GPU 1: 2882.8MB/8630.0MB, GPU 2: 2882.6MB/8596.0MB, GPU 3: 2883.4MB/8608.0MB, GPU 4: 2882.0MB/8622.0MB, GPU 5: 2882.6MB/8604.0MB, GPU 6: 2882.6MB/8600.0MB, GPU 7: 2882.0MB/8622.0MB, GPU 8: 2882.6MB/8596.0MB, Total: 25945.1MB allocated, 77498.0MB reserved, Mean: 2882.8MB allocated, 8610.9MB reserved
2025-03-25 04:56:27,406 - training - INFO - Epoch: 9/200000, Batch: 32/49, Loss: 4.4538, Throughput: 57.56 samples/sec
2025-03-25 04:56:41,846 - training - INFO - GPU Memory: GPU 0: 3530.2MB/8652.0MB, GPU 1: 3523.8MB/8664.0MB, GPU 2: 3526.6MB/8628.0MB, GPU 3: 3520.0MB/8640.0MB, GPU 4: 3517.2MB/8654.0MB, GPU 5: 3522.9MB/8638.0MB, GPU 6: 3516.8MB/8632.0MB, GPU 7: 3515.0MB/8656.0MB, GPU 8: 3522.5MB/8630.0MB, Total: 31694.9MB allocated, 77794.0MB reserved, Mean: 3521.7MB allocated, 8643.8MB reserved
2025-03-25 04:56:41,846 - training - INFO - Epoch: 9/200000, Batch: 48/49, Loss: 4.4663, Throughput: 58.28 samples/sec
2025-03-25 04:56:41,846 - training - INFO - Epoch 9 completed in 45.40s. Average loss: 4.4499
2025-03-25 04:56:41,850 - training - INFO - Starting epoch 10/200000
2025-03-25 04:56:42,716 - training - INFO - GPU Memory: GPU 0: 3562.8MB/8652.0MB, GPU 1: 3547.9MB/8664.0MB, GPU 2: 3545.4MB/8628.0MB, GPU 3: 3545.5MB/8640.0MB, GPU 4: 3546.8MB/8654.0MB, GPU 5: 3545.3MB/8638.0MB, GPU 6: 3545.3MB/8632.0MB, GPU 7: 3545.5MB/8656.0MB, GPU 8: 3548.8MB/8630.0MB, Total: 31933.2MB allocated, 77794.0MB reserved, Mean: 3548.1MB allocated, 8643.8MB reserved
2025-03-25 04:56:42,716 - training - INFO - Epoch: 10/200000, Batch: 0/49, Loss: 4.2395, Throughput: 62.42 samples/sec
2025-03-25 04:56:57,558 - training - INFO - GPU Memory: GPU 0: 3559.9MB/8614.0MB, GPU 1: 3546.8MB/8624.0MB, GPU 2: 3545.3MB/8604.0MB, GPU 3: 3543.1MB/8612.0MB, GPU 4: 3543.6MB/8620.0MB, GPU 5: 3544.6MB/8616.0MB, GPU 6: 3545.6MB/8606.0MB, GPU 7: 3549.4MB/8596.0MB, GPU 8: 3546.3MB/8608.0MB, Total: 31924.8MB allocated, 77500.0MB reserved, Mean: 3547.2MB allocated, 8611.1MB reserved
2025-03-25 04:56:57,559 - training - INFO - Epoch: 10/200000, Batch: 16/49, Loss: 4.3796, Throughput: 58.44 samples/sec
2025-03-25 04:57:12,703 - training - INFO - GPU Memory: GPU 0: 2884.6MB/8614.0MB, GPU 1: 2882.8MB/8624.0MB, GPU 2: 2882.4MB/8604.0MB, GPU 3: 2883.4MB/8612.0MB, GPU 4: 2882.6MB/8620.0MB, GPU 5: 2882.6MB/8616.0MB, GPU 6: 2882.4MB/8606.0MB, GPU 7: 2882.6MB/8596.0MB, GPU 8: 2882.6MB/8608.0MB, Total: 25945.9MB allocated, 77500.0MB reserved, Mean: 2882.9MB allocated, 8611.1MB reserved
2025-03-25 04:57:12,703 - training - INFO - Epoch: 10/200000, Batch: 32/49, Loss: 4.4418, Throughput: 57.76 samples/sec
2025-03-25 04:57:27,138 - training - INFO - GPU Memory: GPU 0: 3528.0MB/8646.0MB, GPU 1: 3520.9MB/8656.0MB, GPU 2: 3528.8MB/8636.0MB, GPU 3: 3520.4MB/8644.0MB, GPU 4: 3528.1MB/8652.0MB, GPU 5: 3517.5MB/8648.0MB, GPU 6: 3523.1MB/8638.0MB, GPU 7: 3521.0MB/8630.0MB, GPU 8: 3525.1MB/8642.0MB, Total: 31712.8MB allocated, 77792.0MB reserved, Mean: 3523.6MB allocated, 8643.6MB reserved
2025-03-25 04:57:27,138 - training - INFO - Epoch: 10/200000, Batch: 48/49, Loss: 4.4171, Throughput: 58.43 samples/sec
2025-03-25 04:57:27,138 - training - INFO - Epoch 10 completed in 45.29s. Average loss: 4.4224
2025-03-25 04:57:27,145 - training - INFO - Starting epoch 11/200000
2025-03-25 04:57:28,016 - training - INFO - GPU Memory: GPU 0: 3559.3MB/8646.0MB, GPU 1: 3546.7MB/8656.0MB, GPU 2: 3544.7MB/8636.0MB, GPU 3: 3542.8MB/8644.0MB, GPU 4: 3546.0MB/8652.0MB, GPU 5: 3543.2MB/8648.0MB, GPU 6: 3545.5MB/8638.0MB, GPU 7: 3545.5MB/8630.0MB, GPU 8: 3543.9MB/8642.0MB, Total: 31917.6MB allocated, 77792.0MB reserved, Mean: 3546.4MB allocated, 8643.6MB reserved
2025-03-25 04:57:28,016 - training - INFO - Epoch: 11/200000, Batch: 0/49, Loss: 4.7105, Throughput: 61.99 samples/sec
2025-03-25 04:57:42,935 - training - INFO - GPU Memory: GPU 0: 3559.9MB/8606.0MB, GPU 1: 3544.4MB/8604.0MB, GPU 2: 3545.8MB/8600.0MB, GPU 3: 3545.0MB/8614.0MB, GPU 4: 3548.0MB/8622.0MB, GPU 5: 3547.1MB/8604.0MB, GPU 6: 3545.1MB/8610.0MB, GPU 7: 3545.3MB/8618.0MB, GPU 8: 3546.2MB/8604.0MB, Total: 31926.8MB allocated, 77482.0MB reserved, Mean: 3547.4MB allocated, 8609.1MB reserved
2025-03-25 04:57:42,936 - training - INFO - Epoch: 11/200000, Batch: 16/49, Loss: 4.4467, Throughput: 58.14 samples/sec
2025-03-25 04:57:58,197 - training - INFO - GPU Memory: GPU 0: 2884.6MB/8606.0MB, GPU 1: 2882.8MB/8604.0MB, GPU 2: 2882.6MB/8600.0MB, GPU 3: 2883.4MB/8614.0MB, GPU 4: 2882.6MB/8622.0MB, GPU 5: 2882.4MB/8604.0MB, GPU 6: 2882.6MB/8610.0MB, GPU 7: 2882.6MB/8618.0MB, GPU 8: 2882.6MB/8604.0MB, Total: 25946.1MB allocated, 77482.0MB reserved, Mean: 2882.9MB allocated, 8609.1MB reserved
2025-03-25 04:57:58,197 - training - INFO - Epoch: 11/200000, Batch: 32/49, Loss: 4.4223, Throughput: 57.39 samples/sec
2025-03-25 04:58:12,653 - training - INFO - GPU Memory: GPU 0: 3529.2MB/8638.0MB, GPU 1: 3516.8MB/8636.0MB, GPU 2: 3523.6MB/8632.0MB, GPU 3: 3517.1MB/8646.0MB, GPU 4: 3520.8MB/8654.0MB, GPU 5: 3526.6MB/8636.0MB, GPU 6: 3516.8MB/8642.0MB, GPU 7: 3520.0MB/8652.0MB, GPU 8: 3524.6MB/8636.0MB, Total: 31695.6MB allocated, 77772.0MB reserved, Mean: 3521.7MB allocated, 8641.3MB reserved
2025-03-25 04:58:12,655 - training - INFO - Epoch: 11/200000, Batch: 48/49, Loss: 4.4360, Throughput: 58.14 samples/sec
2025-03-25 04:58:12,656 - training - INFO - Epoch 11 completed in 45.51s. Average loss: 4.4040
2025-03-25 04:58:12,667 - training - INFO - Starting epoch 12/200000
2025-03-25 04:58:13,542 - training - INFO - GPU Memory: GPU 0: 3559.3MB/8638.0MB, GPU 1: 3543.8MB/8636.0MB, GPU 2: 3545.0MB/8632.0MB, GPU 3: 3544.2MB/8646.0MB, GPU 4: 3547.1MB/8654.0MB, GPU 5: 3546.7MB/8636.0MB, GPU 6: 3544.5MB/8642.0MB, GPU 7: 3545.0MB/8652.0MB, GPU 8: 3546.6MB/8636.0MB, Total: 31922.3MB allocated, 77772.0MB reserved, Mean: 3546.9MB allocated, 8641.3MB reserved
2025-03-25 04:58:13,542 - training - INFO - Epoch: 12/200000, Batch: 0/49, Loss: 4.3718, Throughput: 61.78 samples/sec
2025-03-25 04:58:28,376 - training - INFO - GPU Memory: GPU 0: 3561.4MB/8620.0MB, GPU 1: 3543.2MB/8626.0MB, GPU 2: 3544.0MB/8606.0MB, GPU 3: 3545.5MB/8612.0MB, GPU 4: 3546.1MB/8628.0MB, GPU 5: 3544.1MB/8608.0MB, GPU 6: 3543.5MB/8602.0MB, GPU 7: 3547.0MB/8612.0MB, GPU 8: 3547.1MB/8616.0MB, Total: 31921.7MB allocated, 77530.0MB reserved, Mean: 3546.9MB allocated, 8614.4MB reserved
2025-03-25 04:58:28,376 - training - INFO - Epoch: 12/200000, Batch: 16/49, Loss: 4.4020, Throughput: 58.44 samples/sec
2025-03-25 04:58:43,599 - training - INFO - GPU Memory: GPU 0: 2884.6MB/8620.0MB, GPU 1: 2882.8MB/8626.0MB, GPU 2: 2882.6MB/8606.0MB, GPU 3: 2883.4MB/8612.0MB, GPU 4: 2882.0MB/8628.0MB, GPU 5: 2882.6MB/8608.0MB, GPU 6: 2882.6MB/8602.0MB, GPU 7: 2882.6MB/8612.0MB, GPU 8: 2882.6MB/8616.0MB, Total: 25945.7MB allocated, 77530.0MB reserved, Mean: 2882.9MB allocated, 8614.4MB reserved
2025-03-25 04:58:43,599 - training - INFO - Epoch: 12/200000, Batch: 32/49, Loss: 4.3670, Throughput: 57.61 samples/sec
2025-03-25 04:58:58,043 - training - INFO - GPU Memory: GPU 0: 3532.2MB/8652.0MB, GPU 1: 3518.4MB/8660.0MB, GPU 2: 3528.4MB/8638.0MB, GPU 3: 3521.8MB/8644.0MB, GPU 4: 3519.2MB/8660.0MB, GPU 5: 3519.7MB/8640.0MB, GPU 6: 3519.3MB/8634.0MB, GPU 7: 3517.0MB/8646.0MB, GPU 8: 3521.6MB/8648.0MB, Total: 31697.7MB allocated, 77822.0MB reserved, Mean: 3522.0MB allocated, 8646.9MB reserved
2025-03-25 04:58:58,044 - training - INFO - Epoch: 12/200000, Batch: 48/49, Loss: 4.3714, Throughput: 58.31 samples/sec
2025-03-25 04:58:58,044 - training - INFO - Epoch 12 completed in 45.38s. Average loss: 4.3956
2025-03-25 04:58:58,056 - training - INFO - Starting epoch 13/200000
2025-03-25 04:58:58,914 - training - INFO - GPU Memory: GPU 0: 3560.8MB/8652.0MB, GPU 1: 3542.7MB/8660.0MB, GPU 2: 3543.4MB/8638.0MB, GPU 3: 3542.9MB/8644.0MB, GPU 4: 3546.4MB/8660.0MB, GPU 5: 3543.8MB/8640.0MB, GPU 6: 3543.1MB/8634.0MB, GPU 7: 3546.6MB/8646.0MB, GPU 8: 3547.2MB/8648.0MB, Total: 31916.9MB allocated, 77822.0MB reserved, Mean: 3546.3MB allocated, 8646.9MB reserved
2025-03-25 04:58:58,914 - training - INFO - Epoch: 13/200000, Batch: 0/49, Loss: 4.0631, Throughput: 62.95 samples/sec
2025-03-25 04:59:13,685 - training - INFO - GPU Memory: GPU 0: 3561.6MB/8612.0MB, GPU 1: 3548.8MB/8608.0MB, GPU 2: 3547.1MB/8598.0MB, GPU 3: 3545.0MB/8592.0MB, GPU 4: 3549.3MB/8604.0MB, GPU 5: 3549.3MB/8596.0MB, GPU 6: 3544.6MB/8598.0MB, GPU 7: 3546.6MB/8618.0MB, GPU 8: 3548.1MB/8612.0MB, Total: 31940.5MB allocated, 77438.0MB reserved, Mean: 3548.9MB allocated, 8604.2MB reserved
2025-03-25 04:59:13,686 - training - INFO - Epoch: 13/200000, Batch: 16/49, Loss: 4.3929, Throughput: 58.74 samples/sec
2025-03-25 04:59:28,903 - training - INFO - GPU Memory: GPU 0: 2884.6MB/8612.0MB, GPU 1: 2882.8MB/8608.0MB, GPU 2: 2882.6MB/8598.0MB, GPU 3: 2883.4MB/8592.0MB, GPU 4: 2882.6MB/8604.0MB, GPU 5: 2882.6MB/8596.0MB, GPU 6: 2882.6MB/8598.0MB, GPU 7: 2882.6MB/8618.0MB, GPU 8: 2882.6MB/8612.0MB, Total: 25946.3MB allocated, 77438.0MB reserved, Mean: 2882.9MB allocated, 8604.2MB reserved
2025-03-25 04:59:28,903 - training - INFO - Epoch: 13/200000, Batch: 32/49, Loss: 4.3723, Throughput: 57.77 samples/sec
2025-03-25 04:59:43,396 - training - INFO - GPU Memory: GPU 0: 3529.6MB/8644.0MB, GPU 1: 3517.8MB/8640.0MB, GPU 2: 3522.2MB/8632.0MB, GPU 3: 3518.2MB/8624.0MB, GPU 4: 3519.6MB/8638.0MB, GPU 5: 3522.0MB/8628.0MB, GPU 6: 3515.4MB/8630.0MB, GPU 7: 3519.9MB/8652.0MB, GPU 8: 3518.8MB/8644.0MB, Total: 31683.3MB allocated, 77732.0MB reserved, Mean: 3520.4MB allocated, 8636.9MB reserved
2025-03-25 04:59:43,396 - training - INFO - Epoch: 13/200000, Batch: 48/49, Loss: 4.3568, Throughput: 58.36 samples/sec
2025-03-25 04:59:43,397 - training - INFO - Epoch 13 completed in 45.34s. Average loss: 4.3812
2025-03-25 04:59:43,408 - training - INFO - Starting epoch 14/200000
2025-03-25 04:59:44,298 - training - INFO - GPU Memory: GPU 0: 3561.1MB/8644.0MB, GPU 1: 3544.6MB/8640.0MB, GPU 2: 3546.8MB/8632.0MB, GPU 3: 3544.7MB/8624.0MB, GPU 4: 3550.0MB/8638.0MB, GPU 5: 3549.0MB/8628.0MB, GPU 6: 3544.6MB/8630.0MB, GPU 7: 3546.3MB/8652.0MB, GPU 8: 3548.6MB/8644.0MB, Total: 31935.5MB allocated, 77732.0MB reserved, Mean: 3548.4MB allocated, 8636.9MB reserved
2025-03-25 04:59:44,298 - training - INFO - Epoch: 14/200000, Batch: 0/49, Loss: 4.1652, Throughput: 60.66 samples/sec
2025-03-25 04:59:59,071 - training - INFO - GPU Memory: GPU 0: 3565.3MB/8610.0MB, GPU 1: 3547.3MB/8614.0MB, GPU 2: 3547.2MB/8616.0MB, GPU 3: 3547.7MB/8602.0MB, GPU 4: 3548.3MB/8624.0MB, GPU 5: 3549.6MB/8608.0MB, GPU 6: 3543.9MB/8602.0MB, GPU 7: 3546.1MB/8606.0MB, GPU 8: 3544.7MB/8616.0MB, Total: 31940.0MB allocated, 77498.0MB reserved, Mean: 3548.9MB allocated, 8610.9MB reserved
2025-03-25 04:59:59,071 - training - INFO - Epoch: 14/200000, Batch: 16/49, Loss: 4.3281, Throughput: 58.61 samples/sec
2025-03-25 05:00:14,174 - training - INFO - GPU Memory: GPU 0: 2884.6MB/8610.0MB, GPU 1: 2882.6MB/8614.0MB, GPU 2: 2882.6MB/8616.0MB, GPU 3: 2883.4MB/8602.0MB, GPU 4: 2882.0MB/8624.0MB, GPU 5: 2882.6MB/8608.0MB, GPU 6: 2882.6MB/8602.0MB, GPU 7: 2882.6MB/8606.0MB, GPU 8: 2882.6MB/8616.0MB, Total: 25945.5MB allocated, 77498.0MB reserved, Mean: 2882.8MB allocated, 8610.9MB reserved
2025-03-25 05:00:14,175 - training - INFO - Epoch: 14/200000, Batch: 32/49, Loss: 4.3056, Throughput: 57.92 samples/sec
2025-03-25 05:00:28,674 - training - INFO - GPU Memory: GPU 0: 3533.8MB/8644.0MB, GPU 1: 3524.8MB/8646.0MB, GPU 2: 3516.9MB/8648.0MB, GPU 3: 3523.9MB/8634.0MB, GPU 4: 3516.4MB/8658.0MB, GPU 5: 3513.1MB/8640.0MB, GPU 6: 3520.5MB/8634.0MB, GPU 7: 3520.7MB/8640.0MB, GPU 8: 3520.3MB/8650.0MB, Total: 31690.2MB allocated, 77794.0MB reserved, Mean: 3521.1MB allocated, 8643.8MB reserved
2025-03-25 05:00:28,675 - training - INFO - Epoch: 14/200000, Batch: 48/49, Loss: 4.3411, Throughput: 58.45 samples/sec
2025-03-25 05:00:28,675 - training - INFO - Epoch 14 completed in 45.27s. Average loss: 4.3600
2025-03-25 05:00:28,679 - training - INFO - Starting epoch 15/200000
2025-03-25 05:00:29,542 - training - INFO - GPU Memory: GPU 0: 3564.7MB/8644.0MB, GPU 1: 3546.9MB/8646.0MB, GPU 2: 3546.8MB/8648.0MB, GPU 3: 3546.3MB/8634.0MB, GPU 4: 3543.6MB/8658.0MB, GPU 5: 3548.8MB/8640.0MB, GPU 6: 3543.6MB/8634.0MB, GPU 7: 3544.6MB/8640.0MB, GPU 8: 3543.8MB/8650.0MB, Total: 31929.2MB allocated, 77794.0MB reserved, Mean: 3547.7MB allocated, 8643.8MB reserved
2025-03-25 05:00:29,542 - training - INFO - Epoch: 15/200000, Batch: 0/49, Loss: 4.3157, Throughput: 62.59 samples/sec
2025-03-25 05:02:21,689 - training - INFO - Overriding batch size with command-line value: 7
2025-03-25 05:02:21,689 - training - INFO - Overriding gradient accumulation steps with command-line value: 3
2025-03-25 05:02:21,689 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_6j59ucai.log
2025-03-25 05:02:21,690 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 9
2025-03-25 05:02:21,690 - training - INFO - Device: cuda:0
2025-03-25 05:02:22,176 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 05:02:22,177 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 05:02:22,177 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 05:02:22,178 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 05:02:22,178 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 05:02:27,515 - training - INFO - Starting model initialization...
2025-03-25 05:02:37,961 - training - INFO - Per-GPU batch size: 21 (global batch size: 189, with gradient_accumulation_steps=3)
2025-03-25 05:02:37,969 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-25 05:02:37,972 - training - INFO - Starting epoch 1/200000
2025-03-25 05:02:43,301 - training - INFO - Memory: GPU 0: 2261.3MB allocated, 6108.0MB reserved
2025-03-25 05:02:43,302 - training - INFO - Epoch: 1/200000, Batch: 0/42, Loss: 10.6011, Throughput: 11.83 samples/sec
2025-03-25 05:02:56,756 - training - INFO - Memory: GPU 0: 2907.5MB allocated, 9570.0MB reserved
2025-03-25 05:02:56,757 - training - INFO - Epoch: 1/200000, Batch: 14/42, Loss: 7.9652, Throughput: 50.31 samples/sec
2025-03-25 05:03:09,474 - training - INFO - Memory: GPU 0: 3567.9MB allocated, 9570.0MB reserved
2025-03-25 05:03:09,475 - training - INFO - Epoch: 1/200000, Batch: 28/42, Loss: 6.9827, Throughput: 58.00 samples/sec
2025-03-25 05:03:23,582 - training - INFO - Epoch 1 completed in 45.61s. Average loss: 6.4834
2025-03-25 05:03:23,586 - training - INFO - Starting epoch 2/200000
2025-03-25 05:03:24,554 - training - INFO - Memory: GPU 0: 3574.8MB allocated, 9124.0MB reserved
2025-03-25 05:03:24,554 - training - INFO - Epoch: 2/200000, Batch: 0/42, Loss: 5.0364, Throughput: 65.16 samples/sec
2025-03-25 05:03:38,007 - training - INFO - Memory: GPU 0: 2907.5MB allocated, 9588.0MB reserved
2025-03-25 05:03:38,007 - training - INFO - Epoch: 2/200000, Batch: 14/42, Loss: 5.2404, Throughput: 65.53 samples/sec
2025-03-25 05:03:50,630 - training - INFO - Memory: GPU 0: 3577.7MB allocated, 9588.0MB reserved
2025-03-25 05:03:50,630 - training - INFO - Epoch: 2/200000, Batch: 28/42, Loss: 5.1996, Throughput: 67.56 samples/sec
2025-03-25 05:04:03,016 - training - INFO - Epoch 2 completed in 39.43s. Average loss: 5.1232
2025-03-25 05:04:03,020 - training - INFO - Starting epoch 3/200000
2025-03-25 05:04:03,945 - training - INFO - Memory: GPU 0: 3579.3MB allocated, 9592.0MB reserved
2025-03-25 05:04:03,945 - training - INFO - Epoch: 3/200000, Batch: 0/42, Loss: 5.0821, Throughput: 68.25 samples/sec
2025-03-25 05:04:17,451 - training - INFO - Memory: GPU 0: 2907.5MB allocated, 9666.0MB reserved
2025-03-25 05:04:17,451 - training - INFO - Epoch: 3/200000, Batch: 14/42, Loss: 4.9765, Throughput: 65.49 samples/sec
2025-03-25 05:04:30,145 - training - INFO - Memory: GPU 0: 3571.8MB allocated, 9666.0MB reserved
2025-03-25 05:04:30,146 - training - INFO - Epoch: 3/200000, Batch: 28/42, Loss: 4.9452, Throughput: 67.36 samples/sec
2025-03-25 05:04:42,565 - training - INFO - Epoch 3 completed in 39.55s. Average loss: 4.9057
2025-03-25 05:04:42,569 - training - INFO - Starting epoch 4/200000
2025-03-25 05:04:43,491 - training - INFO - Memory: GPU 0: 3576.1MB allocated, 9670.0MB reserved
2025-03-25 05:04:43,492 - training - INFO - Epoch: 4/200000, Batch: 0/42, Loss: 4.7591, Throughput: 68.44 samples/sec
2025-03-25 05:04:56,985 - training - INFO - Memory: GPU 0: 2907.5MB allocated, 9562.0MB reserved
2025-03-25 05:04:56,985 - training - INFO - Epoch: 4/200000, Batch: 14/42, Loss: 4.8675, Throughput: 65.56 samples/sec
2025-03-25 05:05:09,751 - training - INFO - Memory: GPU 0: 3572.4MB allocated, 9562.0MB reserved
2025-03-25 05:05:09,752 - training - INFO - Epoch: 4/200000, Batch: 28/42, Loss: 4.8361, Throughput: 67.22 samples/sec
2025-03-25 05:05:22,252 - training - INFO - Epoch 4 completed in 39.68s. Average loss: 4.7987
2025-03-25 05:05:22,255 - training - INFO - Starting epoch 5/200000
2025-03-25 05:05:23,163 - training - INFO - Memory: GPU 0: 3572.3MB allocated, 9566.0MB reserved
2025-03-25 05:05:23,164 - training - INFO - Epoch: 5/200000, Batch: 0/42, Loss: 4.4219, Throughput: 69.52 samples/sec
2025-03-25 05:05:36,690 - training - INFO - Memory: GPU 0: 2907.5MB allocated, 9572.0MB reserved
2025-03-25 05:05:36,690 - training - INFO - Epoch: 5/200000, Batch: 14/42, Loss: 4.6757, Throughput: 65.47 samples/sec
2025-03-25 05:05:49,294 - training - INFO - Memory: GPU 0: 3573.6MB allocated, 9572.0MB reserved
2025-03-25 05:05:49,295 - training - INFO - Epoch: 5/200000, Batch: 28/42, Loss: 4.7440, Throughput: 67.57 samples/sec
2025-03-25 05:06:01,718 - training - INFO - Epoch 5 completed in 39.46s. Average loss: 4.7313
2025-03-25 05:06:01,722 - training - INFO - Starting epoch 6/200000
2025-03-25 05:06:02,641 - training - INFO - Memory: GPU 0: 3575.1MB allocated, 9576.0MB reserved
2025-03-25 05:06:02,641 - training - INFO - Epoch: 6/200000, Batch: 0/42, Loss: 4.4040, Throughput: 68.61 samples/sec
2025-03-25 05:06:16,121 - training - INFO - Memory: GPU 0: 2907.5MB allocated, 9576.0MB reserved
2025-03-25 05:06:16,122 - training - INFO - Epoch: 6/200000, Batch: 14/42, Loss: 4.6229, Throughput: 65.63 samples/sec
2025-03-25 05:07:14,938 - training - INFO - Overriding batch size with command-line value: 8
2025-03-25 05:07:14,938 - training - INFO - Overriding gradient accumulation steps with command-line value: 4
2025-03-25 05:07:14,938 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_xqgt7tk1.log
2025-03-25 05:07:14,938 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 6
2025-03-25 05:07:14,938 - training - INFO - Device: cuda:0
2025-03-25 05:07:15,536 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 05:07:15,536 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 05:07:15,536 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 05:07:15,538 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 05:07:15,538 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 05:07:19,692 - training - INFO - Starting model initialization...
2025-03-25 05:07:29,781 - training - INFO - Per-GPU batch size: 32 (global batch size: 192, with gradient_accumulation_steps=4)
2025-03-25 05:07:29,790 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-25 05:07:29,795 - training - INFO - Starting epoch 1/200000
2025-03-25 05:07:34,765 - training - INFO - Memory: GPU 0: 2280.4MB allocated, 6544.0MB reserved
2025-03-25 05:07:34,766 - training - INFO - Epoch: 1/200000, Batch: 0/55, Loss: 10.9207, Throughput: 9.66 samples/sec
2025-03-25 05:07:38,836 - training - ERROR - Critical error in training batch 6: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 2; 10.75 GiB total capacity; 9.19 GiB already allocated; 223.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 05:07:38,838 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 176, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 2; 10.75 GiB total capacity; 9.19 GiB already allocated; 223.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 05:07:38,838 - training - ERROR - Fatal error in training loop: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 2; 10.75 GiB total capacity; 9.19 GiB already allocated; 223.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 05:07:38,838 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 224, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 176, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 2; 10.75 GiB total capacity; 9.19 GiB already allocated; 223.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 05:07:38,839 - training - ERROR - Fatal error in main function: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 2; 10.75 GiB total capacity; 9.19 GiB already allocated; 223.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2025-03-25 05:07:38,839 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 663, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 341, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 224, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 176, in train_model
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 2; 10.75 GiB total capacity; 9.19 GiB already allocated; 223.81 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2025-03-25 05:08:14,337 - training - INFO - Overriding batch size with command-line value: 7
2025-03-25 05:08:14,337 - training - INFO - Overriding gradient accumulation steps with command-line value: 4
2025-03-25 05:08:14,337 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_3xpwz_dl.log
2025-03-25 05:08:14,337 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 6
2025-03-25 05:08:14,338 - training - INFO - Device: cuda:0
2025-03-25 05:08:14,846 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 05:08:14,846 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 05:08:14,846 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 05:08:14,846 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 05:08:14,846 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 05:08:15,467 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-25 05:08:15,467 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 554, in main
    dist.barrier()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2776, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-25 05:08:15,496 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-25 05:08:15,496 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 554, in main
    dist.barrier()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2776, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-25 05:08:15,500 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-25 05:08:15,501 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 554, in main
    dist.barrier()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2776, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-25 05:08:15,581 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-25 05:08:15,588 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 554, in main
    dist.barrier()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2776, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-25 05:08:49,796 - training - INFO - Overriding batch size with command-line value: 7
2025-03-25 05:08:49,796 - training - INFO - Overriding gradient accumulation steps with command-line value: 4
2025-03-25 05:08:49,796 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_0fqi68k2.log
2025-03-25 05:08:49,796 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 9
2025-03-25 05:08:49,797 - training - INFO - Device: cuda:0
2025-03-25 05:08:50,336 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 05:08:50,336 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 05:08:50,336 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 05:08:50,336 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 05:08:50,336 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 05:08:51,663 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-25 05:08:51,663 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 554, in main
    dist.barrier()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2776, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-25 05:08:51,665 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-25 05:08:51,665 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 554, in main
    dist.barrier()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2776, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-25 05:08:51,686 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-25 05:08:51,686 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 554, in main
    dist.barrier()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2776, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-25 05:08:51,700 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-25 05:08:51,701 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 554, in main
    dist.barrier()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2776, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-25 05:08:51,717 - training - ERROR - Fatal error in main function: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-03-25 05:08:51,718 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 554, in main
    dist.barrier()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2776, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

2025-03-25 05:12:26,529 - training - INFO - Overriding batch size with command-line value: 7
2025-03-25 05:12:26,529 - training - INFO - Overriding gradient accumulation steps with command-line value: 4
2025-03-25 05:12:26,529 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_v_6e4fhd.log
2025-03-25 05:12:26,529 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 05:12:26,529 - training - INFO - Device: cuda:0
2025-03-25 05:12:26,711 - training - INFO - Overriding batch size with command-line value: 7
2025-03-25 05:12:26,711 - training - INFO - Overriding gradient accumulation steps with command-line value: 4
2025-03-25 05:12:26,711 - training - INFO - Overriding batch size with command-line value: 7
2025-03-25 05:12:26,711 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_d16rnnu8.log
2025-03-25 05:12:26,711 - training - INFO - Overriding batch size with command-line value: 7
2025-03-25 05:12:26,711 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 05:12:26,711 - training - INFO - Overriding batch size with command-line value: 7
2025-03-25 05:12:26,711 - training - INFO - Overriding gradient accumulation steps with command-line value: 4
2025-03-25 05:12:26,711 - training - INFO - Overriding batch size with command-line value: 7
2025-03-25 05:12:26,711 - training - INFO - Device: cuda:0
2025-03-25 05:12:26,711 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_tvu6edzc.log
2025-03-25 05:12:26,711 - training - INFO - Overriding gradient accumulation steps with command-line value: 4
2025-03-25 05:12:26,711 - training - INFO - Overriding gradient accumulation steps with command-line value: 4
2025-03-25 05:12:26,711 - training - INFO - Overriding gradient accumulation steps with command-line value: 4
2025-03-25 05:12:26,711 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 05:12:26,711 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_jhcol32a.log
2025-03-25 05:12:26,711 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_xhlqb0f6.log
2025-03-25 05:12:26,711 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_lhv5ifdo.log
2025-03-25 05:12:26,712 - training - INFO - Device: cuda:0
2025-03-25 05:12:26,712 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 05:12:26,712 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 05:12:26,712 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 1
2025-03-25 05:12:26,712 - training - INFO - Device: cuda:0
2025-03-25 05:12:26,712 - training - INFO - Device: cuda:0
2025-03-25 05:12:26,712 - training - INFO - Device: cuda:0
2025-03-25 05:12:27,861 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 05:12:27,861 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 05:12:27,861 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 05:12:27,863 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 05:12:27,863 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 05:12:27,864 - training - INFO - Starting model initialization...
2025-03-25 05:12:27,871 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 05:12:27,872 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 05:12:27,872 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 05:12:27,872 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 05:12:27,872 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 05:12:27,872 - training - INFO - Starting model initialization...
2025-03-25 05:12:27,875 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 05:12:27,875 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 05:12:27,875 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 05:12:27,875 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 05:12:27,875 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 05:12:27,875 - training - INFO - Starting model initialization...
2025-03-25 05:12:27,901 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 05:12:27,901 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 05:12:27,901 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 05:12:27,901 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 05:12:27,901 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 05:12:27,901 - training - INFO - Starting model initialization...
2025-03-25 05:12:27,905 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 05:12:27,905 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 05:12:27,905 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 05:12:27,905 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 05:12:27,905 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 05:12:27,905 - training - INFO - Starting model initialization...
2025-03-25 05:12:27,914 - training - INFO - Training on 1 GPUs, distributed mode: False
2025-03-25 05:12:27,915 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 05:12:27,915 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 05:12:27,915 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 05:12:27,915 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 05:12:27,915 - training - INFO - Starting model initialization...
2025-03-25 05:13:05,899 - training - INFO - Overriding batch size with command-line value: 7
2025-03-25 05:13:05,899 - training - INFO - Overriding gradient accumulation steps with command-line value: 4
2025-03-25 05:13:05,899 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_eo_1s7va.log
2025-03-25 05:13:05,899 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 6
2025-03-25 05:13:05,900 - training - INFO - Device: cuda:0
2025-03-25 05:13:08,779 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 05:13:08,779 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 05:13:08,779 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 05:13:08,779 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 05:13:08,779 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 05:13:12,169 - training - INFO - Starting model initialization...
2025-03-25 05:13:37,623 - training - INFO - Per-GPU batch size: 28 (global batch size: 168, with gradient_accumulation_steps=4)
2025-03-25 05:13:37,632 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-25 05:13:37,635 - training - INFO - Starting epoch 1/200000
2025-03-25 05:13:43,033 - training - INFO - Memory: GPU 0: 2261.3MB allocated, 6108.0MB reserved
2025-03-25 05:13:43,033 - training - INFO - Epoch: 1/200000, Batch: 0/62, Loss: 10.8441, Throughput: 7.79 samples/sec
2025-03-25 05:13:57,480 - training - INFO - Memory: GPU 0: 3569.8MB allocated, 9584.0MB reserved
2025-03-25 05:13:57,480 - training - INFO - Epoch: 1/200000, Batch: 20/62, Loss: 7.7252, Throughput: 44.45 samples/sec
2025-03-25 05:14:11,808 - training - INFO - Memory: GPU 0: 3569.8MB allocated, 9584.0MB reserved
2025-03-25 05:14:11,809 - training - INFO - Epoch: 1/200000, Batch: 40/62, Loss: 6.7811, Throughput: 50.39 samples/sec
2025-03-25 05:14:26,295 - training - INFO - Memory: GPU 0: 3574.8MB allocated, 9598.0MB reserved
2025-03-25 05:14:26,295 - training - INFO - Epoch: 1/200000, Batch: 60/62, Loss: 6.3356, Throughput: 52.65 samples/sec
2025-03-25 05:14:26,889 - training - INFO - Epoch 1 completed in 49.25s. Average loss: 6.2854
2025-03-25 05:14:26,895 - training - INFO - Starting epoch 2/200000
2025-03-25 05:14:27,648 - training - INFO - Memory: GPU 0: 3574.6MB allocated, 9598.0MB reserved
2025-03-25 05:14:27,648 - training - INFO - Epoch: 2/200000, Batch: 0/62, Loss: 4.9657, Throughput: 55.92 samples/sec
2025-03-25 05:14:42,260 - training - INFO - Memory: GPU 0: 3576.0MB allocated, 9576.0MB reserved
2025-03-25 05:14:42,261 - training - INFO - Epoch: 2/200000, Batch: 20/62, Loss: 5.1705, Throughput: 57.41 samples/sec
2025-03-25 05:14:56,703 - training - INFO - Memory: GPU 0: 3576.0MB allocated, 9578.0MB reserved
2025-03-25 05:14:56,703 - training - INFO - Epoch: 2/200000, Batch: 40/62, Loss: 5.0941, Throughput: 57.77 samples/sec
2025-03-25 05:15:11,338 - training - INFO - Memory: GPU 0: 3575.7MB allocated, 9678.0MB reserved
2025-03-25 05:15:11,340 - training - INFO - Epoch: 2/200000, Batch: 60/62, Loss: 5.0642, Throughput: 57.65 samples/sec
2025-03-25 05:15:11,933 - training - INFO - Epoch 2 completed in 45.04s. Average loss: 5.0719
2025-03-25 05:15:11,942 - training - INFO - Starting epoch 3/200000
2025-03-25 05:15:12,699 - training - INFO - Memory: GPU 0: 3575.7MB allocated, 9678.0MB reserved
2025-03-25 05:15:12,699 - training - INFO - Epoch: 3/200000, Batch: 0/62, Loss: 5.0828, Throughput: 55.62 samples/sec
2025-03-25 05:15:27,388 - training - INFO - Memory: GPU 0: 3573.0MB allocated, 9568.0MB reserved
2025-03-25 05:15:27,388 - training - INFO - Epoch: 3/200000, Batch: 20/62, Loss: 4.9546, Throughput: 57.11 samples/sec
2025-03-25 05:15:41,909 - training - INFO - Memory: GPU 0: 3573.0MB allocated, 9568.0MB reserved
2025-03-25 05:15:41,909 - training - INFO - Epoch: 3/200000, Batch: 40/62, Loss: 4.9210, Throughput: 57.47 samples/sec
2025-03-25 05:15:56,509 - training - INFO - Memory: GPU 0: 3575.6MB allocated, 9572.0MB reserved
2025-03-25 05:15:56,509 - training - INFO - Epoch: 3/200000, Batch: 60/62, Loss: 4.8847, Throughput: 57.49 samples/sec
2025-03-25 05:15:57,105 - training - INFO - Epoch 3 completed in 45.16s. Average loss: 4.8730
2025-03-25 05:15:57,112 - training - INFO - Starting epoch 4/200000
2025-03-25 05:15:57,849 - training - INFO - Memory: GPU 0: 3575.6MB allocated, 9572.0MB reserved
2025-03-25 05:15:57,849 - training - INFO - Epoch: 4/200000, Batch: 0/62, Loss: 4.8301, Throughput: 57.06 samples/sec
2025-03-25 05:16:12,430 - training - INFO - Memory: GPU 0: 3573.5MB allocated, 9590.0MB reserved
2025-03-25 05:16:12,431 - training - INFO - Epoch: 4/200000, Batch: 20/62, Loss: 4.8469, Throughput: 57.58 samples/sec
2025-03-25 05:16:57,638 - training - INFO - Overriding batch size with command-line value: 7
2025-03-25 05:16:57,638 - training - INFO - Overriding gradient accumulation steps with command-line value: 3
2025-03-25 05:16:57,638 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_rl_srv36.log
2025-03-25 05:16:57,639 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 8
2025-03-25 05:16:57,639 - training - INFO - Device: cuda:0
2025-03-25 05:16:58,123 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 05:16:58,123 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 05:16:58,123 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 05:16:58,125 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 05:16:58,125 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 05:17:02,789 - training - INFO - Starting model initialization...
2025-03-25 05:17:12,540 - training - INFO - Per-GPU batch size: 21 (global batch size: 168, with gradient_accumulation_steps=3)
2025-03-25 05:17:12,549 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-25 05:17:12,552 - training - INFO - Starting epoch 1/200000
2025-03-25 05:17:19,032 - training - INFO - Memory: GPU 0: 2261.3MB allocated, 6108.0MB reserved
2025-03-25 05:17:19,032 - training - INFO - Epoch: 1/200000, Batch: 0/47, Loss: 10.7142, Throughput: 8.64 samples/sec
2025-03-25 05:17:30,858 - training - INFO - Memory: GPU 0: 3567.6MB allocated, 9588.0MB reserved
2025-03-25 05:17:30,858 - training - INFO - Epoch: 1/200000, Batch: 15/47, Loss: 7.7046, Throughput: 48.95 samples/sec
2025-03-25 05:17:42,621 - training - INFO - Memory: GPU 0: 3567.6MB allocated, 9588.0MB reserved
2025-03-25 05:17:42,623 - training - INFO - Epoch: 1/200000, Batch: 30/47, Loss: 6.7739, Throughput: 57.74 samples/sec
2025-03-25 05:17:54,232 - training - INFO - Memory: GPU 0: 3567.6MB allocated, 9588.0MB reserved
2025-03-25 05:17:54,232 - training - INFO - Epoch: 1/200000, Batch: 45/47, Loss: 6.3036, Throughput: 61.81 samples/sec
2025-03-25 05:17:56,471 - training - INFO - Epoch 1 completed in 43.92s. Average loss: 6.2938
2025-03-25 05:17:56,478 - training - INFO - Starting epoch 2/200000
2025-03-25 05:17:57,296 - training - INFO - Memory: GPU 0: 3572.7MB allocated, 9100.0MB reserved
2025-03-25 05:17:57,296 - training - INFO - Epoch: 2/200000, Batch: 0/47, Loss: 5.0414, Throughput: 68.59 samples/sec
2025-03-25 05:18:09,211 - training - INFO - Memory: GPU 0: 3573.9MB allocated, 9686.0MB reserved
2025-03-25 05:18:09,212 - training - INFO - Epoch: 2/200000, Batch: 15/47, Loss: 5.1743, Throughput: 70.37 samples/sec
2025-03-25 05:18:21,005 - training - INFO - Memory: GPU 0: 3573.9MB allocated, 9686.0MB reserved
2025-03-25 05:18:21,005 - training - INFO - Epoch: 2/200000, Batch: 30/47, Loss: 5.1055, Throughput: 70.78 samples/sec
2025-03-25 05:18:32,625 - training - INFO - Memory: GPU 0: 3573.9MB allocated, 9686.0MB reserved
2025-03-25 05:18:32,625 - training - INFO - Epoch: 2/200000, Batch: 45/47, Loss: 5.0533, Throughput: 71.27 samples/sec
2025-03-25 05:18:33,206 - training - INFO - Epoch 2 completed in 36.73s. Average loss: 5.0717
2025-03-25 05:18:33,219 - training - INFO - Starting epoch 3/200000
2025-03-25 05:18:33,988 - training - INFO - Memory: GPU 0: 3573.5MB allocated, 9816.0MB reserved
2025-03-25 05:18:33,989 - training - INFO - Epoch: 3/200000, Batch: 0/47, Loss: 4.8531, Throughput: 73.03 samples/sec
2025-03-25 05:18:45,901 - training - INFO - Memory: GPU 0: 3578.6MB allocated, 9596.0MB reserved
2025-03-25 05:18:45,902 - training - INFO - Epoch: 3/200000, Batch: 15/47, Loss: 4.8885, Throughput: 70.66 samples/sec
2025-03-25 05:18:57,567 - training - INFO - Memory: GPU 0: 3578.6MB allocated, 9596.0MB reserved
2025-03-25 05:18:57,570 - training - INFO - Epoch: 3/200000, Batch: 30/47, Loss: 4.8756, Throughput: 71.31 samples/sec
2025-03-25 05:19:09,165 - training - INFO - Memory: GPU 0: 3578.6MB allocated, 9596.0MB reserved
2025-03-25 05:19:09,166 - training - INFO - Epoch: 3/200000, Batch: 45/47, Loss: 4.8763, Throughput: 71.67 samples/sec
2025-03-25 05:19:09,769 - training - INFO - Epoch 3 completed in 36.55s. Average loss: 4.8771
2025-03-25 05:19:09,778 - training - INFO - Starting epoch 4/200000
2025-03-25 05:19:10,616 - training - INFO - Memory: GPU 0: 3579.1MB allocated, 8852.0MB reserved
2025-03-25 05:19:10,616 - training - INFO - Epoch: 4/200000, Batch: 0/47, Loss: 4.8167, Throughput: 66.96 samples/sec
2025-03-25 05:19:22,555 - training - INFO - Memory: GPU 0: 3579.2MB allocated, 9694.0MB reserved
2025-03-25 05:19:22,555 - training - INFO - Epoch: 4/200000, Batch: 15/47, Loss: 4.8643, Throughput: 70.14 samples/sec
2025-03-25 05:19:34,284 - training - INFO - Memory: GPU 0: 3579.2MB allocated, 9696.0MB reserved
2025-03-25 05:19:34,284 - training - INFO - Epoch: 4/200000, Batch: 30/47, Loss: 4.8085, Throughput: 70.85 samples/sec
2025-03-25 05:19:45,935 - training - INFO - Memory: GPU 0: 3579.2MB allocated, 9696.0MB reserved
2025-03-25 05:19:45,936 - training - INFO - Epoch: 4/200000, Batch: 45/47, Loss: 4.7686, Throughput: 71.25 samples/sec
2025-03-25 05:19:46,546 - training - INFO - Epoch 4 completed in 36.77s. Average loss: 4.7619
2025-03-25 05:19:46,562 - training - INFO - Starting epoch 5/200000
2025-03-25 05:19:47,423 - training - INFO - Memory: GPU 0: 3579.8MB allocated, 8818.0MB reserved
2025-03-25 05:19:47,423 - training - INFO - Epoch: 5/200000, Batch: 0/47, Loss: 4.5250, Throughput: 65.14 samples/sec
2025-03-25 05:19:59,305 - training - INFO - Memory: GPU 0: 3575.3MB allocated, 9692.0MB reserved
2025-03-25 05:19:59,306 - training - INFO - Epoch: 5/200000, Batch: 15/47, Loss: 4.6905, Throughput: 70.32 samples/sec
2025-03-25 05:20:10,957 - training - INFO - Memory: GPU 0: 3575.3MB allocated, 9694.0MB reserved
2025-03-25 05:20:10,957 - training - INFO - Epoch: 5/200000, Batch: 30/47, Loss: 4.6821, Throughput: 71.16 samples/sec
2025-03-25 05:20:22,643 - training - INFO - Memory: GPU 0: 3575.3MB allocated, 9694.0MB reserved
2025-03-25 05:20:22,644 - training - INFO - Epoch: 5/200000, Batch: 45/47, Loss: 4.6766, Throughput: 71.40 samples/sec
2025-03-25 05:20:23,265 - training - INFO - Epoch 5 completed in 36.70s. Average loss: 4.6570
2025-03-25 05:20:23,281 - training - INFO - Starting epoch 6/200000
2025-03-25 05:20:50,138 - training - INFO - Overriding batch size with command-line value: 7
2025-03-25 05:20:50,138 - training - INFO - Overriding gradient accumulation steps with command-line value: 3
2025-03-25 05:20:50,139 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_1163gjrt.log
2025-03-25 05:20:50,139 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 9
2025-03-25 05:20:50,139 - training - INFO - Device: cuda:0
2025-03-25 05:20:50,638 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 05:20:50,639 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 05:20:50,639 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 05:20:50,641 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 05:20:50,641 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 05:20:55,878 - training - INFO - Starting model initialization...
2025-03-25 05:21:08,670 - training - INFO - Per-GPU batch size: 21 (global batch size: 189, with gradient_accumulation_steps=3)
2025-03-25 05:21:08,678 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-25 05:21:08,681 - training - INFO - Starting epoch 1/200000
2025-03-25 05:21:15,229 - training - INFO - Memory: GPU 0: 2261.3MB allocated, 6108.0MB reserved
2025-03-25 05:21:15,230 - training - INFO - Epoch: 1/200000, Batch: 0/42, Loss: 11.1989, Throughput: 9.63 samples/sec
2025-03-25 05:21:28,685 - training - INFO - Memory: GPU 0: 2908.3MB allocated, 9572.0MB reserved
2025-03-25 05:21:28,685 - training - INFO - Epoch: 1/200000, Batch: 14/42, Loss: 8.0126, Throughput: 47.24 samples/sec
2025-03-25 05:21:41,329 - training - INFO - Memory: GPU 0: 3573.6MB allocated, 9572.0MB reserved
2025-03-25 05:21:41,330 - training - INFO - Epoch: 1/200000, Batch: 28/42, Loss: 6.9631, Throughput: 55.97 samples/sec
2025-03-25 05:21:55,352 - training - INFO - Epoch 1 completed in 46.67s. Average loss: 6.4571
2025-03-25 05:21:55,356 - training - INFO - Starting epoch 2/200000
2025-03-25 05:21:56,346 - training - INFO - Memory: GPU 0: 3575.1MB allocated, 9120.0MB reserved
2025-03-25 05:21:56,347 - training - INFO - Epoch: 2/200000, Batch: 0/42, Loss: 5.0282, Throughput: 63.67 samples/sec
2025-03-25 05:22:09,874 - training - INFO - Memory: GPU 0: 2907.8MB allocated, 9586.0MB reserved
2025-03-25 05:22:09,874 - training - INFO - Epoch: 2/200000, Batch: 14/42, Loss: 5.2202, Throughput: 65.10 samples/sec
2025-03-25 05:22:22,509 - training - INFO - Memory: GPU 0: 3577.7MB allocated, 9586.0MB reserved
2025-03-25 05:22:22,510 - training - INFO - Epoch: 2/200000, Batch: 28/42, Loss: 5.1856, Throughput: 67.29 samples/sec
2025-03-25 05:22:34,894 - training - INFO - Epoch 2 completed in 39.54s. Average loss: 5.1208
2025-03-25 05:22:34,898 - training - INFO - Starting epoch 3/200000
2025-03-25 05:22:35,812 - training - INFO - Memory: GPU 0: 3577.7MB allocated, 9590.0MB reserved
2025-03-25 05:22:35,812 - training - INFO - Epoch: 3/200000, Batch: 0/42, Loss: 5.0876, Throughput: 69.08 samples/sec
2025-03-25 05:22:49,414 - training - INFO - Memory: GPU 0: 2908.3MB allocated, 9680.0MB reserved
2025-03-25 05:22:49,414 - training - INFO - Epoch: 3/200000, Batch: 14/42, Loss: 4.9632, Throughput: 65.10 samples/sec
2025-03-25 05:23:02,163 - training - INFO - Memory: GPU 0: 3578.7MB allocated, 9680.0MB reserved
2025-03-25 05:23:02,164 - training - INFO - Epoch: 3/200000, Batch: 28/42, Loss: 4.9389, Throughput: 67.01 samples/sec
2025-03-25 05:23:14,578 - training - INFO - Epoch 3 completed in 39.68s. Average loss: 4.9061
2025-03-25 05:23:14,581 - training - INFO - Starting epoch 4/200000
2025-03-25 05:23:15,476 - training - INFO - Memory: GPU 0: 3577.3MB allocated, 9684.0MB reserved
2025-03-25 05:23:15,476 - training - INFO - Epoch: 4/200000, Batch: 0/42, Loss: 4.7546, Throughput: 70.50 samples/sec
2025-03-25 05:23:28,998 - training - INFO - Memory: GPU 0: 2908.3MB allocated, 9570.0MB reserved
2025-03-25 05:23:28,998 - training - INFO - Epoch: 4/200000, Batch: 14/42, Loss: 4.8714, Throughput: 65.55 samples/sec
2025-03-25 05:23:41,585 - training - INFO - Memory: GPU 0: 3576.6MB allocated, 9570.0MB reserved
2025-03-25 05:23:41,586 - training - INFO - Epoch: 4/200000, Batch: 28/42, Loss: 4.8365, Throughput: 67.66 samples/sec
2025-03-25 05:23:54,012 - training - INFO - Epoch 4 completed in 39.43s. Average loss: 4.7945
2025-03-25 05:23:54,016 - training - INFO - Starting epoch 5/200000
2025-03-25 05:23:54,905 - training - INFO - Memory: GPU 0: 3576.1MB allocated, 9574.0MB reserved
2025-03-25 05:23:54,905 - training - INFO - Epoch: 5/200000, Batch: 0/42, Loss: 4.3668, Throughput: 71.01 samples/sec
2025-03-25 05:24:08,397 - training - INFO - Memory: GPU 0: 2908.3MB allocated, 9588.0MB reserved
2025-03-25 05:24:08,397 - training - INFO - Epoch: 5/200000, Batch: 14/42, Loss: 4.6664, Throughput: 65.72 samples/sec
2025-03-25 05:24:21,051 - training - INFO - Memory: GPU 0: 3574.3MB allocated, 9588.0MB reserved
2025-03-25 05:24:21,051 - training - INFO - Epoch: 5/200000, Batch: 28/42, Loss: 4.7319, Throughput: 67.58 samples/sec
2025-03-25 05:24:33,463 - training - INFO - Epoch 5 completed in 39.45s. Average loss: 4.7142
2025-03-25 05:24:33,467 - training - INFO - Starting epoch 6/200000
2025-03-25 05:24:34,374 - training - INFO - Memory: GPU 0: 3573.8MB allocated, 9592.0MB reserved
2025-03-25 05:24:34,375 - training - INFO - Epoch: 6/200000, Batch: 0/42, Loss: 4.3649, Throughput: 69.53 samples/sec
2025-03-25 05:37:00,917 - training - INFO - Overriding batch size with command-line value: 7
2025-03-25 05:37:00,918 - training - INFO - Overriding gradient accumulation steps with command-line value: 3
2025-03-25 05:37:00,918 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_yfq2kiur.log
2025-03-25 05:37:00,918 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 8
2025-03-25 05:37:00,918 - training - INFO - Device: cuda:0
2025-03-25 05:37:01,453 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 05:37:01,453 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 05:37:01,453 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 05:37:01,455 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 05:37:01,455 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 05:37:06,156 - training - INFO - Starting model initialization...
2025-03-25 05:37:16,065 - training - INFO - Dataset split: 2466 training, 137 validation, 137 test samples
2025-03-25 05:37:16,140 - training - INFO - Test dataset indices saved to /app/UAV-Language-Guided-Navigation/AnsweringAgent/outputs/logs/test_indices.pt
2025-03-25 05:37:16,141 - training - INFO - Per-GPU batch size: 7 (effective batch size: 168, with gradient_accumulation_steps=3)
2025-03-25 05:37:16,156 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-25 05:37:16,163 - training - INFO - Starting epoch 1/200000
2025-03-25 05:37:22,946 - training - INFO - Memory: GPU 0: 2261.3MB allocated, 6108.0MB reserved
2025-03-25 05:37:22,946 - training - INFO - Epoch: 1/200000, Batch: 0/45, Loss: 11.0821, Throughput: 8.26 samples/sec
2025-03-25 05:37:34,704 - training - INFO - Memory: GPU 0: 3572.6MB allocated, 9582.0MB reserved
2025-03-25 05:37:34,704 - training - INFO - Epoch: 1/200000, Batch: 15/45, Loss: 7.8276, Throughput: 48.33 samples/sec
2025-03-25 05:37:46,301 - training - INFO - Memory: GPU 0: 3572.6MB allocated, 9582.0MB reserved
2025-03-25 05:37:46,303 - training - INFO - Epoch: 1/200000, Batch: 30/45, Loss: 6.8516, Throughput: 57.60 samples/sec
2025-03-25 05:37:58,751 - training - INFO - Epoch 1 completed in 42.59s. Average loss: 6.3815
2025-03-25 05:37:58,756 - training - INFO - Starting epoch 2/200000
2025-03-25 05:37:59,562 - training - INFO - Memory: GPU 0: 3577.2MB allocated, 9198.0MB reserved
2025-03-25 05:37:59,563 - training - INFO - Epoch: 2/200000, Batch: 0/45, Loss: 5.0073, Throughput: 69.51 samples/sec
2025-03-25 05:38:11,461 - training - INFO - Memory: GPU 0: 3575.4MB allocated, 9584.0MB reserved
2025-03-25 05:38:11,462 - training - INFO - Epoch: 2/200000, Batch: 15/45, Loss: 5.1950, Throughput: 70.53 samples/sec
2025-03-25 05:38:23,216 - training - INFO - Memory: GPU 0: 3575.4MB allocated, 9584.0MB reserved
2025-03-25 05:38:23,216 - training - INFO - Epoch: 2/200000, Batch: 30/45, Loss: 5.1491, Throughput: 70.98 samples/sec
2025-03-25 05:38:34,351 - training - INFO - Epoch 2 completed in 35.60s. Average loss: 5.1063
2025-03-25 05:38:34,357 - training - INFO - Starting epoch 3/200000
2025-03-25 05:38:35,146 - training - INFO - Memory: GPU 0: 3572.5MB allocated, 9724.0MB reserved
2025-03-25 05:38:35,234 - training - INFO - Epoch: 3/200000, Batch: 0/45, Loss: 4.7621, Throughput: 71.19 samples/sec
2025-03-25 05:38:47,127 - training - INFO - Memory: GPU 0: 3573.3MB allocated, 9666.0MB reserved
2025-03-25 05:38:47,127 - training - INFO - Epoch: 3/200000, Batch: 15/45, Loss: 4.8867, Throughput: 70.18 samples/sec
2025-03-25 05:38:58,782 - training - INFO - Memory: GPU 0: 3573.3MB allocated, 9666.0MB reserved
2025-03-25 05:38:58,782 - training - INFO - Epoch: 3/200000, Batch: 30/45, Loss: 4.9170, Throughput: 71.08 samples/sec
2025-03-25 05:39:09,788 - training - INFO - Epoch 3 completed in 35.43s. Average loss: 4.8882
2025-03-25 05:39:09,792 - training - INFO - Starting epoch 4/200000
2025-03-25 05:39:10,551 - training - INFO - Memory: GPU 0: 3573.3MB allocated, 9804.0MB reserved
2025-03-25 05:39:10,551 - training - INFO - Epoch: 4/200000, Batch: 0/45, Loss: 4.4447, Throughput: 73.88 samples/sec
2025-03-25 05:39:22,489 - training - INFO - Memory: GPU 0: 3574.0MB allocated, 9678.0MB reserved
2025-03-25 05:39:22,489 - training - INFO - Epoch: 4/200000, Batch: 15/45, Loss: 4.8057, Throughput: 70.57 samples/sec
2025-03-25 05:39:34,105 - training - INFO - Memory: GPU 0: 3574.0MB allocated, 9678.0MB reserved
2025-03-25 05:39:34,105 - training - INFO - Epoch: 4/200000, Batch: 30/45, Loss: 4.8146, Throughput: 71.40 samples/sec
2025-03-25 05:39:45,083 - training - INFO - Epoch 4 completed in 35.29s. Average loss: 4.7878
2025-03-25 05:39:45,087 - training - INFO - Starting epoch 5/200000
2025-03-25 05:39:45,887 - training - INFO - Memory: GPU 0: 3574.5MB allocated, 9816.0MB reserved
2025-03-25 05:39:45,887 - training - INFO - Epoch: 5/200000, Batch: 0/45, Loss: 4.7597, Throughput: 70.18 samples/sec
2025-03-25 05:39:57,870 - training - INFO - Memory: GPU 0: 3572.1MB allocated, 9570.0MB reserved
2025-03-25 05:39:57,870 - training - INFO - Epoch: 5/200000, Batch: 15/45, Loss: 4.7923, Throughput: 70.10 samples/sec
2025-03-25 05:40:09,586 - training - INFO - Memory: GPU 0: 3572.1MB allocated, 9572.0MB reserved
2025-03-25 05:40:09,586 - training - INFO - Epoch: 5/200000, Batch: 30/45, Loss: 4.7498, Throughput: 70.87 samples/sec
2025-03-25 05:40:20,636 - training - INFO - Epoch 5 completed in 35.55s. Average loss: 4.7229
2025-03-25 05:40:20,640 - training - INFO - Starting epoch 6/200000
2025-03-25 05:40:21,405 - training - INFO - Memory: GPU 0: 3572.1MB allocated, 9710.0MB reserved
2025-03-25 05:40:21,405 - training - INFO - Epoch: 6/200000, Batch: 0/45, Loss: 4.6973, Throughput: 73.39 samples/sec
2025-03-25 05:40:33,361 - training - INFO - Memory: GPU 0: 3573.0MB allocated, 9576.0MB reserved
2025-03-25 05:40:33,362 - training - INFO - Epoch: 6/200000, Batch: 15/45, Loss: 4.5993, Throughput: 70.44 samples/sec
2025-03-25 05:40:45,020 - training - INFO - Memory: GPU 0: 3573.0MB allocated, 9576.0MB reserved
2025-03-25 05:40:45,021 - training - INFO - Epoch: 6/200000, Batch: 30/45, Loss: 4.6121, Throughput: 71.21 samples/sec
2025-03-25 05:41:08,631 - training - INFO - Overriding batch size with command-line value: 7
2025-03-25 05:41:08,631 - training - INFO - Overriding gradient accumulation steps with command-line value: 3
2025-03-25 05:41:08,631 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_p9ggy08d.log
2025-03-25 05:41:08,631 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 4
2025-03-25 05:41:08,631 - training - INFO - Device: cuda:0
2025-03-25 05:41:09,182 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 05:41:09,182 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 05:41:09,182 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 05:41:09,184 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 05:41:09,184 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 05:41:13,277 - training - INFO - Starting model initialization...
2025-03-25 05:41:23,000 - training - INFO - Dataset split: 2466 training, 137 validation, 137 test samples
2025-03-25 05:41:23,226 - training - INFO - Test dataset indices saved to /app/UAV-Language-Guided-Navigation/AnsweringAgent/outputs/logs/test_indices.pt
2025-03-25 05:41:23,226 - training - INFO - Per-GPU batch size: 7 (effective batch size: 84, with gradient_accumulation_steps=3)
2025-03-25 05:41:23,243 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-25 05:41:23,249 - training - INFO - Starting epoch 1/200000
2025-03-25 05:41:27,980 - training - INFO - Memory: GPU 0: 2261.3MB allocated, 6108.0MB reserved
2025-03-25 05:41:27,981 - training - INFO - Epoch: 1/200000, Batch: 0/89, Loss: 11.2684, Throughput: 5.92 samples/sec
2025-03-25 05:41:48,679 - training - INFO - Memory: GPU 0: 2908.3MB allocated, 9570.0MB reserved
2025-03-25 05:41:48,679 - training - INFO - Epoch: 1/200000, Batch: 29/89, Loss: 6.9164, Throughput: 33.03 samples/sec
2025-03-25 05:42:00,567 - training - INFO - Overriding batch size with command-line value: 7
2025-03-25 05:42:00,567 - training - INFO - Overriding gradient accumulation steps with command-line value: 3
2025-03-25 05:42:00,567 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_v5m5_q97.log
2025-03-25 05:42:00,568 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 8
2025-03-25 05:42:00,568 - training - INFO - Device: cuda:0
2025-03-25 05:42:01,059 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 05:42:01,060 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 05:42:01,060 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 05:42:01,060 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 05:42:01,060 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 05:42:05,937 - training - INFO - Starting model initialization...
2025-03-25 05:42:16,067 - training - INFO - Dataset split: 2466 training, 137 validation, 137 test samples
2025-03-25 05:42:16,244 - training - INFO - Test dataset indices saved to /app/UAV-Language-Guided-Navigation/AnsweringAgent/outputs/logs/test_indices.pt
2025-03-25 05:42:16,246 - training - INFO - Per-GPU batch size: 7 (effective batch size: 168, with gradient_accumulation_steps=3)
2025-03-25 05:42:16,262 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-25 05:42:16,270 - training - INFO - Starting epoch 1/200000
2025-03-25 05:42:22,749 - training - INFO - Memory: GPU 0: 2261.3MB allocated, 6108.0MB reserved
2025-03-25 05:42:22,750 - training - INFO - Epoch: 1/200000, Batch: 0/45, Loss: 11.0842, Throughput: 8.65 samples/sec
2025-03-25 05:42:34,564 - training - INFO - Memory: GPU 0: 3569.7MB allocated, 9570.0MB reserved
2025-03-25 05:42:34,567 - training - INFO - Epoch: 1/200000, Batch: 15/45, Loss: 7.8273, Throughput: 48.98 samples/sec
2025-03-25 05:42:46,309 - training - INFO - Memory: GPU 0: 3569.7MB allocated, 9570.0MB reserved
2025-03-25 05:42:46,310 - training - INFO - Epoch: 1/200000, Batch: 30/45, Loss: 6.8513, Throughput: 57.79 samples/sec
2025-03-25 05:42:58,804 - training - INFO - Epoch 1 completed in 42.53s. Average loss: 6.3815
2025-03-25 05:42:58,808 - training - INFO - Starting epoch 2/200000
2025-03-25 05:42:59,639 - training - INFO - Memory: GPU 0: 3573.7MB allocated, 9202.0MB reserved
2025-03-25 05:42:59,640 - training - INFO - Epoch: 2/200000, Batch: 0/45, Loss: 5.0077, Throughput: 67.53 samples/sec
2025-03-25 05:43:11,557 - training - INFO - Memory: GPU 0: 3575.6MB allocated, 9588.0MB reserved
2025-03-25 05:43:11,560 - training - INFO - Epoch: 2/200000, Batch: 15/45, Loss: 5.1949, Throughput: 70.29 samples/sec
2025-03-25 05:43:23,322 - training - INFO - Memory: GPU 0: 3575.6MB allocated, 9588.0MB reserved
2025-03-25 05:43:23,322 - training - INFO - Epoch: 2/200000, Batch: 30/45, Loss: 5.1491, Throughput: 70.82 samples/sec
2025-03-25 05:43:34,298 - training - INFO - Epoch 2 completed in 35.49s. Average loss: 5.1063
2025-03-25 05:43:34,301 - training - INFO - Starting epoch 3/200000
2025-03-25 05:43:35,102 - training - INFO - Memory: GPU 0: 3575.6MB allocated, 9726.0MB reserved
2025-03-25 05:43:35,102 - training - INFO - Epoch: 3/200000, Batch: 0/45, Loss: 4.7629, Throughput: 70.12 samples/sec
2025-03-25 05:43:47,122 - training - INFO - Memory: GPU 0: 3576.5MB allocated, 9694.0MB reserved
2025-03-25 05:43:47,122 - training - INFO - Epoch: 3/200000, Batch: 15/45, Loss: 4.8868, Throughput: 69.89 samples/sec
2025-03-25 05:43:58,748 - training - INFO - Memory: GPU 0: 3576.5MB allocated, 9694.0MB reserved
2025-03-25 05:43:58,749 - training - INFO - Epoch: 3/200000, Batch: 30/45, Loss: 4.9170, Throughput: 71.02 samples/sec
2025-03-25 05:44:09,810 - training - INFO - Epoch 3 completed in 35.51s. Average loss: 4.8882
2025-03-25 05:44:09,815 - training - INFO - Starting epoch 4/200000
2025-03-25 05:44:10,613 - training - INFO - Memory: GPU 0: 3576.5MB allocated, 9832.0MB reserved
2025-03-25 05:44:10,614 - training - INFO - Epoch: 4/200000, Batch: 0/45, Loss: 4.4440, Throughput: 70.21 samples/sec
2025-03-25 05:44:22,692 - training - INFO - Memory: GPU 0: 3575.4MB allocated, 9576.0MB reserved
2025-03-25 05:44:22,692 - training - INFO - Epoch: 4/200000, Batch: 15/45, Loss: 4.8056, Throughput: 69.58 samples/sec
2025-03-25 05:44:34,430 - training - INFO - Memory: GPU 0: 3573.4MB allocated, 9576.0MB reserved
2025-03-25 05:44:34,431 - training - INFO - Epoch: 4/200000, Batch: 30/45, Loss: 4.8146, Throughput: 70.53 samples/sec
2025-03-25 05:44:45,459 - training - INFO - Epoch 4 completed in 35.64s. Average loss: 4.7878
2025-03-25 05:44:45,462 - training - INFO - Starting epoch 5/200000
2025-03-25 05:44:46,257 - training - INFO - Memory: GPU 0: 3575.6MB allocated, 9714.0MB reserved
2025-03-25 05:44:46,258 - training - INFO - Epoch: 5/200000, Batch: 0/45, Loss: 4.7602, Throughput: 70.63 samples/sec
2025-03-25 05:44:58,295 - training - INFO - Memory: GPU 0: 3574.7MB allocated, 9584.0MB reserved
2025-03-25 05:44:58,295 - training - INFO - Epoch: 5/200000, Batch: 15/45, Loss: 4.7922, Throughput: 69.84 samples/sec
2025-03-25 05:45:10,018 - training - INFO - Memory: GPU 0: 3574.7MB allocated, 9584.0MB reserved
2025-03-25 05:45:10,019 - training - INFO - Epoch: 5/200000, Batch: 30/45, Loss: 4.7498, Throughput: 70.70 samples/sec
2025-03-25 05:45:21,033 - training - INFO - Epoch 5 completed in 35.57s. Average loss: 4.7226
2025-03-25 05:45:21,037 - training - INFO - Starting epoch 6/200000
2025-03-25 05:45:21,806 - training - INFO - Memory: GPU 0: 3574.7MB allocated, 9722.0MB reserved
2025-03-25 05:45:21,806 - training - INFO - Epoch: 6/200000, Batch: 0/45, Loss: 4.6953, Throughput: 72.96 samples/sec
2025-03-25 05:45:33,893 - training - INFO - Memory: GPU 0: 3574.3MB allocated, 9576.0MB reserved
2025-03-25 05:45:33,893 - training - INFO - Epoch: 6/200000, Batch: 15/45, Loss: 4.5972, Throughput: 69.70 samples/sec
2025-03-25 05:45:45,523 - training - INFO - Memory: GPU 0: 3574.3MB allocated, 9576.0MB reserved
2025-03-25 05:45:45,524 - training - INFO - Epoch: 6/200000, Batch: 30/45, Loss: 4.6139, Throughput: 70.90 samples/sec
2025-03-25 05:45:56,512 - training - INFO - Epoch 6 completed in 35.48s. Average loss: 4.6536
2025-03-25 05:45:56,516 - training - INFO - Starting epoch 7/200000
2025-03-25 05:45:57,285 - training - INFO - Memory: GPU 0: 3574.3MB allocated, 9716.0MB reserved
2025-03-25 05:45:57,285 - training - INFO - Epoch: 7/200000, Batch: 0/45, Loss: 4.4543, Throughput: 72.93 samples/sec
2025-03-25 05:46:09,296 - training - INFO - Memory: GPU 0: 3574.0MB allocated, 9576.0MB reserved
2025-03-25 05:46:09,296 - training - INFO - Epoch: 7/200000, Batch: 15/45, Loss: 4.6452, Throughput: 70.12 samples/sec
2025-03-25 05:46:20,905 - training - INFO - Memory: GPU 0: 3574.0MB allocated, 9576.0MB reserved
2025-03-25 05:46:20,905 - training - INFO - Epoch: 7/200000, Batch: 30/45, Loss: 4.6324, Throughput: 71.18 samples/sec
2025-03-25 05:46:31,885 - training - INFO - Epoch 7 completed in 35.37s. Average loss: 4.6068
2025-03-25 05:46:31,889 - training - INFO - Starting epoch 8/200000
2025-03-25 05:46:32,679 - training - INFO - Memory: GPU 0: 3574.0MB allocated, 9716.0MB reserved
2025-03-25 05:46:32,679 - training - INFO - Epoch: 8/200000, Batch: 0/45, Loss: 4.4670, Throughput: 71.04 samples/sec
2025-03-25 05:46:44,657 - training - INFO - Memory: GPU 0: 3572.0MB allocated, 9566.0MB reserved
2025-03-25 05:46:44,658 - training - INFO - Epoch: 8/200000, Batch: 15/45, Loss: 4.5728, Throughput: 70.18 samples/sec
2025-03-25 05:46:56,279 - training - INFO - Memory: GPU 0: 3572.0MB allocated, 9566.0MB reserved
2025-03-25 05:46:56,280 - training - INFO - Epoch: 8/200000, Batch: 30/45, Loss: 4.6069, Throughput: 71.18 samples/sec
2025-03-25 05:47:07,220 - training - INFO - Epoch 8 completed in 35.33s. Average loss: 4.5685
2025-03-25 05:47:07,224 - training - INFO - Starting epoch 9/200000
2025-03-25 05:47:08,018 - training - INFO - Memory: GPU 0: 3572.0MB allocated, 9704.0MB reserved
2025-03-25 05:47:08,019 - training - INFO - Epoch: 9/200000, Batch: 0/45, Loss: 4.5358, Throughput: 70.66 samples/sec
2025-03-25 05:47:19,942 - training - INFO - Memory: GPU 0: 3571.0MB allocated, 9576.0MB reserved
2025-03-25 05:47:19,943 - training - INFO - Epoch: 9/200000, Batch: 15/45, Loss: 4.5244, Throughput: 70.45 samples/sec
2025-03-25 05:47:31,626 - training - INFO - Memory: GPU 0: 3571.0MB allocated, 9576.0MB reserved
2025-03-25 05:47:31,627 - training - INFO - Epoch: 9/200000, Batch: 30/45, Loss: 4.5387, Throughput: 71.14 samples/sec
2025-03-25 05:47:42,601 - training - INFO - Epoch 9 completed in 35.38s. Average loss: 4.5348
2025-03-25 05:47:42,605 - training - INFO - Starting epoch 10/200000
2025-03-25 05:47:43,405 - training - INFO - Memory: GPU 0: 3571.0MB allocated, 9714.0MB reserved
2025-03-25 05:47:43,405 - training - INFO - Epoch: 10/200000, Batch: 0/45, Loss: 4.4121, Throughput: 70.15 samples/sec
2025-03-25 05:47:55,365 - training - INFO - Memory: GPU 0: 3570.2MB allocated, 9574.0MB reserved
2025-03-25 05:47:55,366 - training - INFO - Epoch: 10/200000, Batch: 15/45, Loss: 4.4104, Throughput: 70.23 samples/sec
2025-03-25 05:48:07,107 - training - INFO - Memory: GPU 0: 3570.2MB allocated, 9574.0MB reserved
2025-03-25 05:48:07,107 - training - INFO - Epoch: 10/200000, Batch: 30/45, Loss: 4.4641, Throughput: 70.86 samples/sec
2025-03-25 05:48:18,133 - training - INFO - Epoch 10 completed in 35.53s. Average loss: 4.4981
2025-03-25 05:48:18,137 - training - INFO - Starting epoch 11/200000
2025-03-25 05:48:18,911 - training - INFO - Memory: GPU 0: 3570.2MB allocated, 9714.0MB reserved
2025-03-25 05:48:18,911 - training - INFO - Epoch: 11/200000, Batch: 0/45, Loss: 4.7048, Throughput: 72.51 samples/sec
2025-03-25 05:48:30,886 - training - INFO - Memory: GPU 0: 3573.0MB allocated, 9570.0MB reserved
2025-03-25 05:48:30,886 - training - INFO - Epoch: 11/200000, Batch: 15/45, Loss: 4.4973, Throughput: 70.29 samples/sec
2025-03-25 05:48:42,647 - training - INFO - Memory: GPU 0: 3573.0MB allocated, 9570.0MB reserved
2025-03-25 05:48:42,648 - training - INFO - Epoch: 11/200000, Batch: 30/45, Loss: 4.5224, Throughput: 70.83 samples/sec
2025-03-25 05:48:53,687 - training - INFO - Epoch 11 completed in 35.55s. Average loss: 4.4709
2025-03-25 05:48:53,691 - training - INFO - Starting epoch 12/200000
2025-03-25 05:48:54,493 - training - INFO - Memory: GPU 0: 3573.1MB allocated, 9710.0MB reserved
2025-03-25 05:48:54,493 - training - INFO - Epoch: 12/200000, Batch: 0/45, Loss: 4.3637, Throughput: 69.96 samples/sec
2025-03-25 05:49:06,417 - training - INFO - Memory: GPU 0: 3570.3MB allocated, 9578.0MB reserved
2025-03-25 05:49:06,417 - training - INFO - Epoch: 12/200000, Batch: 15/45, Loss: 4.5082, Throughput: 70.42 samples/sec
2025-03-25 05:49:18,089 - training - INFO - Memory: GPU 0: 3570.3MB allocated, 9578.0MB reserved
2025-03-25 05:49:18,089 - training - INFO - Epoch: 12/200000, Batch: 30/45, Loss: 4.4782, Throughput: 71.16 samples/sec
2025-03-25 05:49:29,098 - training - INFO - Epoch 12 completed in 35.41s. Average loss: 4.4563
2025-03-25 05:49:29,103 - training - INFO - Starting epoch 13/200000
2025-03-25 05:49:29,881 - training - INFO - Memory: GPU 0: 3570.3MB allocated, 9716.0MB reserved
2025-03-25 05:49:29,881 - training - INFO - Epoch: 13/200000, Batch: 0/45, Loss: 4.6325, Throughput: 72.21 samples/sec
2025-03-25 05:49:41,886 - training - INFO - Memory: GPU 0: 3571.3MB allocated, 9586.0MB reserved
2025-03-25 05:49:41,887 - training - INFO - Epoch: 13/200000, Batch: 15/45, Loss: 4.4947, Throughput: 70.11 samples/sec
2025-03-25 05:49:53,582 - training - INFO - Memory: GPU 0: 3571.3MB allocated, 9586.0MB reserved
2025-03-25 05:49:53,582 - training - INFO - Epoch: 13/200000, Batch: 30/45, Loss: 4.4501, Throughput: 70.93 samples/sec
2025-03-25 05:50:04,581 - training - INFO - Epoch 13 completed in 35.48s. Average loss: 4.4488
2025-03-25 05:50:04,585 - training - INFO - Starting epoch 14/200000
2025-03-25 05:50:05,374 - training - INFO - Memory: GPU 0: 3571.3MB allocated, 9726.0MB reserved
2025-03-25 05:50:05,374 - training - INFO - Epoch: 14/200000, Batch: 0/45, Loss: 4.4736, Throughput: 71.03 samples/sec
2025-03-25 05:50:17,313 - training - INFO - Memory: GPU 0: 3571.5MB allocated, 9566.0MB reserved
2025-03-25 05:50:17,313 - training - INFO - Epoch: 14/200000, Batch: 15/45, Loss: 4.4184, Throughput: 70.40 samples/sec
2025-03-25 05:50:28,978 - training - INFO - Memory: GPU 0: 3571.5MB allocated, 9568.0MB reserved
2025-03-25 05:50:28,978 - training - INFO - Epoch: 14/200000, Batch: 30/45, Loss: 4.4318, Throughput: 71.17 samples/sec
2025-03-25 05:50:40,019 - training - INFO - Epoch 14 completed in 35.43s. Average loss: 4.4423
2025-03-25 05:50:40,023 - training - INFO - Starting epoch 15/200000
2025-03-25 05:50:40,794 - training - INFO - Memory: GPU 0: 3571.5MB allocated, 9706.0MB reserved
2025-03-25 05:50:40,794 - training - INFO - Epoch: 15/200000, Batch: 0/45, Loss: 4.7873, Throughput: 72.76 samples/sec
2025-03-25 05:50:52,738 - training - INFO - Memory: GPU 0: 3573.0MB allocated, 9574.0MB reserved
2025-03-25 05:50:52,738 - training - INFO - Epoch: 15/200000, Batch: 15/45, Loss: 4.4429, Throughput: 70.48 samples/sec
2025-03-25 05:51:04,391 - training - INFO - Memory: GPU 0: 3573.0MB allocated, 9574.0MB reserved
2025-03-25 05:51:04,392 - training - INFO - Epoch: 15/200000, Batch: 30/45, Loss: 4.4259, Throughput: 71.25 samples/sec
2025-03-25 05:51:15,362 - training - INFO - Epoch 15 completed in 35.34s. Average loss: 4.4337
2025-03-25 05:51:15,365 - training - INFO - Starting epoch 16/200000
2025-03-25 05:51:16,145 - training - INFO - Memory: GPU 0: 3573.0MB allocated, 9714.0MB reserved
2025-03-25 05:51:16,146 - training - INFO - Epoch: 16/200000, Batch: 0/45, Loss: 4.6486, Throughput: 71.98 samples/sec
2025-03-25 05:51:28,106 - training - INFO - Memory: GPU 0: 3573.2MB allocated, 9580.0MB reserved
2025-03-25 05:51:28,106 - training - INFO - Epoch: 16/200000, Batch: 15/45, Loss: 4.4401, Throughput: 70.34 samples/sec
2025-03-25 05:51:39,806 - training - INFO - Memory: GPU 0: 3573.2MB allocated, 9580.0MB reserved
2025-03-25 05:51:39,806 - training - INFO - Epoch: 16/200000, Batch: 30/45, Loss: 4.4088, Throughput: 71.04 samples/sec
2025-03-25 05:51:50,794 - training - INFO - Epoch 16 completed in 35.43s. Average loss: 4.4295
2025-03-25 05:51:50,798 - training - INFO - Starting epoch 17/200000
2025-03-25 05:51:51,574 - training - INFO - Memory: GPU 0: 3573.2MB allocated, 9720.0MB reserved
2025-03-25 05:51:51,574 - training - INFO - Epoch: 17/200000, Batch: 0/45, Loss: 4.4947, Throughput: 72.28 samples/sec
2025-03-25 05:52:03,565 - training - INFO - Memory: GPU 0: 3571.3MB allocated, 9582.0MB reserved
2025-03-25 05:52:03,565 - training - INFO - Epoch: 17/200000, Batch: 15/45, Loss: 4.4019, Throughput: 70.19 samples/sec
2025-03-25 05:52:15,314 - training - INFO - Memory: GPU 0: 3571.3MB allocated, 9584.0MB reserved
2025-03-25 05:52:15,314 - training - INFO - Epoch: 17/200000, Batch: 30/45, Loss: 4.4245, Throughput: 70.81 samples/sec
2025-03-25 05:52:26,363 - training - INFO - Epoch 17 completed in 35.57s. Average loss: 4.4133
2025-03-25 05:52:26,367 - training - INFO - Starting epoch 18/200000
2025-03-25 05:52:27,156 - training - INFO - Memory: GPU 0: 3571.3MB allocated, 9720.0MB reserved
2025-03-25 05:52:27,156 - training - INFO - Epoch: 18/200000, Batch: 0/45, Loss: 4.5176, Throughput: 71.15 samples/sec
2025-03-25 05:52:39,156 - training - INFO - Memory: GPU 0: 3574.2MB allocated, 9580.0MB reserved
2025-03-25 05:52:39,157 - training - INFO - Epoch: 18/200000, Batch: 15/45, Loss: 4.4105, Throughput: 70.07 samples/sec
2025-03-25 05:52:50,854 - training - INFO - Memory: GPU 0: 3574.2MB allocated, 9582.0MB reserved
2025-03-25 05:52:50,854 - training - INFO - Epoch: 18/200000, Batch: 30/45, Loss: 4.4242, Throughput: 70.90 samples/sec
2025-03-25 05:53:01,836 - training - INFO - Epoch 18 completed in 35.47s. Average loss: 4.4013
2025-03-25 05:53:01,840 - training - INFO - Starting epoch 19/200000
2025-03-25 05:53:02,641 - training - INFO - Memory: GPU 0: 3573.7MB allocated, 9720.0MB reserved
2025-03-25 05:53:02,641 - training - INFO - Epoch: 19/200000, Batch: 0/45, Loss: 4.3814, Throughput: 70.00 samples/sec
2025-03-25 05:53:14,598 - training - INFO - Memory: GPU 0: 3572.0MB allocated, 9682.0MB reserved
2025-03-25 05:53:14,599 - training - INFO - Epoch: 19/200000, Batch: 15/45, Loss: 4.4143, Throughput: 70.24 samples/sec
2025-03-25 05:53:26,184 - training - INFO - Memory: GPU 0: 3572.0MB allocated, 9682.0MB reserved
2025-03-25 05:53:26,185 - training - INFO - Epoch: 19/200000, Batch: 30/45, Loss: 4.3841, Throughput: 71.31 samples/sec
2025-03-25 05:53:37,128 - training - INFO - Epoch 19 completed in 35.29s. Average loss: 4.3870
2025-03-25 05:53:37,132 - training - INFO - Starting epoch 20/200000
2025-03-25 05:53:37,896 - training - INFO - Memory: GPU 0: 3572.0MB allocated, 9822.0MB reserved
2025-03-25 05:53:37,896 - training - INFO - Epoch: 20/200000, Batch: 0/45, Loss: 4.8774, Throughput: 73.51 samples/sec
2025-03-25 05:53:49,902 - training - INFO - Memory: GPU 0: 3570.6MB allocated, 9578.0MB reserved
2025-03-25 05:53:49,903 - training - INFO - Epoch: 20/200000, Batch: 15/45, Loss: 4.3810, Throughput: 70.17 samples/sec
2025-03-25 05:54:01,589 - training - INFO - Memory: GPU 0: 3570.6MB allocated, 9578.0MB reserved
2025-03-25 05:54:01,589 - training - INFO - Epoch: 20/200000, Batch: 30/45, Loss: 4.3917, Throughput: 70.98 samples/sec
2025-03-25 05:54:12,607 - training - INFO - Epoch 20 completed in 35.48s. Average loss: 4.3907
2025-03-25 05:54:12,611 - training - INFO - Starting epoch 21/200000
2025-03-25 05:54:13,402 - training - INFO - Memory: GPU 0: 3570.6MB allocated, 9718.0MB reserved
2025-03-25 05:54:13,402 - training - INFO - Epoch: 21/200000, Batch: 0/45, Loss: 4.1722, Throughput: 70.97 samples/sec
2025-03-25 05:54:25,437 - training - INFO - Memory: GPU 0: 3573.8MB allocated, 9680.0MB reserved
2025-03-25 05:54:25,437 - training - INFO - Epoch: 21/200000, Batch: 15/45, Loss: 4.4576, Throughput: 69.87 samples/sec
2025-03-25 05:54:37,090 - training - INFO - Memory: GPU 0: 3573.8MB allocated, 9680.0MB reserved
2025-03-25 05:54:37,091 - training - INFO - Epoch: 21/200000, Batch: 30/45, Loss: 4.4174, Throughput: 70.92 samples/sec
2025-03-25 05:54:48,082 - training - INFO - Epoch 21 completed in 35.47s. Average loss: 4.3701
2025-03-25 05:54:48,086 - training - INFO - Starting epoch 22/200000
2025-03-25 05:54:48,885 - training - INFO - Memory: GPU 0: 3573.8MB allocated, 9820.0MB reserved
2025-03-25 05:54:48,886 - training - INFO - Epoch: 22/200000, Batch: 0/45, Loss: 4.2639, Throughput: 70.14 samples/sec
2025-03-25 05:55:00,802 - training - INFO - Memory: GPU 0: 3571.6MB allocated, 9596.0MB reserved
2025-03-25 05:55:00,802 - training - INFO - Epoch: 22/200000, Batch: 15/45, Loss: 4.3643, Throughput: 70.48 samples/sec
2025-03-25 05:55:12,383 - training - INFO - Memory: GPU 0: 3572.2MB allocated, 9596.0MB reserved
2025-03-25 05:55:12,383 - training - INFO - Epoch: 22/200000, Batch: 30/45, Loss: 4.3796, Throughput: 71.45 samples/sec
2025-03-25 05:55:23,330 - training - INFO - Epoch 22 completed in 35.24s. Average loss: 4.3623
2025-03-25 05:55:23,333 - training - INFO - Starting epoch 23/200000
2025-03-25 05:55:24,109 - training - INFO - Memory: GPU 0: 3571.6MB allocated, 9736.0MB reserved
2025-03-25 05:55:24,109 - training - INFO - Epoch: 23/200000, Batch: 0/45, Loss: 4.4380, Throughput: 72.26 samples/sec
2025-03-25 05:55:36,078 - training - INFO - Memory: GPU 0: 3573.2MB allocated, 9576.0MB reserved
2025-03-25 05:55:36,078 - training - INFO - Epoch: 23/200000, Batch: 15/45, Loss: 4.3585, Throughput: 70.32 samples/sec
2025-03-25 05:55:47,701 - training - INFO - Memory: GPU 0: 3573.2MB allocated, 9576.0MB reserved
2025-03-25 05:55:47,702 - training - INFO - Epoch: 23/200000, Batch: 30/45, Loss: 4.3752, Throughput: 71.25 samples/sec
2025-03-25 05:55:58,652 - training - INFO - Epoch 23 completed in 35.32s. Average loss: 4.3539
2025-03-25 05:55:58,656 - training - INFO - Starting epoch 24/200000
2025-03-25 05:55:59,440 - training - INFO - Memory: GPU 0: 3573.2MB allocated, 9716.0MB reserved
2025-03-25 05:55:59,440 - training - INFO - Epoch: 24/200000, Batch: 0/45, Loss: 4.4791, Throughput: 71.54 samples/sec
2025-03-25 05:56:11,481 - training - INFO - Memory: GPU 0: 3574.2MB allocated, 9588.0MB reserved
2025-03-25 05:56:11,481 - training - INFO - Epoch: 24/200000, Batch: 15/45, Loss: 4.3258, Throughput: 69.87 samples/sec
2025-03-25 05:56:23,214 - training - INFO - Memory: GPU 0: 3573.5MB allocated, 9588.0MB reserved
2025-03-25 05:56:23,214 - training - INFO - Epoch: 24/200000, Batch: 30/45, Loss: 4.3105, Throughput: 70.69 samples/sec
2025-03-25 05:56:34,216 - training - INFO - Epoch 24 completed in 35.56s. Average loss: 4.3471
2025-03-25 05:56:34,220 - training - INFO - Starting epoch 25/200000
2025-03-25 05:56:34,996 - training - INFO - Memory: GPU 0: 3574.2MB allocated, 9726.0MB reserved
2025-03-25 05:56:34,997 - training - INFO - Epoch: 25/200000, Batch: 0/45, Loss: 4.2292, Throughput: 72.30 samples/sec
2025-03-25 05:56:47,004 - training - INFO - Memory: GPU 0: 3569.5MB allocated, 9586.0MB reserved
2025-03-25 05:56:47,004 - training - INFO - Epoch: 25/200000, Batch: 15/45, Loss: 4.3605, Throughput: 70.10 samples/sec
2025-03-25 05:56:58,706 - training - INFO - Memory: GPU 0: 3569.5MB allocated, 9586.0MB reserved
2025-03-25 05:56:58,706 - training - INFO - Epoch: 25/200000, Batch: 30/45, Loss: 4.2995, Throughput: 70.90 samples/sec
2025-03-25 05:57:09,708 - training - INFO - Epoch 25 completed in 35.49s. Average loss: 4.3367
2025-03-25 05:57:09,712 - training - INFO - Starting epoch 26/200000
2025-03-25 05:57:10,486 - training - INFO - Memory: GPU 0: 3569.5MB allocated, 9726.0MB reserved
2025-03-25 05:57:10,486 - training - INFO - Epoch: 26/200000, Batch: 0/45, Loss: 4.8408, Throughput: 72.48 samples/sec
2025-03-25 05:57:22,416 - training - INFO - Memory: GPU 0: 3572.6MB allocated, 9682.0MB reserved
2025-03-25 05:57:22,417 - training - INFO - Epoch: 26/200000, Batch: 15/45, Loss: 4.3134, Throughput: 70.53 samples/sec
2025-03-25 05:57:34,105 - training - INFO - Memory: GPU 0: 3572.6MB allocated, 9682.0MB reserved
2025-03-25 05:57:34,105 - training - INFO - Epoch: 26/200000, Batch: 30/45, Loss: 4.3369, Throughput: 71.18 samples/sec
2025-03-25 05:57:45,100 - training - INFO - Epoch 26 completed in 35.39s. Average loss: 4.3428
2025-03-25 05:57:45,104 - training - INFO - Starting epoch 27/200000
2025-03-25 05:57:45,860 - training - INFO - Memory: GPU 0: 3572.6MB allocated, 9822.0MB reserved
2025-03-25 05:57:45,861 - training - INFO - Epoch: 27/200000, Batch: 0/45, Loss: 4.5166, Throughput: 74.13 samples/sec
2025-03-25 05:57:57,855 - training - INFO - Memory: GPU 0: 3572.6MB allocated, 9578.0MB reserved
2025-03-25 05:57:57,856 - training - INFO - Epoch: 27/200000, Batch: 15/45, Loss: 4.3429, Throughput: 70.28 samples/sec
2025-03-25 05:58:09,524 - training - INFO - Memory: GPU 0: 3571.2MB allocated, 9578.0MB reserved
2025-03-25 05:58:09,525 - training - INFO - Epoch: 27/200000, Batch: 30/45, Loss: 4.3298, Throughput: 71.09 samples/sec
2025-03-25 05:58:20,526 - training - INFO - Epoch 27 completed in 35.42s. Average loss: 4.3200
2025-03-25 05:58:20,530 - training - INFO - Starting epoch 28/200000
2025-03-25 05:58:21,306 - training - INFO - Memory: GPU 0: 3572.6MB allocated, 9716.0MB reserved
2025-03-25 05:58:21,307 - training - INFO - Epoch: 28/200000, Batch: 0/45, Loss: 4.3959, Throughput: 72.34 samples/sec
2025-03-25 05:58:33,372 - training - INFO - Memory: GPU 0: 3571.8MB allocated, 9584.0MB reserved
2025-03-25 05:58:33,373 - training - INFO - Epoch: 28/200000, Batch: 15/45, Loss: 4.3333, Throughput: 69.78 samples/sec
2025-03-25 05:58:45,066 - training - INFO - Memory: GPU 0: 3571.8MB allocated, 9584.0MB reserved
2025-03-25 05:58:45,066 - training - INFO - Epoch: 28/200000, Batch: 30/45, Loss: 4.3064, Throughput: 70.76 samples/sec
2025-03-25 05:58:56,023 - training - INFO - Epoch 28 completed in 35.49s. Average loss: 4.3041
2025-03-25 05:58:56,027 - training - INFO - Starting epoch 29/200000
2025-03-25 05:58:56,819 - training - INFO - Memory: GPU 0: 3571.8MB allocated, 9724.0MB reserved
2025-03-25 05:58:56,819 - training - INFO - Epoch: 29/200000, Batch: 0/45, Loss: 4.3225, Throughput: 70.78 samples/sec
2025-03-25 05:59:08,833 - training - INFO - Memory: GPU 0: 3575.0MB allocated, 9580.0MB reserved
2025-03-25 05:59:08,834 - training - INFO - Epoch: 29/200000, Batch: 15/45, Loss: 4.2679, Throughput: 69.98 samples/sec
2025-03-25 05:59:20,549 - training - INFO - Memory: GPU 0: 3575.0MB allocated, 9580.0MB reserved
2025-03-25 05:59:20,549 - training - INFO - Epoch: 29/200000, Batch: 30/45, Loss: 4.2715, Throughput: 70.80 samples/sec
2025-03-25 05:59:31,568 - training - INFO - Epoch 29 completed in 35.54s. Average loss: 4.2913
2025-03-25 05:59:31,572 - training - INFO - Starting epoch 30/200000
2025-03-25 05:59:32,350 - training - INFO - Memory: GPU 0: 3575.0MB allocated, 9720.0MB reserved
2025-03-25 05:59:32,350 - training - INFO - Epoch: 30/200000, Batch: 0/45, Loss: 4.0146, Throughput: 72.10 samples/sec
2025-03-25 05:59:44,341 - training - INFO - Memory: GPU 0: 3571.8MB allocated, 9574.0MB reserved
2025-03-25 05:59:44,342 - training - INFO - Epoch: 30/200000, Batch: 15/45, Loss: 4.2200, Throughput: 70.18 samples/sec
2025-03-25 05:59:55,989 - training - INFO - Memory: GPU 0: 3571.8MB allocated, 9574.0MB reserved
2025-03-25 05:59:55,990 - training - INFO - Epoch: 30/200000, Batch: 30/45, Loss: 4.2547, Throughput: 71.10 samples/sec
2025-03-25 06:00:06,959 - training - INFO - Epoch 30 completed in 35.39s. Average loss: 4.2766
2025-03-25 06:00:06,962 - training - INFO - Starting epoch 31/200000
2025-03-25 06:00:07,730 - training - INFO - Memory: GPU 0: 3571.8MB allocated, 9710.0MB reserved
2025-03-25 06:00:07,730 - training - INFO - Epoch: 31/200000, Batch: 0/45, Loss: 4.2677, Throughput: 73.12 samples/sec
2025-03-25 06:00:19,733 - training - INFO - Memory: GPU 0: 3574.7MB allocated, 9584.0MB reserved
2025-03-25 06:00:19,733 - training - INFO - Epoch: 31/200000, Batch: 15/45, Loss: 4.3148, Throughput: 70.18 samples/sec
2025-03-25 06:00:31,457 - training - INFO - Memory: GPU 0: 3574.7MB allocated, 9584.0MB reserved
2025-03-25 06:00:31,457 - training - INFO - Epoch: 31/200000, Batch: 30/45, Loss: 4.2643, Throughput: 70.88 samples/sec
2025-03-25 06:00:42,451 - training - INFO - Epoch 31 completed in 35.49s. Average loss: 4.2582
2025-03-25 06:00:42,454 - training - INFO - Starting epoch 32/200000
2025-03-25 06:00:43,226 - training - INFO - Memory: GPU 0: 3573.4MB allocated, 9724.0MB reserved
2025-03-25 06:00:43,226 - training - INFO - Epoch: 32/200000, Batch: 0/45, Loss: 4.1314, Throughput: 72.73 samples/sec
2025-03-25 06:00:55,123 - training - INFO - Memory: GPU 0: 3577.1MB allocated, 9576.0MB reserved
2025-03-25 06:00:55,124 - training - INFO - Epoch: 32/200000, Batch: 15/45, Loss: 4.3117, Throughput: 70.73 samples/sec
2025-03-25 06:01:06,835 - training - INFO - Memory: GPU 0: 3577.1MB allocated, 9578.0MB reserved
2025-03-25 06:01:06,895 - training - INFO - Epoch: 32/200000, Batch: 30/45, Loss: 4.2869, Throughput: 71.21 samples/sec
2025-03-25 06:01:17,903 - training - INFO - Epoch 32 completed in 35.45s. Average loss: 4.2621
2025-03-25 06:01:17,907 - training - INFO - Starting epoch 33/200000
2025-03-25 06:01:18,675 - training - INFO - Memory: GPU 0: 3577.1MB allocated, 9716.0MB reserved
2025-03-25 06:01:18,675 - training - INFO - Epoch: 33/200000, Batch: 0/45, Loss: 4.5291, Throughput: 73.10 samples/sec
2025-03-25 06:11:04,451 - training - INFO - Overriding batch size with command-line value: 7
2025-03-25 06:11:04,451 - training - INFO - Overriding gradient accumulation steps with command-line value: 3
2025-03-25 06:11:04,451 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_ig30holr.log
2025-03-25 06:11:04,451 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 8
2025-03-25 06:11:04,451 - training - INFO - Device: cuda:0
2025-03-25 06:11:05,699 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-25 06:11:05,699 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-25 06:11:05,699 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-25 06:11:05,701 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-25 06:11:05,701 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-25 06:11:10,712 - training - INFO - Starting model initialization...
2025-03-25 06:11:21,003 - training - INFO - Dataset split: 2466 training, 137 validation, 137 test samples
2025-03-25 06:11:21,231 - training - INFO - Test dataset indices saved to /app/UAV-Language-Guided-Navigation/AnsweringAgent/outputs/logs/test_indices.pt
2025-03-25 06:11:21,231 - training - INFO - Per-GPU batch size: 7 (effective batch size: 168, with gradient_accumulation_steps=3)
2025-03-25 06:11:21,243 - training - INFO - Created 27 gradient buckets for efficient all-reduce
2025-03-25 06:11:21,248 - training - INFO - Starting epoch 1/200000
2025-03-25 06:11:27,229 - training - INFO - Memory: GPU 0: 2253.4MB allocated, 6388.0MB reserved
2025-03-25 06:11:27,229 - training - INFO - Epoch: 1/200000, Batch: 0/45, Loss: 11.2643, Throughput: 9.37 samples/sec
2025-03-25 06:11:38,982 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9474.0MB reserved
2025-03-25 06:11:38,983 - training - INFO - Epoch: 1/200000, Batch: 15/45, Loss: 7.9241, Throughput: 50.53 samples/sec
2025-03-25 06:11:50,759 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9474.0MB reserved
2025-03-25 06:11:50,760 - training - INFO - Epoch: 1/200000, Batch: 30/45, Loss: 6.9646, Throughput: 58.83 samples/sec
2025-03-25 06:12:03,118 - training - INFO - Epoch 1 completed in 41.87s. Average loss: 6.4800
2025-03-25 06:12:03,123 - training - INFO - Starting epoch 2/200000
2025-03-25 06:12:03,896 - training - INFO - Memory: GPU 0: 3564.4MB allocated, 9124.0MB reserved
2025-03-25 06:12:03,897 - training - INFO - Epoch: 2/200000, Batch: 0/45, Loss: 5.0445, Throughput: 72.53 samples/sec
2025-03-25 06:12:15,843 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9486.0MB reserved
2025-03-25 06:12:15,843 - training - INFO - Epoch: 2/200000, Batch: 15/45, Loss: 5.2295, Throughput: 70.45 samples/sec
2025-03-25 06:12:27,556 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9486.0MB reserved
2025-03-25 06:12:27,556 - training - INFO - Epoch: 2/200000, Batch: 30/45, Loss: 5.1716, Throughput: 71.05 samples/sec
2025-03-25 06:12:38,586 - training - INFO - Epoch 2 completed in 35.46s. Average loss: 5.1247
2025-03-25 06:12:38,590 - training - INFO - Starting epoch 3/200000
2025-03-25 06:12:39,328 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9624.0MB reserved
2025-03-25 06:12:39,328 - training - INFO - Epoch: 3/200000, Batch: 0/45, Loss: 4.7757, Throughput: 76.07 samples/sec
2025-03-25 06:12:51,238 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9510.0MB reserved
2025-03-25 06:12:51,238 - training - INFO - Epoch: 3/200000, Batch: 15/45, Loss: 4.8946, Throughput: 70.85 samples/sec
2025-03-25 06:13:02,936 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9510.0MB reserved
2025-03-25 06:13:02,939 - training - INFO - Epoch: 3/200000, Batch: 30/45, Loss: 4.9232, Throughput: 71.31 samples/sec
2025-03-25 06:13:13,943 - training - INFO - Epoch 3 completed in 35.35s. Average loss: 4.8938
2025-03-25 06:13:13,948 - training - INFO - Starting epoch 4/200000
2025-03-25 06:13:14,718 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9648.0MB reserved
2025-03-25 06:13:14,718 - training - INFO - Epoch: 4/200000, Batch: 0/45, Loss: 4.4348, Throughput: 72.81 samples/sec
2025-03-25 06:13:26,682 - training - INFO - Memory: GPU 0: 3562.7MB allocated, 9514.0MB reserved
2025-03-25 06:13:26,682 - training - INFO - Epoch: 4/200000, Batch: 15/45, Loss: 4.8078, Throughput: 70.37 samples/sec
2025-03-25 06:13:38,386 - training - INFO - Memory: GPU 0: 3562.7MB allocated, 9514.0MB reserved
2025-03-25 06:13:38,387 - training - INFO - Epoch: 4/200000, Batch: 30/45, Loss: 4.8179, Throughput: 71.04 samples/sec
2025-03-25 06:13:49,398 - training - INFO - Epoch 4 completed in 35.45s. Average loss: 4.7934
2025-03-25 06:13:49,403 - training - INFO - Starting epoch 5/200000
2025-03-25 06:13:50,158 - training - INFO - Memory: GPU 0: 3563.4MB allocated, 9652.0MB reserved
2025-03-25 06:13:50,159 - training - INFO - Epoch: 5/200000, Batch: 0/45, Loss: 4.7662, Throughput: 74.27 samples/sec
2025-03-25 06:14:02,169 - training - INFO - Memory: GPU 0: 3563.7MB allocated, 9486.0MB reserved
2025-03-25 06:14:02,169 - training - INFO - Epoch: 5/200000, Batch: 15/45, Loss: 4.8024, Throughput: 70.20 samples/sec
2025-03-25 06:14:13,839 - training - INFO - Memory: GPU 0: 3563.7MB allocated, 9486.0MB reserved
2025-03-25 06:14:13,840 - training - INFO - Epoch: 5/200000, Batch: 30/45, Loss: 4.7619, Throughput: 71.05 samples/sec
2025-03-25 06:14:24,845 - training - INFO - Epoch 5 completed in 35.44s. Average loss: 4.7382
2025-03-25 06:14:24,849 - training - INFO - Starting epoch 6/200000
2025-03-25 06:14:25,604 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9624.0MB reserved
2025-03-25 06:14:25,604 - training - INFO - Epoch: 6/200000, Batch: 0/45, Loss: 4.7478, Throughput: 74.18 samples/sec
2025-03-25 06:14:37,573 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9488.0MB reserved
2025-03-25 06:14:37,574 - training - INFO - Epoch: 6/200000, Batch: 15/45, Loss: 4.6388, Throughput: 70.43 samples/sec
2025-03-25 06:14:49,253 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9490.0MB reserved
2025-03-25 06:14:49,253 - training - INFO - Epoch: 6/200000, Batch: 30/45, Loss: 4.6520, Throughput: 71.14 samples/sec
2025-03-25 06:15:00,221 - training - INFO - Epoch 6 completed in 35.37s. Average loss: 4.6788
2025-03-25 06:15:00,225 - training - INFO - Starting epoch 7/200000
2025-03-25 06:15:00,978 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9628.0MB reserved
2025-03-25 06:15:00,978 - training - INFO - Epoch: 7/200000, Batch: 0/45, Loss: 4.5307, Throughput: 74.58 samples/sec
2025-03-25 06:15:13,037 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9484.0MB reserved
2025-03-25 06:15:13,038 - training - INFO - Epoch: 7/200000, Batch: 15/45, Loss: 4.6620, Throughput: 69.94 samples/sec
2025-03-25 06:15:24,773 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9484.0MB reserved
2025-03-25 06:15:24,773 - training - INFO - Epoch: 7/200000, Batch: 30/45, Loss: 4.6430, Throughput: 70.73 samples/sec
2025-03-25 06:15:35,804 - training - INFO - Epoch 7 completed in 35.58s. Average loss: 4.6218
2025-03-25 06:15:35,808 - training - INFO - Starting epoch 8/200000
2025-03-25 06:15:36,552 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9622.0MB reserved
2025-03-25 06:15:36,552 - training - INFO - Epoch: 8/200000, Batch: 0/45, Loss: 4.4853, Throughput: 75.43 samples/sec
2025-03-25 06:15:48,529 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9488.0MB reserved
2025-03-25 06:15:48,530 - training - INFO - Epoch: 8/200000, Batch: 15/45, Loss: 4.5826, Throughput: 70.45 samples/sec
2025-03-25 06:16:00,203 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9488.0MB reserved
2025-03-25 06:16:00,203 - training - INFO - Epoch: 8/200000, Batch: 30/45, Loss: 4.6061, Throughput: 71.17 samples/sec
2025-03-25 06:16:11,202 - training - INFO - Epoch 8 completed in 35.39s. Average loss: 4.5773
2025-03-25 06:16:11,206 - training - INFO - Starting epoch 9/200000
2025-03-25 06:16:11,956 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9626.0MB reserved
2025-03-25 06:16:11,956 - training - INFO - Epoch: 9/200000, Batch: 0/45, Loss: 4.5308, Throughput: 74.87 samples/sec
2025-03-25 06:16:23,877 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9496.0MB reserved
2025-03-25 06:16:23,877 - training - INFO - Epoch: 9/200000, Batch: 15/45, Loss: 4.5379, Throughput: 70.73 samples/sec
2025-03-25 06:16:35,508 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9496.0MB reserved
2025-03-25 06:16:35,509 - training - INFO - Epoch: 9/200000, Batch: 30/45, Loss: 4.5524, Throughput: 71.44 samples/sec
2025-03-25 06:16:46,459 - training - INFO - Epoch 9 completed in 35.25s. Average loss: 4.5498
2025-03-25 06:16:46,463 - training - INFO - Starting epoch 10/200000
2025-03-25 06:16:47,224 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9632.0MB reserved
2025-03-25 06:16:47,225 - training - INFO - Epoch: 10/200000, Batch: 0/45, Loss: 4.3514, Throughput: 73.65 samples/sec
2025-03-25 06:16:59,145 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9494.0MB reserved
2025-03-25 06:16:59,145 - training - INFO - Epoch: 10/200000, Batch: 15/45, Loss: 4.4227, Throughput: 70.66 samples/sec
2025-03-25 06:17:10,766 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9494.0MB reserved
2025-03-25 06:17:10,766 - training - INFO - Epoch: 10/200000, Batch: 30/45, Loss: 4.4789, Throughput: 71.44 samples/sec
2025-03-25 06:17:21,710 - training - INFO - Epoch 10 completed in 35.25s. Average loss: 4.5076
2025-03-25 06:17:21,714 - training - INFO - Starting epoch 11/200000
2025-03-25 06:17:22,461 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9632.0MB reserved
2025-03-25 06:17:22,461 - training - INFO - Epoch: 11/200000, Batch: 0/45, Loss: 4.7194, Throughput: 75.02 samples/sec
2025-03-25 06:17:34,469 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9508.0MB reserved
2025-03-25 06:17:34,469 - training - INFO - Epoch: 11/200000, Batch: 15/45, Loss: 4.4998, Throughput: 70.27 samples/sec
2025-03-25 06:17:46,085 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9508.0MB reserved
2025-03-25 06:17:46,085 - training - INFO - Epoch: 11/200000, Batch: 30/45, Loss: 4.5253, Throughput: 71.24 samples/sec
2025-03-25 06:17:57,005 - training - INFO - Epoch 11 completed in 35.29s. Average loss: 4.4761
2025-03-25 06:17:57,010 - training - INFO - Starting epoch 12/200000
2025-03-25 06:17:57,765 - training - INFO - Memory: GPU 0: 3556.0MB allocated, 9644.0MB reserved
2025-03-25 06:17:57,766 - training - INFO - Epoch: 12/200000, Batch: 0/45, Loss: 4.3616, Throughput: 74.18 samples/sec
2025-03-25 06:18:09,798 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9504.0MB reserved
2025-03-25 06:18:09,799 - training - INFO - Epoch: 12/200000, Batch: 15/45, Loss: 4.5169, Throughput: 70.09 samples/sec
2025-03-25 06:18:21,486 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9504.0MB reserved
2025-03-25 06:18:21,486 - training - INFO - Epoch: 12/200000, Batch: 30/45, Loss: 4.4905, Throughput: 70.93 samples/sec
2025-03-25 06:18:32,497 - training - INFO - Epoch 12 completed in 35.49s. Average loss: 4.4604
2025-03-25 06:18:32,501 - training - INFO - Starting epoch 13/200000
2025-03-25 06:18:33,259 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9642.0MB reserved
2025-03-25 06:18:33,260 - training - INFO - Epoch: 13/200000, Batch: 0/45, Loss: 4.6553, Throughput: 74.01 samples/sec
2025-03-25 06:18:45,177 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9490.0MB reserved
2025-03-25 06:18:45,177 - training - INFO - Epoch: 13/200000, Batch: 15/45, Loss: 4.5114, Throughput: 70.70 samples/sec
2025-03-25 06:18:56,768 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9490.0MB reserved
2025-03-25 06:18:56,769 - training - INFO - Epoch: 13/200000, Batch: 30/45, Loss: 4.4671, Throughput: 71.54 samples/sec
2025-03-25 06:19:07,766 - training - INFO - Epoch 13 completed in 35.26s. Average loss: 4.4533
2025-03-25 06:19:07,770 - training - INFO - Starting epoch 14/200000
2025-03-25 06:19:08,516 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9628.0MB reserved
2025-03-25 06:19:08,517 - training - INFO - Epoch: 14/200000, Batch: 0/45, Loss: 4.3945, Throughput: 75.17 samples/sec
2025-03-25 06:19:20,409 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9492.0MB reserved
2025-03-25 06:19:20,409 - training - INFO - Epoch: 14/200000, Batch: 15/45, Loss: 4.3709, Throughput: 70.90 samples/sec
2025-03-25 06:19:32,025 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9492.0MB reserved
2025-03-25 06:19:32,025 - training - INFO - Epoch: 14/200000, Batch: 30/45, Loss: 4.4057, Throughput: 71.58 samples/sec
2025-03-25 06:19:42,997 - training - INFO - Epoch 14 completed in 35.23s. Average loss: 4.4302
2025-03-25 06:19:43,001 - training - INFO - Starting epoch 15/200000
2025-03-25 06:19:43,754 - training - INFO - Memory: GPU 0: 3563.2MB allocated, 9630.0MB reserved
2025-03-25 06:19:43,754 - training - INFO - Epoch: 15/200000, Batch: 0/45, Loss: 4.7918, Throughput: 74.55 samples/sec
2025-03-25 06:19:55,787 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9484.0MB reserved
2025-03-25 06:19:55,788 - training - INFO - Epoch: 15/200000, Batch: 15/45, Loss: 4.4314, Throughput: 70.09 samples/sec
2025-03-25 06:20:07,401 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9484.0MB reserved
2025-03-25 06:20:07,401 - training - INFO - Epoch: 15/200000, Batch: 30/45, Loss: 4.4179, Throughput: 71.16 samples/sec
2025-03-25 06:20:18,380 - training - INFO - Epoch 15 completed in 35.38s. Average loss: 4.4248
2025-03-25 06:20:18,385 - training - INFO - Starting epoch 16/200000
2025-03-25 06:20:19,135 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9622.0MB reserved
2025-03-25 06:20:19,136 - training - INFO - Epoch: 16/200000, Batch: 0/45, Loss: 4.5992, Throughput: 74.71 samples/sec
2025-03-25 06:20:31,024 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9480.0MB reserved
2025-03-25 06:20:31,024 - training - INFO - Epoch: 16/200000, Batch: 15/45, Loss: 4.4131, Throughput: 70.91 samples/sec
2025-03-25 06:20:42,688 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9480.0MB reserved
2025-03-25 06:20:42,688 - training - INFO - Epoch: 16/200000, Batch: 30/45, Loss: 4.3909, Throughput: 71.44 samples/sec
2025-03-25 06:20:53,673 - training - INFO - Epoch 16 completed in 35.29s. Average loss: 4.4154
2025-03-25 06:20:53,677 - training - INFO - Starting epoch 17/200000
2025-03-25 06:20:54,432 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9618.0MB reserved
2025-03-25 06:20:54,432 - training - INFO - Epoch: 17/200000, Batch: 0/45, Loss: 4.4672, Throughput: 74.37 samples/sec
2025-03-25 06:21:06,371 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9508.0MB reserved
2025-03-25 06:21:06,372 - training - INFO - Epoch: 17/200000, Batch: 15/45, Loss: 4.3785, Throughput: 70.60 samples/sec
2025-03-25 06:21:18,021 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9508.0MB reserved
2025-03-25 06:21:18,022 - training - INFO - Epoch: 17/200000, Batch: 30/45, Loss: 4.4082, Throughput: 71.32 samples/sec
2025-03-25 06:21:28,984 - training - INFO - Epoch 17 completed in 35.31s. Average loss: 4.4039
2025-03-25 06:21:28,988 - training - INFO - Starting epoch 18/200000
2025-03-25 06:21:29,747 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9646.0MB reserved
2025-03-25 06:21:29,748 - training - INFO - Epoch: 18/200000, Batch: 0/45, Loss: 4.5153, Throughput: 73.99 samples/sec
2025-03-25 06:21:41,640 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9502.0MB reserved
2025-03-25 06:21:41,640 - training - INFO - Epoch: 18/200000, Batch: 15/45, Loss: 4.4013, Throughput: 70.83 samples/sec
2025-03-25 06:21:53,284 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9502.0MB reserved
2025-03-25 06:21:53,284 - training - INFO - Epoch: 18/200000, Batch: 30/45, Loss: 4.4096, Throughput: 71.46 samples/sec
2025-03-25 06:22:04,237 - training - INFO - Epoch 18 completed in 35.25s. Average loss: 4.3918
2025-03-25 06:22:04,241 - training - INFO - Starting epoch 19/200000
2025-03-25 06:22:05,001 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9640.0MB reserved
2025-03-25 06:22:05,001 - training - INFO - Epoch: 19/200000, Batch: 0/45, Loss: 4.3461, Throughput: 73.84 samples/sec
2025-03-25 06:22:16,939 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9514.0MB reserved
2025-03-25 06:22:16,939 - training - INFO - Epoch: 19/200000, Batch: 15/45, Loss: 4.4138, Throughput: 70.58 samples/sec
2025-03-25 06:22:28,597 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9514.0MB reserved
2025-03-25 06:22:28,598 - training - INFO - Epoch: 19/200000, Batch: 30/45, Loss: 4.3855, Throughput: 71.29 samples/sec
2025-03-25 06:22:39,591 - training - INFO - Epoch 19 completed in 35.35s. Average loss: 4.3832
2025-03-25 06:22:39,595 - training - INFO - Starting epoch 20/200000
2025-03-25 06:22:40,331 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9652.0MB reserved
2025-03-25 06:22:40,331 - training - INFO - Epoch: 20/200000, Batch: 0/45, Loss: 4.8958, Throughput: 76.23 samples/sec
2025-03-25 06:22:52,256 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9498.0MB reserved
2025-03-25 06:22:52,257 - training - INFO - Epoch: 20/200000, Batch: 15/45, Loss: 4.3928, Throughput: 70.78 samples/sec
2025-03-25 06:23:03,894 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9498.0MB reserved
2025-03-25 06:23:03,895 - training - INFO - Epoch: 20/200000, Batch: 30/45, Loss: 4.3931, Throughput: 71.45 samples/sec
2025-03-25 06:23:14,812 - training - INFO - Epoch 20 completed in 35.22s. Average loss: 4.3815
2025-03-25 06:23:14,816 - training - INFO - Starting epoch 21/200000
2025-03-25 06:23:15,554 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9636.0MB reserved
2025-03-25 06:23:15,554 - training - INFO - Epoch: 21/200000, Batch: 0/45, Loss: 4.1957, Throughput: 76.17 samples/sec
2025-03-25 06:23:27,387 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9496.0MB reserved
2025-03-25 06:23:27,387 - training - INFO - Epoch: 21/200000, Batch: 15/45, Loss: 4.4452, Throughput: 71.29 samples/sec
2025-03-25 06:23:38,965 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9496.0MB reserved
2025-03-25 06:23:38,965 - training - INFO - Epoch: 21/200000, Batch: 30/45, Loss: 4.4043, Throughput: 71.89 samples/sec
2025-03-25 06:23:49,938 - training - INFO - Epoch 21 completed in 35.12s. Average loss: 4.3614
2025-03-25 06:23:49,946 - training - INFO - Starting epoch 22/200000
2025-03-25 06:23:50,693 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9634.0MB reserved
2025-03-25 06:23:50,694 - training - INFO - Epoch: 22/200000, Batch: 0/45, Loss: 4.1439, Throughput: 75.10 samples/sec
2025-03-25 06:24:02,664 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9504.0MB reserved
2025-03-25 06:24:02,665 - training - INFO - Epoch: 22/200000, Batch: 15/45, Loss: 4.3445, Throughput: 70.46 samples/sec
2025-03-25 06:24:14,259 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9504.0MB reserved
2025-03-25 06:24:14,259 - training - INFO - Epoch: 22/200000, Batch: 30/45, Loss: 4.3528, Throughput: 71.41 samples/sec
2025-03-25 06:24:25,201 - training - INFO - Epoch 22 completed in 35.26s. Average loss: 4.3506
2025-03-25 06:24:25,205 - training - INFO - Starting epoch 23/200000
2025-03-25 06:24:25,950 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9642.0MB reserved
2025-03-25 06:24:25,951 - training - INFO - Epoch: 23/200000, Batch: 0/45, Loss: 4.4645, Throughput: 75.29 samples/sec
2025-03-25 06:24:37,845 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9500.0MB reserved
2025-03-25 06:24:37,845 - training - INFO - Epoch: 23/200000, Batch: 15/45, Loss: 4.3354, Throughput: 70.90 samples/sec
2025-03-25 06:24:49,412 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9500.0MB reserved
2025-03-25 06:24:49,412 - training - INFO - Epoch: 23/200000, Batch: 30/45, Loss: 4.3638, Throughput: 71.72 samples/sec
2025-03-25 06:25:00,361 - training - INFO - Epoch 23 completed in 35.16s. Average loss: 4.3412
2025-03-25 06:25:00,365 - training - INFO - Starting epoch 24/200000
2025-03-25 06:25:01,105 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9638.0MB reserved
2025-03-25 06:25:01,106 - training - INFO - Epoch: 24/200000, Batch: 0/45, Loss: 4.4684, Throughput: 75.82 samples/sec
2025-03-25 06:25:13,087 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9500.0MB reserved
2025-03-25 06:25:13,088 - training - INFO - Epoch: 24/200000, Batch: 15/45, Loss: 4.3031, Throughput: 70.44 samples/sec
2025-03-25 06:25:24,711 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9500.0MB reserved
2025-03-25 06:25:24,712 - training - INFO - Epoch: 24/200000, Batch: 30/45, Loss: 4.3005, Throughput: 71.31 samples/sec
2025-03-25 06:25:35,611 - training - INFO - Epoch 24 completed in 35.25s. Average loss: 4.3282
2025-03-25 06:25:35,615 - training - INFO - Starting epoch 25/200000
2025-03-25 06:25:36,376 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9638.0MB reserved
2025-03-25 06:25:36,376 - training - INFO - Epoch: 25/200000, Batch: 0/45, Loss: 4.2092, Throughput: 73.72 samples/sec
2025-03-25 06:25:48,345 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9480.0MB reserved
2025-03-25 06:25:48,346 - training - INFO - Epoch: 25/200000, Batch: 15/45, Loss: 4.3412, Throughput: 70.39 samples/sec
2025-03-25 06:26:00,018 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9480.0MB reserved
2025-03-25 06:26:00,018 - training - INFO - Epoch: 25/200000, Batch: 30/45, Loss: 4.2731, Throughput: 71.14 samples/sec
2025-03-25 06:26:11,009 - training - INFO - Epoch 25 completed in 35.39s. Average loss: 4.3172
2025-03-25 06:26:11,013 - training - INFO - Starting epoch 26/200000
2025-03-25 06:26:11,765 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9618.0MB reserved
2025-03-25 06:26:11,765 - training - INFO - Epoch: 26/200000, Batch: 0/45, Loss: 4.7741, Throughput: 74.54 samples/sec
2025-03-25 06:26:23,708 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9486.0MB reserved
2025-03-25 06:26:23,708 - training - INFO - Epoch: 26/200000, Batch: 15/45, Loss: 4.2594, Throughput: 70.59 samples/sec
2025-03-25 06:26:35,335 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9486.0MB reserved
2025-03-25 06:26:35,335 - training - INFO - Epoch: 26/200000, Batch: 30/45, Loss: 4.2827, Throughput: 71.38 samples/sec
2025-03-25 06:26:46,276 - training - INFO - Epoch 26 completed in 35.26s. Average loss: 4.3006
2025-03-25 06:26:46,281 - training - INFO - Starting epoch 27/200000
2025-03-25 06:26:47,028 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9622.0MB reserved
2025-03-25 06:26:47,028 - training - INFO - Epoch: 27/200000, Batch: 0/45, Loss: 4.4349, Throughput: 75.19 samples/sec
2025-03-25 06:26:58,983 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9502.0MB reserved
2025-03-25 06:26:58,983 - training - INFO - Epoch: 27/200000, Batch: 15/45, Loss: 4.2957, Throughput: 70.55 samples/sec
2025-03-25 06:27:10,572 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9502.0MB reserved
2025-03-25 06:27:10,572 - training - INFO - Epoch: 27/200000, Batch: 30/45, Loss: 4.2963, Throughput: 71.47 samples/sec
2025-03-25 06:27:21,517 - training - INFO - Epoch 27 completed in 35.24s. Average loss: 4.2873
2025-03-25 06:27:21,521 - training - INFO - Starting epoch 28/200000
2025-03-25 06:27:22,286 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9640.0MB reserved
2025-03-25 06:27:22,287 - training - INFO - Epoch: 28/200000, Batch: 0/45, Loss: 4.2571, Throughput: 73.38 samples/sec
2025-03-25 06:27:34,271 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9500.0MB reserved
2025-03-25 06:27:34,271 - training - INFO - Epoch: 28/200000, Batch: 15/45, Loss: 4.2845, Throughput: 70.29 samples/sec
2025-03-25 06:27:45,988 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9502.0MB reserved
2025-03-25 06:27:45,989 - training - INFO - Epoch: 28/200000, Batch: 30/45, Loss: 4.2711, Throughput: 70.96 samples/sec
2025-03-25 06:27:56,922 - training - INFO - Epoch 28 completed in 35.40s. Average loss: 4.2648
2025-03-25 06:27:56,926 - training - INFO - Starting epoch 29/200000
2025-03-25 06:27:57,693 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9640.0MB reserved
2025-03-25 06:27:57,693 - training - INFO - Epoch: 29/200000, Batch: 0/45, Loss: 4.3045, Throughput: 73.06 samples/sec
2025-03-25 06:28:09,534 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9394.0MB reserved
2025-03-25 06:28:09,534 - training - INFO - Epoch: 29/200000, Batch: 15/45, Loss: 4.2238, Throughput: 71.07 samples/sec
2025-03-25 06:28:21,199 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9500.0MB reserved
2025-03-25 06:28:21,200 - training - INFO - Epoch: 29/200000, Batch: 30/45, Loss: 4.2342, Throughput: 71.52 samples/sec
2025-03-25 06:28:32,195 - training - INFO - Epoch 29 completed in 35.27s. Average loss: 4.2539
2025-03-25 06:28:32,201 - training - INFO - Starting epoch 30/200000
2025-03-25 06:28:32,962 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9638.0MB reserved
2025-03-25 06:28:32,963 - training - INFO - Epoch: 30/200000, Batch: 0/45, Loss: 4.0059, Throughput: 73.66 samples/sec
2025-03-25 06:28:44,848 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9484.0MB reserved
2025-03-25 06:28:44,849 - training - INFO - Epoch: 30/200000, Batch: 15/45, Loss: 4.1659, Throughput: 70.85 samples/sec
2025-03-25 06:28:56,428 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9486.0MB reserved
2025-03-25 06:28:56,428 - training - INFO - Epoch: 30/200000, Batch: 30/45, Loss: 4.2134, Throughput: 71.66 samples/sec
2025-03-25 06:29:07,345 - training - INFO - Epoch 30 completed in 35.14s. Average loss: 4.2416
2025-03-25 06:29:07,349 - training - INFO - Starting epoch 31/200000
2025-03-25 06:29:08,094 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9624.0MB reserved
2025-03-25 06:29:08,094 - training - INFO - Epoch: 31/200000, Batch: 0/45, Loss: 4.3045, Throughput: 75.32 samples/sec
2025-03-25 06:29:19,947 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9500.0MB reserved
2025-03-25 06:29:19,948 - training - INFO - Epoch: 31/200000, Batch: 15/45, Loss: 4.2827, Throughput: 71.13 samples/sec
2025-03-25 06:29:31,650 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9502.0MB reserved
2025-03-25 06:29:31,651 - training - INFO - Epoch: 31/200000, Batch: 30/45, Loss: 4.2286, Throughput: 71.44 samples/sec
2025-03-25 06:29:42,636 - training - INFO - Epoch 31 completed in 35.29s. Average loss: 4.2142
2025-03-25 06:29:42,641 - training - INFO - Starting epoch 32/200000
2025-03-25 06:29:43,391 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9640.0MB reserved
2025-03-25 06:29:43,392 - training - INFO - Epoch: 32/200000, Batch: 0/45, Loss: 4.1627, Throughput: 74.77 samples/sec
2025-03-25 06:29:55,348 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9502.0MB reserved
2025-03-25 06:29:55,349 - training - INFO - Epoch: 32/200000, Batch: 15/45, Loss: 4.2709, Throughput: 70.53 samples/sec
2025-03-25 06:30:07,015 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9502.0MB reserved
2025-03-25 06:30:07,016 - training - INFO - Epoch: 32/200000, Batch: 30/45, Loss: 4.2410, Throughput: 71.23 samples/sec
2025-03-25 06:30:18,007 - training - INFO - Epoch 32 completed in 35.37s. Average loss: 4.2168
2025-03-25 06:30:18,011 - training - INFO - Starting epoch 33/200000
2025-03-25 06:30:18,755 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9638.0MB reserved
2025-03-25 06:30:18,755 - training - INFO - Epoch: 33/200000, Batch: 0/45, Loss: 4.4720, Throughput: 75.41 samples/sec
2025-03-25 06:30:30,772 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9508.0MB reserved
2025-03-25 06:30:30,772 - training - INFO - Epoch: 33/200000, Batch: 15/45, Loss: 4.2019, Throughput: 70.22 samples/sec
2025-03-25 06:30:42,397 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9508.0MB reserved
2025-03-25 06:30:42,397 - training - INFO - Epoch: 33/200000, Batch: 30/45, Loss: 4.1463, Throughput: 71.19 samples/sec
2025-03-25 06:30:53,402 - training - INFO - Epoch 33 completed in 35.39s. Average loss: 4.1978
2025-03-25 06:30:53,406 - training - INFO - Starting epoch 34/200000
2025-03-25 06:30:54,206 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9646.0MB reserved
2025-03-25 06:30:54,206 - training - INFO - Epoch: 34/200000, Batch: 0/45, Loss: 3.9436, Throughput: 70.15 samples/sec
2025-03-25 06:31:06,191 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9500.0MB reserved
2025-03-25 06:31:06,191 - training - INFO - Epoch: 34/200000, Batch: 15/45, Loss: 4.2205, Throughput: 70.09 samples/sec
2025-03-25 06:31:17,745 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9500.0MB reserved
2025-03-25 06:31:17,745 - training - INFO - Epoch: 34/200000, Batch: 30/45, Loss: 4.2345, Throughput: 71.33 samples/sec
2025-03-25 06:31:28,667 - training - INFO - Epoch 34 completed in 35.26s. Average loss: 4.1990
2025-03-25 06:31:28,671 - training - INFO - Starting epoch 35/200000
2025-03-25 06:31:29,425 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9636.0MB reserved
2025-03-25 06:31:29,425 - training - INFO - Epoch: 35/200000, Batch: 0/45, Loss: 4.2932, Throughput: 74.36 samples/sec
2025-03-25 06:31:41,409 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9510.0MB reserved
2025-03-25 06:31:41,410 - training - INFO - Epoch: 35/200000, Batch: 15/45, Loss: 3.9847, Throughput: 70.35 samples/sec
2025-03-25 06:31:52,969 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9510.0MB reserved
2025-03-25 06:31:52,970 - training - INFO - Epoch: 35/200000, Batch: 30/45, Loss: 4.0626, Throughput: 71.45 samples/sec
2025-03-25 06:32:03,908 - training - INFO - Epoch 35 completed in 35.24s. Average loss: 4.1620
2025-03-25 06:32:03,911 - training - INFO - Starting epoch 36/200000
2025-03-25 06:32:04,665 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9648.0MB reserved
2025-03-25 06:32:04,665 - training - INFO - Epoch: 36/200000, Batch: 0/45, Loss: 4.1993, Throughput: 74.50 samples/sec
2025-03-25 06:32:16,663 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9502.0MB reserved
2025-03-25 06:32:16,663 - training - INFO - Epoch: 36/200000, Batch: 15/45, Loss: 4.1630, Throughput: 70.27 samples/sec
2025-03-25 06:32:28,296 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9504.0MB reserved
2025-03-25 06:32:28,297 - training - INFO - Epoch: 36/200000, Batch: 30/45, Loss: 4.1535, Throughput: 71.20 samples/sec
2025-03-25 06:32:39,263 - training - INFO - Epoch 36 completed in 35.35s. Average loss: 4.1458
2025-03-25 06:32:39,267 - training - INFO - Starting epoch 37/200000
2025-03-25 06:32:40,040 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9642.0MB reserved
2025-03-25 06:32:40,040 - training - INFO - Epoch: 37/200000, Batch: 0/45, Loss: 3.8605, Throughput: 72.57 samples/sec
2025-03-25 06:32:51,993 - training - INFO - Memory: GPU 0: 3563.1MB allocated, 9506.0MB reserved
2025-03-25 06:32:51,994 - training - INFO - Epoch: 37/200000, Batch: 15/45, Loss: 4.1370, Throughput: 70.41 samples/sec
2025-03-25 06:33:03,594 - training - INFO - Memory: GPU 0: 3563.1MB allocated, 9506.0MB reserved
2025-03-25 06:33:03,594 - training - INFO - Epoch: 37/200000, Batch: 30/45, Loss: 4.1522, Throughput: 71.37 samples/sec
2025-03-25 06:33:14,501 - training - INFO - Epoch 37 completed in 35.23s. Average loss: 4.1630
2025-03-25 06:33:14,505 - training - INFO - Starting epoch 38/200000
2025-03-25 06:33:15,249 - training - INFO - Memory: GPU 0: 3563.1MB allocated, 9644.0MB reserved
2025-03-25 06:33:15,249 - training - INFO - Epoch: 38/200000, Batch: 0/45, Loss: 3.7163, Throughput: 75.45 samples/sec
2025-03-25 06:33:27,141 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9488.0MB reserved
2025-03-25 06:33:27,141 - training - INFO - Epoch: 38/200000, Batch: 15/45, Loss: 4.1683, Throughput: 70.92 samples/sec
2025-03-25 06:33:38,747 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9488.0MB reserved
2025-03-25 06:33:38,747 - training - INFO - Epoch: 38/200000, Batch: 30/45, Loss: 4.1560, Throughput: 71.62 samples/sec
2025-03-25 06:33:49,667 - training - INFO - Epoch 38 completed in 35.16s. Average loss: 4.1436
2025-03-25 06:33:49,671 - training - INFO - Starting epoch 39/200000
2025-03-25 06:33:50,404 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9626.0MB reserved
2025-03-25 06:33:50,404 - training - INFO - Epoch: 39/200000, Batch: 0/45, Loss: 4.1653, Throughput: 76.49 samples/sec
2025-03-25 06:34:02,311 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9500.0MB reserved
2025-03-25 06:34:02,311 - training - INFO - Epoch: 39/200000, Batch: 15/45, Loss: 4.1782, Throughput: 70.90 samples/sec
2025-03-25 06:34:13,952 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9502.0MB reserved
2025-03-25 06:34:13,952 - training - INFO - Epoch: 39/200000, Batch: 30/45, Loss: 4.1462, Throughput: 71.50 samples/sec
2025-03-25 06:34:24,885 - training - INFO - Epoch 39 completed in 35.21s. Average loss: 4.1223
2025-03-25 06:34:24,889 - training - INFO - Starting epoch 40/200000
2025-03-25 06:34:25,661 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9640.0MB reserved
2025-03-25 06:34:25,661 - training - INFO - Epoch: 40/200000, Batch: 0/45, Loss: 4.3063, Throughput: 72.76 samples/sec
2025-03-25 06:34:37,554 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9498.0MB reserved
2025-03-25 06:34:37,554 - training - INFO - Epoch: 40/200000, Batch: 15/45, Loss: 4.2031, Throughput: 70.75 samples/sec
2025-03-25 06:34:49,162 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9498.0MB reserved
2025-03-25 06:34:49,162 - training - INFO - Epoch: 40/200000, Batch: 30/45, Loss: 4.1651, Throughput: 71.52 samples/sec
2025-03-25 06:35:00,139 - training - INFO - Epoch 40 completed in 35.25s. Average loss: 4.1304
2025-03-25 06:35:00,143 - training - INFO - Starting epoch 41/200000
2025-03-25 06:35:00,908 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9636.0MB reserved
2025-03-25 06:35:00,908 - training - INFO - Epoch: 41/200000, Batch: 0/45, Loss: 4.3045, Throughput: 73.46 samples/sec
2025-03-25 06:35:12,930 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9490.0MB reserved
2025-03-25 06:35:12,931 - training - INFO - Epoch: 41/200000, Batch: 15/45, Loss: 4.0742, Throughput: 70.08 samples/sec
2025-03-25 06:35:24,622 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9490.0MB reserved
2025-03-25 06:35:24,623 - training - INFO - Epoch: 41/200000, Batch: 30/45, Loss: 4.1114, Throughput: 70.93 samples/sec
2025-03-25 06:35:35,601 - training - INFO - Epoch 41 completed in 35.46s. Average loss: 4.1147
2025-03-25 06:35:35,605 - training - INFO - Starting epoch 42/200000
2025-03-25 06:35:36,353 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9626.0MB reserved
2025-03-25 06:35:36,354 - training - INFO - Epoch: 42/200000, Batch: 0/45, Loss: 3.9559, Throughput: 75.03 samples/sec
2025-03-25 06:35:48,320 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9402.0MB reserved
2025-03-25 06:35:48,320 - training - INFO - Epoch: 42/200000, Batch: 15/45, Loss: 4.0672, Throughput: 70.47 samples/sec
2025-03-25 06:36:00,006 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9404.0MB reserved
2025-03-25 06:36:00,006 - training - INFO - Epoch: 42/200000, Batch: 30/45, Loss: 4.1105, Throughput: 71.15 samples/sec
2025-03-25 06:36:11,006 - training - INFO - Epoch 42 completed in 35.40s. Average loss: 4.0940
2025-03-25 06:36:11,011 - training - INFO - Starting epoch 43/200000
2025-03-25 06:36:11,757 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9540.0MB reserved
2025-03-25 06:36:11,757 - training - INFO - Epoch: 43/200000, Batch: 0/45, Loss: 4.1226, Throughput: 75.33 samples/sec
2025-03-25 06:36:23,645 - training - INFO - Memory: GPU 0: 3556.6MB allocated, 9498.0MB reserved
2025-03-25 06:36:23,645 - training - INFO - Epoch: 43/200000, Batch: 15/45, Loss: 4.1592, Throughput: 70.94 samples/sec
2025-03-25 06:36:35,245 - training - INFO - Memory: GPU 0: 3556.6MB allocated, 9498.0MB reserved
2025-03-25 06:36:35,245 - training - INFO - Epoch: 43/200000, Batch: 30/45, Loss: 4.0995, Throughput: 71.64 samples/sec
2025-03-25 06:36:46,257 - training - INFO - Epoch 43 completed in 35.25s. Average loss: 4.0770
2025-03-25 06:36:46,261 - training - INFO - Starting epoch 44/200000
2025-03-25 06:36:47,002 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9636.0MB reserved
2025-03-25 06:36:47,002 - training - INFO - Epoch: 44/200000, Batch: 0/45, Loss: 4.1152, Throughput: 75.74 samples/sec
2025-03-25 06:36:58,859 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9494.0MB reserved
2025-03-25 06:36:58,859 - training - INFO - Epoch: 44/200000, Batch: 15/45, Loss: 4.0689, Throughput: 71.13 samples/sec
2025-03-25 06:37:10,456 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9494.0MB reserved
2025-03-25 06:37:10,456 - training - INFO - Epoch: 44/200000, Batch: 30/45, Loss: 4.0986, Throughput: 71.75 samples/sec
2025-03-25 06:37:21,434 - training - INFO - Epoch 44 completed in 35.17s. Average loss: 4.0744
2025-03-25 06:37:21,438 - training - INFO - Starting epoch 45/200000
2025-03-25 06:37:22,185 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9632.0MB reserved
2025-03-25 06:37:22,186 - training - INFO - Epoch: 45/200000, Batch: 0/45, Loss: 4.2083, Throughput: 75.12 samples/sec
2025-03-25 06:37:34,156 - training - INFO - Memory: GPU 0: 3555.9MB allocated, 9500.0MB reserved
2025-03-25 06:37:34,156 - training - INFO - Epoch: 45/200000, Batch: 15/45, Loss: 4.0046, Throughput: 70.46 samples/sec
2025-03-25 06:37:45,754 - training - INFO - Memory: GPU 0: 3555.9MB allocated, 9500.0MB reserved
2025-03-25 06:37:45,754 - training - INFO - Epoch: 45/200000, Batch: 30/45, Loss: 3.9999, Throughput: 71.40 samples/sec
2025-03-25 06:37:56,764 - training - INFO - Epoch 45 completed in 35.33s. Average loss: 4.0726
2025-03-25 06:37:56,768 - training - INFO - Starting epoch 46/200000
2025-03-25 06:37:57,527 - training - INFO - Memory: GPU 0: 3555.9MB allocated, 9638.0MB reserved
2025-03-25 06:37:57,527 - training - INFO - Epoch: 46/200000, Batch: 0/45, Loss: 4.3461, Throughput: 74.01 samples/sec
2025-03-25 06:38:09,470 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9490.0MB reserved
2025-03-25 06:38:09,470 - training - INFO - Epoch: 46/200000, Batch: 15/45, Loss: 4.0016, Throughput: 70.55 samples/sec
2025-03-25 06:38:21,045 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9490.0MB reserved
2025-03-25 06:38:21,045 - training - INFO - Epoch: 46/200000, Batch: 30/45, Loss: 4.0253, Throughput: 71.51 samples/sec
2025-03-25 06:38:31,954 - training - INFO - Epoch 46 completed in 35.19s. Average loss: 4.0557
2025-03-25 06:38:31,957 - training - INFO - Starting epoch 47/200000
2025-03-25 06:38:32,715 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9628.0MB reserved
2025-03-25 06:38:32,716 - training - INFO - Epoch: 47/200000, Batch: 0/45, Loss: 4.1755, Throughput: 74.04 samples/sec
2025-03-25 06:38:44,561 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9498.0MB reserved
2025-03-25 06:38:44,561 - training - INFO - Epoch: 47/200000, Batch: 15/45, Loss: 4.0477, Throughput: 71.10 samples/sec
2025-03-25 06:38:56,124 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9498.0MB reserved
2025-03-25 06:38:56,125 - training - INFO - Epoch: 47/200000, Batch: 30/45, Loss: 4.0657, Throughput: 71.84 samples/sec
2025-03-25 06:39:07,135 - training - INFO - Epoch 47 completed in 35.18s. Average loss: 4.0540
2025-03-25 06:39:07,139 - training - INFO - Starting epoch 48/200000
2025-03-25 06:39:07,897 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9636.0MB reserved
2025-03-25 06:39:07,897 - training - INFO - Epoch: 48/200000, Batch: 0/45, Loss: 3.9830, Throughput: 74.08 samples/sec
2025-03-25 06:39:19,770 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9498.0MB reserved
2025-03-25 06:39:19,770 - training - INFO - Epoch: 48/200000, Batch: 15/45, Loss: 4.0464, Throughput: 70.95 samples/sec
2025-03-25 06:39:31,400 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9498.0MB reserved
2025-03-25 06:39:31,400 - training - INFO - Epoch: 48/200000, Batch: 30/45, Loss: 4.0299, Throughput: 71.56 samples/sec
2025-03-25 06:39:42,360 - training - INFO - Epoch 48 completed in 35.22s. Average loss: 4.0391
2025-03-25 06:39:42,363 - training - INFO - Starting epoch 49/200000
2025-03-25 06:39:43,109 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9636.0MB reserved
2025-03-25 06:39:43,109 - training - INFO - Epoch: 49/200000, Batch: 0/45, Loss: 4.3660, Throughput: 75.20 samples/sec
2025-03-25 06:39:54,947 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9498.0MB reserved
2025-03-25 06:39:54,947 - training - INFO - Epoch: 49/200000, Batch: 15/45, Loss: 4.1450, Throughput: 71.21 samples/sec
2025-03-25 06:40:06,604 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9500.0MB reserved
2025-03-25 06:40:06,604 - training - INFO - Epoch: 49/200000, Batch: 30/45, Loss: 4.0840, Throughput: 71.62 samples/sec
2025-03-25 06:40:17,583 - training - INFO - Epoch 49 completed in 35.22s. Average loss: 4.0260
2025-03-25 06:40:17,587 - training - INFO - Starting epoch 50/200000
2025-03-25 06:40:18,350 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9636.0MB reserved
2025-03-25 06:40:18,350 - training - INFO - Epoch: 50/200000, Batch: 0/45, Loss: 3.9221, Throughput: 73.63 samples/sec
2025-03-25 06:40:30,297 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9504.0MB reserved
2025-03-25 06:40:30,298 - training - INFO - Epoch: 50/200000, Batch: 15/45, Loss: 3.9123, Throughput: 70.51 samples/sec
2025-03-25 06:40:41,897 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9504.0MB reserved
2025-03-25 06:40:41,897 - training - INFO - Epoch: 50/200000, Batch: 30/45, Loss: 3.9512, Throughput: 71.42 samples/sec
2025-03-25 06:40:52,826 - training - INFO - Epoch 50 completed in 35.24s. Average loss: 4.0058
2025-03-25 06:40:52,830 - training - INFO - Starting epoch 51/200000
2025-03-25 06:40:53,584 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9642.0MB reserved
2025-03-25 06:40:53,584 - training - INFO - Epoch: 51/200000, Batch: 0/45, Loss: 4.0272, Throughput: 74.43 samples/sec
2025-03-25 06:41:05,519 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9490.0MB reserved
2025-03-25 06:41:05,520 - training - INFO - Epoch: 51/200000, Batch: 15/45, Loss: 4.0471, Throughput: 70.62 samples/sec
2025-03-25 06:41:17,110 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9492.0MB reserved
2025-03-25 06:41:17,110 - training - INFO - Epoch: 51/200000, Batch: 30/45, Loss: 4.0192, Throughput: 71.50 samples/sec
2025-03-25 06:41:28,043 - training - INFO - Epoch 51 completed in 35.21s. Average loss: 3.9942
2025-03-25 06:41:28,047 - training - INFO - Starting epoch 52/200000
2025-03-25 06:41:28,799 - training - INFO - Memory: GPU 0: 3564.1MB allocated, 9630.0MB reserved
2025-03-25 06:41:28,799 - training - INFO - Epoch: 52/200000, Batch: 0/45, Loss: 4.0108, Throughput: 74.68 samples/sec
2025-03-25 06:41:40,768 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9480.0MB reserved
2025-03-25 06:41:40,768 - training - INFO - Epoch: 52/200000, Batch: 15/45, Loss: 3.9991, Throughput: 70.45 samples/sec
2025-03-25 06:41:52,465 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9480.0MB reserved
2025-03-25 06:41:52,465 - training - INFO - Epoch: 52/200000, Batch: 30/45, Loss: 3.9892, Throughput: 71.10 samples/sec
2025-03-25 06:42:03,490 - training - INFO - Epoch 52 completed in 35.44s. Average loss: 3.9804
2025-03-25 06:42:03,495 - training - INFO - Starting epoch 53/200000
2025-03-25 06:42:04,254 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9618.0MB reserved
2025-03-25 06:42:04,254 - training - INFO - Epoch: 53/200000, Batch: 0/45, Loss: 4.1963, Throughput: 73.82 samples/sec
2025-03-25 06:42:16,291 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9494.0MB reserved
2025-03-25 06:42:16,292 - training - INFO - Epoch: 53/200000, Batch: 15/45, Loss: 3.9006, Throughput: 70.03 samples/sec
2025-03-25 06:42:27,857 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9494.0MB reserved
2025-03-25 06:42:27,857 - training - INFO - Epoch: 53/200000, Batch: 30/45, Loss: 3.9235, Throughput: 71.26 samples/sec
2025-03-25 06:42:38,786 - training - INFO - Epoch 53 completed in 35.29s. Average loss: 3.9795
2025-03-25 06:42:38,790 - training - INFO - Starting epoch 54/200000
2025-03-25 06:42:39,549 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9632.0MB reserved
2025-03-25 06:42:39,549 - training - INFO - Epoch: 54/200000, Batch: 0/45, Loss: 3.7592, Throughput: 73.90 samples/sec
2025-03-25 06:42:51,504 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9516.0MB reserved
2025-03-25 06:42:51,504 - training - INFO - Epoch: 54/200000, Batch: 15/45, Loss: 3.9063, Throughput: 70.49 samples/sec
2025-03-25 06:43:03,126 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9516.0MB reserved
2025-03-25 06:43:03,126 - training - INFO - Epoch: 54/200000, Batch: 30/45, Loss: 3.9571, Throughput: 71.34 samples/sec
2025-03-25 06:43:14,056 - training - INFO - Epoch 54 completed in 35.27s. Average loss: 3.9547
2025-03-25 06:43:14,059 - training - INFO - Starting epoch 55/200000
2025-03-25 06:43:14,818 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9654.0MB reserved
2025-03-25 06:43:14,818 - training - INFO - Epoch: 55/200000, Batch: 0/45, Loss: 4.1571, Throughput: 73.88 samples/sec
2025-03-25 06:43:26,712 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9504.0MB reserved
2025-03-25 06:43:26,713 - training - INFO - Epoch: 55/200000, Batch: 15/45, Loss: 3.9601, Throughput: 70.82 samples/sec
2025-03-25 06:43:38,314 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9506.0MB reserved
2025-03-25 06:43:38,315 - training - INFO - Epoch: 55/200000, Batch: 30/45, Loss: 3.9649, Throughput: 71.58 samples/sec
2025-03-25 06:43:49,264 - training - INFO - Epoch 55 completed in 35.21s. Average loss: 3.9367
2025-03-25 06:43:49,268 - training - INFO - Starting epoch 56/200000
2025-03-25 06:43:50,005 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9642.0MB reserved
2025-03-25 06:43:50,005 - training - INFO - Epoch: 56/200000, Batch: 0/45, Loss: 3.7882, Throughput: 76.18 samples/sec
2025-03-25 06:44:01,990 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9514.0MB reserved
2025-03-25 06:44:01,990 - training - INFO - Epoch: 56/200000, Batch: 15/45, Loss: 3.9927, Throughput: 70.44 samples/sec
2025-03-25 06:44:13,654 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9514.0MB reserved
2025-03-25 06:44:13,654 - training - INFO - Epoch: 56/200000, Batch: 30/45, Loss: 3.8870, Throughput: 71.19 samples/sec
2025-03-25 06:44:24,637 - training - INFO - Epoch 56 completed in 35.37s. Average loss: 3.9483
2025-03-25 06:44:24,641 - training - INFO - Starting epoch 57/200000
2025-03-25 06:44:25,383 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9650.0MB reserved
2025-03-25 06:44:25,383 - training - INFO - Epoch: 57/200000, Batch: 0/45, Loss: 4.2032, Throughput: 75.54 samples/sec
2025-03-25 06:44:37,315 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9512.0MB reserved
2025-03-25 06:44:37,316 - training - INFO - Epoch: 57/200000, Batch: 15/45, Loss: 3.9885, Throughput: 70.70 samples/sec
2025-03-25 06:44:48,952 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9512.0MB reserved
2025-03-25 06:44:48,952 - training - INFO - Epoch: 57/200000, Batch: 30/45, Loss: 3.9023, Throughput: 71.41 samples/sec
2025-03-25 06:44:59,945 - training - INFO - Epoch 57 completed in 35.30s. Average loss: 3.9269
2025-03-25 06:44:59,949 - training - INFO - Starting epoch 58/200000
2025-03-25 06:45:00,708 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9650.0MB reserved
2025-03-25 06:45:00,708 - training - INFO - Epoch: 58/200000, Batch: 0/45, Loss: 3.9967, Throughput: 73.83 samples/sec
2025-03-25 06:45:12,640 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9504.0MB reserved
2025-03-25 06:45:12,640 - training - INFO - Epoch: 58/200000, Batch: 15/45, Loss: 3.8100, Throughput: 70.61 samples/sec
2025-03-25 06:45:24,302 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9504.0MB reserved
2025-03-25 06:45:24,302 - training - INFO - Epoch: 58/200000, Batch: 30/45, Loss: 3.8628, Throughput: 71.29 samples/sec
2025-03-25 06:45:35,301 - training - INFO - Epoch 58 completed in 35.35s. Average loss: 3.9335
2025-03-25 06:45:35,305 - training - INFO - Starting epoch 59/200000
2025-03-25 06:45:36,075 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9642.0MB reserved
2025-03-25 06:45:36,076 - training - INFO - Epoch: 59/200000, Batch: 0/45, Loss: 3.7424, Throughput: 72.76 samples/sec
2025-03-25 06:45:48,055 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9500.0MB reserved
2025-03-25 06:45:48,055 - training - INFO - Epoch: 59/200000, Batch: 15/45, Loss: 3.8730, Throughput: 70.28 samples/sec
2025-03-25 06:45:59,656 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9500.0MB reserved
2025-03-25 06:45:59,656 - training - INFO - Epoch: 59/200000, Batch: 30/45, Loss: 3.8553, Throughput: 71.29 samples/sec
2025-03-25 06:46:10,580 - training - INFO - Epoch 59 completed in 35.28s. Average loss: 3.9092
2025-03-25 06:46:10,584 - training - INFO - Starting epoch 60/200000
2025-03-25 06:46:11,330 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9638.0MB reserved
2025-03-25 06:46:11,330 - training - INFO - Epoch: 60/200000, Batch: 0/45, Loss: 4.1019, Throughput: 75.24 samples/sec
2025-03-25 06:46:23,329 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9498.0MB reserved
2025-03-25 06:46:23,330 - training - INFO - Epoch: 60/200000, Batch: 15/45, Loss: 3.9172, Throughput: 70.31 samples/sec
2025-03-25 06:46:34,952 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9498.0MB reserved
2025-03-25 06:46:34,953 - training - INFO - Epoch: 60/200000, Batch: 30/45, Loss: 3.9177, Throughput: 71.25 samples/sec
2025-03-25 06:46:45,881 - training - INFO - Epoch 60 completed in 35.30s. Average loss: 3.8948
2025-03-25 06:46:45,885 - training - INFO - Starting epoch 61/200000
2025-03-25 06:46:46,630 - training - INFO - Memory: GPU 0: 3556.9MB allocated, 9636.0MB reserved
2025-03-25 06:46:46,630 - training - INFO - Epoch: 61/200000, Batch: 0/45, Loss: 3.7531, Throughput: 75.26 samples/sec
2025-03-25 06:46:58,567 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9512.0MB reserved
2025-03-25 06:46:58,568 - training - INFO - Epoch: 61/200000, Batch: 15/45, Loss: 3.8768, Throughput: 70.65 samples/sec
2025-03-25 06:47:10,114 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9512.0MB reserved
2025-03-25 06:47:10,114 - training - INFO - Epoch: 61/200000, Batch: 30/45, Loss: 3.9172, Throughput: 71.65 samples/sec
2025-03-25 06:47:21,043 - training - INFO - Epoch 61 completed in 35.16s. Average loss: 3.9043
2025-03-25 06:47:21,046 - training - INFO - Starting epoch 62/200000
2025-03-25 06:47:21,820 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9650.0MB reserved
2025-03-25 06:47:21,820 - training - INFO - Epoch: 62/200000, Batch: 0/45, Loss: 3.8357, Throughput: 72.47 samples/sec
2025-03-25 06:47:33,768 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9508.0MB reserved
2025-03-25 06:47:33,768 - training - INFO - Epoch: 62/200000, Batch: 15/45, Loss: 3.8853, Throughput: 70.44 samples/sec
2025-03-25 06:47:45,327 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9508.0MB reserved
2025-03-25 06:47:45,327 - training - INFO - Epoch: 62/200000, Batch: 30/45, Loss: 3.9008, Throughput: 71.50 samples/sec
2025-03-25 06:47:56,294 - training - INFO - Epoch 62 completed in 35.25s. Average loss: 3.8731
2025-03-25 06:47:56,298 - training - INFO - Starting epoch 63/200000
2025-03-25 06:47:57,059 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9646.0MB reserved
2025-03-25 06:47:57,059 - training - INFO - Epoch: 63/200000, Batch: 0/45, Loss: 3.3779, Throughput: 73.65 samples/sec
2025-03-25 06:48:08,968 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9512.0MB reserved
2025-03-25 06:48:08,968 - training - INFO - Epoch: 63/200000, Batch: 15/45, Loss: 3.8545, Throughput: 70.73 samples/sec
2025-03-25 06:48:20,588 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9512.0MB reserved
2025-03-25 06:48:20,588 - training - INFO - Epoch: 63/200000, Batch: 30/45, Loss: 3.8243, Throughput: 71.48 samples/sec
2025-03-25 06:48:31,523 - training - INFO - Epoch 63 completed in 35.23s. Average loss: 3.8718
2025-03-25 06:48:31,526 - training - INFO - Starting epoch 64/200000
2025-03-25 06:48:32,289 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9650.0MB reserved
2025-03-25 06:48:32,289 - training - INFO - Epoch: 64/200000, Batch: 0/45, Loss: 3.8734, Throughput: 73.67 samples/sec
2025-03-25 06:48:44,288 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9496.0MB reserved
2025-03-25 06:48:44,289 - training - INFO - Epoch: 64/200000, Batch: 15/45, Loss: 3.8442, Throughput: 70.22 samples/sec
2025-03-25 06:48:55,978 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9496.0MB reserved
2025-03-25 06:48:55,978 - training - INFO - Epoch: 64/200000, Batch: 30/45, Loss: 3.8391, Throughput: 71.00 samples/sec
2025-03-25 06:49:06,975 - training - INFO - Epoch 64 completed in 35.45s. Average loss: 3.8725
2025-03-25 06:49:06,980 - training - INFO - Starting epoch 65/200000
2025-03-25 06:49:07,735 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9634.0MB reserved
2025-03-25 06:49:07,735 - training - INFO - Epoch: 65/200000, Batch: 0/45, Loss: 3.3344, Throughput: 74.19 samples/sec
2025-03-25 06:49:19,609 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9484.0MB reserved
2025-03-25 06:49:19,610 - training - INFO - Epoch: 65/200000, Batch: 15/45, Loss: 3.9065, Throughput: 70.95 samples/sec
2025-03-25 06:49:31,223 - training - INFO - Memory: GPU 0: 3556.9MB allocated, 9484.0MB reserved
2025-03-25 06:49:31,223 - training - INFO - Epoch: 65/200000, Batch: 30/45, Loss: 3.8609, Throughput: 71.61 samples/sec
2025-03-25 06:49:42,219 - training - INFO - Epoch 65 completed in 35.24s. Average loss: 3.8662
2025-03-25 06:49:42,222 - training - INFO - Starting epoch 66/200000
2025-03-25 06:49:42,963 - training - INFO - Memory: GPU 0: 3556.9MB allocated, 9622.0MB reserved
2025-03-25 06:49:42,964 - training - INFO - Epoch: 66/200000, Batch: 0/45, Loss: 3.9397, Throughput: 75.61 samples/sec
2025-03-25 06:49:54,916 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9514.0MB reserved
2025-03-25 06:49:54,917 - training - INFO - Epoch: 66/200000, Batch: 15/45, Loss: 3.8669, Throughput: 70.59 samples/sec
2025-03-25 06:50:06,512 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9514.0MB reserved
2025-03-25 06:50:06,513 - training - INFO - Epoch: 66/200000, Batch: 30/45, Loss: 3.8818, Throughput: 71.48 samples/sec
2025-03-25 06:50:17,518 - training - INFO - Epoch 66 completed in 35.30s. Average loss: 3.8628
2025-03-25 06:50:17,522 - training - INFO - Starting epoch 67/200000
2025-03-25 06:50:18,258 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9654.0MB reserved
2025-03-25 06:50:18,259 - training - INFO - Epoch: 67/200000, Batch: 0/45, Loss: 3.7962, Throughput: 76.17 samples/sec
2025-03-25 06:50:30,159 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9498.0MB reserved
2025-03-25 06:50:30,159 - training - INFO - Epoch: 67/200000, Batch: 15/45, Loss: 3.8453, Throughput: 70.92 samples/sec
2025-03-25 06:50:41,749 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9500.0MB reserved
2025-03-25 06:50:41,749 - training - INFO - Epoch: 67/200000, Batch: 30/45, Loss: 3.8046, Throughput: 71.66 samples/sec
2025-03-25 06:50:52,691 - training - INFO - Epoch 67 completed in 35.17s. Average loss: 3.8336
2025-03-25 06:50:52,694 - training - INFO - Starting epoch 68/200000
2025-03-25 06:50:53,455 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9638.0MB reserved
2025-03-25 06:50:53,455 - training - INFO - Epoch: 68/200000, Batch: 0/45, Loss: 4.0403, Throughput: 73.79 samples/sec
2025-03-25 06:51:05,335 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9508.0MB reserved
2025-03-25 06:51:05,335 - training - INFO - Epoch: 68/200000, Batch: 15/45, Loss: 3.9311, Throughput: 70.89 samples/sec
2025-03-25 06:51:16,995 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9508.0MB reserved
2025-03-25 06:51:16,996 - training - INFO - Epoch: 68/200000, Batch: 30/45, Loss: 3.8390, Throughput: 71.44 samples/sec
2025-03-25 06:51:27,955 - training - INFO - Epoch 68 completed in 35.26s. Average loss: 3.8262
2025-03-25 06:51:27,958 - training - INFO - Starting epoch 69/200000
2025-03-25 06:51:28,683 - training - INFO - Memory: GPU 0: 3556.6MB allocated, 9646.0MB reserved
2025-03-25 06:51:28,683 - training - INFO - Epoch: 69/200000, Batch: 0/45, Loss: 4.0037, Throughput: 77.37 samples/sec
2025-03-25 06:51:40,537 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9500.0MB reserved
2025-03-25 06:51:40,537 - training - INFO - Epoch: 69/200000, Batch: 15/45, Loss: 3.7706, Throughput: 71.24 samples/sec
2025-03-25 06:51:52,180 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9500.0MB reserved
2025-03-25 06:51:52,180 - training - INFO - Epoch: 69/200000, Batch: 30/45, Loss: 3.8246, Throughput: 71.68 samples/sec
2025-03-25 06:52:03,142 - training - INFO - Epoch 69 completed in 35.18s. Average loss: 3.8108
2025-03-25 06:52:03,146 - training - INFO - Starting epoch 70/200000
2025-03-25 06:52:03,894 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9638.0MB reserved
2025-03-25 06:52:03,894 - training - INFO - Epoch: 70/200000, Batch: 0/45, Loss: 3.3967, Throughput: 75.16 samples/sec
2025-03-25 06:52:15,861 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9490.0MB reserved
2025-03-25 06:52:15,862 - training - INFO - Epoch: 70/200000, Batch: 15/45, Loss: 3.8245, Throughput: 70.48 samples/sec
2025-03-25 06:52:27,477 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9492.0MB reserved
2025-03-25 06:52:27,477 - training - INFO - Epoch: 70/200000, Batch: 30/45, Loss: 3.8143, Throughput: 71.35 samples/sec
2025-03-25 06:52:38,420 - training - INFO - Epoch 70 completed in 35.27s. Average loss: 3.7956
2025-03-25 06:52:38,424 - training - INFO - Starting epoch 71/200000
2025-03-25 06:52:39,157 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9630.0MB reserved
2025-03-25 06:52:39,157 - training - INFO - Epoch: 71/200000, Batch: 0/45, Loss: 3.8484, Throughput: 76.53 samples/sec
2025-03-25 06:52:51,039 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9496.0MB reserved
2025-03-25 06:52:51,040 - training - INFO - Epoch: 71/200000, Batch: 15/45, Loss: 3.6802, Throughput: 71.04 samples/sec
2025-03-25 06:53:02,582 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9496.0MB reserved
2025-03-25 06:53:02,582 - training - INFO - Epoch: 71/200000, Batch: 30/45, Loss: 3.7577, Throughput: 71.86 samples/sec
2025-03-25 06:53:13,481 - training - INFO - Epoch 71 completed in 35.06s. Average loss: 3.8177
2025-03-25 06:53:13,484 - training - INFO - Starting epoch 72/200000
2025-03-25 06:53:14,245 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9634.0MB reserved
2025-03-25 06:53:14,245 - training - INFO - Epoch: 72/200000, Batch: 0/45, Loss: 3.8349, Throughput: 73.74 samples/sec
2025-03-25 06:53:26,236 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9486.0MB reserved
2025-03-25 06:53:26,237 - training - INFO - Epoch: 72/200000, Batch: 15/45, Loss: 3.8281, Throughput: 70.27 samples/sec
2025-03-25 06:53:37,897 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9488.0MB reserved
2025-03-25 06:53:37,897 - training - INFO - Epoch: 72/200000, Batch: 30/45, Loss: 3.8047, Throughput: 71.12 samples/sec
2025-03-25 06:53:48,820 - training - INFO - Epoch 72 completed in 35.34s. Average loss: 3.7894
2025-03-25 06:53:48,824 - training - INFO - Starting epoch 73/200000
2025-03-25 06:53:49,572 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9626.0MB reserved
2025-03-25 06:53:49,572 - training - INFO - Epoch: 73/200000, Batch: 0/45, Loss: 3.9841, Throughput: 75.01 samples/sec
2025-03-25 06:54:01,482 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9506.0MB reserved
2025-03-25 06:54:01,483 - training - INFO - Epoch: 73/200000, Batch: 15/45, Loss: 3.8131, Throughput: 70.79 samples/sec
2025-03-25 06:54:13,079 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9506.0MB reserved
2025-03-25 06:54:13,079 - training - INFO - Epoch: 73/200000, Batch: 30/45, Loss: 3.8255, Throughput: 71.58 samples/sec
2025-03-25 06:54:24,070 - training - INFO - Epoch 73 completed in 35.25s. Average loss: 3.8282
2025-03-25 06:54:24,073 - training - INFO - Starting epoch 74/200000
2025-03-25 06:54:24,809 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9644.0MB reserved
2025-03-25 06:54:24,809 - training - INFO - Epoch: 74/200000, Batch: 0/45, Loss: 3.9670, Throughput: 76.28 samples/sec
2025-03-25 06:54:36,694 - training - INFO - Memory: GPU 0: 3562.6MB allocated, 9506.0MB reserved
2025-03-25 06:54:36,694 - training - INFO - Epoch: 74/200000, Batch: 15/45, Loss: 3.7869, Throughput: 71.01 samples/sec
2025-03-25 06:54:48,279 - training - INFO - Memory: GPU 0: 3562.6MB allocated, 9506.0MB reserved
2025-03-25 06:54:48,280 - training - INFO - Epoch: 74/200000, Batch: 30/45, Loss: 3.7977, Throughput: 71.72 samples/sec
2025-03-25 06:54:59,199 - training - INFO - Epoch 74 completed in 35.13s. Average loss: 3.7716
2025-03-25 06:54:59,202 - training - INFO - Starting epoch 75/200000
2025-03-25 06:54:59,949 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9646.0MB reserved
2025-03-25 06:54:59,949 - training - INFO - Epoch: 75/200000, Batch: 0/45, Loss: 3.6751, Throughput: 75.22 samples/sec
2025-03-25 06:55:11,904 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9498.0MB reserved
2025-03-25 06:55:11,905 - training - INFO - Epoch: 75/200000, Batch: 15/45, Loss: 3.8413, Throughput: 70.55 samples/sec
2025-03-25 06:55:23,606 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9498.0MB reserved
2025-03-25 06:55:23,606 - training - INFO - Epoch: 75/200000, Batch: 30/45, Loss: 3.8279, Throughput: 71.14 samples/sec
2025-03-25 06:55:34,552 - training - INFO - Epoch 75 completed in 35.35s. Average loss: 3.7797
2025-03-25 06:55:34,557 - training - INFO - Starting epoch 76/200000
2025-03-25 06:55:35,301 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9636.0MB reserved
2025-03-25 06:55:35,301 - training - INFO - Epoch: 76/200000, Batch: 0/45, Loss: 3.8041, Throughput: 75.32 samples/sec
2025-03-25 06:55:47,246 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9500.0MB reserved
2025-03-25 06:55:47,246 - training - INFO - Epoch: 76/200000, Batch: 15/45, Loss: 3.6821, Throughput: 70.62 samples/sec
2025-03-25 06:55:58,806 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9500.0MB reserved
2025-03-25 06:55:58,806 - training - INFO - Epoch: 76/200000, Batch: 30/45, Loss: 3.7314, Throughput: 71.59 samples/sec
2025-03-25 06:56:09,740 - training - INFO - Epoch 76 completed in 35.18s. Average loss: 3.7589
2025-03-25 06:56:09,744 - training - INFO - Starting epoch 77/200000
2025-03-25 06:56:10,489 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9638.0MB reserved
2025-03-25 06:56:10,489 - training - INFO - Epoch: 77/200000, Batch: 0/45, Loss: 3.5487, Throughput: 75.34 samples/sec
2025-03-25 06:56:22,482 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9496.0MB reserved
2025-03-25 06:56:22,482 - training - INFO - Epoch: 77/200000, Batch: 15/45, Loss: 3.6687, Throughput: 70.35 samples/sec
2025-03-25 06:56:34,167 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9496.0MB reserved
2025-03-25 06:56:34,168 - training - INFO - Epoch: 77/200000, Batch: 30/45, Loss: 3.7389, Throughput: 71.08 samples/sec
2025-03-25 06:56:45,175 - training - INFO - Epoch 77 completed in 35.43s. Average loss: 3.7389
2025-03-25 06:56:45,178 - training - INFO - Starting epoch 78/200000
2025-03-25 06:56:45,940 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9634.0MB reserved
2025-03-25 06:56:45,940 - training - INFO - Epoch: 78/200000, Batch: 0/45, Loss: 3.9553, Throughput: 73.64 samples/sec
2025-03-25 06:56:57,833 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9494.0MB reserved
2025-03-25 06:56:57,833 - training - INFO - Epoch: 78/200000, Batch: 15/45, Loss: 3.7989, Throughput: 70.81 samples/sec
2025-03-25 06:57:09,413 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9494.0MB reserved
2025-03-25 06:57:09,413 - training - INFO - Epoch: 78/200000, Batch: 30/45, Loss: 3.7770, Throughput: 71.64 samples/sec
2025-03-25 06:57:20,330 - training - INFO - Epoch 78 completed in 35.15s. Average loss: 3.7501
2025-03-25 06:57:20,334 - training - INFO - Starting epoch 79/200000
2025-03-25 06:57:21,105 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9632.0MB reserved
2025-03-25 06:57:21,106 - training - INFO - Epoch: 79/200000, Batch: 0/45, Loss: 3.6795, Throughput: 72.70 samples/sec
2025-03-25 06:57:33,084 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9510.0MB reserved
2025-03-25 06:57:33,084 - training - INFO - Epoch: 79/200000, Batch: 15/45, Loss: 3.8124, Throughput: 70.29 samples/sec
2025-03-25 06:57:44,726 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9510.0MB reserved
2025-03-25 06:57:44,726 - training - INFO - Epoch: 79/200000, Batch: 30/45, Loss: 3.7275, Throughput: 71.17 samples/sec
2025-03-25 06:57:55,727 - training - INFO - Epoch 79 completed in 35.39s. Average loss: 3.7648
2025-03-25 06:57:55,731 - training - INFO - Starting epoch 80/200000
2025-03-25 06:57:56,503 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9648.0MB reserved
2025-03-25 06:57:56,504 - training - INFO - Epoch: 80/200000, Batch: 0/45, Loss: 4.0419, Throughput: 72.57 samples/sec
2025-03-25 06:58:08,292 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9510.0MB reserved
2025-03-25 06:58:08,292 - training - INFO - Epoch: 80/200000, Batch: 15/45, Loss: 3.7025, Throughput: 71.34 samples/sec
2025-03-25 06:58:19,842 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9510.0MB reserved
2025-03-25 06:58:19,842 - training - INFO - Epoch: 80/200000, Batch: 30/45, Loss: 3.6202, Throughput: 72.00 samples/sec
2025-03-25 06:58:30,759 - training - INFO - Epoch 80 completed in 35.03s. Average loss: 3.7422
2025-03-25 06:58:30,762 - training - INFO - Starting epoch 81/200000
2025-03-25 06:58:31,513 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9648.0MB reserved
2025-03-25 06:58:31,513 - training - INFO - Epoch: 81/200000, Batch: 0/45, Loss: 3.6483, Throughput: 74.69 samples/sec
2025-03-25 06:58:43,407 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9496.0MB reserved
2025-03-25 06:58:43,407 - training - INFO - Epoch: 81/200000, Batch: 15/45, Loss: 3.6847, Throughput: 70.87 samples/sec
2025-03-25 06:58:54,991 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9498.0MB reserved
2025-03-25 06:58:54,991 - training - INFO - Epoch: 81/200000, Batch: 30/45, Loss: 3.7009, Throughput: 71.65 samples/sec
2025-03-25 06:59:05,966 - training - INFO - Epoch 81 completed in 35.20s. Average loss: 3.7433
2025-03-25 06:59:05,969 - training - INFO - Starting epoch 82/200000
2025-03-25 06:59:06,716 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9636.0MB reserved
2025-03-25 06:59:06,717 - training - INFO - Epoch: 82/200000, Batch: 0/45, Loss: 3.6349, Throughput: 75.15 samples/sec
2025-03-25 06:59:18,587 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9520.0MB reserved
2025-03-25 06:59:18,587 - training - INFO - Epoch: 82/200000, Batch: 15/45, Loss: 3.7911, Throughput: 71.02 samples/sec
2025-03-25 06:59:30,188 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9520.0MB reserved
2025-03-25 06:59:30,189 - training - INFO - Epoch: 82/200000, Batch: 30/45, Loss: 3.7624, Throughput: 71.69 samples/sec
2025-03-25 06:59:41,101 - training - INFO - Epoch 82 completed in 35.13s. Average loss: 3.7338
2025-03-25 06:59:41,105 - training - INFO - Starting epoch 83/200000
2025-03-25 06:59:41,848 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9658.0MB reserved
2025-03-25 06:59:41,849 - training - INFO - Epoch: 83/200000, Batch: 0/45, Loss: 3.3991, Throughput: 75.58 samples/sec
2025-03-25 06:59:53,756 - training - INFO - Memory: GPU 0: 3556.0MB allocated, 9496.0MB reserved
2025-03-25 06:59:53,756 - training - INFO - Epoch: 83/200000, Batch: 15/45, Loss: 3.7207, Throughput: 70.84 samples/sec
2025-03-25 07:00:05,401 - training - INFO - Memory: GPU 0: 3556.0MB allocated, 9498.0MB reserved
2025-03-25 07:00:05,401 - training - INFO - Epoch: 83/200000, Batch: 30/45, Loss: 3.6746, Throughput: 71.46 samples/sec
2025-03-25 07:00:16,359 - training - INFO - Epoch 83 completed in 35.25s. Average loss: 3.7013
2025-03-25 07:00:16,363 - training - INFO - Starting epoch 84/200000
2025-03-25 07:00:17,111 - training - INFO - Memory: GPU 0: 3556.0MB allocated, 9636.0MB reserved
2025-03-25 07:00:17,111 - training - INFO - Epoch: 84/200000, Batch: 0/45, Loss: 3.6407, Throughput: 74.87 samples/sec
2025-03-25 07:00:29,017 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9506.0MB reserved
2025-03-25 07:00:29,017 - training - INFO - Epoch: 84/200000, Batch: 15/45, Loss: 3.7356, Throughput: 70.82 samples/sec
2025-03-25 07:00:40,640 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9506.0MB reserved
2025-03-25 07:00:40,640 - training - INFO - Epoch: 84/200000, Batch: 30/45, Loss: 3.7121, Throughput: 71.51 samples/sec
2025-03-25 07:00:51,564 - training - INFO - Epoch 84 completed in 35.20s. Average loss: 3.6884
2025-03-25 07:00:51,568 - training - INFO - Starting epoch 85/200000
2025-03-25 07:00:52,326 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9644.0MB reserved
2025-03-25 07:00:52,327 - training - INFO - Epoch: 85/200000, Batch: 0/45, Loss: 3.8317, Throughput: 73.87 samples/sec
2025-03-25 07:01:04,173 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9500.0MB reserved
2025-03-25 07:01:04,174 - training - INFO - Epoch: 85/200000, Batch: 15/45, Loss: 3.7239, Throughput: 71.08 samples/sec
2025-03-25 07:01:15,735 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9500.0MB reserved
2025-03-25 07:01:15,735 - training - INFO - Epoch: 85/200000, Batch: 30/45, Loss: 3.6672, Throughput: 71.84 samples/sec
2025-03-25 07:01:26,623 - training - INFO - Epoch 85 completed in 35.06s. Average loss: 3.6817
2025-03-25 07:01:26,627 - training - INFO - Starting epoch 86/200000
2025-03-25 07:01:27,359 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9638.0MB reserved
2025-03-25 07:01:27,360 - training - INFO - Epoch: 86/200000, Batch: 0/45, Loss: 3.4523, Throughput: 76.67 samples/sec
2025-03-25 07:01:39,320 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9488.0MB reserved
2025-03-25 07:01:39,320 - training - INFO - Epoch: 86/200000, Batch: 15/45, Loss: 3.6053, Throughput: 70.60 samples/sec
2025-03-25 07:01:50,988 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9488.0MB reserved
2025-03-25 07:01:50,988 - training - INFO - Epoch: 86/200000, Batch: 30/45, Loss: 3.6733, Throughput: 71.26 samples/sec
2025-03-25 07:02:01,922 - training - INFO - Epoch 86 completed in 35.29s. Average loss: 3.6820
2025-03-25 07:02:01,926 - training - INFO - Starting epoch 87/200000
2025-03-25 07:02:02,701 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9626.0MB reserved
2025-03-25 07:02:02,701 - training - INFO - Epoch: 87/200000, Batch: 0/45, Loss: 3.7419, Throughput: 72.39 samples/sec
2025-03-25 07:02:14,593 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9500.0MB reserved
2025-03-25 07:02:14,594 - training - INFO - Epoch: 87/200000, Batch: 15/45, Loss: 3.7296, Throughput: 70.75 samples/sec
2025-03-25 07:02:26,126 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9502.0MB reserved
2025-03-25 07:02:26,126 - training - INFO - Epoch: 87/200000, Batch: 30/45, Loss: 3.6396, Throughput: 71.74 samples/sec
2025-03-25 07:02:37,028 - training - INFO - Epoch 87 completed in 35.10s. Average loss: 3.6827
2025-03-25 07:02:37,032 - training - INFO - Starting epoch 88/200000
2025-03-25 07:02:37,779 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9640.0MB reserved
2025-03-25 07:02:37,779 - training - INFO - Epoch: 88/200000, Batch: 0/45, Loss: 3.6747, Throughput: 75.10 samples/sec
2025-03-25 07:02:49,623 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9484.0MB reserved
2025-03-25 07:02:49,624 - training - INFO - Epoch: 88/200000, Batch: 15/45, Loss: 3.6704, Throughput: 71.16 samples/sec
2025-03-25 07:03:01,256 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9484.0MB reserved
2025-03-25 07:03:01,256 - training - INFO - Epoch: 88/200000, Batch: 30/45, Loss: 3.6684, Throughput: 71.67 samples/sec
2025-03-25 07:03:12,278 - training - INFO - Epoch 88 completed in 35.25s. Average loss: 3.6689
2025-03-25 07:03:12,282 - training - INFO - Starting epoch 89/200000
2025-03-25 07:03:13,026 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9624.0MB reserved
2025-03-25 07:03:13,026 - training - INFO - Epoch: 89/200000, Batch: 0/45, Loss: 3.4272, Throughput: 75.36 samples/sec
2025-03-25 07:03:24,931 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9494.0MB reserved
2025-03-25 07:03:24,931 - training - INFO - Epoch: 89/200000, Batch: 15/45, Loss: 3.5717, Throughput: 70.84 samples/sec
2025-03-25 07:03:36,543 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9494.0MB reserved
2025-03-25 07:03:36,543 - training - INFO - Epoch: 89/200000, Batch: 30/45, Loss: 3.5836, Throughput: 71.56 samples/sec
2025-03-25 07:03:47,470 - training - INFO - Epoch 89 completed in 35.19s. Average loss: 3.6574
2025-03-25 07:03:47,474 - training - INFO - Starting epoch 90/200000
2025-03-25 07:03:48,259 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9632.0MB reserved
2025-03-25 07:03:48,259 - training - INFO - Epoch: 90/200000, Batch: 0/45, Loss: 3.6505, Throughput: 71.47 samples/sec
2025-03-25 07:04:00,172 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9500.0MB reserved
2025-03-25 07:04:00,173 - training - INFO - Epoch: 90/200000, Batch: 15/45, Loss: 3.7257, Throughput: 70.57 samples/sec
2025-03-25 07:04:11,883 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9500.0MB reserved
2025-03-25 07:04:11,883 - training - INFO - Epoch: 90/200000, Batch: 30/45, Loss: 3.6996, Throughput: 71.12 samples/sec
2025-03-25 07:04:22,881 - training - INFO - Epoch 90 completed in 35.41s. Average loss: 3.6806
2025-03-25 07:04:22,885 - training - INFO - Starting epoch 91/200000
2025-03-25 07:04:23,641 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9638.0MB reserved
2025-03-25 07:04:23,641 - training - INFO - Epoch: 91/200000, Batch: 0/45, Loss: 3.4823, Throughput: 74.23 samples/sec
2025-03-25 07:04:35,587 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9500.0MB reserved
2025-03-25 07:04:35,587 - training - INFO - Epoch: 91/200000, Batch: 15/45, Loss: 3.6328, Throughput: 70.55 samples/sec
2025-03-25 07:04:47,272 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9502.0MB reserved
2025-03-25 07:04:47,273 - training - INFO - Epoch: 91/200000, Batch: 30/45, Loss: 3.6636, Throughput: 71.19 samples/sec
2025-03-25 07:04:58,245 - training - INFO - Epoch 91 completed in 35.36s. Average loss: 3.6246
2025-03-25 07:04:58,249 - training - INFO - Starting epoch 92/200000
2025-03-25 07:04:59,016 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9638.0MB reserved
2025-03-25 07:04:59,016 - training - INFO - Epoch: 92/200000, Batch: 0/45, Loss: 3.5341, Throughput: 73.06 samples/sec
2025-03-25 07:05:10,926 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9482.0MB reserved
2025-03-25 07:05:10,926 - training - INFO - Epoch: 92/200000, Batch: 15/45, Loss: 3.5298, Throughput: 70.69 samples/sec
2025-03-25 07:05:22,478 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9482.0MB reserved
2025-03-25 07:05:22,479 - training - INFO - Epoch: 92/200000, Batch: 30/45, Loss: 3.5959, Throughput: 71.65 samples/sec
2025-03-25 07:05:33,407 - training - INFO - Epoch 92 completed in 35.16s. Average loss: 3.6265
2025-03-25 07:05:33,412 - training - INFO - Starting epoch 93/200000
2025-03-25 07:05:34,170 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9620.0MB reserved
2025-03-25 07:05:34,171 - training - INFO - Epoch: 93/200000, Batch: 0/45, Loss: 3.6169, Throughput: 73.95 samples/sec
2025-03-25 07:05:46,084 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9500.0MB reserved
2025-03-25 07:05:46,084 - training - INFO - Epoch: 93/200000, Batch: 15/45, Loss: 3.5891, Throughput: 70.72 samples/sec
2025-03-25 07:05:57,637 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9500.0MB reserved
2025-03-25 07:05:57,637 - training - INFO - Epoch: 93/200000, Batch: 30/45, Loss: 3.6061, Throughput: 71.67 samples/sec
2025-03-25 07:06:08,578 - training - INFO - Epoch 93 completed in 35.17s. Average loss: 3.6307
2025-03-25 07:06:08,582 - training - INFO - Starting epoch 94/200000
2025-03-25 07:06:09,336 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9638.0MB reserved
2025-03-25 07:06:09,337 - training - INFO - Epoch: 94/200000, Batch: 0/45, Loss: 3.6725, Throughput: 74.29 samples/sec
2025-03-25 07:06:21,191 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9512.0MB reserved
2025-03-25 07:06:21,191 - training - INFO - Epoch: 94/200000, Batch: 15/45, Loss: 3.6070, Throughput: 71.07 samples/sec
2025-03-25 07:06:32,719 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9512.0MB reserved
2025-03-25 07:06:32,720 - training - INFO - Epoch: 94/200000, Batch: 30/45, Loss: 3.6011, Throughput: 71.93 samples/sec
2025-03-25 07:06:43,627 - training - INFO - Epoch 94 completed in 35.05s. Average loss: 3.6261
2025-03-25 07:06:43,631 - training - INFO - Starting epoch 95/200000
2025-03-25 07:06:44,400 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9650.0MB reserved
2025-03-25 07:06:44,400 - training - INFO - Epoch: 95/200000, Batch: 0/45, Loss: 3.1399, Throughput: 72.96 samples/sec
2025-03-25 07:06:56,342 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9500.0MB reserved
2025-03-25 07:06:56,343 - training - INFO - Epoch: 95/200000, Batch: 15/45, Loss: 3.5938, Throughput: 70.50 samples/sec
2025-03-25 07:07:08,056 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9500.0MB reserved
2025-03-25 07:07:08,057 - training - INFO - Epoch: 95/200000, Batch: 30/45, Loss: 3.5856, Throughput: 71.08 samples/sec
2025-03-25 07:07:18,977 - training - INFO - Epoch 95 completed in 35.35s. Average loss: 3.6282
2025-03-25 07:07:18,980 - training - INFO - Starting epoch 96/200000
2025-03-25 07:07:19,724 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9638.0MB reserved
2025-03-25 07:07:19,725 - training - INFO - Epoch: 96/200000, Batch: 0/45, Loss: 4.6665, Throughput: 75.45 samples/sec
2025-03-25 07:07:31,578 - training - INFO - Memory: GPU 0: 3556.2MB allocated, 9496.0MB reserved
2025-03-25 07:07:31,579 - training - INFO - Epoch: 96/200000, Batch: 15/45, Loss: 3.6246, Throughput: 71.13 samples/sec
2025-03-25 07:07:43,149 - training - INFO - Memory: GPU 0: 3556.2MB allocated, 9496.0MB reserved
2025-03-25 07:07:43,149 - training - INFO - Epoch: 96/200000, Batch: 30/45, Loss: 3.6552, Throughput: 71.84 samples/sec
2025-03-25 07:07:54,068 - training - INFO - Epoch 96 completed in 35.09s. Average loss: 3.6228
2025-03-25 07:07:54,072 - training - INFO - Starting epoch 97/200000
2025-03-25 07:07:54,818 - training - INFO - Memory: GPU 0: 3556.7MB allocated, 9636.0MB reserved
2025-03-25 07:07:54,819 - training - INFO - Epoch: 97/200000, Batch: 0/45, Loss: 3.8281, Throughput: 75.21 samples/sec
2025-03-25 07:08:06,719 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9496.0MB reserved
2025-03-25 07:08:06,719 - training - INFO - Epoch: 97/200000, Batch: 15/45, Loss: 3.5748, Throughput: 70.86 samples/sec
2025-03-25 07:08:18,248 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9496.0MB reserved
2025-03-25 07:08:18,248 - training - INFO - Epoch: 97/200000, Batch: 30/45, Loss: 3.5188, Throughput: 71.81 samples/sec
2025-03-25 07:08:29,158 - training - INFO - Epoch 97 completed in 35.09s. Average loss: 3.6027
2025-03-25 07:08:29,163 - training - INFO - Starting epoch 98/200000
2025-03-25 07:08:29,904 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9634.0MB reserved
2025-03-25 07:08:29,905 - training - INFO - Epoch: 98/200000, Batch: 0/45, Loss: 3.8919, Throughput: 75.71 samples/sec
2025-03-25 07:08:41,855 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9494.0MB reserved
2025-03-25 07:08:41,855 - training - INFO - Epoch: 98/200000, Batch: 15/45, Loss: 3.5465, Throughput: 70.60 samples/sec
2025-03-25 07:08:53,549 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9494.0MB reserved
2025-03-25 07:08:53,550 - training - INFO - Epoch: 98/200000, Batch: 30/45, Loss: 3.5576, Throughput: 71.19 samples/sec
2025-03-25 07:09:04,587 - training - INFO - Epoch 98 completed in 35.42s. Average loss: 3.5970
2025-03-25 07:09:04,591 - training - INFO - Starting epoch 99/200000
2025-03-25 07:09:05,342 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9630.0MB reserved
2025-03-25 07:09:05,342 - training - INFO - Epoch: 99/200000, Batch: 0/45, Loss: 3.5010, Throughput: 74.63 samples/sec
2025-03-25 07:09:17,406 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9400.0MB reserved
2025-03-25 07:09:17,407 - training - INFO - Epoch: 99/200000, Batch: 15/45, Loss: 3.5235, Throughput: 69.92 samples/sec
2025-03-25 07:09:29,172 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9400.0MB reserved
2025-03-25 07:09:29,173 - training - INFO - Epoch: 99/200000, Batch: 30/45, Loss: 3.5698, Throughput: 70.63 samples/sec
2025-03-25 07:09:40,200 - training - INFO - Epoch 99 completed in 35.61s. Average loss: 3.5984
2025-03-25 07:09:40,204 - training - INFO - Starting epoch 100/200000
2025-03-25 07:09:40,949 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9538.0MB reserved
2025-03-25 07:09:40,949 - training - INFO - Epoch: 100/200000, Batch: 0/45, Loss: 3.1189, Throughput: 75.28 samples/sec
2025-03-25 07:09:52,820 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9396.0MB reserved
2025-03-25 07:09:52,821 - training - INFO - Epoch: 100/200000, Batch: 15/45, Loss: 3.5257, Throughput: 71.03 samples/sec
2025-03-25 07:10:04,388 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9398.0MB reserved
2025-03-25 07:10:04,389 - training - INFO - Epoch: 100/200000, Batch: 30/45, Loss: 3.5632, Throughput: 71.79 samples/sec
2025-03-25 07:10:15,357 - training - INFO - Epoch 100 completed in 35.15s. Average loss: 3.5784
2025-03-25 07:10:15,361 - training - INFO - Starting validation...
2025-03-25 07:10:18,539 - training - INFO - Validation Loss: 7.7029
2025-03-25 07:10:18,539 - training - INFO - Validation loss improved from inf to 7.7029
2025-03-25 07:17:28,210 - training - INFO - New best model saved at epoch 100 (val_loss: 7.7029)
2025-03-25 07:17:28,233 - training - INFO - Starting epoch 101/200000
2025-03-25 07:17:29,018 - training - INFO - Memory: GPU 0: 3569.8MB allocated, 8736.0MB reserved
2025-03-25 07:17:29,018 - training - INFO - Epoch: 101/200000, Batch: 0/45, Loss: 3.6779, Throughput: 71.37 samples/sec
2025-03-25 07:17:40,754 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9462.0MB reserved
2025-03-25 07:17:40,754 - training - INFO - Epoch: 101/200000, Batch: 15/45, Loss: 3.6310, Throughput: 71.57 samples/sec
2025-03-25 07:17:52,372 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9462.0MB reserved
2025-03-25 07:17:52,373 - training - INFO - Epoch: 101/200000, Batch: 30/45, Loss: 3.6695, Throughput: 71.92 samples/sec
2025-03-25 07:18:03,313 - training - INFO - Epoch 101 completed in 35.08s. Average loss: 3.5811
2025-03-25 07:18:03,317 - training - INFO - Starting epoch 102/200000
2025-03-25 07:18:04,054 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9600.0MB reserved
2025-03-25 07:18:04,054 - training - INFO - Epoch: 102/200000, Batch: 0/45, Loss: 3.2904, Throughput: 76.14 samples/sec
2025-03-25 07:18:15,913 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9504.0MB reserved
2025-03-25 07:18:15,914 - training - INFO - Epoch: 102/200000, Batch: 15/45, Loss: 3.5650, Throughput: 71.15 samples/sec
2025-03-25 07:18:27,562 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9506.0MB reserved
2025-03-25 07:18:27,563 - training - INFO - Epoch: 102/200000, Batch: 30/45, Loss: 3.5791, Throughput: 71.61 samples/sec
2025-03-25 07:18:38,503 - training - INFO - Epoch 102 completed in 35.19s. Average loss: 3.6050
2025-03-25 07:18:38,507 - training - INFO - Starting epoch 103/200000
2025-03-25 07:18:39,246 - training - INFO - Memory: GPU 0: 3562.6MB allocated, 9644.0MB reserved
2025-03-25 07:18:39,246 - training - INFO - Epoch: 103/200000, Batch: 0/45, Loss: 3.6317, Throughput: 75.85 samples/sec
2025-03-25 07:18:51,110 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9490.0MB reserved
2025-03-25 07:18:51,110 - training - INFO - Epoch: 103/200000, Batch: 15/45, Loss: 3.4279, Throughput: 71.10 samples/sec
2025-03-25 07:19:02,747 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9490.0MB reserved
2025-03-25 07:19:02,748 - training - INFO - Epoch: 103/200000, Batch: 30/45, Loss: 3.5096, Throughput: 71.62 samples/sec
2025-03-25 07:19:13,721 - training - INFO - Epoch 103 completed in 35.21s. Average loss: 3.5445
2025-03-25 07:19:13,724 - training - INFO - Starting epoch 104/200000
2025-03-25 07:19:14,462 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9628.0MB reserved
2025-03-25 07:19:14,463 - training - INFO - Epoch: 104/200000, Batch: 0/45, Loss: 3.8049, Throughput: 76.07 samples/sec
2025-03-25 07:19:26,297 - training - INFO - Memory: GPU 0: 3562.5MB allocated, 9502.0MB reserved
2025-03-25 07:19:26,298 - training - INFO - Epoch: 104/200000, Batch: 15/45, Loss: 3.5425, Throughput: 71.27 samples/sec
2025-03-25 07:19:37,943 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9502.0MB reserved
2025-03-25 07:19:37,943 - training - INFO - Epoch: 104/200000, Batch: 30/45, Loss: 3.4900, Throughput: 71.68 samples/sec
2025-03-25 07:19:48,916 - training - INFO - Epoch 104 completed in 35.19s. Average loss: 3.5497
2025-03-25 07:19:48,920 - training - INFO - Starting epoch 105/200000
2025-03-25 07:19:49,667 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9640.0MB reserved
2025-03-25 07:19:49,667 - training - INFO - Epoch: 105/200000, Batch: 0/45, Loss: 3.6376, Throughput: 75.17 samples/sec
2025-03-25 07:20:01,597 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9504.0MB reserved
2025-03-25 07:20:01,597 - training - INFO - Epoch: 105/200000, Batch: 15/45, Loss: 3.5837, Throughput: 70.69 samples/sec
2025-03-25 07:20:13,151 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9504.0MB reserved
2025-03-25 07:20:13,152 - training - INFO - Epoch: 105/200000, Batch: 30/45, Loss: 3.5402, Throughput: 71.65 samples/sec
2025-03-25 07:20:24,093 - training - INFO - Epoch 105 completed in 35.17s. Average loss: 3.5443
2025-03-25 07:20:24,097 - training - INFO - Starting epoch 106/200000
2025-03-25 07:20:24,824 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9642.0MB reserved
2025-03-25 07:20:24,825 - training - INFO - Epoch: 106/200000, Batch: 0/45, Loss: 2.7592, Throughput: 77.09 samples/sec
2025-03-25 07:20:36,649 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9486.0MB reserved
2025-03-25 07:20:36,650 - training - INFO - Epoch: 106/200000, Batch: 15/45, Loss: 3.4118, Throughput: 71.39 samples/sec
2025-03-25 07:20:48,274 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9486.0MB reserved
2025-03-25 07:20:48,275 - training - INFO - Epoch: 106/200000, Batch: 30/45, Loss: 3.4577, Throughput: 71.81 samples/sec
2025-03-25 07:20:59,223 - training - INFO - Epoch 106 completed in 35.13s. Average loss: 3.5385
2025-03-25 07:20:59,226 - training - INFO - Starting epoch 107/200000
2025-03-25 07:20:59,967 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9624.0MB reserved
2025-03-25 07:20:59,968 - training - INFO - Epoch: 107/200000, Batch: 0/45, Loss: 3.2881, Throughput: 75.74 samples/sec
2025-03-25 07:21:11,888 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9486.0MB reserved
2025-03-25 07:21:11,889 - training - INFO - Epoch: 107/200000, Batch: 15/45, Loss: 3.4721, Throughput: 70.77 samples/sec
2025-03-25 07:21:23,422 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9486.0MB reserved
2025-03-25 07:21:23,423 - training - INFO - Epoch: 107/200000, Batch: 30/45, Loss: 3.4608, Throughput: 71.75 samples/sec
2025-03-25 07:21:34,326 - training - INFO - Epoch 107 completed in 35.10s. Average loss: 3.5052
2025-03-25 07:21:34,330 - training - INFO - Starting epoch 108/200000
2025-03-25 07:21:35,069 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9624.0MB reserved
2025-03-25 07:21:35,126 - training - INFO - Epoch: 108/200000, Batch: 0/45, Loss: 3.8320, Throughput: 75.95 samples/sec
2025-03-25 07:21:46,903 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9388.0MB reserved
2025-03-25 07:21:46,903 - training - INFO - Epoch: 108/200000, Batch: 15/45, Loss: 3.5618, Throughput: 71.27 samples/sec
2025-03-25 07:21:58,468 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9388.0MB reserved
2025-03-25 07:21:58,469 - training - INFO - Epoch: 108/200000, Batch: 30/45, Loss: 3.5495, Throughput: 71.93 samples/sec
2025-03-25 07:22:09,397 - training - INFO - Epoch 108 completed in 35.07s. Average loss: 3.5277
2025-03-25 07:22:09,400 - training - INFO - Starting epoch 109/200000
2025-03-25 07:22:10,125 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9526.0MB reserved
2025-03-25 07:22:10,125 - training - INFO - Epoch: 109/200000, Batch: 0/45, Loss: 3.1886, Throughput: 77.34 samples/sec
2025-03-25 07:22:22,030 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9488.0MB reserved
2025-03-25 07:22:22,030 - training - INFO - Epoch: 109/200000, Batch: 15/45, Loss: 3.4778, Throughput: 70.96 samples/sec
2025-03-25 07:22:33,686 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9488.0MB reserved
2025-03-25 07:22:33,686 - training - INFO - Epoch: 109/200000, Batch: 30/45, Loss: 3.5270, Throughput: 71.49 samples/sec
2025-03-25 07:22:44,652 - training - INFO - Epoch 109 completed in 35.25s. Average loss: 3.5287
2025-03-25 07:22:44,656 - training - INFO - Starting epoch 110/200000
2025-03-25 07:22:45,402 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9626.0MB reserved
2025-03-25 07:22:45,403 - training - INFO - Epoch: 110/200000, Batch: 0/45, Loss: 3.5068, Throughput: 75.06 samples/sec
2025-03-25 07:22:57,441 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9490.0MB reserved
2025-03-25 07:22:57,441 - training - INFO - Epoch: 110/200000, Batch: 15/45, Loss: 3.4499, Throughput: 70.09 samples/sec
2025-03-25 07:23:09,092 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9490.0MB reserved
2025-03-25 07:23:09,092 - training - INFO - Epoch: 110/200000, Batch: 30/45, Loss: 3.4640, Throughput: 71.05 samples/sec
2025-03-25 07:23:20,085 - training - INFO - Epoch 110 completed in 35.43s. Average loss: 3.5066
2025-03-25 07:23:20,089 - training - INFO - Starting epoch 111/200000
2025-03-25 07:23:20,830 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9626.0MB reserved
2025-03-25 07:23:20,830 - training - INFO - Epoch: 111/200000, Batch: 0/45, Loss: 3.5556, Throughput: 75.76 samples/sec
2025-03-25 07:23:32,701 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9500.0MB reserved
2025-03-25 07:23:32,701 - training - INFO - Epoch: 111/200000, Batch: 15/45, Loss: 3.6496, Throughput: 71.06 samples/sec
2025-03-25 07:23:44,285 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9502.0MB reserved
2025-03-25 07:23:44,285 - training - INFO - Epoch: 111/200000, Batch: 30/45, Loss: 3.5897, Throughput: 71.75 samples/sec
2025-03-25 07:23:55,195 - training - INFO - Epoch 111 completed in 35.11s. Average loss: 3.4857
2025-03-25 07:23:55,199 - training - INFO - Starting epoch 112/200000
2025-03-25 07:23:55,943 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9640.0MB reserved
2025-03-25 07:23:55,943 - training - INFO - Epoch: 112/200000, Batch: 0/45, Loss: 3.4901, Throughput: 75.40 samples/sec
2025-03-25 07:24:07,845 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9502.0MB reserved
2025-03-25 07:24:07,845 - training - INFO - Epoch: 112/200000, Batch: 15/45, Loss: 3.5716, Throughput: 70.86 samples/sec
2025-03-25 07:24:19,578 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9502.0MB reserved
2025-03-25 07:24:19,579 - training - INFO - Epoch: 112/200000, Batch: 30/45, Loss: 3.4821, Throughput: 71.21 samples/sec
2025-03-25 07:24:30,504 - training - INFO - Epoch 112 completed in 35.31s. Average loss: 3.4779
2025-03-25 07:24:30,508 - training - INFO - Starting epoch 113/200000
2025-03-25 07:24:31,243 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9640.0MB reserved
2025-03-25 07:24:31,243 - training - INFO - Epoch: 113/200000, Batch: 0/45, Loss: 3.5291, Throughput: 76.31 samples/sec
2025-03-25 07:24:43,178 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9490.0MB reserved
2025-03-25 07:24:43,178 - training - INFO - Epoch: 113/200000, Batch: 15/45, Loss: 3.4102, Throughput: 70.73 samples/sec
2025-03-25 07:24:54,823 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9490.0MB reserved
2025-03-25 07:24:54,824 - training - INFO - Epoch: 113/200000, Batch: 30/45, Loss: 3.4257, Throughput: 71.40 samples/sec
2025-03-25 07:25:05,792 - training - INFO - Epoch 113 completed in 35.28s. Average loss: 3.4657
2025-03-25 07:25:05,796 - training - INFO - Starting epoch 114/200000
2025-03-25 07:25:06,543 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9628.0MB reserved
2025-03-25 07:25:06,543 - training - INFO - Epoch: 114/200000, Batch: 0/45, Loss: 3.7638, Throughput: 75.06 samples/sec
2025-03-25 07:25:18,539 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9504.0MB reserved
2025-03-25 07:25:18,539 - training - INFO - Epoch: 114/200000, Batch: 15/45, Loss: 3.4030, Throughput: 70.33 samples/sec
2025-03-25 07:25:30,188 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9504.0MB reserved
2025-03-25 07:25:30,188 - training - INFO - Epoch: 114/200000, Batch: 30/45, Loss: 3.4396, Throughput: 71.17 samples/sec
2025-03-25 07:25:41,116 - training - INFO - Epoch 114 completed in 35.32s. Average loss: 3.4698
2025-03-25 07:25:41,130 - training - INFO - Starting epoch 115/200000
2025-03-25 07:25:41,851 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9642.0MB reserved
2025-03-25 07:25:41,851 - training - INFO - Epoch: 115/200000, Batch: 0/45, Loss: 3.6343, Throughput: 77.71 samples/sec
2025-03-25 07:25:53,689 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9500.0MB reserved
2025-03-25 07:25:53,690 - training - INFO - Epoch: 115/200000, Batch: 15/45, Loss: 3.3349, Throughput: 71.35 samples/sec
2025-03-25 07:26:05,238 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9500.0MB reserved
2025-03-25 07:26:05,239 - training - INFO - Epoch: 115/200000, Batch: 30/45, Loss: 3.4416, Throughput: 72.01 samples/sec
2025-03-25 07:26:16,131 - training - INFO - Epoch 115 completed in 35.00s. Average loss: 3.4444
2025-03-25 07:26:16,135 - training - INFO - Starting epoch 116/200000
2025-03-25 07:26:16,863 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9638.0MB reserved
2025-03-25 07:26:16,864 - training - INFO - Epoch: 116/200000, Batch: 0/45, Loss: 3.8861, Throughput: 76.95 samples/sec
2025-03-25 07:26:28,724 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9500.0MB reserved
2025-03-25 07:26:28,725 - training - INFO - Epoch: 116/200000, Batch: 15/45, Loss: 3.4611, Throughput: 71.18 samples/sec
2025-03-25 07:26:40,336 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9502.0MB reserved
2025-03-25 07:26:40,337 - training - INFO - Epoch: 116/200000, Batch: 30/45, Loss: 3.4443, Throughput: 71.74 samples/sec
2025-03-25 07:26:51,284 - training - INFO - Epoch 116 completed in 35.15s. Average loss: 3.4978
2025-03-25 07:26:51,288 - training - INFO - Starting epoch 117/200000
2025-03-25 07:26:52,036 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9640.0MB reserved
2025-03-25 07:26:52,037 - training - INFO - Epoch: 117/200000, Batch: 0/45, Loss: 3.2181, Throughput: 75.04 samples/sec
2025-03-25 07:27:03,933 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9404.0MB reserved
2025-03-25 07:27:03,933 - training - INFO - Epoch: 117/200000, Batch: 15/45, Loss: 3.5558, Throughput: 70.87 samples/sec
2025-03-25 07:27:15,624 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9404.0MB reserved
2025-03-25 07:27:15,624 - training - INFO - Epoch: 117/200000, Batch: 30/45, Loss: 3.5049, Throughput: 71.34 samples/sec
2025-03-25 07:27:26,567 - training - INFO - Epoch 117 completed in 35.28s. Average loss: 3.4685
2025-03-25 07:27:26,571 - training - INFO - Starting epoch 118/200000
2025-03-25 07:27:27,324 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9542.0MB reserved
2025-03-25 07:27:27,325 - training - INFO - Epoch: 118/200000, Batch: 0/45, Loss: 4.0421, Throughput: 74.44 samples/sec
2025-03-25 07:27:39,176 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9506.0MB reserved
2025-03-25 07:27:39,177 - training - INFO - Epoch: 118/200000, Batch: 15/45, Loss: 3.4882, Throughput: 71.09 samples/sec
2025-03-25 07:27:50,737 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9506.0MB reserved
2025-03-25 07:27:50,737 - training - INFO - Epoch: 118/200000, Batch: 30/45, Loss: 3.4843, Throughput: 71.84 samples/sec
2025-03-25 07:28:01,674 - training - INFO - Epoch 118 completed in 35.10s. Average loss: 3.4540
2025-03-25 07:28:01,678 - training - INFO - Starting epoch 119/200000
2025-03-25 07:28:02,420 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9644.0MB reserved
2025-03-25 07:28:02,420 - training - INFO - Epoch: 119/200000, Batch: 0/45, Loss: 3.5353, Throughput: 75.63 samples/sec
2025-03-25 07:28:14,279 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9514.0MB reserved
2025-03-25 07:28:14,280 - training - INFO - Epoch: 119/200000, Batch: 15/45, Loss: 3.4358, Throughput: 71.11 samples/sec
2025-03-25 07:28:25,861 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9514.0MB reserved
2025-03-25 07:28:25,862 - training - INFO - Epoch: 119/200000, Batch: 30/45, Loss: 3.4221, Throughput: 71.79 samples/sec
2025-03-25 07:28:36,794 - training - INFO - Epoch 119 completed in 35.12s. Average loss: 3.4554
2025-03-25 07:28:36,798 - training - INFO - Starting epoch 120/200000
2025-03-25 07:28:37,562 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9652.0MB reserved
2025-03-25 07:28:37,562 - training - INFO - Epoch: 120/200000, Batch: 0/45, Loss: 3.5321, Throughput: 73.43 samples/sec
2025-03-25 07:28:49,437 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9510.0MB reserved
2025-03-25 07:28:49,438 - training - INFO - Epoch: 120/200000, Batch: 15/45, Loss: 3.4748, Throughput: 70.90 samples/sec
2025-03-25 07:29:01,151 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9510.0MB reserved
2025-03-25 07:29:01,151 - training - INFO - Epoch: 120/200000, Batch: 30/45, Loss: 3.4650, Throughput: 71.29 samples/sec
2025-03-25 07:29:12,098 - training - INFO - Epoch 120 completed in 35.30s. Average loss: 3.4394
2025-03-25 07:29:12,102 - training - INFO - Starting epoch 121/200000
2025-03-25 07:29:12,834 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9646.0MB reserved
2025-03-25 07:29:12,834 - training - INFO - Epoch: 121/200000, Batch: 0/45, Loss: 3.3149, Throughput: 76.60 samples/sec
2025-03-25 07:29:24,646 - training - INFO - Memory: GPU 0: 3562.7MB allocated, 9518.0MB reserved
2025-03-25 07:29:24,646 - training - INFO - Epoch: 121/200000, Batch: 15/45, Loss: 3.3757, Throughput: 71.43 samples/sec
2025-03-25 07:29:36,200 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9518.0MB reserved
2025-03-25 07:29:36,201 - training - INFO - Epoch: 121/200000, Batch: 30/45, Loss: 3.4215, Throughput: 72.04 samples/sec
2025-03-25 07:29:47,093 - training - INFO - Epoch 121 completed in 34.99s. Average loss: 3.4380
2025-03-25 07:29:47,097 - training - INFO - Starting epoch 122/200000
2025-03-25 07:29:47,831 - training - INFO - Memory: GPU 0: 3562.7MB allocated, 9656.0MB reserved
2025-03-25 07:29:47,831 - training - INFO - Epoch: 122/200000, Batch: 0/45, Loss: 3.3527, Throughput: 76.47 samples/sec
2025-03-25 07:29:59,710 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9490.0MB reserved
2025-03-25 07:29:59,710 - training - INFO - Epoch: 122/200000, Batch: 15/45, Loss: 3.3321, Throughput: 71.05 samples/sec
2025-03-25 07:30:11,317 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9492.0MB reserved
2025-03-25 07:30:11,317 - training - INFO - Epoch: 122/200000, Batch: 30/45, Loss: 3.3651, Throughput: 71.68 samples/sec
2025-03-25 07:30:22,286 - training - INFO - Epoch 122 completed in 35.19s. Average loss: 3.4236
2025-03-25 07:30:22,290 - training - INFO - Starting epoch 123/200000
2025-03-25 07:30:23,026 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9630.0MB reserved
2025-03-25 07:30:23,026 - training - INFO - Epoch: 123/200000, Batch: 0/45, Loss: 3.3889, Throughput: 76.25 samples/sec
2025-03-25 07:30:34,957 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9496.0MB reserved
2025-03-25 07:30:34,958 - training - INFO - Epoch: 123/200000, Batch: 15/45, Loss: 3.3067, Throughput: 70.74 samples/sec
2025-03-25 07:30:46,557 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9496.0MB reserved
2025-03-25 07:30:46,558 - training - INFO - Epoch: 123/200000, Batch: 30/45, Loss: 3.4021, Throughput: 71.54 samples/sec
2025-03-25 07:30:57,477 - training - INFO - Epoch 123 completed in 35.19s. Average loss: 3.3995
2025-03-25 07:30:57,480 - training - INFO - Starting epoch 124/200000
2025-03-25 07:30:58,204 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9632.0MB reserved
2025-03-25 07:30:58,204 - training - INFO - Epoch: 124/200000, Batch: 0/45, Loss: 3.6591, Throughput: 77.63 samples/sec
2025-03-25 07:31:10,188 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9510.0MB reserved
2025-03-25 07:31:10,189 - training - INFO - Epoch: 124/200000, Batch: 15/45, Loss: 3.3654, Throughput: 70.51 samples/sec
2025-03-25 07:31:21,843 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9510.0MB reserved
2025-03-25 07:31:21,843 - training - INFO - Epoch: 124/200000, Batch: 30/45, Loss: 3.4260, Throughput: 71.26 samples/sec
2025-03-25 07:31:32,832 - training - INFO - Epoch 124 completed in 35.35s. Average loss: 3.4256
2025-03-25 07:31:32,835 - training - INFO - Starting epoch 125/200000
2025-03-25 07:31:33,580 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9648.0MB reserved
2025-03-25 07:31:33,580 - training - INFO - Epoch: 125/200000, Batch: 0/45, Loss: 3.1008, Throughput: 75.32 samples/sec
2025-03-25 07:31:45,571 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9504.0MB reserved
2025-03-25 07:31:45,572 - training - INFO - Epoch: 125/200000, Batch: 15/45, Loss: 3.4479, Throughput: 70.36 samples/sec
2025-03-25 07:31:57,131 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9504.0MB reserved
2025-03-25 07:31:57,132 - training - INFO - Epoch: 125/200000, Batch: 30/45, Loss: 3.3891, Throughput: 71.46 samples/sec
2025-03-25 07:32:08,035 - training - INFO - Epoch 125 completed in 35.20s. Average loss: 3.3876
2025-03-25 07:32:08,039 - training - INFO - Starting epoch 126/200000
2025-03-25 07:32:08,774 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9642.0MB reserved
2025-03-25 07:32:08,774 - training - INFO - Epoch: 126/200000, Batch: 0/45, Loss: 3.5633, Throughput: 76.35 samples/sec
2025-03-25 07:32:20,695 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9508.0MB reserved
2025-03-25 07:32:20,695 - training - INFO - Epoch: 126/200000, Batch: 15/45, Loss: 3.4426, Throughput: 70.80 samples/sec
2025-03-25 07:32:32,312 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9508.0MB reserved
2025-03-25 07:32:32,312 - training - INFO - Epoch: 126/200000, Batch: 30/45, Loss: 3.4463, Throughput: 71.53 samples/sec
2025-03-25 07:32:43,249 - training - INFO - Epoch 126 completed in 35.21s. Average loss: 3.4628
2025-03-25 07:32:43,253 - training - INFO - Starting epoch 127/200000
2025-03-25 07:32:44,006 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9646.0MB reserved
2025-03-25 07:32:44,006 - training - INFO - Epoch: 127/200000, Batch: 0/45, Loss: 3.2700, Throughput: 74.54 samples/sec
2025-03-25 07:32:55,948 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9510.0MB reserved
2025-03-25 07:32:55,949 - training - INFO - Epoch: 127/200000, Batch: 15/45, Loss: 3.4737, Throughput: 70.59 samples/sec
2025-03-25 07:33:07,485 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9512.0MB reserved
2025-03-25 07:33:07,485 - training - INFO - Epoch: 127/200000, Batch: 30/45, Loss: 3.4110, Throughput: 71.64 samples/sec
2025-03-25 07:33:18,401 - training - INFO - Epoch 127 completed in 35.15s. Average loss: 3.3778
2025-03-25 07:33:18,407 - training - INFO - Starting epoch 128/200000
2025-03-25 07:33:19,148 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9650.0MB reserved
2025-03-25 07:33:19,149 - training - INFO - Epoch: 128/200000, Batch: 0/45, Loss: 3.0979, Throughput: 75.58 samples/sec
2025-03-25 07:33:31,102 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9494.0MB reserved
2025-03-25 07:33:31,103 - training - INFO - Epoch: 128/200000, Batch: 15/45, Loss: 3.4130, Throughput: 70.58 samples/sec
2025-03-25 07:33:42,782 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9494.0MB reserved
2025-03-25 07:33:42,783 - training - INFO - Epoch: 128/200000, Batch: 30/45, Loss: 3.4153, Throughput: 71.22 samples/sec
2025-03-25 07:33:53,765 - training - INFO - Epoch 128 completed in 35.36s. Average loss: 3.4181
2025-03-25 07:33:53,769 - training - INFO - Starting epoch 129/200000
2025-03-25 07:33:54,507 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9632.0MB reserved
2025-03-25 07:33:54,507 - training - INFO - Epoch: 129/200000, Batch: 0/45, Loss: 3.5303, Throughput: 75.96 samples/sec
2025-03-25 07:34:06,464 - training - INFO - Memory: GPU 0: 3556.6MB allocated, 9496.0MB reserved
2025-03-25 07:34:06,465 - training - INFO - Epoch: 129/200000, Batch: 15/45, Loss: 3.3988, Throughput: 70.59 samples/sec
2025-03-25 07:34:18,104 - training - INFO - Memory: GPU 0: 3556.6MB allocated, 9496.0MB reserved
2025-03-25 07:34:18,104 - training - INFO - Epoch: 129/200000, Batch: 30/45, Loss: 3.4053, Throughput: 71.34 samples/sec
2025-03-25 07:34:29,058 - training - INFO - Epoch 129 completed in 35.29s. Average loss: 3.3766
2025-03-25 07:34:29,062 - training - INFO - Starting epoch 130/200000
2025-03-25 07:34:29,808 - training - INFO - Memory: GPU 0: 3557.2MB allocated, 9634.0MB reserved
2025-03-25 07:34:29,808 - training - INFO - Epoch: 130/200000, Batch: 0/45, Loss: 3.5371, Throughput: 75.30 samples/sec
2025-03-25 07:34:41,841 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9510.0MB reserved
2025-03-25 07:34:41,842 - training - INFO - Epoch: 130/200000, Batch: 15/45, Loss: 3.4764, Throughput: 70.12 samples/sec
2025-03-25 07:34:53,514 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9510.0MB reserved
2025-03-25 07:34:53,514 - training - INFO - Epoch: 130/200000, Batch: 30/45, Loss: 3.4227, Throughput: 71.00 samples/sec
2025-03-25 07:35:04,414 - training - INFO - Epoch 130 completed in 35.35s. Average loss: 3.4029
2025-03-25 07:35:04,417 - training - INFO - Starting epoch 131/200000
2025-03-25 07:35:05,156 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9648.0MB reserved
2025-03-25 07:35:05,156 - training - INFO - Epoch: 131/200000, Batch: 0/45, Loss: 3.3621, Throughput: 76.05 samples/sec
2025-03-25 07:35:17,092 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9498.0MB reserved
2025-03-25 07:35:17,092 - training - INFO - Epoch: 131/200000, Batch: 15/45, Loss: 3.3919, Throughput: 70.70 samples/sec
2025-03-25 07:35:28,742 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9498.0MB reserved
2025-03-25 07:35:28,742 - training - INFO - Epoch: 131/200000, Batch: 30/45, Loss: 3.3475, Throughput: 71.37 samples/sec
2025-03-25 07:35:39,706 - training - INFO - Epoch 131 completed in 35.29s. Average loss: 3.3376
2025-03-25 07:35:39,710 - training - INFO - Starting epoch 132/200000
2025-03-25 07:35:40,442 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9636.0MB reserved
2025-03-25 07:35:40,442 - training - INFO - Epoch: 132/200000, Batch: 0/45, Loss: 3.5330, Throughput: 76.65 samples/sec
2025-03-25 07:35:52,328 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9492.0MB reserved
2025-03-25 07:35:52,329 - training - INFO - Epoch: 132/200000, Batch: 15/45, Loss: 3.4601, Throughput: 71.02 samples/sec
2025-03-25 07:36:03,937 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9492.0MB reserved
2025-03-25 07:36:03,937 - training - INFO - Epoch: 132/200000, Batch: 30/45, Loss: 3.3970, Throughput: 71.66 samples/sec
2025-03-25 07:36:14,855 - training - INFO - Epoch 132 completed in 35.15s. Average loss: 3.4020
2025-03-25 07:36:14,859 - training - INFO - Starting epoch 133/200000
2025-03-25 07:36:15,604 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9630.0MB reserved
2025-03-25 07:36:15,605 - training - INFO - Epoch: 133/200000, Batch: 0/45, Loss: 2.8357, Throughput: 75.25 samples/sec
2025-03-25 07:36:27,520 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9494.0MB reserved
2025-03-25 07:36:27,521 - training - INFO - Epoch: 133/200000, Batch: 15/45, Loss: 3.2808, Throughput: 70.77 samples/sec
2025-03-25 07:36:39,121 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9494.0MB reserved
2025-03-25 07:36:39,122 - training - INFO - Epoch: 133/200000, Batch: 30/45, Loss: 3.3575, Throughput: 71.56 samples/sec
2025-03-25 07:36:50,088 - training - INFO - Epoch 133 completed in 35.23s. Average loss: 3.3381
2025-03-25 07:36:50,092 - training - INFO - Starting epoch 134/200000
2025-03-25 07:36:50,825 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9630.0MB reserved
2025-03-25 07:36:50,825 - training - INFO - Epoch: 134/200000, Batch: 0/45, Loss: 3.1364, Throughput: 76.45 samples/sec
2025-03-25 07:37:02,806 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9494.0MB reserved
2025-03-25 07:37:02,806 - training - INFO - Epoch: 134/200000, Batch: 15/45, Loss: 3.3362, Throughput: 70.48 samples/sec
2025-03-25 07:37:14,501 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9496.0MB reserved
2025-03-25 07:37:14,502 - training - INFO - Epoch: 134/200000, Batch: 30/45, Loss: 3.3327, Throughput: 71.13 samples/sec
2025-03-25 07:37:25,451 - training - INFO - Epoch 134 completed in 35.36s. Average loss: 3.3404
2025-03-25 07:37:25,455 - training - INFO - Starting epoch 135/200000
2025-03-25 07:37:26,204 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9634.0MB reserved
2025-03-25 07:37:26,205 - training - INFO - Epoch: 135/200000, Batch: 0/45, Loss: 3.7533, Throughput: 74.76 samples/sec
2025-03-25 07:37:38,031 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9510.0MB reserved
2025-03-25 07:37:38,031 - training - INFO - Epoch: 135/200000, Batch: 15/45, Loss: 3.4874, Throughput: 71.26 samples/sec
2025-03-25 07:37:49,613 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9510.0MB reserved
2025-03-25 07:37:49,613 - training - INFO - Epoch: 135/200000, Batch: 30/45, Loss: 3.3478, Throughput: 71.86 samples/sec
2025-03-25 07:38:00,536 - training - INFO - Epoch 135 completed in 35.08s. Average loss: 3.3921
2025-03-25 07:38:00,540 - training - INFO - Starting epoch 136/200000
2025-03-25 07:38:01,284 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9650.0MB reserved
2025-03-25 07:38:01,284 - training - INFO - Epoch: 136/200000, Batch: 0/45, Loss: 3.4602, Throughput: 75.32 samples/sec
2025-03-25 07:38:13,220 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9516.0MB reserved
2025-03-25 07:38:13,221 - training - INFO - Epoch: 136/200000, Batch: 15/45, Loss: 3.3540, Throughput: 70.67 samples/sec
2025-03-25 07:38:24,795 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9516.0MB reserved
2025-03-25 07:38:24,796 - training - INFO - Epoch: 136/200000, Batch: 30/45, Loss: 3.2833, Throughput: 71.58 samples/sec
2025-03-25 07:38:35,743 - training - INFO - Epoch 136 completed in 35.20s. Average loss: 3.3300
2025-03-25 07:38:35,746 - training - INFO - Starting epoch 137/200000
2025-03-25 07:38:36,498 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9654.0MB reserved
2025-03-25 07:38:36,498 - training - INFO - Epoch: 137/200000, Batch: 0/45, Loss: 3.6959, Throughput: 74.71 samples/sec
2025-03-25 07:38:48,424 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9486.0MB reserved
2025-03-25 07:38:48,424 - training - INFO - Epoch: 137/200000, Batch: 15/45, Loss: 3.3027, Throughput: 70.69 samples/sec
2025-03-25 07:39:00,036 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9486.0MB reserved
2025-03-25 07:39:00,037 - training - INFO - Epoch: 137/200000, Batch: 30/45, Loss: 3.3017, Throughput: 71.48 samples/sec
2025-03-25 07:39:11,052 - training - INFO - Epoch 137 completed in 35.31s. Average loss: 3.3408
2025-03-25 07:39:11,056 - training - INFO - Starting epoch 138/200000
2025-03-25 07:39:11,792 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9622.0MB reserved
2025-03-25 07:39:11,792 - training - INFO - Epoch: 138/200000, Batch: 0/45, Loss: 3.5466, Throughput: 76.27 samples/sec
2025-03-25 07:39:23,790 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9500.0MB reserved
2025-03-25 07:39:23,791 - training - INFO - Epoch: 138/200000, Batch: 15/45, Loss: 3.2916, Throughput: 70.37 samples/sec
2025-03-25 07:39:35,568 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9500.0MB reserved
2025-03-25 07:39:35,569 - training - INFO - Epoch: 138/200000, Batch: 30/45, Loss: 3.2416, Throughput: 70.83 samples/sec
2025-03-25 07:39:46,527 - training - INFO - Epoch 138 completed in 35.47s. Average loss: 3.3289
2025-03-25 07:39:46,531 - training - INFO - Starting epoch 139/200000
2025-03-25 07:39:47,280 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9638.0MB reserved
2025-03-25 07:39:47,280 - training - INFO - Epoch: 139/200000, Batch: 0/45, Loss: 3.4548, Throughput: 74.86 samples/sec
2025-03-25 07:39:59,189 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9492.0MB reserved
2025-03-25 07:39:59,189 - training - INFO - Epoch: 139/200000, Batch: 15/45, Loss: 3.2704, Throughput: 70.79 samples/sec
2025-03-25 07:40:10,841 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9492.0MB reserved
2025-03-25 07:40:10,841 - training - INFO - Epoch: 139/200000, Batch: 30/45, Loss: 3.2615, Throughput: 71.42 samples/sec
2025-03-25 07:40:21,822 - training - INFO - Epoch 139 completed in 35.29s. Average loss: 3.3006
2025-03-25 07:40:21,825 - training - INFO - Starting epoch 140/200000
2025-03-25 07:40:22,556 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9630.0MB reserved
2025-03-25 07:40:22,556 - training - INFO - Epoch: 140/200000, Batch: 0/45, Loss: 3.3113, Throughput: 76.76 samples/sec
2025-03-25 07:40:34,580 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9504.0MB reserved
2025-03-25 07:40:34,580 - training - INFO - Epoch: 140/200000, Batch: 15/45, Loss: 3.3587, Throughput: 70.26 samples/sec
2025-03-25 07:40:46,298 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9504.0MB reserved
2025-03-25 07:40:46,299 - training - INFO - Epoch: 140/200000, Batch: 30/45, Loss: 3.3684, Throughput: 70.94 samples/sec
2025-03-25 07:40:57,264 - training - INFO - Epoch 140 completed in 35.44s. Average loss: 3.3479
2025-03-25 07:40:57,268 - training - INFO - Starting epoch 141/200000
2025-03-25 07:40:58,013 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9642.0MB reserved
2025-03-25 07:40:58,013 - training - INFO - Epoch: 141/200000, Batch: 0/45, Loss: 2.8755, Throughput: 75.26 samples/sec
2025-03-25 07:41:09,833 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9512.0MB reserved
2025-03-25 07:41:09,833 - training - INFO - Epoch: 141/200000, Batch: 15/45, Loss: 3.3691, Throughput: 71.32 samples/sec
2025-03-25 07:41:21,373 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9512.0MB reserved
2025-03-25 07:41:21,373 - training - INFO - Epoch: 141/200000, Batch: 30/45, Loss: 3.3337, Throughput: 72.03 samples/sec
2025-03-25 07:41:32,276 - training - INFO - Epoch 141 completed in 35.01s. Average loss: 3.3138
2025-03-25 07:41:32,279 - training - INFO - Starting epoch 142/200000
2025-03-25 07:41:33,010 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9652.0MB reserved
2025-03-25 07:41:33,010 - training - INFO - Epoch: 142/200000, Batch: 0/45, Loss: 2.9422, Throughput: 76.81 samples/sec
2025-03-25 07:41:44,971 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9486.0MB reserved
2025-03-25 07:41:44,972 - training - INFO - Epoch: 142/200000, Batch: 15/45, Loss: 3.2977, Throughput: 70.61 samples/sec
2025-03-25 07:41:56,709 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9486.0MB reserved
2025-03-25 07:41:56,709 - training - INFO - Epoch: 142/200000, Batch: 30/45, Loss: 3.2978, Throughput: 71.06 samples/sec
2025-03-25 07:42:07,686 - training - INFO - Epoch 142 completed in 35.41s. Average loss: 3.3072
2025-03-25 07:42:07,690 - training - INFO - Starting epoch 143/200000
2025-03-25 07:42:08,444 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9624.0MB reserved
2025-03-25 07:42:08,444 - training - INFO - Epoch: 143/200000, Batch: 0/45, Loss: 3.2268, Throughput: 74.46 samples/sec
2025-03-25 07:42:20,356 - training - INFO - Memory: GPU 0: 3563.2MB allocated, 9500.0MB reserved
2025-03-25 07:42:20,356 - training - INFO - Epoch: 143/200000, Batch: 15/45, Loss: 3.1548, Throughput: 70.75 samples/sec
2025-03-25 07:42:31,973 - training - INFO - Memory: GPU 0: 3563.2MB allocated, 9500.0MB reserved
2025-03-25 07:42:31,974 - training - INFO - Epoch: 143/200000, Batch: 30/45, Loss: 3.2470, Throughput: 71.49 samples/sec
2025-03-25 07:42:42,871 - training - INFO - Epoch 143 completed in 35.18s. Average loss: 3.3111
2025-03-25 07:42:42,874 - training - INFO - Starting epoch 144/200000
2025-03-25 07:42:43,640 - training - INFO - Memory: GPU 0: 3563.2MB allocated, 9636.0MB reserved
2025-03-25 07:42:43,640 - training - INFO - Epoch: 144/200000, Batch: 0/45, Loss: 3.6261, Throughput: 73.21 samples/sec
2025-03-25 07:42:55,617 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9506.0MB reserved
2025-03-25 07:42:55,617 - training - INFO - Epoch: 144/200000, Batch: 15/45, Loss: 3.2891, Throughput: 70.32 samples/sec
2025-03-25 07:43:07,309 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9506.0MB reserved
2025-03-25 07:43:07,309 - training - INFO - Epoch: 144/200000, Batch: 30/45, Loss: 3.2221, Throughput: 71.05 samples/sec
2025-03-25 07:43:18,292 - training - INFO - Epoch 144 completed in 35.42s. Average loss: 3.3035
2025-03-25 07:43:18,296 - training - INFO - Starting epoch 145/200000
2025-03-25 07:43:19,041 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9644.0MB reserved
2025-03-25 07:43:19,041 - training - INFO - Epoch: 145/200000, Batch: 0/45, Loss: 3.6406, Throughput: 75.29 samples/sec
2025-03-25 07:43:30,906 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9518.0MB reserved
2025-03-25 07:43:30,906 - training - INFO - Epoch: 145/200000, Batch: 15/45, Loss: 3.1910, Throughput: 71.07 samples/sec
2025-03-25 07:43:42,547 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9518.0MB reserved
2025-03-25 07:43:42,548 - training - INFO - Epoch: 145/200000, Batch: 30/45, Loss: 3.2318, Throughput: 71.59 samples/sec
2025-03-25 07:43:53,523 - training - INFO - Epoch 145 completed in 35.23s. Average loss: 3.2291
2025-03-25 07:43:53,527 - training - INFO - Starting epoch 146/200000
2025-03-25 07:43:54,282 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9656.0MB reserved
2025-03-25 07:43:54,282 - training - INFO - Epoch: 146/200000, Batch: 0/45, Loss: 3.4948, Throughput: 74.20 samples/sec
2025-03-25 07:44:06,163 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9500.0MB reserved
2025-03-25 07:44:06,163 - training - INFO - Epoch: 146/200000, Batch: 15/45, Loss: 3.3954, Throughput: 70.91 samples/sec
2025-03-25 07:44:17,801 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9500.0MB reserved
2025-03-25 07:44:17,801 - training - INFO - Epoch: 146/200000, Batch: 30/45, Loss: 3.3096, Throughput: 71.52 samples/sec
2025-03-25 07:44:28,676 - training - INFO - Epoch 146 completed in 35.15s. Average loss: 3.2629
2025-03-25 07:44:28,680 - training - INFO - Starting epoch 147/200000
2025-03-25 07:44:29,414 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9638.0MB reserved
2025-03-25 07:44:29,414 - training - INFO - Epoch: 147/200000, Batch: 0/45, Loss: 3.4648, Throughput: 76.48 samples/sec
2025-03-25 07:44:41,299 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9494.0MB reserved
2025-03-25 07:44:41,299 - training - INFO - Epoch: 147/200000, Batch: 15/45, Loss: 3.3074, Throughput: 71.01 samples/sec
2025-03-25 07:44:52,882 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9494.0MB reserved
2025-03-25 07:44:52,882 - training - INFO - Epoch: 147/200000, Batch: 30/45, Loss: 3.3090, Throughput: 71.73 samples/sec
2025-03-25 07:45:03,820 - training - INFO - Epoch 147 completed in 35.14s. Average loss: 3.2878
2025-03-25 07:45:03,824 - training - INFO - Starting epoch 148/200000
2025-03-25 07:45:04,576 - training - INFO - Memory: GPU 0: 3556.7MB allocated, 9634.0MB reserved
2025-03-25 07:45:04,576 - training - INFO - Epoch: 148/200000, Batch: 0/45, Loss: 3.3192, Throughput: 74.64 samples/sec
2025-03-25 07:45:16,411 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9496.0MB reserved
2025-03-25 07:45:16,412 - training - INFO - Epoch: 148/200000, Batch: 15/45, Loss: 3.1930, Throughput: 71.19 samples/sec
2025-03-25 07:45:27,957 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9498.0MB reserved
2025-03-25 07:45:27,958 - training - INFO - Epoch: 148/200000, Batch: 30/45, Loss: 3.2411, Throughput: 71.94 samples/sec
2025-03-25 07:45:38,862 - training - INFO - Epoch 148 completed in 35.04s. Average loss: 3.2700
2025-03-25 07:45:39,015 - training - INFO - Starting epoch 149/200000
2025-03-25 07:45:39,707 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9636.0MB reserved
2025-03-25 07:45:39,707 - training - INFO - Epoch: 149/200000, Batch: 0/45, Loss: 3.5926, Throughput: 81.21 samples/sec
2025-03-25 07:45:51,647 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9498.0MB reserved
2025-03-25 07:45:51,647 - training - INFO - Epoch: 149/200000, Batch: 15/45, Loss: 3.1799, Throughput: 70.94 samples/sec
2025-03-25 07:46:03,311 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9500.0MB reserved
2025-03-25 07:46:03,311 - training - INFO - Epoch: 149/200000, Batch: 30/45, Loss: 3.2365, Throughput: 71.46 samples/sec
2025-03-25 07:46:14,248 - training - INFO - Epoch 149 completed in 35.23s. Average loss: 3.2626
2025-03-25 07:46:14,252 - training - INFO - Starting epoch 150/200000
2025-03-25 07:46:15,009 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9638.0MB reserved
2025-03-25 07:46:15,010 - training - INFO - Epoch: 150/200000, Batch: 0/45, Loss: 3.4525, Throughput: 74.00 samples/sec
2025-03-25 07:46:26,888 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9482.0MB reserved
2025-03-25 07:46:26,889 - training - INFO - Epoch: 150/200000, Batch: 15/45, Loss: 3.1185, Throughput: 70.91 samples/sec
2025-03-25 07:46:38,410 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9482.0MB reserved
2025-03-25 07:46:38,410 - training - INFO - Epoch: 150/200000, Batch: 30/45, Loss: 3.1760, Throughput: 71.86 samples/sec
2025-03-25 07:46:49,350 - training - INFO - Epoch 150 completed in 35.10s. Average loss: 3.2336
2025-03-25 07:46:49,354 - training - INFO - Starting epoch 151/200000
2025-03-25 07:46:50,088 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9620.0MB reserved
2025-03-25 07:46:50,088 - training - INFO - Epoch: 151/200000, Batch: 0/45, Loss: 3.0973, Throughput: 76.64 samples/sec
2025-03-25 07:47:01,971 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9484.0MB reserved
2025-03-25 07:47:01,971 - training - INFO - Epoch: 151/200000, Batch: 15/45, Loss: 3.1902, Throughput: 71.03 samples/sec
2025-03-25 07:47:13,658 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9484.0MB reserved
2025-03-25 07:47:13,659 - training - INFO - Epoch: 151/200000, Batch: 30/45, Loss: 3.2134, Throughput: 71.43 samples/sec
2025-03-25 07:47:24,669 - training - INFO - Epoch 151 completed in 35.32s. Average loss: 3.2565
2025-03-25 07:47:24,674 - training - INFO - Starting epoch 152/200000
2025-03-25 07:47:25,426 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9622.0MB reserved
2025-03-25 07:47:25,427 - training - INFO - Epoch: 152/200000, Batch: 0/45, Loss: 3.1651, Throughput: 74.72 samples/sec
2025-03-25 07:47:37,281 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9484.0MB reserved
2025-03-25 07:47:37,281 - training - INFO - Epoch: 152/200000, Batch: 15/45, Loss: 3.1258, Throughput: 71.08 samples/sec
2025-03-25 07:47:48,817 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9484.0MB reserved
2025-03-25 07:47:48,818 - training - INFO - Epoch: 152/200000, Batch: 30/45, Loss: 3.1300, Throughput: 71.91 samples/sec
2025-03-25 07:47:59,743 - training - INFO - Epoch 152 completed in 35.07s. Average loss: 3.2396
2025-03-25 07:47:59,747 - training - INFO - Starting epoch 153/200000
2025-03-25 07:48:00,489 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9622.0MB reserved
2025-03-25 07:48:00,489 - training - INFO - Epoch: 153/200000, Batch: 0/45, Loss: 3.4437, Throughput: 75.64 samples/sec
2025-03-25 07:48:12,384 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9508.0MB reserved
2025-03-25 07:48:12,593 - training - INFO - Epoch: 153/200000, Batch: 15/45, Loss: 3.2762, Throughput: 70.91 samples/sec
2025-03-25 07:48:24,170 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9510.0MB reserved
2025-03-25 07:48:24,170 - training - INFO - Epoch: 153/200000, Batch: 30/45, Loss: 3.2463, Throughput: 71.09 samples/sec
2025-03-25 07:48:35,104 - training - INFO - Epoch 153 completed in 35.36s. Average loss: 3.2396
2025-03-25 07:48:35,108 - training - INFO - Starting epoch 154/200000
2025-03-25 07:48:35,854 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9648.0MB reserved
2025-03-25 07:48:35,855 - training - INFO - Epoch: 154/200000, Batch: 0/45, Loss: 3.2344, Throughput: 75.20 samples/sec
2025-03-25 07:48:47,773 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9500.0MB reserved
2025-03-25 07:48:47,774 - training - INFO - Epoch: 154/200000, Batch: 15/45, Loss: 3.1396, Throughput: 70.76 samples/sec
2025-03-25 07:48:59,363 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9500.0MB reserved
2025-03-25 07:48:59,363 - training - INFO - Epoch: 154/200000, Batch: 30/45, Loss: 3.2136, Throughput: 71.58 samples/sec
2025-03-25 07:49:10,302 - training - INFO - Epoch 154 completed in 35.19s. Average loss: 3.2324
2025-03-25 07:49:10,305 - training - INFO - Starting epoch 155/200000
2025-03-25 07:49:11,044 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9636.0MB reserved
2025-03-25 07:49:11,045 - training - INFO - Epoch: 155/200000, Batch: 0/45, Loss: 3.5247, Throughput: 75.93 samples/sec
2025-03-25 07:49:22,910 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9508.0MB reserved
2025-03-25 07:49:22,910 - training - INFO - Epoch: 155/200000, Batch: 15/45, Loss: 3.2460, Throughput: 71.10 samples/sec
2025-03-25 07:49:34,562 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9508.0MB reserved
2025-03-25 07:49:34,562 - training - INFO - Epoch: 155/200000, Batch: 30/45, Loss: 3.2499, Throughput: 71.57 samples/sec
2025-03-25 07:49:45,545 - training - INFO - Epoch 155 completed in 35.24s. Average loss: 3.2022
2025-03-25 07:49:45,548 - training - INFO - Starting epoch 156/200000
2025-03-25 07:49:46,302 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9644.0MB reserved
2025-03-25 07:49:46,302 - training - INFO - Epoch: 156/200000, Batch: 0/45, Loss: 3.8858, Throughput: 74.48 samples/sec
2025-03-25 07:49:58,233 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9498.0MB reserved
2025-03-25 07:49:58,234 - training - INFO - Epoch: 156/200000, Batch: 15/45, Loss: 3.2511, Throughput: 70.64 samples/sec
2025-03-25 07:50:09,823 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9500.0MB reserved
2025-03-25 07:50:09,823 - training - INFO - Epoch: 156/200000, Batch: 30/45, Loss: 3.2330, Throughput: 71.52 samples/sec
2025-03-25 07:50:20,755 - training - INFO - Epoch 156 completed in 35.21s. Average loss: 3.2096
2025-03-25 07:50:20,759 - training - INFO - Starting epoch 157/200000
2025-03-25 07:50:21,507 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9636.0MB reserved
2025-03-25 07:50:21,508 - training - INFO - Epoch: 157/200000, Batch: 0/45, Loss: 3.1551, Throughput: 75.06 samples/sec
2025-03-25 07:50:33,439 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9508.0MB reserved
2025-03-25 07:50:33,439 - training - INFO - Epoch: 157/200000, Batch: 15/45, Loss: 3.1592, Throughput: 70.67 samples/sec
2025-03-25 07:50:45,067 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9508.0MB reserved
2025-03-25 07:50:45,067 - training - INFO - Epoch: 157/200000, Batch: 30/45, Loss: 3.1967, Throughput: 71.43 samples/sec
2025-03-25 07:50:56,014 - training - INFO - Epoch 157 completed in 35.25s. Average loss: 3.1785
2025-03-25 07:50:56,019 - training - INFO - Starting epoch 158/200000
2025-03-25 07:50:56,767 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9646.0MB reserved
2025-03-25 07:50:56,767 - training - INFO - Epoch: 158/200000, Batch: 0/45, Loss: 2.8075, Throughput: 74.93 samples/sec
2025-03-25 07:51:08,669 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9502.0MB reserved
2025-03-25 07:51:08,669 - training - INFO - Epoch: 158/200000, Batch: 15/45, Loss: 3.2134, Throughput: 70.84 samples/sec
2025-03-25 07:51:20,335 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9502.0MB reserved
2025-03-25 07:51:20,336 - training - INFO - Epoch: 158/200000, Batch: 30/45, Loss: 3.2567, Throughput: 71.40 samples/sec
2025-03-25 07:51:31,321 - training - INFO - Epoch 158 completed in 35.30s. Average loss: 3.1848
2025-03-25 07:51:31,324 - training - INFO - Starting epoch 159/200000
2025-03-25 07:51:32,071 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9640.0MB reserved
2025-03-25 07:51:32,071 - training - INFO - Epoch: 159/200000, Batch: 0/45, Loss: 3.5304, Throughput: 75.16 samples/sec
2025-03-25 07:51:44,022 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9508.0MB reserved
2025-03-25 07:51:44,022 - training - INFO - Epoch: 159/200000, Batch: 15/45, Loss: 3.2747, Throughput: 70.57 samples/sec
2025-03-25 07:51:55,581 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9508.0MB reserved
2025-03-25 07:51:55,581 - training - INFO - Epoch: 159/200000, Batch: 30/45, Loss: 3.2752, Throughput: 71.57 samples/sec
2025-03-25 07:52:06,488 - training - INFO - Epoch 159 completed in 35.16s. Average loss: 3.1967
2025-03-25 07:52:06,491 - training - INFO - Starting epoch 160/200000
2025-03-25 07:52:07,229 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9646.0MB reserved
2025-03-25 07:52:07,229 - training - INFO - Epoch: 160/200000, Batch: 0/45, Loss: 3.6657, Throughput: 76.09 samples/sec
2025-03-25 07:52:19,181 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9496.0MB reserved
2025-03-25 07:52:19,181 - training - INFO - Epoch: 160/200000, Batch: 15/45, Loss: 3.1931, Throughput: 70.62 samples/sec
2025-03-25 07:52:30,801 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9496.0MB reserved
2025-03-25 07:52:30,801 - training - INFO - Epoch: 160/200000, Batch: 30/45, Loss: 3.2116, Throughput: 71.42 samples/sec
2025-03-25 07:52:41,806 - training - INFO - Epoch 160 completed in 35.32s. Average loss: 3.2116
2025-03-25 07:52:41,810 - training - INFO - Starting epoch 161/200000
2025-03-25 07:52:42,572 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9636.0MB reserved
2025-03-25 07:52:42,572 - training - INFO - Epoch: 161/200000, Batch: 0/45, Loss: 3.7744, Throughput: 73.65 samples/sec
2025-03-25 07:52:54,653 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9510.0MB reserved
2025-03-25 07:52:54,654 - training - INFO - Epoch: 161/200000, Batch: 15/45, Loss: 3.1336, Throughput: 69.78 samples/sec
2025-03-25 07:53:06,283 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9510.0MB reserved
2025-03-25 07:53:06,284 - training - INFO - Epoch: 161/200000, Batch: 30/45, Loss: 3.1531, Throughput: 70.94 samples/sec
2025-03-25 07:53:17,185 - training - INFO - Epoch 161 completed in 35.37s. Average loss: 3.2417
2025-03-25 07:53:17,188 - training - INFO - Starting epoch 162/200000
2025-03-25 07:53:17,935 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9648.0MB reserved
2025-03-25 07:53:17,935 - training - INFO - Epoch: 162/200000, Batch: 0/45, Loss: 2.9661, Throughput: 75.19 samples/sec
2025-03-25 07:53:29,797 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9502.0MB reserved
2025-03-25 07:53:29,797 - training - INFO - Epoch: 162/200000, Batch: 15/45, Loss: 3.3072, Throughput: 71.07 samples/sec
2025-03-25 07:53:41,359 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9502.0MB reserved
2025-03-25 07:53:41,359 - training - INFO - Epoch: 162/200000, Batch: 30/45, Loss: 3.2225, Throughput: 71.83 samples/sec
2025-03-25 07:53:52,279 - training - INFO - Epoch 162 completed in 35.09s. Average loss: 3.1927
2025-03-25 07:53:52,282 - training - INFO - Starting epoch 163/200000
2025-03-25 07:53:53,013 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9640.0MB reserved
2025-03-25 07:53:53,014 - training - INFO - Epoch: 163/200000, Batch: 0/45, Loss: 3.0167, Throughput: 76.81 samples/sec
2025-03-25 07:54:04,832 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9510.0MB reserved
2025-03-25 07:54:04,832 - training - INFO - Epoch: 163/200000, Batch: 15/45, Loss: 3.2001, Throughput: 71.40 samples/sec
2025-03-25 07:54:16,475 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9510.0MB reserved
2025-03-25 07:54:16,475 - training - INFO - Epoch: 163/200000, Batch: 30/45, Loss: 3.1843, Throughput: 71.76 samples/sec
2025-03-25 07:54:27,505 - training - INFO - Epoch 163 completed in 35.22s. Average loss: 3.1721
2025-03-25 07:54:27,509 - training - INFO - Starting epoch 164/200000
2025-03-25 07:54:28,256 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9646.0MB reserved
2025-03-25 07:54:28,256 - training - INFO - Epoch: 164/200000, Batch: 0/45, Loss: 3.7881, Throughput: 75.03 samples/sec
2025-03-25 07:54:40,241 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9510.0MB reserved
2025-03-25 07:54:40,241 - training - INFO - Epoch: 164/200000, Batch: 15/45, Loss: 3.1359, Throughput: 70.38 samples/sec
2025-03-25 07:54:51,823 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9510.0MB reserved
2025-03-25 07:54:52,000 - training - INFO - Epoch: 164/200000, Batch: 30/45, Loss: 3.1492, Throughput: 71.40 samples/sec
2025-03-25 07:55:02,921 - training - INFO - Epoch 164 completed in 35.41s. Average loss: 3.1887
2025-03-25 07:55:02,925 - training - INFO - Starting epoch 165/200000
2025-03-25 07:55:03,670 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9648.0MB reserved
2025-03-25 07:55:03,670 - training - INFO - Epoch: 165/200000, Batch: 0/45, Loss: 3.2048, Throughput: 75.39 samples/sec
2025-03-25 07:55:15,621 - training - INFO - Memory: GPU 0: 3562.9MB allocated, 9498.0MB reserved
2025-03-25 07:55:15,621 - training - INFO - Epoch: 165/200000, Batch: 15/45, Loss: 3.1563, Throughput: 70.59 samples/sec
2025-03-25 07:55:27,349 - training - INFO - Memory: GPU 0: 3562.9MB allocated, 9498.0MB reserved
2025-03-25 07:55:27,349 - training - INFO - Epoch: 165/200000, Batch: 30/45, Loss: 3.1193, Throughput: 71.08 samples/sec
2025-03-25 07:55:38,353 - training - INFO - Epoch 165 completed in 35.43s. Average loss: 3.1565
2025-03-25 07:55:38,357 - training - INFO - Starting epoch 166/200000
2025-03-25 07:55:39,111 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9636.0MB reserved
2025-03-25 07:55:39,112 - training - INFO - Epoch: 166/200000, Batch: 0/45, Loss: 3.2381, Throughput: 74.46 samples/sec
2025-03-25 07:55:50,954 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9498.0MB reserved
2025-03-25 07:55:50,954 - training - INFO - Epoch: 166/200000, Batch: 15/45, Loss: 3.1929, Throughput: 71.14 samples/sec
2025-03-25 07:56:02,549 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9498.0MB reserved
2025-03-25 07:56:02,549 - training - INFO - Epoch: 166/200000, Batch: 30/45, Loss: 3.1196, Throughput: 71.76 samples/sec
2025-03-25 07:56:13,543 - training - INFO - Epoch 166 completed in 35.19s. Average loss: 3.1505
2025-03-25 07:56:13,547 - training - INFO - Starting epoch 167/200000
2025-03-25 07:56:14,304 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9636.0MB reserved
2025-03-25 07:56:14,304 - training - INFO - Epoch: 167/200000, Batch: 0/45, Loss: 2.8661, Throughput: 74.08 samples/sec
2025-03-25 07:56:26,175 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9514.0MB reserved
2025-03-25 07:56:26,175 - training - INFO - Epoch: 167/200000, Batch: 15/45, Loss: 3.0977, Throughput: 70.96 samples/sec
2025-03-25 07:56:37,983 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9514.0MB reserved
2025-03-25 07:56:37,983 - training - INFO - Epoch: 167/200000, Batch: 30/45, Loss: 3.0594, Throughput: 71.05 samples/sec
2025-03-25 07:56:49,032 - training - INFO - Epoch 167 completed in 35.48s. Average loss: 3.1229
2025-03-25 07:56:49,035 - training - INFO - Starting epoch 168/200000
2025-03-25 07:56:49,785 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9652.0MB reserved
2025-03-25 07:56:49,785 - training - INFO - Epoch: 168/200000, Batch: 0/45, Loss: 3.3377, Throughput: 74.90 samples/sec
2025-03-25 07:57:01,652 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9504.0MB reserved
2025-03-25 07:57:01,652 - training - INFO - Epoch: 168/200000, Batch: 15/45, Loss: 3.1246, Throughput: 71.02 samples/sec
2025-03-25 07:57:13,292 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9504.0MB reserved
2025-03-25 07:57:13,292 - training - INFO - Epoch: 168/200000, Batch: 30/45, Loss: 3.1336, Throughput: 71.57 samples/sec
2025-03-25 07:57:24,279 - training - INFO - Epoch 168 completed in 35.24s. Average loss: 3.1543
2025-03-25 07:57:24,283 - training - INFO - Starting epoch 169/200000
2025-03-25 07:57:25,014 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9642.0MB reserved
2025-03-25 07:57:25,014 - training - INFO - Epoch: 169/200000, Batch: 0/45, Loss: 3.3905, Throughput: 76.78 samples/sec
2025-03-25 07:57:36,845 - training - INFO - Memory: GPU 0: 3562.6MB allocated, 9508.0MB reserved
2025-03-25 07:57:36,846 - training - INFO - Epoch: 169/200000, Batch: 15/45, Loss: 3.2099, Throughput: 71.34 samples/sec
2025-03-25 07:57:48,469 - training - INFO - Memory: GPU 0: 3562.6MB allocated, 9508.0MB reserved
2025-03-25 07:57:48,469 - training - INFO - Epoch: 169/200000, Batch: 30/45, Loss: 3.1497, Throughput: 71.78 samples/sec
2025-03-25 07:57:59,391 - training - INFO - Epoch 169 completed in 35.11s. Average loss: 3.1949
2025-03-25 07:57:59,395 - training - INFO - Starting epoch 170/200000
2025-03-25 07:58:00,131 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9646.0MB reserved
2025-03-25 07:58:00,131 - training - INFO - Epoch: 170/200000, Batch: 0/45, Loss: 3.2819, Throughput: 76.27 samples/sec
2025-03-25 07:58:12,065 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9516.0MB reserved
2025-03-25 07:58:12,065 - training - INFO - Epoch: 170/200000, Batch: 15/45, Loss: 3.2158, Throughput: 70.72 samples/sec
2025-03-25 07:58:23,681 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9516.0MB reserved
2025-03-25 07:58:23,681 - training - INFO - Epoch: 170/200000, Batch: 30/45, Loss: 3.0908, Throughput: 71.49 samples/sec
2025-03-25 07:58:34,606 - training - INFO - Epoch 170 completed in 35.21s. Average loss: 3.1381
2025-03-25 07:58:34,609 - training - INFO - Starting epoch 171/200000
2025-03-25 07:58:35,361 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9654.0MB reserved
2025-03-25 07:58:35,361 - training - INFO - Epoch: 171/200000, Batch: 0/45, Loss: 3.3100, Throughput: 74.57 samples/sec
2025-03-25 07:58:47,257 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9510.0MB reserved
2025-03-25 07:58:47,258 - training - INFO - Epoch: 171/200000, Batch: 15/45, Loss: 3.0629, Throughput: 70.85 samples/sec
2025-03-25 07:58:58,830 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9510.0MB reserved
2025-03-25 07:58:58,831 - training - INFO - Epoch: 171/200000, Batch: 30/45, Loss: 3.1332, Throughput: 71.68 samples/sec
2025-03-25 07:59:09,726 - training - INFO - Epoch 171 completed in 35.12s. Average loss: 3.1216
2025-03-25 07:59:09,729 - training - INFO - Starting epoch 172/200000
2025-03-25 07:59:10,468 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9648.0MB reserved
2025-03-25 07:59:10,468 - training - INFO - Epoch: 172/200000, Batch: 0/45, Loss: 3.2300, Throughput: 75.91 samples/sec
2025-03-25 07:59:22,381 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9498.0MB reserved
2025-03-25 07:59:22,382 - training - INFO - Epoch: 172/200000, Batch: 15/45, Loss: 3.2554, Throughput: 70.83 samples/sec
2025-03-25 07:59:34,027 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9500.0MB reserved
2025-03-25 07:59:34,027 - training - INFO - Epoch: 172/200000, Batch: 30/45, Loss: 3.1487, Throughput: 71.45 samples/sec
2025-03-25 07:59:44,999 - training - INFO - Epoch 172 completed in 35.27s. Average loss: 3.1039
2025-03-25 07:59:45,002 - training - INFO - Starting epoch 173/200000
2025-03-25 07:59:45,737 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9638.0MB reserved
2025-03-25 07:59:45,737 - training - INFO - Epoch: 173/200000, Batch: 0/45, Loss: 3.2002, Throughput: 76.27 samples/sec
2025-03-25 07:59:57,781 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9504.0MB reserved
2025-03-25 07:59:57,781 - training - INFO - Epoch: 173/200000, Batch: 15/45, Loss: 3.0360, Throughput: 70.12 samples/sec
2025-03-25 08:00:09,412 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9506.0MB reserved
2025-03-25 08:00:09,412 - training - INFO - Epoch: 173/200000, Batch: 30/45, Loss: 3.0731, Throughput: 71.12 samples/sec
2025-03-25 08:00:20,393 - training - INFO - Epoch 173 completed in 35.39s. Average loss: 3.1308
2025-03-25 08:00:20,397 - training - INFO - Starting epoch 174/200000
2025-03-25 08:00:21,160 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9644.0MB reserved
2025-03-25 08:00:21,160 - training - INFO - Epoch: 174/200000, Batch: 0/45, Loss: 3.3038, Throughput: 73.61 samples/sec
2025-03-25 08:00:33,188 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9498.0MB reserved
2025-03-25 08:00:33,188 - training - INFO - Epoch: 174/200000, Batch: 15/45, Loss: 3.0469, Throughput: 70.06 samples/sec
2025-03-25 08:00:44,821 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9498.0MB reserved
2025-03-25 08:00:44,821 - training - INFO - Epoch: 174/200000, Batch: 30/45, Loss: 3.0542, Throughput: 71.09 samples/sec
2025-03-25 08:00:55,853 - training - INFO - Epoch 174 completed in 35.46s. Average loss: 3.1548
2025-03-25 08:00:55,857 - training - INFO - Starting epoch 175/200000
2025-03-25 08:00:56,596 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9638.0MB reserved
2025-03-25 08:00:56,596 - training - INFO - Epoch: 175/200000, Batch: 0/45, Loss: 3.4759, Throughput: 75.92 samples/sec
2025-03-25 08:01:08,567 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9398.0MB reserved
2025-03-25 08:01:08,567 - training - INFO - Epoch: 175/200000, Batch: 15/45, Loss: 3.2055, Throughput: 70.50 samples/sec
2025-03-25 08:01:20,226 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9398.0MB reserved
2025-03-25 08:01:20,226 - training - INFO - Epoch: 175/200000, Batch: 30/45, Loss: 3.1449, Throughput: 71.24 samples/sec
2025-03-25 08:01:31,216 - training - INFO - Epoch 175 completed in 35.36s. Average loss: 3.1480
2025-03-25 08:01:31,313 - training - INFO - Starting epoch 176/200000
2025-03-25 08:01:31,992 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9536.0MB reserved
2025-03-25 08:01:31,993 - training - INFO - Epoch: 176/200000, Batch: 0/45, Loss: 3.2875, Throughput: 82.82 samples/sec
2025-03-25 08:01:43,906 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9502.0MB reserved
2025-03-25 08:01:43,907 - training - INFO - Epoch: 176/200000, Batch: 15/45, Loss: 3.1767, Throughput: 71.16 samples/sec
2025-03-25 08:01:55,623 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9502.0MB reserved
2025-03-25 08:01:55,623 - training - INFO - Epoch: 176/200000, Batch: 30/45, Loss: 3.1668, Throughput: 71.42 samples/sec
2025-03-25 08:02:06,665 - training - INFO - Epoch 176 completed in 35.35s. Average loss: 3.1484
2025-03-25 08:02:06,668 - training - INFO - Starting epoch 177/200000
2025-03-25 08:02:07,414 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9638.0MB reserved
2025-03-25 08:02:07,414 - training - INFO - Epoch: 177/200000, Batch: 0/45, Loss: 3.7846, Throughput: 75.16 samples/sec
2025-03-25 08:02:19,303 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9498.0MB reserved
2025-03-25 08:02:19,303 - training - INFO - Epoch: 177/200000, Batch: 15/45, Loss: 3.2186, Throughput: 70.93 samples/sec
2025-03-25 08:02:30,867 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9498.0MB reserved
2025-03-25 08:02:30,867 - training - INFO - Epoch: 177/200000, Batch: 30/45, Loss: 3.1556, Throughput: 71.75 samples/sec
2025-03-25 08:02:41,776 - training - INFO - Epoch 177 completed in 35.11s. Average loss: 3.1247
2025-03-25 08:02:41,780 - training - INFO - Starting epoch 178/200000
2025-03-25 08:02:42,534 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9636.0MB reserved
2025-03-25 08:02:42,534 - training - INFO - Epoch: 178/200000, Batch: 0/45, Loss: 3.2464, Throughput: 74.33 samples/sec
2025-03-25 08:02:54,491 - training - INFO - Memory: GPU 0: 3556.9MB allocated, 9508.0MB reserved
2025-03-25 08:02:54,491 - training - INFO - Epoch: 178/200000, Batch: 15/45, Loss: 3.1074, Throughput: 70.49 samples/sec
2025-03-25 08:03:06,177 - training - INFO - Memory: GPU 0: 3556.9MB allocated, 9508.0MB reserved
2025-03-25 08:03:06,177 - training - INFO - Epoch: 178/200000, Batch: 30/45, Loss: 3.0131, Throughput: 71.16 samples/sec
2025-03-25 08:03:17,197 - training - INFO - Epoch 178 completed in 35.42s. Average loss: 3.0843
2025-03-25 08:03:17,201 - training - INFO - Starting epoch 179/200000
2025-03-25 08:03:17,944 - training - INFO - Memory: GPU 0: 3556.9MB allocated, 9646.0MB reserved
2025-03-25 08:03:17,944 - training - INFO - Epoch: 179/200000, Batch: 0/45, Loss: 2.9331, Throughput: 75.53 samples/sec
2025-03-25 08:03:29,840 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9500.0MB reserved
2025-03-25 08:03:29,840 - training - INFO - Epoch: 179/200000, Batch: 15/45, Loss: 3.1114, Throughput: 70.90 samples/sec
2025-03-25 08:03:41,541 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9500.0MB reserved
2025-03-25 08:03:41,541 - training - INFO - Epoch: 179/200000, Batch: 30/45, Loss: 3.1106, Throughput: 71.33 samples/sec
2025-03-25 08:03:52,611 - training - INFO - Epoch 179 completed in 35.41s. Average loss: 3.1492
2025-03-25 08:03:52,616 - training - INFO - Starting epoch 180/200000
2025-03-25 08:03:53,374 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9638.0MB reserved
2025-03-25 08:03:53,374 - training - INFO - Epoch: 180/200000, Batch: 0/45, Loss: 3.0588, Throughput: 73.92 samples/sec
2025-03-25 08:04:05,313 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9488.0MB reserved
2025-03-25 08:04:05,314 - training - INFO - Epoch: 180/200000, Batch: 15/45, Loss: 3.1391, Throughput: 70.57 samples/sec
2025-03-25 08:04:16,941 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9488.0MB reserved
2025-03-25 08:04:16,941 - training - INFO - Epoch: 180/200000, Batch: 30/45, Loss: 3.1696, Throughput: 71.37 samples/sec
2025-03-25 08:04:27,897 - training - INFO - Epoch 180 completed in 35.28s. Average loss: 3.0756
2025-03-25 08:04:27,901 - training - INFO - Starting epoch 181/200000
2025-03-25 08:04:28,638 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9626.0MB reserved
2025-03-25 08:04:28,638 - training - INFO - Epoch: 181/200000, Batch: 0/45, Loss: 2.9208, Throughput: 76.05 samples/sec
2025-03-25 08:04:40,505 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9500.0MB reserved
2025-03-25 08:04:40,506 - training - INFO - Epoch: 181/200000, Batch: 15/45, Loss: 3.0568, Throughput: 71.09 samples/sec
2025-03-25 08:04:52,091 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9500.0MB reserved
2025-03-25 08:04:52,091 - training - INFO - Epoch: 181/200000, Batch: 30/45, Loss: 3.0670, Throughput: 71.77 samples/sec
2025-03-25 08:05:03,020 - training - INFO - Epoch 181 completed in 35.12s. Average loss: 3.1149
2025-03-25 08:05:03,024 - training - INFO - Starting epoch 182/200000
2025-03-25 08:05:03,756 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9638.0MB reserved
2025-03-25 08:05:03,756 - training - INFO - Epoch: 182/200000, Batch: 0/45, Loss: 3.1721, Throughput: 76.74 samples/sec
2025-03-25 08:05:15,640 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9508.0MB reserved
2025-03-25 08:05:15,641 - training - INFO - Epoch: 182/200000, Batch: 15/45, Loss: 3.0726, Throughput: 71.03 samples/sec
2025-03-25 08:05:27,252 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9508.0MB reserved
2025-03-25 08:05:27,253 - training - INFO - Epoch: 182/200000, Batch: 30/45, Loss: 3.0913, Throughput: 71.66 samples/sec
2025-03-25 08:05:38,256 - training - INFO - Epoch 182 completed in 35.23s. Average loss: 3.0988
2025-03-25 08:05:38,260 - training - INFO - Starting epoch 183/200000
2025-03-25 08:05:39,005 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9646.0MB reserved
2025-03-25 08:05:39,005 - training - INFO - Epoch: 183/200000, Batch: 0/45, Loss: 3.0972, Throughput: 75.34 samples/sec
2025-03-25 08:05:50,867 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9404.0MB reserved
2025-03-25 08:05:50,867 - training - INFO - Epoch: 183/200000, Batch: 15/45, Loss: 2.9411, Throughput: 71.09 samples/sec
2025-03-25 08:06:02,427 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9404.0MB reserved
2025-03-25 08:06:02,427 - training - INFO - Epoch: 183/200000, Batch: 30/45, Loss: 2.9985, Throughput: 71.84 samples/sec
2025-03-25 08:06:13,352 - training - INFO - Epoch 183 completed in 35.09s. Average loss: 3.0545
2025-03-25 08:06:13,355 - training - INFO - Starting epoch 184/200000
2025-03-25 08:06:14,100 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9542.0MB reserved
2025-03-25 08:06:14,101 - training - INFO - Epoch: 184/200000, Batch: 0/45, Loss: 3.4241, Throughput: 75.23 samples/sec
2025-03-25 08:06:26,055 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9480.0MB reserved
2025-03-25 08:06:26,056 - training - INFO - Epoch: 184/200000, Batch: 15/45, Loss: 3.0257, Throughput: 70.57 samples/sec
2025-03-25 08:06:37,710 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9482.0MB reserved
2025-03-25 08:06:37,711 - training - INFO - Epoch: 184/200000, Batch: 30/45, Loss: 3.0398, Throughput: 71.29 samples/sec
2025-03-25 08:06:48,649 - training - INFO - Epoch 184 completed in 35.29s. Average loss: 3.0898
2025-03-25 08:06:48,653 - training - INFO - Starting epoch 185/200000
2025-03-25 08:06:49,420 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9620.0MB reserved
2025-03-25 08:06:49,421 - training - INFO - Epoch: 185/200000, Batch: 0/45, Loss: 2.7746, Throughput: 73.10 samples/sec
2025-03-25 08:07:01,412 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9512.0MB reserved
2025-03-25 08:07:01,412 - training - INFO - Epoch: 185/200000, Batch: 15/45, Loss: 3.0849, Throughput: 70.23 samples/sec
2025-03-25 08:07:13,112 - training - INFO - Memory: GPU 0: 3556.4MB allocated, 9512.0MB reserved
2025-03-25 08:07:13,112 - training - INFO - Epoch: 185/200000, Batch: 30/45, Loss: 3.0749, Throughput: 70.98 samples/sec
2025-03-25 08:07:24,106 - training - INFO - Epoch 185 completed in 35.45s. Average loss: 3.0791
2025-03-25 08:07:24,109 - training - INFO - Starting epoch 186/200000
2025-03-25 08:07:24,843 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9648.0MB reserved
2025-03-25 08:07:24,843 - training - INFO - Epoch: 186/200000, Batch: 0/45, Loss: 2.8786, Throughput: 76.59 samples/sec
2025-03-25 08:07:36,731 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9498.0MB reserved
2025-03-25 08:07:36,732 - training - INFO - Epoch: 186/200000, Batch: 15/45, Loss: 2.9410, Throughput: 71.00 samples/sec
2025-03-25 08:07:48,305 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9498.0MB reserved
2025-03-25 08:07:48,306 - training - INFO - Epoch: 186/200000, Batch: 30/45, Loss: 3.0903, Throughput: 71.75 samples/sec
2025-03-25 08:07:59,317 - training - INFO - Epoch 186 completed in 35.21s. Average loss: 3.0887
2025-03-25 08:07:59,321 - training - INFO - Starting epoch 187/200000
2025-03-25 08:08:00,064 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9636.0MB reserved
2025-03-25 08:08:00,064 - training - INFO - Epoch: 187/200000, Batch: 0/45, Loss: 3.3214, Throughput: 75.41 samples/sec
2025-03-25 08:08:12,001 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9502.0MB reserved
2025-03-25 08:08:12,001 - training - INFO - Epoch: 187/200000, Batch: 15/45, Loss: 3.0519, Throughput: 70.67 samples/sec
2025-03-25 08:08:23,630 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9502.0MB reserved
2025-03-25 08:08:23,630 - training - INFO - Epoch: 187/200000, Batch: 30/45, Loss: 3.1145, Throughput: 71.42 samples/sec
2025-03-25 08:08:34,587 - training - INFO - Epoch 187 completed in 35.27s. Average loss: 3.0851
2025-03-25 08:08:34,591 - training - INFO - Starting epoch 188/200000
2025-03-25 08:08:35,332 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9640.0MB reserved
2025-03-25 08:08:35,333 - training - INFO - Epoch: 188/200000, Batch: 0/45, Loss: 2.5407, Throughput: 75.72 samples/sec
2025-03-25 08:08:47,338 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9506.0MB reserved
2025-03-25 08:08:47,338 - training - INFO - Epoch: 188/200000, Batch: 15/45, Loss: 2.9380, Throughput: 70.30 samples/sec
2025-03-25 08:08:58,986 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9508.0MB reserved
2025-03-25 08:08:58,987 - training - INFO - Epoch: 188/200000, Batch: 30/45, Loss: 3.0128, Throughput: 71.17 samples/sec
2025-03-25 08:09:09,943 - training - INFO - Epoch 188 completed in 35.35s. Average loss: 3.0608
2025-03-25 08:09:09,947 - training - INFO - Starting epoch 189/200000
2025-03-25 08:09:10,672 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9646.0MB reserved
2025-03-25 08:09:10,672 - training - INFO - Epoch: 189/200000, Batch: 0/45, Loss: 3.2383, Throughput: 77.36 samples/sec
2025-03-25 08:09:22,760 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9514.0MB reserved
2025-03-25 08:09:22,761 - training - INFO - Epoch: 189/200000, Batch: 15/45, Loss: 2.9573, Throughput: 69.94 samples/sec
2025-03-25 08:09:34,437 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9514.0MB reserved
2025-03-25 08:09:34,437 - training - INFO - Epoch: 189/200000, Batch: 30/45, Loss: 2.9419, Throughput: 70.89 samples/sec
2025-03-25 08:09:45,373 - training - INFO - Epoch 189 completed in 35.43s. Average loss: 3.0258
2025-03-25 08:09:45,376 - training - INFO - Starting epoch 190/200000
2025-03-25 08:09:46,126 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9652.0MB reserved
2025-03-25 08:09:46,126 - training - INFO - Epoch: 190/200000, Batch: 0/45, Loss: 2.9214, Throughput: 74.92 samples/sec
2025-03-25 08:09:58,061 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9514.0MB reserved
2025-03-25 08:09:58,062 - training - INFO - Epoch: 190/200000, Batch: 15/45, Loss: 2.9704, Throughput: 70.64 samples/sec
2025-03-25 08:10:09,826 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9514.0MB reserved
2025-03-25 08:10:09,826 - training - INFO - Epoch: 190/200000, Batch: 30/45, Loss: 3.0333, Throughput: 71.01 samples/sec
2025-03-25 08:10:20,764 - training - INFO - Epoch 190 completed in 35.39s. Average loss: 3.0600
2025-03-25 08:10:20,767 - training - INFO - Starting epoch 191/200000
2025-03-25 08:10:21,511 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9650.0MB reserved
2025-03-25 08:10:21,512 - training - INFO - Epoch: 191/200000, Batch: 0/45, Loss: 3.3272, Throughput: 75.45 samples/sec
2025-03-25 08:10:33,364 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9502.0MB reserved
2025-03-25 08:10:33,364 - training - INFO - Epoch: 191/200000, Batch: 15/45, Loss: 2.9376, Throughput: 71.14 samples/sec
2025-03-25 08:10:45,011 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9502.0MB reserved
2025-03-25 08:10:45,011 - training - INFO - Epoch: 191/200000, Batch: 30/45, Loss: 2.9357, Throughput: 71.61 samples/sec
2025-03-25 08:10:55,969 - training - INFO - Epoch 191 completed in 35.20s. Average loss: 2.9752
2025-03-25 08:10:55,973 - training - INFO - Starting epoch 192/200000
2025-03-25 08:10:56,731 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9640.0MB reserved
2025-03-25 08:10:56,731 - training - INFO - Epoch: 192/200000, Batch: 0/45, Loss: 3.1643, Throughput: 74.05 samples/sec
2025-03-25 08:11:08,634 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9498.0MB reserved
2025-03-25 08:11:08,634 - training - INFO - Epoch: 192/200000, Batch: 15/45, Loss: 3.0606, Throughput: 70.77 samples/sec
2025-03-25 08:11:20,248 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9498.0MB reserved
2025-03-25 08:11:20,249 - training - INFO - Epoch: 192/200000, Batch: 30/45, Loss: 3.0694, Throughput: 71.52 samples/sec
2025-03-25 08:11:31,186 - training - INFO - Epoch 192 completed in 35.21s. Average loss: 2.9811
2025-03-25 08:11:31,190 - training - INFO - Starting epoch 193/200000
2025-03-25 08:11:31,923 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9636.0MB reserved
2025-03-25 08:11:31,923 - training - INFO - Epoch: 193/200000, Batch: 0/45, Loss: 3.1968, Throughput: 76.52 samples/sec
2025-03-25 08:11:43,906 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9500.0MB reserved
2025-03-25 08:11:43,907 - training - INFO - Epoch: 193/200000, Batch: 15/45, Loss: 3.1008, Throughput: 70.47 samples/sec
2025-03-25 08:11:55,620 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9500.0MB reserved
2025-03-25 08:11:55,620 - training - INFO - Epoch: 193/200000, Batch: 30/45, Loss: 3.0515, Throughput: 71.06 samples/sec
2025-03-25 08:12:06,583 - training - INFO - Epoch 193 completed in 35.39s. Average loss: 3.0424
2025-03-25 08:12:06,586 - training - INFO - Starting epoch 194/200000
2025-03-25 08:12:07,334 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9638.0MB reserved
2025-03-25 08:12:07,334 - training - INFO - Epoch: 194/200000, Batch: 0/45, Loss: 3.2211, Throughput: 75.11 samples/sec
2025-03-25 08:12:19,226 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9498.0MB reserved
2025-03-25 08:12:19,227 - training - INFO - Epoch: 194/200000, Batch: 15/45, Loss: 3.1003, Throughput: 70.90 samples/sec
2025-03-25 08:12:30,896 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9498.0MB reserved
2025-03-25 08:12:30,896 - training - INFO - Epoch: 194/200000, Batch: 30/45, Loss: 3.0618, Throughput: 71.42 samples/sec
2025-03-25 08:12:41,890 - training - INFO - Epoch 194 completed in 35.30s. Average loss: 2.9896
2025-03-25 08:12:41,896 - training - INFO - Starting epoch 195/200000
2025-03-25 08:12:42,633 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9636.0MB reserved
2025-03-25 08:12:42,634 - training - INFO - Epoch: 195/200000, Batch: 0/45, Loss: 2.9040, Throughput: 76.02 samples/sec
2025-03-25 08:12:54,565 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9498.0MB reserved
2025-03-25 08:12:54,565 - training - INFO - Epoch: 195/200000, Batch: 15/45, Loss: 2.9820, Throughput: 70.73 samples/sec
2025-03-25 08:13:06,300 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9498.0MB reserved
2025-03-25 08:13:06,300 - training - INFO - Epoch: 195/200000, Batch: 30/45, Loss: 2.9641, Throughput: 71.14 samples/sec
2025-03-25 08:13:17,289 - training - INFO - Epoch 195 completed in 35.39s. Average loss: 2.9986
2025-03-25 08:13:17,293 - training - INFO - Starting epoch 196/200000
2025-03-25 08:13:18,027 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9638.0MB reserved
2025-03-25 08:13:18,027 - training - INFO - Epoch: 196/200000, Batch: 0/45, Loss: 2.8751, Throughput: 76.41 samples/sec
2025-03-25 08:13:29,957 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9498.0MB reserved
2025-03-25 08:13:29,957 - training - INFO - Epoch: 196/200000, Batch: 15/45, Loss: 2.8778, Throughput: 70.76 samples/sec
2025-03-25 08:13:41,617 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9498.0MB reserved
2025-03-25 08:13:41,618 - training - INFO - Epoch: 196/200000, Batch: 30/45, Loss: 2.8750, Throughput: 71.37 samples/sec
2025-03-25 08:13:52,546 - training - INFO - Epoch 196 completed in 35.25s. Average loss: 3.0165
2025-03-25 08:13:52,550 - training - INFO - Starting epoch 197/200000
2025-03-25 08:13:53,287 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9638.0MB reserved
2025-03-25 08:13:53,287 - training - INFO - Epoch: 197/200000, Batch: 0/45, Loss: 2.9700, Throughput: 76.02 samples/sec
2025-03-25 08:14:05,167 - training - INFO - Memory: GPU 0: 3562.5MB allocated, 9504.0MB reserved
2025-03-25 08:14:05,167 - training - INFO - Epoch: 197/200000, Batch: 15/45, Loss: 2.9547, Throughput: 71.02 samples/sec
2025-03-25 08:14:16,844 - training - INFO - Memory: GPU 0: 3562.5MB allocated, 9506.0MB reserved
2025-03-25 08:14:16,845 - training - INFO - Epoch: 197/200000, Batch: 30/45, Loss: 2.9450, Throughput: 71.47 samples/sec
2025-03-25 08:14:27,821 - training - INFO - Epoch 197 completed in 35.27s. Average loss: 2.9993
2025-03-25 08:14:27,825 - training - INFO - Starting epoch 198/200000
2025-03-25 08:14:28,578 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9642.0MB reserved
2025-03-25 08:14:28,578 - training - INFO - Epoch: 198/200000, Batch: 0/45, Loss: 2.9491, Throughput: 74.53 samples/sec
2025-03-25 08:14:40,601 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9508.0MB reserved
2025-03-25 08:14:40,601 - training - INFO - Epoch: 198/200000, Batch: 15/45, Loss: 2.9648, Throughput: 70.14 samples/sec
2025-03-25 08:14:52,294 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9508.0MB reserved
2025-03-25 08:14:52,294 - training - INFO - Epoch: 198/200000, Batch: 30/45, Loss: 3.0015, Throughput: 70.95 samples/sec
2025-03-25 08:15:03,261 - training - INFO - Epoch 198 completed in 35.44s. Average loss: 3.0038
2025-03-25 08:15:03,265 - training - INFO - Starting epoch 199/200000
2025-03-25 08:15:04,008 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9644.0MB reserved
2025-03-25 08:15:04,008 - training - INFO - Epoch: 199/200000, Batch: 0/45, Loss: 3.0489, Throughput: 75.55 samples/sec
2025-03-25 08:15:16,037 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9500.0MB reserved
2025-03-25 08:15:16,037 - training - INFO - Epoch: 199/200000, Batch: 15/45, Loss: 2.9607, Throughput: 70.17 samples/sec
2025-03-25 08:15:27,724 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9500.0MB reserved
2025-03-25 08:15:27,725 - training - INFO - Epoch: 199/200000, Batch: 30/45, Loss: 2.9751, Throughput: 70.98 samples/sec
2025-03-25 08:15:38,726 - training - INFO - Epoch 199 completed in 35.46s. Average loss: 2.9906
2025-03-25 08:15:38,730 - training - INFO - Starting epoch 200/200000
2025-03-25 08:15:39,465 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9638.0MB reserved
2025-03-25 08:15:39,465 - training - INFO - Epoch: 200/200000, Batch: 0/45, Loss: 3.2980, Throughput: 76.39 samples/sec
2025-03-25 08:15:51,418 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9498.0MB reserved
2025-03-25 08:15:51,432 - training - INFO - Epoch: 200/200000, Batch: 15/45, Loss: 3.1050, Throughput: 70.62 samples/sec
2025-03-25 08:16:03,109 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9498.0MB reserved
2025-03-25 08:16:03,109 - training - INFO - Epoch: 200/200000, Batch: 30/45, Loss: 3.0499, Throughput: 71.21 samples/sec
2025-03-25 08:16:14,068 - training - INFO - Epoch 200 completed in 35.34s. Average loss: 3.0248
2025-03-25 08:16:14,072 - training - INFO - Starting validation...
2025-03-25 08:16:14,398 - training - INFO - Validation Loss: 8.6388
2025-03-25 08:16:14,398 - training - INFO - Validation loss did not improve. Counter: 1/10
2025-03-25 08:16:14,721 - training - INFO - Starting epoch 201/200000
2025-03-25 08:16:15,490 - training - INFO - Memory: GPU 0: 3565.6MB allocated, 8742.0MB reserved
2025-03-25 08:16:15,491 - training - INFO - Epoch: 201/200000, Batch: 0/45, Loss: 2.8463, Throughput: 72.96 samples/sec
2025-03-25 08:16:27,326 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9476.0MB reserved
2025-03-25 08:16:27,326 - training - INFO - Epoch: 201/200000, Batch: 15/45, Loss: 2.9510, Throughput: 71.10 samples/sec
2025-03-25 08:16:38,981 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9476.0MB reserved
2025-03-25 08:16:38,981 - training - INFO - Epoch: 201/200000, Batch: 30/45, Loss: 3.0132, Throughput: 71.57 samples/sec
2025-03-25 08:16:49,936 - training - INFO - Epoch 201 completed in 35.21s. Average loss: 3.0123
2025-03-25 08:16:49,939 - training - INFO - Starting epoch 202/200000
2025-03-25 08:16:50,678 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9614.0MB reserved
2025-03-25 08:16:50,678 - training - INFO - Epoch: 202/200000, Batch: 0/45, Loss: 2.8157, Throughput: 75.88 samples/sec
2025-03-25 08:17:02,505 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9492.0MB reserved
2025-03-25 08:17:02,506 - training - INFO - Epoch: 202/200000, Batch: 15/45, Loss: 3.0582, Throughput: 71.31 samples/sec
2025-03-25 08:17:14,024 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9492.0MB reserved
2025-03-25 08:17:14,025 - training - INFO - Epoch: 202/200000, Batch: 30/45, Loss: 3.0533, Throughput: 72.08 samples/sec
2025-03-25 08:17:24,936 - training - INFO - Epoch 202 completed in 35.00s. Average loss: 3.0404
2025-03-25 08:17:24,940 - training - INFO - Starting epoch 203/200000
2025-03-25 08:17:25,682 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9630.0MB reserved
2025-03-25 08:17:25,682 - training - INFO - Epoch: 203/200000, Batch: 0/45, Loss: 3.1273, Throughput: 75.71 samples/sec
2025-03-25 08:17:37,590 - training - INFO - Memory: GPU 0: 3562.9MB allocated, 9504.0MB reserved
2025-03-25 08:17:37,590 - training - INFO - Epoch: 203/200000, Batch: 15/45, Loss: 3.0549, Throughput: 70.83 samples/sec
2025-03-25 08:17:49,201 - training - INFO - Memory: GPU 0: 3562.9MB allocated, 9504.0MB reserved
2025-03-25 08:17:49,201 - training - INFO - Epoch: 203/200000, Batch: 30/45, Loss: 3.0050, Throughput: 71.56 samples/sec
2025-03-25 08:18:00,130 - training - INFO - Epoch 203 completed in 35.19s. Average loss: 2.9944
2025-03-25 08:18:00,134 - training - INFO - Starting epoch 204/200000
2025-03-25 08:18:00,865 - training - INFO - Memory: GPU 0: 3562.7MB allocated, 9642.0MB reserved
2025-03-25 08:18:00,865 - training - INFO - Epoch: 204/200000, Batch: 0/45, Loss: 2.8569, Throughput: 76.77 samples/sec
2025-03-25 08:18:12,692 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9502.0MB reserved
2025-03-25 08:18:12,692 - training - INFO - Epoch: 204/200000, Batch: 15/45, Loss: 2.9689, Throughput: 71.36 samples/sec
2025-03-25 08:18:24,260 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9502.0MB reserved
2025-03-25 08:18:24,261 - training - INFO - Epoch: 204/200000, Batch: 30/45, Loss: 2.9630, Throughput: 71.96 samples/sec
2025-03-25 08:18:35,224 - training - INFO - Epoch 204 completed in 35.09s. Average loss: 2.9969
2025-03-25 08:18:35,227 - training - INFO - Starting epoch 205/200000
2025-03-25 08:18:35,965 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9640.0MB reserved
2025-03-25 08:18:35,965 - training - INFO - Epoch: 205/200000, Batch: 0/45, Loss: 3.0656, Throughput: 75.99 samples/sec
2025-03-25 08:18:47,895 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9498.0MB reserved
2025-03-25 08:18:47,896 - training - INFO - Epoch: 205/200000, Batch: 15/45, Loss: 2.9358, Throughput: 70.74 samples/sec
2025-03-25 08:18:59,592 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9498.0MB reserved
2025-03-25 08:18:59,593 - training - INFO - Epoch: 205/200000, Batch: 30/45, Loss: 2.9666, Throughput: 71.25 samples/sec
2025-03-25 08:19:10,571 - training - INFO - Epoch 205 completed in 35.34s. Average loss: 2.9409
2025-03-25 08:19:10,574 - training - INFO - Starting epoch 206/200000
2025-03-25 08:19:11,309 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9636.0MB reserved
2025-03-25 08:19:11,310 - training - INFO - Epoch: 206/200000, Batch: 0/45, Loss: 3.2357, Throughput: 76.28 samples/sec
2025-03-25 08:19:23,171 - training - INFO - Memory: GPU 0: 3556.2MB allocated, 9508.0MB reserved
2025-03-25 08:19:23,171 - training - INFO - Epoch: 206/200000, Batch: 15/45, Loss: 2.9100, Throughput: 71.13 samples/sec
2025-03-25 08:19:34,802 - training - INFO - Memory: GPU 0: 3556.2MB allocated, 9508.0MB reserved
2025-03-25 08:19:34,802 - training - INFO - Epoch: 206/200000, Batch: 30/45, Loss: 2.9108, Throughput: 71.66 samples/sec
2025-03-25 08:19:45,793 - training - INFO - Epoch 206 completed in 35.22s. Average loss: 2.9357
2025-03-25 08:19:45,797 - training - INFO - Starting epoch 207/200000
2025-03-25 08:19:46,549 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9648.0MB reserved
2025-03-25 08:19:46,550 - training - INFO - Epoch: 207/200000, Batch: 0/45, Loss: 2.6450, Throughput: 74.49 samples/sec
2025-03-25 08:19:58,488 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9488.0MB reserved
2025-03-25 08:19:58,488 - training - INFO - Epoch: 207/200000, Batch: 15/45, Loss: 2.8380, Throughput: 70.60 samples/sec
2025-03-25 08:20:10,104 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9490.0MB reserved
2025-03-25 08:20:10,105 - training - INFO - Epoch: 207/200000, Batch: 30/45, Loss: 2.8946, Throughput: 71.42 samples/sec
2025-03-25 08:20:21,016 - training - INFO - Epoch 207 completed in 35.22s. Average loss: 3.0194
2025-03-25 08:20:21,020 - training - INFO - Starting epoch 208/200000
2025-03-25 08:20:21,756 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9626.0MB reserved
2025-03-25 08:20:21,757 - training - INFO - Epoch: 208/200000, Batch: 0/45, Loss: 2.8440, Throughput: 76.18 samples/sec
2025-03-25 08:20:33,695 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9484.0MB reserved
2025-03-25 08:20:33,696 - training - INFO - Epoch: 208/200000, Batch: 15/45, Loss: 2.8656, Throughput: 70.70 samples/sec
2025-03-25 08:20:45,278 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9486.0MB reserved
2025-03-25 08:20:45,278 - training - INFO - Epoch: 208/200000, Batch: 30/45, Loss: 2.9342, Throughput: 71.57 samples/sec
2025-03-25 08:20:56,202 - training - INFO - Epoch 208 completed in 35.18s. Average loss: 2.9887
2025-03-25 08:20:56,205 - training - INFO - Starting epoch 209/200000
2025-03-25 08:20:56,945 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9624.0MB reserved
2025-03-25 08:20:56,946 - training - INFO - Epoch: 209/200000, Batch: 0/45, Loss: 3.0562, Throughput: 75.81 samples/sec
2025-03-25 08:21:08,904 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9492.0MB reserved
2025-03-25 08:21:08,905 - training - INFO - Epoch: 209/200000, Batch: 15/45, Loss: 3.0449, Throughput: 70.57 samples/sec
2025-03-25 08:21:20,569 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9492.0MB reserved
2025-03-25 08:21:20,570 - training - INFO - Epoch: 209/200000, Batch: 30/45, Loss: 3.0085, Throughput: 71.26 samples/sec
2025-03-25 08:21:31,533 - training - INFO - Epoch 209 completed in 35.33s. Average loss: 3.0024
2025-03-25 08:21:31,537 - training - INFO - Starting epoch 210/200000
2025-03-25 08:21:32,284 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9632.0MB reserved
2025-03-25 08:21:32,284 - training - INFO - Epoch: 210/200000, Batch: 0/45, Loss: 3.3125, Throughput: 75.15 samples/sec
2025-03-25 08:21:44,203 - training - INFO - Memory: GPU 0: 3557.2MB allocated, 9478.0MB reserved
2025-03-25 08:21:44,203 - training - INFO - Epoch: 210/200000, Batch: 15/45, Loss: 3.0158, Throughput: 70.75 samples/sec
2025-03-25 08:21:55,850 - training - INFO - Memory: GPU 0: 3557.2MB allocated, 9478.0MB reserved
2025-03-25 08:21:55,851 - training - INFO - Epoch: 210/200000, Batch: 30/45, Loss: 2.9870, Throughput: 71.41 samples/sec
2025-03-25 08:22:06,778 - training - INFO - Epoch 210 completed in 35.24s. Average loss: 2.9538
2025-03-25 08:22:06,781 - training - INFO - Starting epoch 211/200000
2025-03-25 08:22:07,527 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9616.0MB reserved
2025-03-25 08:22:07,528 - training - INFO - Epoch: 211/200000, Batch: 0/45, Loss: 3.2492, Throughput: 75.21 samples/sec
2025-03-25 08:22:19,348 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9508.0MB reserved
2025-03-25 08:22:19,348 - training - INFO - Epoch: 211/200000, Batch: 15/45, Loss: 2.8046, Throughput: 71.31 samples/sec
2025-03-25 08:22:30,846 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9508.0MB reserved
2025-03-25 08:22:30,847 - training - INFO - Epoch: 211/200000, Batch: 30/45, Loss: 2.9090, Throughput: 72.14 samples/sec
2025-03-25 08:22:41,731 - training - INFO - Epoch 211 completed in 34.95s. Average loss: 2.9487
2025-03-25 08:22:41,735 - training - INFO - Starting epoch 212/200000
2025-03-25 08:22:42,470 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9646.0MB reserved
2025-03-25 08:22:42,470 - training - INFO - Epoch: 212/200000, Batch: 0/45, Loss: 3.0570, Throughput: 76.29 samples/sec
2025-03-25 08:22:54,373 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9496.0MB reserved
2025-03-25 08:22:54,373 - training - INFO - Epoch: 212/200000, Batch: 15/45, Loss: 3.0062, Throughput: 70.90 samples/sec
2025-03-25 08:23:05,950 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9496.0MB reserved
2025-03-25 08:23:05,951 - training - INFO - Epoch: 212/200000, Batch: 30/45, Loss: 2.9623, Throughput: 71.69 samples/sec
2025-03-25 08:23:16,900 - training - INFO - Epoch 212 completed in 35.17s. Average loss: 2.9757
2025-03-25 08:23:16,904 - training - INFO - Starting epoch 213/200000
2025-03-25 08:23:17,655 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9636.0MB reserved
2025-03-25 08:23:17,655 - training - INFO - Epoch: 213/200000, Batch: 0/45, Loss: 3.0884, Throughput: 74.74 samples/sec
2025-03-25 08:23:29,629 - training - INFO - Memory: GPU 0: 3562.5MB allocated, 9504.0MB reserved
2025-03-25 08:23:29,629 - training - INFO - Epoch: 213/200000, Batch: 15/45, Loss: 2.9736, Throughput: 70.42 samples/sec
2025-03-25 08:23:41,329 - training - INFO - Memory: GPU 0: 3562.5MB allocated, 9506.0MB reserved
2025-03-25 08:23:41,330 - training - INFO - Epoch: 213/200000, Batch: 30/45, Loss: 2.9393, Throughput: 71.08 samples/sec
2025-03-25 08:23:52,350 - training - INFO - Epoch 213 completed in 35.45s. Average loss: 2.9630
2025-03-25 08:23:52,354 - training - INFO - Starting epoch 214/200000
2025-03-25 08:23:53,082 - training - INFO - Memory: GPU 0: 3562.5MB allocated, 9644.0MB reserved
2025-03-25 08:23:53,082 - training - INFO - Epoch: 214/200000, Batch: 0/45, Loss: 2.8598, Throughput: 77.08 samples/sec
2025-03-25 08:24:05,042 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9492.0MB reserved
2025-03-25 08:24:05,043 - training - INFO - Epoch: 214/200000, Batch: 15/45, Loss: 2.8635, Throughput: 70.63 samples/sec
2025-03-25 08:24:16,647 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9492.0MB reserved
2025-03-25 08:24:16,648 - training - INFO - Epoch: 214/200000, Batch: 30/45, Loss: 2.8623, Throughput: 71.46 samples/sec
2025-03-25 08:24:27,568 - training - INFO - Epoch 214 completed in 35.21s. Average loss: 2.9148
2025-03-25 08:24:27,572 - training - INFO - Starting epoch 215/200000
2025-03-25 08:24:28,324 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9630.0MB reserved
2025-03-25 08:24:28,325 - training - INFO - Epoch: 215/200000, Batch: 0/45, Loss: 2.8684, Throughput: 74.63 samples/sec
2025-03-25 08:24:40,287 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9502.0MB reserved
2025-03-25 08:24:40,287 - training - INFO - Epoch: 215/200000, Batch: 15/45, Loss: 2.8630, Throughput: 70.48 samples/sec
2025-03-25 08:24:52,042 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9504.0MB reserved
2025-03-25 08:24:52,043 - training - INFO - Epoch: 215/200000, Batch: 30/45, Loss: 2.9898, Throughput: 70.95 samples/sec
2025-03-25 08:25:03,002 - training - INFO - Epoch 215 completed in 35.43s. Average loss: 2.9874
2025-03-25 08:25:03,006 - training - INFO - Starting epoch 216/200000
2025-03-25 08:25:03,740 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9642.0MB reserved
2025-03-25 08:25:03,740 - training - INFO - Epoch: 216/200000, Batch: 0/45, Loss: 2.9993, Throughput: 76.45 samples/sec
2025-03-25 08:25:15,590 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9510.0MB reserved
2025-03-25 08:25:15,590 - training - INFO - Epoch: 216/200000, Batch: 15/45, Loss: 3.0157, Throughput: 71.21 samples/sec
2025-03-25 08:25:27,161 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9510.0MB reserved
2025-03-25 08:25:27,161 - training - INFO - Epoch: 216/200000, Batch: 30/45, Loss: 2.9548, Throughput: 71.88 samples/sec
2025-03-25 08:25:38,084 - training - INFO - Epoch 216 completed in 35.08s. Average loss: 2.9895
2025-03-25 08:25:38,088 - training - INFO - Starting epoch 217/200000
2025-03-25 08:25:38,825 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9648.0MB reserved
2025-03-25 08:25:38,826 - training - INFO - Epoch: 217/200000, Batch: 0/45, Loss: 2.8037, Throughput: 76.14 samples/sec
2025-03-25 08:25:50,744 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9504.0MB reserved
2025-03-25 08:25:50,744 - training - INFO - Epoch: 217/200000, Batch: 15/45, Loss: 2.9347, Throughput: 70.80 samples/sec
2025-03-25 08:26:02,361 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9504.0MB reserved
2025-03-25 08:26:02,361 - training - INFO - Epoch: 217/200000, Batch: 30/45, Loss: 2.9382, Throughput: 71.52 samples/sec
2025-03-25 08:26:13,315 - training - INFO - Epoch 217 completed in 35.23s. Average loss: 2.9244
2025-03-25 08:26:13,319 - training - INFO - Starting epoch 218/200000
2025-03-25 08:26:14,056 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9642.0MB reserved
2025-03-25 08:26:14,057 - training - INFO - Epoch: 218/200000, Batch: 0/45, Loss: 2.7374, Throughput: 76.15 samples/sec
2025-03-25 08:26:25,895 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9496.0MB reserved
2025-03-25 08:26:25,895 - training - INFO - Epoch: 218/200000, Batch: 15/45, Loss: 2.9318, Throughput: 71.26 samples/sec
2025-03-25 08:26:37,588 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9496.0MB reserved
2025-03-25 08:26:37,589 - training - INFO - Epoch: 218/200000, Batch: 30/45, Loss: 2.9202, Throughput: 71.54 samples/sec
2025-03-25 08:26:48,558 - training - INFO - Epoch 218 completed in 35.24s. Average loss: 2.9269
2025-03-25 08:26:48,562 - training - INFO - Starting epoch 219/200000
2025-03-25 08:26:49,281 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9634.0MB reserved
2025-03-25 08:26:49,281 - training - INFO - Epoch: 219/200000, Batch: 0/45, Loss: 2.8896, Throughput: 78.13 samples/sec
2025-03-25 08:27:01,111 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9494.0MB reserved
2025-03-25 08:27:01,111 - training - INFO - Epoch: 219/200000, Batch: 15/45, Loss: 2.9341, Throughput: 71.41 samples/sec
2025-03-25 08:27:12,714 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9494.0MB reserved
2025-03-25 08:27:12,714 - training - INFO - Epoch: 219/200000, Batch: 30/45, Loss: 2.9094, Throughput: 71.88 samples/sec
2025-03-25 08:27:23,618 - training - INFO - Epoch 219 completed in 35.06s. Average loss: 2.9101
2025-03-25 08:27:23,621 - training - INFO - Starting epoch 220/200000
2025-03-25 08:27:24,379 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9632.0MB reserved
2025-03-25 08:27:24,379 - training - INFO - Epoch: 220/200000, Batch: 0/45, Loss: 2.9664, Throughput: 74.00 samples/sec
2025-03-25 08:27:36,294 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9504.0MB reserved
2025-03-25 08:27:36,295 - training - INFO - Epoch: 220/200000, Batch: 15/45, Loss: 2.7956, Throughput: 70.71 samples/sec
2025-03-25 08:27:47,914 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9506.0MB reserved
2025-03-25 08:27:47,914 - training - INFO - Epoch: 220/200000, Batch: 30/45, Loss: 2.8655, Throughput: 71.47 samples/sec
2025-03-25 08:27:58,857 - training - INFO - Epoch 220 completed in 35.24s. Average loss: 2.8973
2025-03-25 08:27:58,860 - training - INFO - Starting epoch 221/200000
2025-03-25 08:27:59,594 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9644.0MB reserved
2025-03-25 08:27:59,595 - training - INFO - Epoch: 221/200000, Batch: 0/45, Loss: 2.7169, Throughput: 76.51 samples/sec
2025-03-25 08:28:11,523 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9504.0MB reserved
2025-03-25 08:28:11,523 - training - INFO - Epoch: 221/200000, Batch: 15/45, Loss: 2.8568, Throughput: 70.77 samples/sec
2025-03-25 08:28:23,182 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9506.0MB reserved
2025-03-25 08:28:23,183 - training - INFO - Epoch: 221/200000, Batch: 30/45, Loss: 2.8678, Throughput: 71.38 samples/sec
2025-03-25 08:28:34,107 - training - INFO - Epoch 221 completed in 35.25s. Average loss: 2.9113
2025-03-25 08:28:34,111 - training - INFO - Starting epoch 222/200000
2025-03-25 08:28:34,871 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9644.0MB reserved
2025-03-25 08:28:34,871 - training - INFO - Epoch: 222/200000, Batch: 0/45, Loss: 2.5946, Throughput: 73.86 samples/sec
2025-03-25 08:28:46,832 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9504.0MB reserved
2025-03-25 08:28:46,833 - training - INFO - Epoch: 222/200000, Batch: 15/45, Loss: 2.9031, Throughput: 70.43 samples/sec
2025-03-25 08:28:58,493 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9504.0MB reserved
2025-03-25 08:28:58,494 - training - INFO - Epoch: 222/200000, Batch: 30/45, Loss: 2.8767, Throughput: 71.20 samples/sec
2025-03-25 08:29:09,449 - training - INFO - Epoch 222 completed in 35.34s. Average loss: 2.9037
2025-03-25 08:29:09,452 - training - INFO - Starting epoch 223/200000
2025-03-25 08:29:10,201 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9642.0MB reserved
2025-03-25 08:29:10,206 - training - INFO - Epoch: 223/200000, Batch: 0/45, Loss: 3.0229, Throughput: 74.99 samples/sec
2025-03-25 08:29:22,100 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9488.0MB reserved
2025-03-25 08:29:22,100 - training - INFO - Epoch: 223/200000, Batch: 15/45, Loss: 2.9484, Throughput: 70.86 samples/sec
2025-03-25 08:29:33,856 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9488.0MB reserved
2025-03-25 08:29:33,857 - training - INFO - Epoch: 223/200000, Batch: 30/45, Loss: 2.8767, Throughput: 71.14 samples/sec
2025-03-25 08:29:44,874 - training - INFO - Epoch 223 completed in 35.42s. Average loss: 2.9252
2025-03-25 08:29:44,878 - training - INFO - Starting epoch 224/200000
2025-03-25 08:29:45,617 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9626.0MB reserved
2025-03-25 08:29:45,617 - training - INFO - Epoch: 224/200000, Batch: 0/45, Loss: 2.6699, Throughput: 75.92 samples/sec
2025-03-25 08:29:57,521 - training - INFO - Memory: GPU 0: 3555.9MB allocated, 9500.0MB reserved
2025-03-25 08:29:57,521 - training - INFO - Epoch: 224/200000, Batch: 15/45, Loss: 2.9363, Throughput: 70.88 samples/sec
2025-03-25 08:30:09,221 - training - INFO - Memory: GPU 0: 3555.9MB allocated, 9500.0MB reserved
2025-03-25 08:30:09,221 - training - INFO - Epoch: 224/200000, Batch: 30/45, Loss: 2.8846, Throughput: 71.32 samples/sec
2025-03-25 08:30:20,205 - training - INFO - Epoch 224 completed in 35.33s. Average loss: 2.9354
2025-03-25 08:30:20,208 - training - INFO - Starting epoch 225/200000
2025-03-25 08:30:20,962 - training - INFO - Memory: GPU 0: 3555.9MB allocated, 9638.0MB reserved
2025-03-25 08:30:20,963 - training - INFO - Epoch: 225/200000, Batch: 0/45, Loss: 2.5441, Throughput: 74.45 samples/sec
2025-03-25 08:30:32,875 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9508.0MB reserved
2025-03-25 08:30:32,875 - training - INFO - Epoch: 225/200000, Batch: 15/45, Loss: 2.8624, Throughput: 70.74 samples/sec
2025-03-25 08:30:44,595 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9508.0MB reserved
2025-03-25 08:30:44,595 - training - INFO - Epoch: 225/200000, Batch: 30/45, Loss: 2.8435, Throughput: 71.19 samples/sec
2025-03-25 08:30:55,595 - training - INFO - Epoch 225 completed in 35.39s. Average loss: 2.9125
2025-03-25 08:30:55,599 - training - INFO - Starting epoch 226/200000
2025-03-25 08:30:56,340 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9646.0MB reserved
2025-03-25 08:30:56,341 - training - INFO - Epoch: 226/200000, Batch: 0/45, Loss: 2.7409, Throughput: 75.69 samples/sec
2025-03-25 08:31:08,198 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9490.0MB reserved
2025-03-25 08:31:08,198 - training - INFO - Epoch: 226/200000, Batch: 15/45, Loss: 2.8976, Throughput: 71.12 samples/sec
2025-03-25 08:31:19,759 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9490.0MB reserved
2025-03-25 08:31:19,759 - training - INFO - Epoch: 226/200000, Batch: 30/45, Loss: 2.8507, Throughput: 71.86 samples/sec
2025-03-25 08:31:30,716 - training - INFO - Epoch 226 completed in 35.12s. Average loss: 2.8995
2025-03-25 08:31:30,719 - training - INFO - Starting epoch 227/200000
2025-03-25 08:31:31,484 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9628.0MB reserved
2025-03-25 08:31:31,484 - training - INFO - Epoch: 227/200000, Batch: 0/45, Loss: 2.8726, Throughput: 73.41 samples/sec
2025-03-25 08:31:43,447 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9508.0MB reserved
2025-03-25 08:31:43,447 - training - INFO - Epoch: 227/200000, Batch: 15/45, Loss: 2.9164, Throughput: 70.41 samples/sec
2025-03-25 08:31:55,106 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9508.0MB reserved
2025-03-25 08:31:55,107 - training - INFO - Epoch: 227/200000, Batch: 30/45, Loss: 2.8908, Throughput: 71.19 samples/sec
2025-03-25 08:32:06,051 - training - INFO - Epoch 227 completed in 35.33s. Average loss: 2.8858
2025-03-25 08:32:06,055 - training - INFO - Starting epoch 228/200000
2025-03-25 08:32:06,798 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9646.0MB reserved
2025-03-25 08:32:06,798 - training - INFO - Epoch: 228/200000, Batch: 0/45, Loss: 2.9924, Throughput: 75.55 samples/sec
2025-03-25 08:32:18,666 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9498.0MB reserved
2025-03-25 08:32:18,666 - training - INFO - Epoch: 228/200000, Batch: 15/45, Loss: 2.7298, Throughput: 71.06 samples/sec
2025-03-25 08:32:30,274 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9498.0MB reserved
2025-03-25 08:32:30,274 - training - INFO - Epoch: 228/200000, Batch: 30/45, Loss: 2.8082, Throughput: 71.69 samples/sec
2025-03-25 08:32:41,182 - training - INFO - Epoch 228 completed in 35.13s. Average loss: 2.8428
2025-03-25 08:32:41,187 - training - INFO - Starting epoch 229/200000
2025-03-25 08:32:41,940 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9636.0MB reserved
2025-03-25 08:32:41,940 - training - INFO - Epoch: 229/200000, Batch: 0/45, Loss: 2.6836, Throughput: 74.60 samples/sec
2025-03-25 08:32:53,877 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9512.0MB reserved
2025-03-25 08:32:53,877 - training - INFO - Epoch: 229/200000, Batch: 15/45, Loss: 2.8949, Throughput: 70.61 samples/sec
2025-03-25 08:33:05,553 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9512.0MB reserved
2025-03-25 08:33:05,554 - training - INFO - Epoch: 229/200000, Batch: 30/45, Loss: 2.9064, Throughput: 71.25 samples/sec
2025-03-25 08:33:16,526 - training - INFO - Epoch 229 completed in 35.34s. Average loss: 2.8820
2025-03-25 08:33:16,530 - training - INFO - Starting epoch 230/200000
2025-03-25 08:33:17,280 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9648.0MB reserved
2025-03-25 08:33:17,280 - training - INFO - Epoch: 230/200000, Batch: 0/45, Loss: 3.1778, Throughput: 74.76 samples/sec
2025-03-25 08:33:29,254 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9488.0MB reserved
2025-03-25 08:33:29,254 - training - INFO - Epoch: 230/200000, Batch: 15/45, Loss: 2.9020, Throughput: 70.43 samples/sec
2025-03-25 08:33:40,903 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9488.0MB reserved
2025-03-25 08:33:40,903 - training - INFO - Epoch: 230/200000, Batch: 30/45, Loss: 2.8235, Throughput: 71.23 samples/sec
2025-03-25 08:33:51,808 - training - INFO - Epoch 230 completed in 35.28s. Average loss: 2.8699
2025-03-25 08:33:51,812 - training - INFO - Starting epoch 231/200000
2025-03-25 08:33:52,556 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9628.0MB reserved
2025-03-25 08:33:52,556 - training - INFO - Epoch: 231/200000, Batch: 0/45, Loss: 3.1264, Throughput: 75.45 samples/sec
2025-03-25 08:34:04,425 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9490.0MB reserved
2025-03-25 08:34:04,425 - training - INFO - Epoch: 231/200000, Batch: 15/45, Loss: 2.7858, Throughput: 71.04 samples/sec
2025-03-25 08:34:16,008 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9490.0MB reserved
2025-03-25 08:34:16,009 - training - INFO - Epoch: 231/200000, Batch: 30/45, Loss: 2.8373, Throughput: 71.75 samples/sec
2025-03-25 08:34:26,944 - training - INFO - Epoch 231 completed in 35.13s. Average loss: 2.8416
2025-03-25 08:34:26,947 - training - INFO - Starting epoch 232/200000
2025-03-25 08:34:27,684 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9628.0MB reserved
2025-03-25 08:34:27,684 - training - INFO - Epoch: 232/200000, Batch: 0/45, Loss: 2.5087, Throughput: 76.19 samples/sec
2025-03-25 08:34:39,567 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9494.0MB reserved
2025-03-25 08:34:39,568 - training - INFO - Epoch: 232/200000, Batch: 15/45, Loss: 2.8363, Throughput: 71.01 samples/sec
2025-03-25 08:34:51,169 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9496.0MB reserved
2025-03-25 08:34:51,169 - training - INFO - Epoch: 232/200000, Batch: 30/45, Loss: 2.8224, Throughput: 71.68 samples/sec
2025-03-25 08:35:02,104 - training - INFO - Epoch 232 completed in 35.16s. Average loss: 2.8389
2025-03-25 08:35:02,108 - training - INFO - Starting epoch 233/200000
2025-03-25 08:35:02,860 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9634.0MB reserved
2025-03-25 08:35:02,860 - training - INFO - Epoch: 233/200000, Batch: 0/45, Loss: 2.8154, Throughput: 74.52 samples/sec
2025-03-25 08:35:14,756 - training - INFO - Memory: GPU 0: 3562.9MB allocated, 9500.0MB reserved
2025-03-25 08:35:14,756 - training - INFO - Epoch: 233/200000, Batch: 15/45, Loss: 2.8737, Throughput: 70.85 samples/sec
2025-03-25 08:35:26,341 - training - INFO - Memory: GPU 0: 3562.9MB allocated, 9500.0MB reserved
2025-03-25 08:35:26,342 - training - INFO - Epoch: 233/200000, Batch: 30/45, Loss: 2.8485, Throughput: 71.64 samples/sec
2025-03-25 08:35:37,307 - training - INFO - Epoch 233 completed in 35.20s. Average loss: 2.8412
2025-03-25 08:35:37,310 - training - INFO - Starting epoch 234/200000
2025-03-25 08:35:38,060 - training - INFO - Memory: GPU 0: 3562.9MB allocated, 9638.0MB reserved
2025-03-25 08:35:38,060 - training - INFO - Epoch: 234/200000, Batch: 0/45, Loss: 2.8996, Throughput: 74.75 samples/sec
2025-03-25 08:35:50,093 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9498.0MB reserved
2025-03-25 08:35:50,093 - training - INFO - Epoch: 234/200000, Batch: 15/45, Loss: 2.7927, Throughput: 70.10 samples/sec
2025-03-25 08:36:01,767 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9498.0MB reserved
2025-03-25 08:36:01,768 - training - INFO - Epoch: 234/200000, Batch: 30/45, Loss: 2.8890, Throughput: 70.99 samples/sec
2025-03-25 08:36:12,719 - training - INFO - Epoch 234 completed in 35.41s. Average loss: 2.8894
2025-03-25 08:36:12,723 - training - INFO - Starting epoch 235/200000
2025-03-25 08:36:13,480 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9636.0MB reserved
2025-03-25 08:36:13,480 - training - INFO - Epoch: 235/200000, Batch: 0/45, Loss: 2.4934, Throughput: 74.02 samples/sec
2025-03-25 08:36:25,366 - training - INFO - Memory: GPU 0: 3562.6MB allocated, 9504.0MB reserved
2025-03-25 08:36:25,367 - training - INFO - Epoch: 235/200000, Batch: 15/45, Loss: 2.8019, Throughput: 70.88 samples/sec
2025-03-25 08:36:36,972 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9504.0MB reserved
2025-03-25 08:36:36,973 - training - INFO - Epoch: 235/200000, Batch: 30/45, Loss: 2.8161, Throughput: 71.60 samples/sec
2025-03-25 08:36:48,013 - training - INFO - Epoch 235 completed in 35.29s. Average loss: 2.8151
2025-03-25 08:36:48,017 - training - INFO - Starting epoch 236/200000
2025-03-25 08:36:48,771 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9640.0MB reserved
2025-03-25 08:36:48,771 - training - INFO - Epoch: 236/200000, Batch: 0/45, Loss: 2.7705, Throughput: 74.30 samples/sec
2025-03-25 08:37:00,750 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9502.0MB reserved
2025-03-25 08:37:00,751 - training - INFO - Epoch: 236/200000, Batch: 15/45, Loss: 2.8514, Throughput: 70.38 samples/sec
2025-03-25 08:37:12,382 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9502.0MB reserved
2025-03-25 08:37:12,382 - training - INFO - Epoch: 236/200000, Batch: 30/45, Loss: 2.8445, Throughput: 71.25 samples/sec
2025-03-25 08:37:23,341 - training - INFO - Epoch 236 completed in 35.32s. Average loss: 2.9017
2025-03-25 08:37:23,345 - training - INFO - Starting epoch 237/200000
2025-03-25 08:37:24,091 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9640.0MB reserved
2025-03-25 08:37:24,092 - training - INFO - Epoch: 237/200000, Batch: 0/45, Loss: 3.1319, Throughput: 75.23 samples/sec
2025-03-25 08:37:36,050 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9498.0MB reserved
2025-03-25 08:37:36,050 - training - INFO - Epoch: 237/200000, Batch: 15/45, Loss: 2.9890, Throughput: 70.53 samples/sec
2025-03-25 08:37:47,760 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9498.0MB reserved
2025-03-25 08:37:47,760 - training - INFO - Epoch: 237/200000, Batch: 30/45, Loss: 2.8903, Throughput: 71.11 samples/sec
2025-03-25 08:37:58,761 - training - INFO - Epoch 237 completed in 35.42s. Average loss: 2.8714
2025-03-25 08:37:58,766 - training - INFO - Starting epoch 238/200000
2025-03-25 08:37:59,505 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9636.0MB reserved
2025-03-25 08:37:59,505 - training - INFO - Epoch: 238/200000, Batch: 0/45, Loss: 3.3709, Throughput: 76.01 samples/sec
2025-03-25 08:38:11,404 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9510.0MB reserved
2025-03-25 08:38:11,404 - training - INFO - Epoch: 238/200000, Batch: 15/45, Loss: 2.7821, Throughput: 70.91 samples/sec
2025-03-25 08:38:22,911 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9510.0MB reserved
2025-03-25 08:38:22,911 - training - INFO - Epoch: 238/200000, Batch: 30/45, Loss: 2.7669, Throughput: 71.91 samples/sec
2025-03-25 08:38:33,805 - training - INFO - Epoch 238 completed in 35.04s. Average loss: 2.8897
2025-03-25 08:38:33,809 - training - INFO - Starting epoch 239/200000
2025-03-25 08:38:34,547 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9646.0MB reserved
2025-03-25 08:38:34,547 - training - INFO - Epoch: 239/200000, Batch: 0/45, Loss: 2.9194, Throughput: 75.98 samples/sec
2025-03-25 08:38:46,474 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9506.0MB reserved
2025-03-25 08:38:46,474 - training - INFO - Epoch: 239/200000, Batch: 15/45, Loss: 2.8969, Throughput: 70.75 samples/sec
2025-03-25 08:38:58,022 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9506.0MB reserved
2025-03-25 08:38:58,022 - training - INFO - Epoch: 239/200000, Batch: 30/45, Loss: 2.8875, Throughput: 71.70 samples/sec
2025-03-25 08:39:08,964 - training - INFO - Epoch 239 completed in 35.16s. Average loss: 2.8308
2025-03-25 08:39:08,968 - training - INFO - Starting epoch 240/200000
2025-03-25 08:39:09,711 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9644.0MB reserved
2025-03-25 08:39:09,711 - training - INFO - Epoch: 240/200000, Batch: 0/45, Loss: 2.6678, Throughput: 75.44 samples/sec
2025-03-25 08:39:21,619 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9510.0MB reserved
2025-03-25 08:39:21,619 - training - INFO - Epoch: 240/200000, Batch: 15/45, Loss: 2.6720, Throughput: 70.83 samples/sec
2025-03-25 08:39:33,262 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9510.0MB reserved
2025-03-25 08:39:33,263 - training - INFO - Epoch: 240/200000, Batch: 30/45, Loss: 2.7432, Throughput: 71.46 samples/sec
2025-03-25 08:39:44,225 - training - INFO - Epoch 240 completed in 35.26s. Average loss: 2.7914
2025-03-25 08:39:44,231 - training - INFO - Starting epoch 241/200000
2025-03-25 08:39:44,967 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9646.0MB reserved
2025-03-25 08:39:44,967 - training - INFO - Epoch: 241/200000, Batch: 0/45, Loss: 2.5278, Throughput: 76.26 samples/sec
2025-03-25 08:39:56,870 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9508.0MB reserved
2025-03-25 08:39:56,871 - training - INFO - Epoch: 241/200000, Batch: 15/45, Loss: 2.8866, Throughput: 70.91 samples/sec
2025-03-25 08:40:08,443 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9508.0MB reserved
2025-03-25 08:40:08,443 - training - INFO - Epoch: 241/200000, Batch: 30/45, Loss: 2.8176, Throughput: 71.71 samples/sec
2025-03-25 08:40:19,400 - training - INFO - Epoch 241 completed in 35.17s. Average loss: 2.8238
2025-03-25 08:40:19,403 - training - INFO - Starting epoch 242/200000
2025-03-25 08:40:20,147 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9646.0MB reserved
2025-03-25 08:40:20,147 - training - INFO - Epoch: 242/200000, Batch: 0/45, Loss: 2.3742, Throughput: 75.63 samples/sec
2025-03-25 08:40:31,985 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9500.0MB reserved
2025-03-25 08:40:31,986 - training - INFO - Epoch: 242/200000, Batch: 15/45, Loss: 2.7500, Throughput: 71.23 samples/sec
2025-03-25 08:40:43,633 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9500.0MB reserved
2025-03-25 08:40:43,633 - training - INFO - Epoch: 242/200000, Batch: 30/45, Loss: 2.8396, Throughput: 71.65 samples/sec
2025-03-25 08:40:54,592 - training - INFO - Epoch 242 completed in 35.19s. Average loss: 2.8388
2025-03-25 08:40:54,596 - training - INFO - Starting epoch 243/200000
2025-03-25 08:40:55,331 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9638.0MB reserved
2025-03-25 08:40:55,331 - training - INFO - Epoch: 243/200000, Batch: 0/45, Loss: 2.2498, Throughput: 76.18 samples/sec
2025-03-25 08:41:07,307 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9494.0MB reserved
2025-03-25 08:41:07,307 - training - INFO - Epoch: 243/200000, Batch: 15/45, Loss: 2.7769, Throughput: 70.49 samples/sec
2025-03-25 08:41:19,046 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9494.0MB reserved
2025-03-25 08:41:19,046 - training - INFO - Epoch: 243/200000, Batch: 30/45, Loss: 2.8055, Throughput: 71.00 samples/sec
2025-03-25 08:41:30,013 - training - INFO - Epoch 243 completed in 35.42s. Average loss: 2.8031
2025-03-25 08:41:30,017 - training - INFO - Starting epoch 244/200000
2025-03-25 08:41:30,768 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9632.0MB reserved
2025-03-25 08:41:30,769 - training - INFO - Epoch: 244/200000, Batch: 0/45, Loss: 3.4360, Throughput: 74.76 samples/sec
2025-03-25 08:41:42,711 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9500.0MB reserved
2025-03-25 08:41:42,712 - training - INFO - Epoch: 244/200000, Batch: 15/45, Loss: 2.9431, Throughput: 70.60 samples/sec
2025-03-25 08:41:54,317 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9500.0MB reserved
2025-03-25 08:41:54,317 - training - INFO - Epoch: 244/200000, Batch: 30/45, Loss: 2.8788, Throughput: 71.45 samples/sec
2025-03-25 08:42:05,265 - training - INFO - Epoch 244 completed in 35.25s. Average loss: 2.8131
2025-03-25 08:42:05,269 - training - INFO - Starting epoch 245/200000
2025-03-25 08:42:06,001 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9638.0MB reserved
2025-03-25 08:42:06,001 - training - INFO - Epoch: 245/200000, Batch: 0/45, Loss: 2.6616, Throughput: 76.57 samples/sec
2025-03-25 08:42:17,869 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9508.0MB reserved
2025-03-25 08:42:17,869 - training - INFO - Epoch: 245/200000, Batch: 15/45, Loss: 2.6813, Throughput: 71.12 samples/sec
2025-03-25 08:42:29,384 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9508.0MB reserved
2025-03-25 08:42:29,385 - training - INFO - Epoch: 245/200000, Batch: 30/45, Loss: 2.7388, Throughput: 71.99 samples/sec
2025-03-25 08:42:40,330 - training - INFO - Epoch 245 completed in 35.06s. Average loss: 2.7878
2025-03-25 08:42:40,334 - training - INFO - Starting epoch 246/200000
2025-03-25 08:42:41,069 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9648.0MB reserved
2025-03-25 08:42:41,069 - training - INFO - Epoch: 246/200000, Batch: 0/45, Loss: 3.0302, Throughput: 76.35 samples/sec
2025-03-25 08:42:52,921 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9502.0MB reserved
2025-03-25 08:42:52,921 - training - INFO - Epoch: 246/200000, Batch: 15/45, Loss: 2.8100, Throughput: 71.19 samples/sec
2025-03-25 08:43:04,439 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9502.0MB reserved
2025-03-25 08:43:04,440 - training - INFO - Epoch: 246/200000, Batch: 30/45, Loss: 2.7893, Throughput: 72.02 samples/sec
2025-03-25 08:43:15,401 - training - INFO - Epoch 246 completed in 35.07s. Average loss: 2.8335
2025-03-25 08:43:15,405 - training - INFO - Starting epoch 247/200000
2025-03-25 08:43:16,180 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9640.0MB reserved
2025-03-25 08:43:16,180 - training - INFO - Epoch: 247/200000, Batch: 0/45, Loss: 3.0266, Throughput: 72.48 samples/sec
2025-03-25 08:43:28,070 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9492.0MB reserved
2025-03-25 08:43:28,071 - training - INFO - Epoch: 247/200000, Batch: 15/45, Loss: 2.8080, Throughput: 70.76 samples/sec
2025-03-25 08:43:39,674 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9494.0MB reserved
2025-03-25 08:43:39,675 - training - INFO - Epoch: 247/200000, Batch: 30/45, Loss: 2.7927, Throughput: 71.54 samples/sec
2025-03-25 08:43:50,578 - training - INFO - Epoch 247 completed in 35.17s. Average loss: 2.8138
2025-03-25 08:43:50,582 - training - INFO - Starting epoch 248/200000
2025-03-25 08:43:51,328 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9632.0MB reserved
2025-03-25 08:43:51,328 - training - INFO - Epoch: 248/200000, Batch: 0/45, Loss: 3.0468, Throughput: 75.12 samples/sec
2025-03-25 08:44:03,249 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9508.0MB reserved
2025-03-25 08:44:03,250 - training - INFO - Epoch: 248/200000, Batch: 15/45, Loss: 2.8846, Throughput: 70.74 samples/sec
2025-03-25 08:44:14,854 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9510.0MB reserved
2025-03-25 08:44:14,854 - training - INFO - Epoch: 248/200000, Batch: 30/45, Loss: 2.8316, Throughput: 71.52 samples/sec
2025-03-25 08:44:25,801 - training - INFO - Epoch 248 completed in 35.22s. Average loss: 2.8351
2025-03-25 08:44:25,804 - training - INFO - Starting epoch 249/200000
2025-03-25 08:44:26,566 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9648.0MB reserved
2025-03-25 08:44:26,566 - training - INFO - Epoch: 249/200000, Batch: 0/45, Loss: 2.3901, Throughput: 73.66 samples/sec
2025-03-25 08:44:38,488 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9494.0MB reserved
2025-03-25 08:44:38,489 - training - INFO - Epoch: 249/200000, Batch: 15/45, Loss: 2.7203, Throughput: 70.66 samples/sec
2025-03-25 08:44:50,155 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9494.0MB reserved
2025-03-25 08:44:50,156 - training - INFO - Epoch: 249/200000, Batch: 30/45, Loss: 2.7086, Throughput: 71.30 samples/sec
2025-03-25 08:45:01,114 - training - INFO - Epoch 249 completed in 35.31s. Average loss: 2.7661
2025-03-25 08:45:01,118 - training - INFO - Starting epoch 250/200000
2025-03-25 08:45:01,862 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9632.0MB reserved
2025-03-25 08:45:01,862 - training - INFO - Epoch: 250/200000, Batch: 0/45, Loss: 2.9875, Throughput: 75.40 samples/sec
2025-03-25 08:45:13,739 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9500.0MB reserved
2025-03-25 08:45:13,740 - training - INFO - Epoch: 250/200000, Batch: 15/45, Loss: 2.7587, Throughput: 71.00 samples/sec
2025-03-25 08:45:25,330 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9500.0MB reserved
2025-03-25 08:45:25,330 - training - INFO - Epoch: 250/200000, Batch: 30/45, Loss: 2.7521, Throughput: 71.71 samples/sec
2025-03-25 08:45:36,238 - training - INFO - Epoch 250 completed in 35.12s. Average loss: 2.7600
2025-03-25 08:45:36,242 - training - INFO - Starting epoch 251/200000
2025-03-25 08:45:36,975 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9638.0MB reserved
2025-03-25 08:45:36,975 - training - INFO - Epoch: 251/200000, Batch: 0/45, Loss: 2.9946, Throughput: 76.59 samples/sec
2025-03-25 08:45:48,957 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9500.0MB reserved
2025-03-25 08:45:48,957 - training - INFO - Epoch: 251/200000, Batch: 15/45, Loss: 2.8106, Throughput: 70.48 samples/sec
2025-03-25 08:46:00,619 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9500.0MB reserved
2025-03-25 08:46:00,619 - training - INFO - Epoch: 251/200000, Batch: 30/45, Loss: 2.7250, Throughput: 71.22 samples/sec
2025-03-25 08:46:11,608 - training - INFO - Epoch 251 completed in 35.37s. Average loss: 2.7754
2025-03-25 08:46:11,612 - training - INFO - Starting epoch 252/200000
2025-03-25 08:46:12,348 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9636.0MB reserved
2025-03-25 08:46:12,348 - training - INFO - Epoch: 252/200000, Batch: 0/45, Loss: 2.8522, Throughput: 76.31 samples/sec
2025-03-25 08:46:24,249 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9504.0MB reserved
2025-03-25 08:46:24,249 - training - INFO - Epoch: 252/200000, Batch: 15/45, Loss: 2.8067, Throughput: 70.92 samples/sec
2025-03-25 08:46:35,955 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9504.0MB reserved
2025-03-25 08:46:35,955 - training - INFO - Epoch: 252/200000, Batch: 30/45, Loss: 2.7466, Throughput: 71.32 samples/sec
2025-03-25 08:46:46,931 - training - INFO - Epoch 252 completed in 35.32s. Average loss: 2.7915
2025-03-25 08:46:46,935 - training - INFO - Starting epoch 253/200000
2025-03-25 08:46:47,693 - training - INFO - Memory: GPU 0: 3563.3MB allocated, 9642.0MB reserved
2025-03-25 08:46:47,694 - training - INFO - Epoch: 253/200000, Batch: 0/45, Loss: 2.6297, Throughput: 73.98 samples/sec
2025-03-25 08:46:59,588 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9492.0MB reserved
2025-03-25 08:46:59,589 - training - INFO - Epoch: 253/200000, Batch: 15/45, Loss: 2.6845, Throughput: 70.82 samples/sec
2025-03-25 08:47:11,100 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9492.0MB reserved
2025-03-25 08:47:11,101 - training - INFO - Epoch: 253/200000, Batch: 30/45, Loss: 2.7898, Throughput: 71.85 samples/sec
2025-03-25 08:47:22,028 - training - INFO - Epoch 253 completed in 35.09s. Average loss: 2.8170
2025-03-25 08:47:22,031 - training - INFO - Starting epoch 254/200000
2025-03-25 08:47:22,779 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9630.0MB reserved
2025-03-25 08:47:22,779 - training - INFO - Epoch: 254/200000, Batch: 0/45, Loss: 2.6997, Throughput: 74.98 samples/sec
2025-03-25 08:47:34,742 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9508.0MB reserved
2025-03-25 08:47:34,743 - training - INFO - Epoch: 254/200000, Batch: 15/45, Loss: 2.7660, Throughput: 70.50 samples/sec
2025-03-25 08:47:46,348 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9508.0MB reserved
2025-03-25 08:47:46,348 - training - INFO - Epoch: 254/200000, Batch: 30/45, Loss: 2.7803, Throughput: 71.40 samples/sec
2025-03-25 08:47:57,308 - training - INFO - Epoch 254 completed in 35.28s. Average loss: 2.7474
2025-03-25 08:47:57,311 - training - INFO - Starting epoch 255/200000
2025-03-25 08:47:58,044 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9646.0MB reserved
2025-03-25 08:47:58,044 - training - INFO - Epoch: 255/200000, Batch: 0/45, Loss: 3.0567, Throughput: 76.57 samples/sec
2025-03-25 08:48:09,914 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9498.0MB reserved
2025-03-25 08:48:09,915 - training - INFO - Epoch: 255/200000, Batch: 15/45, Loss: 2.8183, Throughput: 71.10 samples/sec
2025-03-25 08:48:21,502 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9498.0MB reserved
2025-03-25 08:48:21,502 - training - INFO - Epoch: 255/200000, Batch: 30/45, Loss: 2.7947, Throughput: 71.77 samples/sec
2025-03-25 08:48:32,446 - training - INFO - Epoch 255 completed in 35.13s. Average loss: 2.8165
2025-03-25 08:48:32,450 - training - INFO - Starting epoch 256/200000
2025-03-25 08:48:33,189 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9636.0MB reserved
2025-03-25 08:48:33,189 - training - INFO - Epoch: 256/200000, Batch: 0/45, Loss: 2.7931, Throughput: 75.97 samples/sec
2025-03-25 08:48:45,131 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9516.0MB reserved
2025-03-25 08:48:45,132 - training - INFO - Epoch: 256/200000, Batch: 15/45, Loss: 2.6854, Throughput: 70.68 samples/sec
2025-03-25 08:48:56,785 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9516.0MB reserved
2025-03-25 08:48:56,785 - training - INFO - Epoch: 256/200000, Batch: 30/45, Loss: 2.6948, Throughput: 71.34 samples/sec
2025-03-25 08:49:07,732 - training - INFO - Epoch 256 completed in 35.28s. Average loss: 2.7626
2025-03-25 08:49:07,735 - training - INFO - Starting epoch 257/200000
2025-03-25 08:49:08,477 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9654.0MB reserved
2025-03-25 08:49:08,477 - training - INFO - Epoch: 257/200000, Batch: 0/45, Loss: 2.3475, Throughput: 75.65 samples/sec
2025-03-25 08:49:20,378 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9500.0MB reserved
2025-03-25 08:49:20,379 - training - INFO - Epoch: 257/200000, Batch: 15/45, Loss: 2.6257, Throughput: 70.88 samples/sec
2025-03-25 08:49:31,928 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9500.0MB reserved
2025-03-25 08:49:31,928 - training - INFO - Epoch: 257/200000, Batch: 30/45, Loss: 2.7156, Throughput: 71.76 samples/sec
2025-03-25 08:49:42,802 - training - INFO - Epoch 257 completed in 35.07s. Average loss: 2.7791
2025-03-25 08:49:42,806 - training - INFO - Starting epoch 258/200000
2025-03-25 08:49:43,562 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9638.0MB reserved
2025-03-25 08:49:43,562 - training - INFO - Epoch: 258/200000, Batch: 0/45, Loss: 2.5812, Throughput: 74.16 samples/sec
2025-03-25 08:49:55,559 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9498.0MB reserved
2025-03-25 08:49:55,560 - training - INFO - Epoch: 258/200000, Batch: 15/45, Loss: 2.8668, Throughput: 70.27 samples/sec
2025-03-25 08:50:07,210 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9500.0MB reserved
2025-03-25 08:50:07,210 - training - INFO - Epoch: 258/200000, Batch: 30/45, Loss: 2.8552, Throughput: 71.14 samples/sec
2025-03-25 08:50:18,216 - training - INFO - Epoch 258 completed in 35.41s. Average loss: 2.8063
2025-03-25 08:50:18,220 - training - INFO - Starting epoch 259/200000
2025-03-25 08:50:18,954 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9638.0MB reserved
2025-03-25 08:50:18,955 - training - INFO - Epoch: 259/200000, Batch: 0/45, Loss: 2.8894, Throughput: 76.35 samples/sec
2025-03-25 08:50:30,850 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9486.0MB reserved
2025-03-25 08:50:30,850 - training - INFO - Epoch: 259/200000, Batch: 15/45, Loss: 2.9105, Throughput: 70.95 samples/sec
2025-03-25 08:50:42,474 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9486.0MB reserved
2025-03-25 08:50:42,475 - training - INFO - Epoch: 259/200000, Batch: 30/45, Loss: 2.8167, Throughput: 71.58 samples/sec
2025-03-25 08:50:53,385 - training - INFO - Epoch 259 completed in 35.16s. Average loss: 2.8041
2025-03-25 08:50:53,388 - training - INFO - Starting epoch 260/200000
2025-03-25 08:50:54,113 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9624.0MB reserved
2025-03-25 08:50:54,113 - training - INFO - Epoch: 260/200000, Batch: 0/45, Loss: 2.8057, Throughput: 77.41 samples/sec
2025-03-25 08:51:06,076 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9484.0MB reserved
2025-03-25 08:51:06,076 - training - INFO - Epoch: 260/200000, Batch: 15/45, Loss: 2.6801, Throughput: 70.63 samples/sec
2025-03-25 08:51:17,670 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9484.0MB reserved
2025-03-25 08:51:17,670 - training - INFO - Epoch: 260/200000, Batch: 30/45, Loss: 2.7463, Throughput: 71.50 samples/sec
2025-03-25 08:51:28,610 - training - INFO - Epoch 260 completed in 35.22s. Average loss: 2.8007
2025-03-25 08:51:28,613 - training - INFO - Starting epoch 261/200000
2025-03-25 08:51:29,351 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9622.0MB reserved
2025-03-25 08:51:29,351 - training - INFO - Epoch: 261/200000, Batch: 0/45, Loss: 3.3848, Throughput: 76.10 samples/sec
2025-03-25 08:51:41,342 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9506.0MB reserved
2025-03-25 08:51:41,343 - training - INFO - Epoch: 261/200000, Batch: 15/45, Loss: 2.8582, Throughput: 70.40 samples/sec
2025-03-25 08:51:53,042 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9506.0MB reserved
2025-03-25 08:51:53,043 - training - INFO - Epoch: 261/200000, Batch: 30/45, Loss: 2.8655, Throughput: 71.07 samples/sec
2025-03-25 08:52:04,015 - training - INFO - Epoch 261 completed in 35.40s. Average loss: 2.8116
2025-03-25 08:52:04,019 - training - INFO - Starting epoch 262/200000
2025-03-25 08:52:04,765 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9644.0MB reserved
2025-03-25 08:52:04,765 - training - INFO - Epoch: 262/200000, Batch: 0/45, Loss: 2.8288, Throughput: 75.21 samples/sec
2025-03-25 08:52:16,709 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9514.0MB reserved
2025-03-25 08:52:16,710 - training - INFO - Epoch: 262/200000, Batch: 15/45, Loss: 2.8144, Throughput: 70.62 samples/sec
2025-03-25 08:52:28,344 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9516.0MB reserved
2025-03-25 08:52:28,344 - training - INFO - Epoch: 262/200000, Batch: 30/45, Loss: 2.7142, Throughput: 71.37 samples/sec
2025-03-25 08:52:39,254 - training - INFO - Epoch 262 completed in 35.23s. Average loss: 2.7318
2025-03-25 08:52:39,257 - training - INFO - Starting epoch 263/200000
2025-03-25 08:52:39,990 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9654.0MB reserved
2025-03-25 08:52:39,991 - training - INFO - Epoch: 263/200000, Batch: 0/45, Loss: 2.3640, Throughput: 76.57 samples/sec
2025-03-25 08:52:51,910 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9496.0MB reserved
2025-03-25 08:52:51,910 - training - INFO - Epoch: 263/200000, Batch: 15/45, Loss: 2.7299, Throughput: 70.83 samples/sec
2025-03-25 08:53:03,599 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9496.0MB reserved
2025-03-25 08:53:03,599 - training - INFO - Epoch: 263/200000, Batch: 30/45, Loss: 2.7333, Throughput: 71.32 samples/sec
2025-03-25 08:53:14,526 - training - INFO - Epoch 263 completed in 35.27s. Average loss: 2.7439
2025-03-25 08:53:14,530 - training - INFO - Starting epoch 264/200000
2025-03-25 08:53:15,270 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9636.0MB reserved
2025-03-25 08:53:15,270 - training - INFO - Epoch: 264/200000, Batch: 0/45, Loss: 2.3119, Throughput: 75.81 samples/sec
2025-03-25 08:53:27,223 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9504.0MB reserved
2025-03-25 08:53:27,223 - training - INFO - Epoch: 264/200000, Batch: 15/45, Loss: 2.6921, Throughput: 70.60 samples/sec
2025-03-25 08:53:38,860 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9504.0MB reserved
2025-03-25 08:53:38,860 - training - INFO - Epoch: 264/200000, Batch: 30/45, Loss: 2.7289, Throughput: 71.36 samples/sec
2025-03-25 08:53:49,864 - training - INFO - Epoch 264 completed in 35.33s. Average loss: 2.7331
2025-03-25 08:53:49,867 - training - INFO - Starting epoch 265/200000
2025-03-25 08:53:50,595 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9642.0MB reserved
2025-03-25 08:53:50,595 - training - INFO - Epoch: 265/200000, Batch: 0/45, Loss: 2.7918, Throughput: 77.09 samples/sec
2025-03-25 08:54:02,513 - training - INFO - Memory: GPU 0: 3562.6MB allocated, 9402.0MB reserved
2025-03-25 08:54:02,514 - training - INFO - Epoch: 265/200000, Batch: 15/45, Loss: 2.6722, Throughput: 70.87 samples/sec
2025-03-25 08:54:14,170 - training - INFO - Memory: GPU 0: 3562.6MB allocated, 9404.0MB reserved
2025-03-25 08:54:14,170 - training - INFO - Epoch: 265/200000, Batch: 30/45, Loss: 2.7364, Throughput: 71.44 samples/sec
2025-03-25 08:54:25,148 - training - INFO - Epoch 265 completed in 35.28s. Average loss: 2.7113
2025-03-25 08:54:25,151 - training - INFO - Starting epoch 266/200000
2025-03-25 08:54:25,876 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9646.0MB reserved
2025-03-25 08:54:25,876 - training - INFO - Epoch: 266/200000, Batch: 0/45, Loss: 3.4410, Throughput: 77.30 samples/sec
2025-03-25 08:54:37,823 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9494.0MB reserved
2025-03-25 08:54:37,824 - training - INFO - Epoch: 266/200000, Batch: 15/45, Loss: 2.6938, Throughput: 70.71 samples/sec
2025-03-25 08:54:49,481 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9494.0MB reserved
2025-03-25 08:54:49,481 - training - INFO - Epoch: 266/200000, Batch: 30/45, Loss: 2.7472, Throughput: 71.36 samples/sec
2025-03-25 08:55:00,497 - training - INFO - Epoch 266 completed in 35.35s. Average loss: 2.7788
2025-03-25 08:55:00,501 - training - INFO - Starting epoch 267/200000
2025-03-25 08:55:01,249 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9632.0MB reserved
2025-03-25 08:55:01,250 - training - INFO - Epoch: 267/200000, Batch: 0/45, Loss: 2.4719, Throughput: 74.97 samples/sec
2025-03-25 08:55:13,091 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9496.0MB reserved
2025-03-25 08:55:13,092 - training - INFO - Epoch: 267/200000, Batch: 15/45, Loss: 2.7039, Throughput: 71.18 samples/sec
2025-03-25 08:55:24,645 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9496.0MB reserved
2025-03-25 08:55:24,645 - training - INFO - Epoch: 267/200000, Batch: 30/45, Loss: 2.7387, Throughput: 71.91 samples/sec
2025-03-25 08:55:35,553 - training - INFO - Epoch 267 completed in 35.05s. Average loss: 2.7406
2025-03-25 08:55:35,557 - training - INFO - Starting epoch 268/200000
2025-03-25 08:55:36,312 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9634.0MB reserved
2025-03-25 08:55:36,312 - training - INFO - Epoch: 268/200000, Batch: 0/45, Loss: 2.7661, Throughput: 74.37 samples/sec
2025-03-25 08:55:48,266 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9506.0MB reserved
2025-03-25 08:55:48,267 - training - INFO - Epoch: 268/200000, Batch: 15/45, Loss: 2.7330, Throughput: 70.51 samples/sec
2025-03-25 08:55:59,895 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9506.0MB reserved
2025-03-25 08:55:59,895 - training - INFO - Epoch: 268/200000, Batch: 30/45, Loss: 2.7260, Throughput: 71.33 samples/sec
2025-03-25 08:56:10,815 - training - INFO - Epoch 268 completed in 35.26s. Average loss: 2.7117
2025-03-25 08:56:10,819 - training - INFO - Starting epoch 269/200000
2025-03-25 08:56:11,570 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9644.0MB reserved
2025-03-25 08:56:11,570 - training - INFO - Epoch: 269/200000, Batch: 0/45, Loss: 2.4623, Throughput: 74.74 samples/sec
2025-03-25 08:56:23,473 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9504.0MB reserved
2025-03-25 08:56:23,473 - training - INFO - Epoch: 269/200000, Batch: 15/45, Loss: 2.6455, Throughput: 70.81 samples/sec
2025-03-25 08:56:35,105 - training - INFO - Memory: GPU 0: 3564.2MB allocated, 9504.0MB reserved
2025-03-25 08:56:35,106 - training - INFO - Epoch: 269/200000, Batch: 30/45, Loss: 2.7024, Throughput: 71.49 samples/sec
2025-03-25 08:56:46,074 - training - INFO - Epoch 269 completed in 35.26s. Average loss: 2.7198
2025-03-25 08:56:46,078 - training - INFO - Starting epoch 270/200000
2025-03-25 08:56:46,828 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9642.0MB reserved
2025-03-25 08:56:46,828 - training - INFO - Epoch: 270/200000, Batch: 0/45, Loss: 2.1661, Throughput: 74.77 samples/sec
2025-03-25 08:56:58,730 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9500.0MB reserved
2025-03-25 08:56:58,730 - training - INFO - Epoch: 270/200000, Batch: 15/45, Loss: 2.6852, Throughput: 70.82 samples/sec
2025-03-25 08:57:10,296 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9502.0MB reserved
2025-03-25 08:57:10,296 - training - INFO - Epoch: 270/200000, Batch: 30/45, Loss: 2.6963, Throughput: 71.69 samples/sec
2025-03-25 08:57:21,283 - training - INFO - Epoch 270 completed in 35.20s. Average loss: 2.7135
2025-03-25 08:57:21,286 - training - INFO - Starting epoch 271/200000
2025-03-25 08:57:22,030 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9640.0MB reserved
2025-03-25 08:57:22,030 - training - INFO - Epoch: 271/200000, Batch: 0/45, Loss: 2.8830, Throughput: 75.42 samples/sec
2025-03-25 08:57:34,013 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9502.0MB reserved
2025-03-25 08:57:34,013 - training - INFO - Epoch: 271/200000, Batch: 15/45, Loss: 2.6851, Throughput: 70.41 samples/sec
2025-03-25 08:57:45,621 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9502.0MB reserved
2025-03-25 08:57:45,621 - training - INFO - Epoch: 271/200000, Batch: 30/45, Loss: 2.6536, Throughput: 71.34 samples/sec
2025-03-25 08:57:56,544 - training - INFO - Epoch 271 completed in 35.26s. Average loss: 2.7178
2025-03-25 08:57:56,548 - training - INFO - Starting epoch 272/200000
2025-03-25 08:57:57,317 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9640.0MB reserved
2025-03-25 08:57:57,317 - training - INFO - Epoch: 272/200000, Batch: 0/45, Loss: 2.3360, Throughput: 72.95 samples/sec
2025-03-25 08:58:09,140 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9510.0MB reserved
2025-03-25 08:58:09,141 - training - INFO - Epoch: 272/200000, Batch: 15/45, Loss: 2.6866, Throughput: 71.17 samples/sec
2025-03-25 08:58:20,687 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9510.0MB reserved
2025-03-25 08:58:20,688 - training - INFO - Epoch: 272/200000, Batch: 30/45, Loss: 2.6867, Throughput: 71.92 samples/sec
2025-03-25 08:58:31,609 - training - INFO - Epoch 272 completed in 35.06s. Average loss: 2.7555
2025-03-25 08:58:31,613 - training - INFO - Starting epoch 273/200000
2025-03-25 08:58:32,338 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9648.0MB reserved
2025-03-25 08:58:32,339 - training - INFO - Epoch: 273/200000, Batch: 0/45, Loss: 2.2224, Throughput: 77.48 samples/sec
2025-03-25 08:58:44,252 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9502.0MB reserved
2025-03-25 08:58:44,252 - training - INFO - Epoch: 273/200000, Batch: 15/45, Loss: 2.7439, Throughput: 70.90 samples/sec
2025-03-25 08:58:55,852 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9502.0MB reserved
2025-03-25 08:58:55,853 - training - INFO - Epoch: 273/200000, Batch: 30/45, Loss: 2.7195, Throughput: 71.62 samples/sec
2025-03-25 08:59:06,762 - training - INFO - Epoch 273 completed in 35.15s. Average loss: 2.7119
2025-03-25 08:59:06,766 - training - INFO - Starting epoch 274/200000
2025-03-25 08:59:07,498 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9640.0MB reserved
2025-03-25 08:59:07,498 - training - INFO - Epoch: 274/200000, Batch: 0/45, Loss: 2.3434, Throughput: 76.76 samples/sec
2025-03-25 08:59:19,464 - training - INFO - Memory: GPU 0: 3555.0MB allocated, 9498.0MB reserved
2025-03-25 08:59:19,465 - training - INFO - Epoch: 274/200000, Batch: 15/45, Loss: 2.6618, Throughput: 70.57 samples/sec
2025-03-25 08:59:31,157 - training - INFO - Memory: GPU 0: 3555.0MB allocated, 9498.0MB reserved
2025-03-25 08:59:31,157 - training - INFO - Epoch: 274/200000, Batch: 30/45, Loss: 2.6632, Throughput: 71.18 samples/sec
2025-03-25 08:59:42,221 - training - INFO - Epoch 274 completed in 35.46s. Average loss: 2.6780
2025-03-25 08:59:42,225 - training - INFO - Starting epoch 275/200000
2025-03-25 08:59:42,970 - training - INFO - Memory: GPU 0: 3555.0MB allocated, 9634.0MB reserved
2025-03-25 08:59:42,970 - training - INFO - Epoch: 275/200000, Batch: 0/45, Loss: 2.8110, Throughput: 75.39 samples/sec
2025-03-25 08:59:54,821 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9504.0MB reserved
2025-03-25 08:59:54,821 - training - INFO - Epoch: 275/200000, Batch: 15/45, Loss: 2.7262, Throughput: 71.15 samples/sec
2025-03-25 09:00:06,383 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9504.0MB reserved
2025-03-25 09:00:06,383 - training - INFO - Epoch: 275/200000, Batch: 30/45, Loss: 2.7299, Throughput: 71.87 samples/sec
2025-03-25 09:00:17,293 - training - INFO - Epoch 275 completed in 35.07s. Average loss: 2.7479
2025-03-25 09:00:17,298 - training - INFO - Starting epoch 276/200000
2025-03-25 09:00:18,041 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9642.0MB reserved
2025-03-25 09:00:18,041 - training - INFO - Epoch: 276/200000, Batch: 0/45, Loss: 3.4156, Throughput: 75.63 samples/sec
2025-03-25 09:00:29,937 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9508.0MB reserved
2025-03-25 09:00:29,938 - training - INFO - Epoch: 276/200000, Batch: 15/45, Loss: 2.7305, Throughput: 70.91 samples/sec
2025-03-25 09:00:41,508 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9508.0MB reserved
2025-03-25 09:00:41,508 - training - INFO - Epoch: 276/200000, Batch: 30/45, Loss: 2.6780, Throughput: 71.71 samples/sec
2025-03-25 09:00:52,432 - training - INFO - Epoch 276 completed in 35.13s. Average loss: 2.6671
2025-03-25 09:00:52,436 - training - INFO - Starting epoch 277/200000
2025-03-25 09:00:53,185 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9646.0MB reserved
2025-03-25 09:00:53,185 - training - INFO - Epoch: 277/200000, Batch: 0/45, Loss: 2.3639, Throughput: 74.97 samples/sec
2025-03-25 09:01:05,140 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9504.0MB reserved
2025-03-25 09:01:05,141 - training - INFO - Epoch: 277/200000, Batch: 15/45, Loss: 2.5453, Throughput: 70.54 samples/sec
2025-03-25 09:01:16,787 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9504.0MB reserved
2025-03-25 09:01:16,788 - training - INFO - Epoch: 277/200000, Batch: 30/45, Loss: 2.6301, Throughput: 71.30 samples/sec
2025-03-25 09:01:27,750 - training - INFO - Epoch 277 completed in 35.31s. Average loss: 2.7270
2025-03-25 09:01:27,753 - training - INFO - Starting epoch 278/200000
2025-03-25 09:01:28,507 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9642.0MB reserved
2025-03-25 09:01:28,508 - training - INFO - Epoch: 278/200000, Batch: 0/45, Loss: 2.7329, Throughput: 74.46 samples/sec
2025-03-25 09:01:40,462 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9498.0MB reserved
2025-03-25 09:01:40,463 - training - INFO - Epoch: 278/200000, Batch: 15/45, Loss: 2.6344, Throughput: 70.51 samples/sec
2025-03-25 09:01:52,121 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9498.0MB reserved
2025-03-25 09:01:52,122 - training - INFO - Epoch: 278/200000, Batch: 30/45, Loss: 2.6828, Throughput: 71.25 samples/sec
2025-03-25 09:02:03,092 - training - INFO - Epoch 278 completed in 35.34s. Average loss: 2.6703
2025-03-25 09:02:03,096 - training - INFO - Starting epoch 279/200000
2025-03-25 09:02:03,833 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9634.0MB reserved
2025-03-25 09:02:03,834 - training - INFO - Epoch: 279/200000, Batch: 0/45, Loss: 2.6033, Throughput: 76.10 samples/sec
2025-03-25 09:02:15,801 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9410.0MB reserved
2025-03-25 09:02:15,801 - training - INFO - Epoch: 279/200000, Batch: 15/45, Loss: 2.6078, Throughput: 70.54 samples/sec
2025-03-25 09:02:27,410 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9518.0MB reserved
2025-03-25 09:02:27,411 - training - INFO - Epoch: 279/200000, Batch: 30/45, Loss: 2.6153, Throughput: 71.41 samples/sec
2025-03-25 09:02:38,391 - training - INFO - Epoch 279 completed in 35.29s. Average loss: 2.6861
2025-03-25 09:02:38,395 - training - INFO - Starting epoch 280/200000
2025-03-25 09:02:39,117 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9656.0MB reserved
2025-03-25 09:02:39,117 - training - INFO - Epoch: 280/200000, Batch: 0/45, Loss: 2.4326, Throughput: 77.60 samples/sec
2025-03-25 09:02:51,032 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9506.0MB reserved
2025-03-25 09:02:51,032 - training - INFO - Epoch: 280/200000, Batch: 15/45, Loss: 2.5644, Throughput: 70.91 samples/sec
2025-03-25 09:03:02,549 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9506.0MB reserved
2025-03-25 09:03:02,550 - training - INFO - Epoch: 280/200000, Batch: 30/45, Loss: 2.6301, Throughput: 71.88 samples/sec
2025-03-25 09:03:13,432 - training - INFO - Epoch 280 completed in 35.04s. Average loss: 2.6353
2025-03-25 09:03:13,435 - training - INFO - Starting epoch 281/200000
2025-03-25 09:03:14,163 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9644.0MB reserved
2025-03-25 09:03:14,164 - training - INFO - Epoch: 281/200000, Batch: 0/45, Loss: 2.6642, Throughput: 77.10 samples/sec
2025-03-25 09:03:26,001 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9506.0MB reserved
2025-03-25 09:03:26,001 - training - INFO - Epoch: 281/200000, Batch: 15/45, Loss: 2.7020, Throughput: 71.31 samples/sec
2025-03-25 09:03:37,544 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9506.0MB reserved
2025-03-25 09:03:37,544 - training - INFO - Epoch: 281/200000, Batch: 30/45, Loss: 2.7249, Throughput: 72.01 samples/sec
2025-03-25 09:03:48,453 - training - INFO - Epoch 281 completed in 35.02s. Average loss: 2.7162
2025-03-25 09:03:48,456 - training - INFO - Starting epoch 282/200000
2025-03-25 09:03:49,201 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9646.0MB reserved
2025-03-25 09:03:49,201 - training - INFO - Epoch: 282/200000, Batch: 0/45, Loss: 3.2485, Throughput: 75.27 samples/sec
2025-03-25 09:04:01,161 - training - INFO - Memory: GPU 0: 3563.1MB allocated, 9502.0MB reserved
2025-03-25 09:04:01,161 - training - INFO - Epoch: 282/200000, Batch: 15/45, Loss: 2.8468, Throughput: 70.54 samples/sec
2025-03-25 09:04:12,831 - training - INFO - Memory: GPU 0: 3563.1MB allocated, 9502.0MB reserved
2025-03-25 09:04:12,831 - training - INFO - Epoch: 282/200000, Batch: 30/45, Loss: 2.7308, Throughput: 71.23 samples/sec
2025-03-25 09:04:23,792 - training - INFO - Epoch 282 completed in 35.34s. Average loss: 2.7098
2025-03-25 09:04:23,796 - training - INFO - Starting epoch 283/200000
2025-03-25 09:04:24,535 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9640.0MB reserved
2025-03-25 09:04:24,536 - training - INFO - Epoch: 283/200000, Batch: 0/45, Loss: 2.3441, Throughput: 75.91 samples/sec
2025-03-25 09:04:36,449 - training - INFO - Memory: GPU 0: 3563.8MB allocated, 9490.0MB reserved
2025-03-25 09:04:36,449 - training - INFO - Epoch: 283/200000, Batch: 15/45, Loss: 2.7714, Throughput: 70.83 samples/sec
2025-03-25 09:04:48,100 - training - INFO - Memory: GPU 0: 3563.8MB allocated, 9490.0MB reserved
2025-03-25 09:04:48,100 - training - INFO - Epoch: 283/200000, Batch: 30/45, Loss: 2.7405, Throughput: 71.44 samples/sec
2025-03-25 09:04:59,034 - training - INFO - Epoch 283 completed in 35.24s. Average loss: 2.6943
2025-03-25 09:04:59,038 - training - INFO - Starting epoch 284/200000
2025-03-25 09:04:59,802 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9628.0MB reserved
2025-03-25 09:04:59,802 - training - INFO - Epoch: 284/200000, Batch: 0/45, Loss: 2.3987, Throughput: 73.41 samples/sec
2025-03-25 09:05:11,780 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9504.0MB reserved
2025-03-25 09:05:11,780 - training - INFO - Epoch: 284/200000, Batch: 15/45, Loss: 2.6035, Throughput: 70.33 samples/sec
2025-03-25 09:05:23,520 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9504.0MB reserved
2025-03-25 09:05:23,520 - training - INFO - Epoch: 284/200000, Batch: 30/45, Loss: 2.6195, Throughput: 70.91 samples/sec
2025-03-25 09:05:34,546 - training - INFO - Epoch 284 completed in 35.51s. Average loss: 2.6442
2025-03-25 09:05:34,550 - training - INFO - Starting epoch 285/200000
2025-03-25 09:05:35,293 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9642.0MB reserved
2025-03-25 09:05:35,293 - training - INFO - Epoch: 285/200000, Batch: 0/45, Loss: 2.5273, Throughput: 75.54 samples/sec
2025-03-25 09:05:47,302 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9404.0MB reserved
2025-03-25 09:05:47,302 - training - INFO - Epoch: 285/200000, Batch: 15/45, Loss: 2.6459, Throughput: 70.28 samples/sec
2025-03-25 09:05:58,942 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9512.0MB reserved
2025-03-25 09:05:58,942 - training - INFO - Epoch: 285/200000, Batch: 30/45, Loss: 2.7203, Throughput: 71.17 samples/sec
2025-03-25 09:06:09,836 - training - INFO - Epoch 285 completed in 35.29s. Average loss: 2.6637
2025-03-25 09:06:09,839 - training - INFO - Starting epoch 286/200000
2025-03-25 09:06:10,562 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9650.0MB reserved
2025-03-25 09:06:10,563 - training - INFO - Epoch: 286/200000, Batch: 0/45, Loss: 2.2222, Throughput: 77.67 samples/sec
2025-03-25 09:06:22,537 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9502.0MB reserved
2025-03-25 09:06:22,538 - training - INFO - Epoch: 286/200000, Batch: 15/45, Loss: 2.5163, Throughput: 70.57 samples/sec
2025-03-25 09:06:34,288 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9504.0MB reserved
2025-03-25 09:06:34,288 - training - INFO - Epoch: 286/200000, Batch: 30/45, Loss: 2.6135, Throughput: 71.01 samples/sec
2025-03-25 09:06:45,293 - training - INFO - Epoch 286 completed in 35.45s. Average loss: 2.6357
2025-03-25 09:06:45,297 - training - INFO - Starting epoch 287/200000
2025-03-25 09:06:46,036 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9642.0MB reserved
2025-03-25 09:06:46,037 - training - INFO - Epoch: 287/200000, Batch: 0/45, Loss: 2.9051, Throughput: 75.89 samples/sec
2025-03-25 09:06:57,941 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9512.0MB reserved
2025-03-25 09:06:57,942 - training - INFO - Epoch: 287/200000, Batch: 15/45, Loss: 2.7094, Throughput: 70.88 samples/sec
2025-03-25 09:07:09,511 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9512.0MB reserved
2025-03-25 09:07:09,512 - training - INFO - Epoch: 287/200000, Batch: 30/45, Loss: 2.6999, Throughput: 71.70 samples/sec
2025-03-25 09:07:20,440 - training - INFO - Epoch 287 completed in 35.14s. Average loss: 2.6900
2025-03-25 09:07:20,444 - training - INFO - Starting epoch 288/200000
2025-03-25 09:07:21,179 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9648.0MB reserved
2025-03-25 09:07:21,179 - training - INFO - Epoch: 288/200000, Batch: 0/45, Loss: 2.5344, Throughput: 76.20 samples/sec
2025-03-25 09:07:33,120 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9504.0MB reserved
2025-03-25 09:07:33,121 - training - INFO - Epoch: 288/200000, Batch: 15/45, Loss: 2.6810, Throughput: 70.70 samples/sec
2025-03-25 09:07:44,717 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9504.0MB reserved
2025-03-25 09:07:44,718 - training - INFO - Epoch: 288/200000, Batch: 30/45, Loss: 2.6346, Throughput: 71.52 samples/sec
2025-03-25 09:07:55,636 - training - INFO - Epoch 288 completed in 35.19s. Average loss: 2.6920
2025-03-25 09:07:55,640 - training - INFO - Starting epoch 289/200000
2025-03-25 09:07:56,390 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9642.0MB reserved
2025-03-25 09:07:56,391 - training - INFO - Epoch: 289/200000, Batch: 0/45, Loss: 2.1903, Throughput: 74.77 samples/sec
2025-03-25 09:08:08,258 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9500.0MB reserved
2025-03-25 09:08:08,259 - training - INFO - Epoch: 289/200000, Batch: 15/45, Loss: 2.6048, Throughput: 71.02 samples/sec
2025-03-25 09:08:19,843 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9500.0MB reserved
2025-03-25 09:08:19,843 - training - INFO - Epoch: 289/200000, Batch: 30/45, Loss: 2.6238, Throughput: 71.73 samples/sec
2025-03-25 09:08:30,739 - training - INFO - Epoch 289 completed in 35.10s. Average loss: 2.6633
2025-03-25 09:08:30,743 - training - INFO - Starting epoch 290/200000
2025-03-25 09:08:31,496 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9638.0MB reserved
2025-03-25 09:08:31,497 - training - INFO - Epoch: 290/200000, Batch: 0/45, Loss: 2.9575, Throughput: 74.41 samples/sec
2025-03-25 09:08:43,422 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9500.0MB reserved
2025-03-25 09:08:43,422 - training - INFO - Epoch: 290/200000, Batch: 15/45, Loss: 2.5560, Throughput: 70.67 samples/sec
2025-03-25 09:08:55,143 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9500.0MB reserved
2025-03-25 09:08:55,143 - training - INFO - Epoch: 290/200000, Batch: 30/45, Loss: 2.5233, Throughput: 71.15 samples/sec
2025-03-25 09:09:06,073 - training - INFO - Epoch 290 completed in 35.33s. Average loss: 2.6878
2025-03-25 09:09:06,077 - training - INFO - Starting epoch 291/200000
2025-03-25 09:09:06,806 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9638.0MB reserved
2025-03-25 09:09:06,806 - training - INFO - Epoch: 291/200000, Batch: 0/45, Loss: 2.6235, Throughput: 76.88 samples/sec
2025-03-25 09:09:18,710 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9506.0MB reserved
2025-03-25 09:09:18,710 - training - INFO - Epoch: 291/200000, Batch: 15/45, Loss: 2.6750, Throughput: 70.93 samples/sec
2025-03-25 09:09:30,392 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9508.0MB reserved
2025-03-25 09:09:30,392 - training - INFO - Epoch: 291/200000, Batch: 30/45, Loss: 2.7094, Throughput: 71.40 samples/sec
2025-03-25 09:09:41,385 - training - INFO - Epoch 291 completed in 35.31s. Average loss: 2.6463
2025-03-25 09:09:41,389 - training - INFO - Starting epoch 292/200000
2025-03-25 09:09:42,132 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9646.0MB reserved
2025-03-25 09:09:42,132 - training - INFO - Epoch: 292/200000, Batch: 0/45, Loss: 2.1954, Throughput: 75.56 samples/sec
2025-03-25 09:09:54,107 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9494.0MB reserved
2025-03-25 09:09:54,107 - training - INFO - Epoch: 292/200000, Batch: 15/45, Loss: 2.5860, Throughput: 70.46 samples/sec
2025-03-25 09:10:05,736 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9494.0MB reserved
2025-03-25 09:10:05,736 - training - INFO - Epoch: 292/200000, Batch: 30/45, Loss: 2.6325, Throughput: 71.31 samples/sec
2025-03-25 09:10:16,700 - training - INFO - Epoch 292 completed in 35.31s. Average loss: 2.6398
2025-03-25 09:10:16,704 - training - INFO - Starting epoch 293/200000
2025-03-25 09:10:17,457 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9630.0MB reserved
2025-03-25 09:10:17,457 - training - INFO - Epoch: 293/200000, Batch: 0/45, Loss: 2.6651, Throughput: 74.44 samples/sec
2025-03-25 09:10:29,449 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9508.0MB reserved
2025-03-25 09:10:29,449 - training - INFO - Epoch: 293/200000, Batch: 15/45, Loss: 2.6412, Throughput: 70.31 samples/sec
2025-03-25 09:10:41,110 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9508.0MB reserved
2025-03-25 09:10:41,110 - training - INFO - Epoch: 293/200000, Batch: 30/45, Loss: 2.6260, Throughput: 71.13 samples/sec
2025-03-25 09:10:52,066 - training - INFO - Epoch 293 completed in 35.36s. Average loss: 2.6314
2025-03-25 09:10:52,069 - training - INFO - Starting epoch 294/200000
2025-03-25 09:10:52,827 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9644.0MB reserved
2025-03-25 09:10:52,827 - training - INFO - Epoch: 294/200000, Batch: 0/45, Loss: 2.2658, Throughput: 74.14 samples/sec
2025-03-25 09:11:04,728 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9498.0MB reserved
2025-03-25 09:11:04,728 - training - INFO - Epoch: 294/200000, Batch: 15/45, Loss: 2.5541, Throughput: 70.80 samples/sec
2025-03-25 09:11:16,281 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9500.0MB reserved
2025-03-25 09:11:16,282 - training - INFO - Epoch: 294/200000, Batch: 30/45, Loss: 2.5628, Throughput: 71.71 samples/sec
2025-03-25 09:11:27,238 - training - INFO - Epoch 294 completed in 35.17s. Average loss: 2.6340
2025-03-25 09:11:27,242 - training - INFO - Starting epoch 295/200000
2025-03-25 09:11:27,990 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9638.0MB reserved
2025-03-25 09:11:27,990 - training - INFO - Epoch: 295/200000, Batch: 0/45, Loss: 2.5753, Throughput: 75.04 samples/sec
2025-03-25 09:11:39,989 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9492.0MB reserved
2025-03-25 09:11:39,989 - training - INFO - Epoch: 295/200000, Batch: 15/45, Loss: 2.6766, Throughput: 70.30 samples/sec
2025-03-25 09:11:51,660 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9492.0MB reserved
2025-03-25 09:11:51,661 - training - INFO - Epoch: 295/200000, Batch: 30/45, Loss: 2.6827, Throughput: 71.10 samples/sec
2025-03-25 09:12:02,640 - training - INFO - Epoch 295 completed in 35.40s. Average loss: 2.6995
2025-03-25 09:12:02,644 - training - INFO - Starting epoch 296/200000
2025-03-25 09:12:03,392 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9630.0MB reserved
2025-03-25 09:12:03,392 - training - INFO - Epoch: 296/200000, Batch: 0/45, Loss: 2.8243, Throughput: 75.11 samples/sec
2025-03-25 09:12:15,330 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9500.0MB reserved
2025-03-25 09:12:15,331 - training - INFO - Epoch: 296/200000, Batch: 15/45, Loss: 2.7054, Throughput: 70.64 samples/sec
2025-03-25 09:12:27,044 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9500.0MB reserved
2025-03-25 09:12:27,044 - training - INFO - Epoch: 296/200000, Batch: 30/45, Loss: 2.6790, Throughput: 71.16 samples/sec
2025-03-25 09:12:37,984 - training - INFO - Epoch 296 completed in 35.34s. Average loss: 2.6811
2025-03-25 09:12:37,988 - training - INFO - Starting epoch 297/200000
2025-03-25 09:12:38,725 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9638.0MB reserved
2025-03-25 09:12:38,726 - training - INFO - Epoch: 297/200000, Batch: 0/45, Loss: 3.2011, Throughput: 76.16 samples/sec
2025-03-25 09:12:50,650 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9492.0MB reserved
2025-03-25 09:12:50,650 - training - INFO - Epoch: 297/200000, Batch: 15/45, Loss: 2.6685, Throughput: 70.77 samples/sec
2025-03-25 09:13:02,386 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9494.0MB reserved
2025-03-25 09:13:02,386 - training - INFO - Epoch: 297/200000, Batch: 30/45, Loss: 2.6105, Throughput: 71.16 samples/sec
2025-03-25 09:13:13,338 - training - INFO - Epoch 297 completed in 35.35s. Average loss: 2.6268
2025-03-25 09:13:13,342 - training - INFO - Starting epoch 298/200000
2025-03-25 09:13:14,081 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9632.0MB reserved
2025-03-25 09:13:14,082 - training - INFO - Epoch: 298/200000, Batch: 0/45, Loss: 2.5189, Throughput: 75.91 samples/sec
2025-03-25 09:13:26,041 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9492.0MB reserved
2025-03-25 09:13:26,041 - training - INFO - Epoch: 298/200000, Batch: 15/45, Loss: 2.6955, Throughput: 70.57 samples/sec
2025-03-25 09:13:37,634 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9492.0MB reserved
2025-03-25 09:13:37,634 - training - INFO - Epoch: 298/200000, Batch: 30/45, Loss: 2.6790, Throughput: 71.47 samples/sec
2025-03-25 09:13:48,569 - training - INFO - Epoch 298 completed in 35.23s. Average loss: 2.6150
2025-03-25 09:13:48,573 - training - INFO - Starting epoch 299/200000
2025-03-25 09:13:49,330 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9630.0MB reserved
2025-03-25 09:13:49,330 - training - INFO - Epoch: 299/200000, Batch: 0/45, Loss: 2.6952, Throughput: 74.16 samples/sec
2025-03-25 09:14:01,205 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9400.0MB reserved
2025-03-25 09:14:01,205 - training - INFO - Epoch: 299/200000, Batch: 15/45, Loss: 2.7724, Throughput: 70.94 samples/sec
2025-03-25 09:14:12,801 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9400.0MB reserved
2025-03-25 09:14:12,801 - training - INFO - Epoch: 299/200000, Batch: 30/45, Loss: 2.7053, Throughput: 71.65 samples/sec
2025-03-25 09:14:23,746 - training - INFO - Epoch 299 completed in 35.17s. Average loss: 2.6743
2025-03-25 09:14:23,750 - training - INFO - Starting epoch 300/200000
2025-03-25 09:14:24,494 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9538.0MB reserved
2025-03-25 09:14:24,494 - training - INFO - Epoch: 300/200000, Batch: 0/45, Loss: 2.1450, Throughput: 75.51 samples/sec
2025-03-25 09:14:36,394 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9496.0MB reserved
2025-03-25 09:14:36,394 - training - INFO - Epoch: 300/200000, Batch: 15/45, Loss: 2.5356, Throughput: 70.88 samples/sec
2025-03-25 09:14:47,977 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9498.0MB reserved
2025-03-25 09:14:47,978 - training - INFO - Epoch: 300/200000, Batch: 30/45, Loss: 2.6029, Throughput: 71.66 samples/sec
2025-03-25 09:14:58,994 - training - INFO - Epoch 300 completed in 35.24s. Average loss: 2.6582
2025-03-25 09:14:58,998 - training - INFO - Starting validation...
2025-03-25 09:14:59,320 - training - INFO - Validation Loss: 18.1480
2025-03-25 09:14:59,320 - training - INFO - Validation loss did not improve. Counter: 2/10
2025-03-25 09:14:59,625 - training - INFO - Starting epoch 301/200000
2025-03-25 09:15:00,401 - training - INFO - Memory: GPU 0: 3565.6MB allocated, 8742.0MB reserved
2025-03-25 09:15:00,401 - training - INFO - Epoch: 301/200000, Batch: 0/45, Loss: 3.2495, Throughput: 72.38 samples/sec
2025-03-25 09:15:12,148 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9464.0MB reserved
2025-03-25 09:15:12,149 - training - INFO - Epoch: 301/200000, Batch: 15/45, Loss: 2.5580, Throughput: 71.56 samples/sec
2025-03-25 09:15:23,679 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9466.0MB reserved
2025-03-25 09:15:23,679 - training - INFO - Epoch: 301/200000, Batch: 30/45, Loss: 2.5918, Throughput: 72.18 samples/sec
2025-03-25 09:15:34,617 - training - INFO - Epoch 301 completed in 34.99s. Average loss: 2.6328
2025-03-25 09:15:34,621 - training - INFO - Starting epoch 302/200000
2025-03-25 09:15:35,367 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9604.0MB reserved
2025-03-25 09:15:35,368 - training - INFO - Epoch: 302/200000, Batch: 0/45, Loss: 2.6926, Throughput: 75.25 samples/sec
2025-03-25 09:15:47,356 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9490.0MB reserved
2025-03-25 09:15:47,356 - training - INFO - Epoch: 302/200000, Batch: 15/45, Loss: 2.7303, Throughput: 70.37 samples/sec
2025-03-25 09:15:59,020 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9492.0MB reserved
2025-03-25 09:15:59,020 - training - INFO - Epoch: 302/200000, Batch: 30/45, Loss: 2.6458, Throughput: 71.16 samples/sec
2025-03-25 09:16:09,938 - training - INFO - Epoch 302 completed in 35.32s. Average loss: 2.6259
2025-03-25 09:16:09,942 - training - INFO - Starting epoch 303/200000
2025-03-25 09:16:10,670 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9630.0MB reserved
2025-03-25 09:16:10,670 - training - INFO - Epoch: 303/200000, Batch: 0/45, Loss: 2.4479, Throughput: 77.04 samples/sec
2025-03-25 09:16:22,546 - training - INFO - Memory: GPU 0: 3563.4MB allocated, 9500.0MB reserved
2025-03-25 09:16:22,546 - training - INFO - Epoch: 303/200000, Batch: 15/45, Loss: 2.5794, Throughput: 71.10 samples/sec
2025-03-25 09:16:34,133 - training - INFO - Memory: GPU 0: 3563.4MB allocated, 9500.0MB reserved
2025-03-25 09:16:34,134 - training - INFO - Epoch: 303/200000, Batch: 30/45, Loss: 2.6122, Throughput: 71.77 samples/sec
2025-03-25 09:16:45,061 - training - INFO - Epoch 303 completed in 35.12s. Average loss: 2.6346
2025-03-25 09:16:45,064 - training - INFO - Starting epoch 304/200000
2025-03-25 09:16:45,803 - training - INFO - Memory: GPU 0: 3563.4MB allocated, 9638.0MB reserved
2025-03-25 09:16:45,803 - training - INFO - Epoch: 304/200000, Batch: 0/45, Loss: 2.1203, Throughput: 75.97 samples/sec
2025-03-25 09:16:57,819 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9392.0MB reserved
2025-03-25 09:16:57,819 - training - INFO - Epoch: 304/200000, Batch: 15/45, Loss: 2.6019, Throughput: 70.26 samples/sec
2025-03-25 09:17:09,538 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9498.0MB reserved
2025-03-25 09:17:09,539 - training - INFO - Epoch: 304/200000, Batch: 30/45, Loss: 2.6473, Throughput: 70.94 samples/sec
2025-03-25 09:17:20,515 - training - INFO - Epoch 304 completed in 35.45s. Average loss: 2.6403
2025-03-25 09:17:20,518 - training - INFO - Starting epoch 305/200000
2025-03-25 09:17:21,265 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9636.0MB reserved
2025-03-25 09:17:21,265 - training - INFO - Epoch: 305/200000, Batch: 0/45, Loss: 2.9691, Throughput: 75.18 samples/sec
2025-03-25 09:17:33,138 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9488.0MB reserved
2025-03-25 09:17:33,138 - training - INFO - Epoch: 305/200000, Batch: 15/45, Loss: 2.6577, Throughput: 71.01 samples/sec
2025-03-25 09:17:44,666 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9488.0MB reserved
2025-03-25 09:17:44,666 - training - INFO - Epoch: 305/200000, Batch: 30/45, Loss: 2.6318, Throughput: 71.89 samples/sec
2025-03-25 09:17:55,601 - training - INFO - Epoch 305 completed in 35.08s. Average loss: 2.6499
2025-03-25 09:17:55,605 - training - INFO - Starting epoch 306/200000
2025-03-25 09:17:56,354 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9626.0MB reserved
2025-03-25 09:17:56,354 - training - INFO - Epoch: 306/200000, Batch: 0/45, Loss: 2.4544, Throughput: 75.00 samples/sec
2025-03-25 09:18:08,345 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9494.0MB reserved
2025-03-25 09:18:08,345 - training - INFO - Epoch: 306/200000, Batch: 15/45, Loss: 2.6611, Throughput: 70.33 samples/sec
2025-03-25 09:18:19,982 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9496.0MB reserved
2025-03-25 09:18:19,982 - training - INFO - Epoch: 306/200000, Batch: 30/45, Loss: 2.6914, Throughput: 71.22 samples/sec
2025-03-25 09:18:30,872 - training - INFO - Epoch 306 completed in 35.27s. Average loss: 2.6390
2025-03-25 09:18:30,875 - training - INFO - Starting epoch 307/200000
2025-03-25 09:18:31,616 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9634.0MB reserved
2025-03-25 09:18:31,616 - training - INFO - Epoch: 307/200000, Batch: 0/45, Loss: 2.8018, Throughput: 75.70 samples/sec
2025-03-25 09:18:43,468 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9490.0MB reserved
2025-03-25 09:18:43,468 - training - INFO - Epoch: 307/200000, Batch: 15/45, Loss: 2.6124, Throughput: 71.16 samples/sec
2025-03-25 09:18:55,033 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9492.0MB reserved
2025-03-25 09:18:55,033 - training - INFO - Epoch: 307/200000, Batch: 30/45, Loss: 2.5769, Throughput: 71.86 samples/sec
2025-03-25 09:19:05,938 - training - INFO - Epoch 307 completed in 35.06s. Average loss: 2.6093
2025-03-25 09:19:05,942 - training - INFO - Starting epoch 308/200000
2025-03-25 09:19:06,685 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9630.0MB reserved
2025-03-25 09:19:06,686 - training - INFO - Epoch: 308/200000, Batch: 0/45, Loss: 2.2397, Throughput: 75.51 samples/sec
2025-03-25 09:19:18,527 - training - INFO - Memory: GPU 0: 3557.2MB allocated, 9498.0MB reserved
2025-03-25 09:19:18,527 - training - INFO - Epoch: 308/200000, Batch: 15/45, Loss: 2.5180, Throughput: 71.21 samples/sec
2025-03-25 09:19:30,145 - training - INFO - Memory: GPU 0: 3557.2MB allocated, 9498.0MB reserved
2025-03-25 09:19:30,146 - training - INFO - Epoch: 308/200000, Batch: 30/45, Loss: 2.5881, Throughput: 71.73 samples/sec
2025-03-25 09:19:41,067 - training - INFO - Epoch 308 completed in 35.12s. Average loss: 2.5968
2025-03-25 09:19:41,070 - training - INFO - Starting epoch 309/200000
2025-03-25 09:19:41,805 - training - INFO - Memory: GPU 0: 3557.2MB allocated, 9636.0MB reserved
2025-03-25 09:19:41,806 - training - INFO - Epoch: 309/200000, Batch: 0/45, Loss: 2.6955, Throughput: 76.35 samples/sec
2025-03-25 09:19:53,784 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9508.0MB reserved
2025-03-25 09:19:53,785 - training - INFO - Epoch: 309/200000, Batch: 15/45, Loss: 2.6112, Throughput: 70.49 samples/sec
2025-03-25 09:20:05,376 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9508.0MB reserved
2025-03-25 09:20:05,376 - training - INFO - Epoch: 309/200000, Batch: 30/45, Loss: 2.5465, Throughput: 71.43 samples/sec
2025-03-25 09:20:16,323 - training - INFO - Epoch 309 completed in 35.25s. Average loss: 2.6101
2025-03-25 09:20:16,327 - training - INFO - Starting epoch 310/200000
2025-03-25 09:20:17,071 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9646.0MB reserved
2025-03-25 09:20:17,071 - training - INFO - Epoch: 310/200000, Batch: 0/45, Loss: 2.4200, Throughput: 75.45 samples/sec
2025-03-25 09:20:29,044 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9494.0MB reserved
2025-03-25 09:20:29,044 - training - INFO - Epoch: 310/200000, Batch: 15/45, Loss: 2.5859, Throughput: 70.46 samples/sec
2025-03-25 09:20:40,706 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9494.0MB reserved
2025-03-25 09:20:40,707 - training - INFO - Epoch: 310/200000, Batch: 30/45, Loss: 2.6101, Throughput: 71.21 samples/sec
2025-03-25 09:20:51,680 - training - INFO - Epoch 310 completed in 35.35s. Average loss: 2.6137
2025-03-25 09:20:51,684 - training - INFO - Starting epoch 311/200000
2025-03-25 09:20:52,429 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9632.0MB reserved
2025-03-25 09:20:52,429 - training - INFO - Epoch: 311/200000, Batch: 0/45, Loss: 2.5410, Throughput: 75.26 samples/sec
2025-03-25 09:21:04,332 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9498.0MB reserved
2025-03-25 09:21:04,333 - training - INFO - Epoch: 311/200000, Batch: 15/45, Loss: 2.6594, Throughput: 70.85 samples/sec
2025-03-25 09:21:15,936 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9500.0MB reserved
2025-03-25 09:21:15,936 - training - INFO - Epoch: 311/200000, Batch: 30/45, Loss: 2.6420, Throughput: 71.58 samples/sec
2025-03-25 09:21:26,849 - training - INFO - Epoch 311 completed in 35.16s. Average loss: 2.6080
2025-03-25 09:21:26,852 - training - INFO - Starting epoch 312/200000
2025-03-25 09:21:27,590 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9638.0MB reserved
2025-03-25 09:21:27,590 - training - INFO - Epoch: 312/200000, Batch: 0/45, Loss: 2.7891, Throughput: 76.11 samples/sec
2025-03-25 09:21:39,469 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9396.0MB reserved
2025-03-25 09:21:39,469 - training - INFO - Epoch: 312/200000, Batch: 15/45, Loss: 2.5487, Throughput: 71.03 samples/sec
2025-03-25 09:21:51,075 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9502.0MB reserved
2025-03-25 09:21:51,075 - training - INFO - Epoch: 312/200000, Batch: 30/45, Loss: 2.5813, Throughput: 71.67 samples/sec
2025-03-25 09:22:01,991 - training - INFO - Epoch 312 completed in 35.14s. Average loss: 2.5927
2025-03-25 09:22:01,995 - training - INFO - Starting epoch 313/200000
2025-03-25 09:22:02,752 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9640.0MB reserved
2025-03-25 09:22:02,752 - training - INFO - Epoch: 313/200000, Batch: 0/45, Loss: 3.2957, Throughput: 73.99 samples/sec
2025-03-25 09:22:14,653 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9512.0MB reserved
2025-03-25 09:22:14,654 - training - INFO - Epoch: 313/200000, Batch: 15/45, Loss: 2.6151, Throughput: 70.79 samples/sec
2025-03-25 09:22:26,312 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9514.0MB reserved
2025-03-25 09:22:26,312 - training - INFO - Epoch: 313/200000, Batch: 30/45, Loss: 2.5936, Throughput: 71.40 samples/sec
2025-03-25 09:22:37,288 - training - INFO - Epoch 313 completed in 35.29s. Average loss: 2.5959
2025-03-25 09:22:37,292 - training - INFO - Starting epoch 314/200000
2025-03-25 09:22:38,037 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9652.0MB reserved
2025-03-25 09:22:38,037 - training - INFO - Epoch: 314/200000, Batch: 0/45, Loss: 2.8312, Throughput: 75.25 samples/sec
2025-03-25 09:22:49,909 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9506.0MB reserved
2025-03-25 09:22:49,909 - training - INFO - Epoch: 314/200000, Batch: 15/45, Loss: 2.6317, Throughput: 71.02 samples/sec
2025-03-25 09:23:01,501 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9506.0MB reserved
2025-03-25 09:23:01,501 - training - INFO - Epoch: 314/200000, Batch: 30/45, Loss: 2.5348, Throughput: 71.71 samples/sec
2025-03-25 09:23:12,439 - training - INFO - Epoch 314 completed in 35.15s. Average loss: 2.5626
2025-03-25 09:23:12,442 - training - INFO - Starting epoch 315/200000
2025-03-25 09:23:13,180 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9644.0MB reserved
2025-03-25 09:23:13,180 - training - INFO - Epoch: 315/200000, Batch: 0/45, Loss: 2.2147, Throughput: 76.15 samples/sec
2025-03-25 09:23:25,149 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9496.0MB reserved
2025-03-25 09:23:25,149 - training - INFO - Epoch: 315/200000, Batch: 15/45, Loss: 2.5542, Throughput: 70.53 samples/sec
2025-03-25 09:23:36,831 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9496.0MB reserved
2025-03-25 09:23:36,831 - training - INFO - Epoch: 315/200000, Batch: 30/45, Loss: 2.5513, Throughput: 71.19 samples/sec
2025-03-25 09:23:47,831 - training - INFO - Epoch 315 completed in 35.39s. Average loss: 2.6520
2025-03-25 09:23:47,835 - training - INFO - Starting epoch 316/200000
2025-03-25 09:23:48,584 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9632.0MB reserved
2025-03-25 09:23:48,585 - training - INFO - Epoch: 316/200000, Batch: 0/45, Loss: 2.6611, Throughput: 74.94 samples/sec
2025-03-25 09:24:00,471 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9508.0MB reserved
2025-03-25 09:24:00,472 - training - INFO - Epoch: 316/200000, Batch: 15/45, Loss: 2.5843, Throughput: 70.92 samples/sec
2025-03-25 09:24:12,027 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9508.0MB reserved
2025-03-25 09:24:12,027 - training - INFO - Epoch: 316/200000, Batch: 30/45, Loss: 2.6111, Throughput: 71.77 samples/sec
2025-03-25 09:24:23,013 - training - INFO - Epoch 316 completed in 35.18s. Average loss: 2.6503
2025-03-25 09:24:23,018 - training - INFO - Starting epoch 317/200000
2025-03-25 09:24:23,762 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9646.0MB reserved
2025-03-25 09:24:23,763 - training - INFO - Epoch: 317/200000, Batch: 0/45, Loss: 2.8413, Throughput: 75.43 samples/sec
2025-03-25 09:24:35,682 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9510.0MB reserved
2025-03-25 09:24:35,682 - training - INFO - Epoch: 317/200000, Batch: 15/45, Loss: 2.6184, Throughput: 70.75 samples/sec
2025-03-25 09:24:47,339 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9510.0MB reserved
2025-03-25 09:24:47,339 - training - INFO - Epoch: 317/200000, Batch: 30/45, Loss: 2.5385, Throughput: 71.38 samples/sec
2025-03-25 09:24:58,325 - training - INFO - Epoch 317 completed in 35.31s. Average loss: 2.6031
2025-03-25 09:24:58,329 - training - INFO - Starting epoch 318/200000
2025-03-25 09:24:59,073 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9648.0MB reserved
2025-03-25 09:24:59,073 - training - INFO - Epoch: 318/200000, Batch: 0/45, Loss: 3.1107, Throughput: 75.45 samples/sec
2025-03-25 09:25:10,926 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9496.0MB reserved
2025-03-25 09:25:10,927 - training - INFO - Epoch: 318/200000, Batch: 15/45, Loss: 2.5563, Throughput: 71.14 samples/sec
2025-03-25 09:25:22,499 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9496.0MB reserved
2025-03-25 09:25:22,500 - training - INFO - Epoch: 318/200000, Batch: 30/45, Loss: 2.5029, Throughput: 71.83 samples/sec
2025-03-25 09:25:33,431 - training - INFO - Epoch 318 completed in 35.10s. Average loss: 2.5762
2025-03-25 09:25:33,435 - training - INFO - Starting epoch 319/200000
2025-03-25 09:25:34,157 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9634.0MB reserved
2025-03-25 09:25:34,157 - training - INFO - Epoch: 319/200000, Batch: 0/45, Loss: 2.5091, Throughput: 77.75 samples/sec
2025-03-25 09:25:46,082 - training - INFO - Memory: GPU 0: 3563.3MB allocated, 9512.0MB reserved
2025-03-25 09:25:46,082 - training - INFO - Epoch: 319/200000, Batch: 15/45, Loss: 2.6384, Throughput: 70.85 samples/sec
2025-03-25 09:25:57,831 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9514.0MB reserved
2025-03-25 09:25:57,831 - training - INFO - Epoch: 319/200000, Batch: 30/45, Loss: 2.5668, Throughput: 71.16 samples/sec
2025-03-25 09:26:08,804 - training - INFO - Epoch 319 completed in 35.37s. Average loss: 2.5592
2025-03-25 09:26:08,807 - training - INFO - Starting epoch 320/200000
2025-03-25 09:26:09,564 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9652.0MB reserved
2025-03-25 09:26:09,565 - training - INFO - Epoch: 320/200000, Batch: 0/45, Loss: 2.5953, Throughput: 74.05 samples/sec
2025-03-25 09:26:21,481 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9398.0MB reserved
2025-03-25 09:26:21,481 - training - INFO - Epoch: 320/200000, Batch: 15/45, Loss: 2.5181, Throughput: 70.71 samples/sec
2025-03-25 09:26:33,132 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9398.0MB reserved
2025-03-25 09:26:33,132 - training - INFO - Epoch: 320/200000, Batch: 30/45, Loss: 2.5642, Throughput: 71.37 samples/sec
2025-03-25 09:26:44,061 - training - INFO - Epoch 320 completed in 35.25s. Average loss: 2.5762
2025-03-25 09:26:44,065 - training - INFO - Starting epoch 321/200000
2025-03-25 09:26:44,803 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9536.0MB reserved
2025-03-25 09:26:44,803 - training - INFO - Epoch: 321/200000, Batch: 0/45, Loss: 2.6451, Throughput: 75.96 samples/sec
2025-03-25 09:26:56,729 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9494.0MB reserved
2025-03-25 09:26:56,729 - training - INFO - Epoch: 321/200000, Batch: 15/45, Loss: 2.5334, Throughput: 70.76 samples/sec
2025-03-25 09:27:08,369 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9496.0MB reserved
2025-03-25 09:27:08,370 - training - INFO - Epoch: 321/200000, Batch: 30/45, Loss: 2.5582, Throughput: 71.43 samples/sec
2025-03-25 09:27:19,304 - training - INFO - Epoch 321 completed in 35.24s. Average loss: 2.5627
2025-03-25 09:27:19,309 - training - INFO - Starting epoch 322/200000
2025-03-25 09:27:20,050 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9632.0MB reserved
2025-03-25 09:27:20,051 - training - INFO - Epoch: 322/200000, Batch: 0/45, Loss: 2.5784, Throughput: 75.70 samples/sec
2025-03-25 09:27:31,936 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9394.0MB reserved
2025-03-25 09:27:31,937 - training - INFO - Epoch: 322/200000, Batch: 15/45, Loss: 2.4313, Throughput: 70.97 samples/sec
2025-03-25 09:27:43,466 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9394.0MB reserved
2025-03-25 09:27:43,466 - training - INFO - Epoch: 322/200000, Batch: 30/45, Loss: 2.4935, Throughput: 71.87 samples/sec
2025-03-25 09:27:54,384 - training - INFO - Epoch 322 completed in 35.08s. Average loss: 2.5824
2025-03-25 09:27:54,388 - training - INFO - Starting epoch 323/200000
2025-03-25 09:27:55,143 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9532.0MB reserved
2025-03-25 09:27:55,143 - training - INFO - Epoch: 323/200000, Batch: 0/45, Loss: 2.6681, Throughput: 74.32 samples/sec
2025-03-25 09:28:07,040 - training - INFO - Memory: GPU 0: 3562.6MB allocated, 9514.0MB reserved
2025-03-25 09:28:07,041 - training - INFO - Epoch: 323/200000, Batch: 15/45, Loss: 2.5508, Throughput: 70.83 samples/sec
2025-03-25 09:28:18,706 - training - INFO - Memory: GPU 0: 3562.6MB allocated, 9514.0MB reserved
2025-03-25 09:28:18,706 - training - INFO - Epoch: 323/200000, Batch: 30/45, Loss: 2.5622, Throughput: 71.39 samples/sec
2025-03-25 09:28:29,622 - training - INFO - Epoch 323 completed in 35.23s. Average loss: 2.5593
2025-03-25 09:28:29,625 - training - INFO - Starting epoch 324/200000
2025-03-25 09:28:30,361 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9652.0MB reserved
2025-03-25 09:28:30,361 - training - INFO - Epoch: 324/200000, Batch: 0/45, Loss: 2.5746, Throughput: 76.21 samples/sec
2025-03-25 09:28:42,233 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9510.0MB reserved
2025-03-25 09:28:42,233 - training - INFO - Epoch: 324/200000, Batch: 15/45, Loss: 2.4661, Throughput: 71.08 samples/sec
2025-03-25 09:28:53,812 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9510.0MB reserved
2025-03-25 09:28:53,812 - training - INFO - Epoch: 324/200000, Batch: 30/45, Loss: 2.4930, Throughput: 71.78 samples/sec
2025-03-25 09:29:04,801 - training - INFO - Epoch 324 completed in 35.18s. Average loss: 2.5663
2025-03-25 09:29:04,805 - training - INFO - Starting epoch 325/200000
2025-03-25 09:29:05,558 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9648.0MB reserved
2025-03-25 09:29:05,558 - training - INFO - Epoch: 325/200000, Batch: 0/45, Loss: 3.1022, Throughput: 74.48 samples/sec
2025-03-25 09:29:17,519 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9508.0MB reserved
2025-03-25 09:29:17,520 - training - INFO - Epoch: 325/200000, Batch: 15/45, Loss: 2.5186, Throughput: 70.48 samples/sec
2025-03-25 09:29:29,176 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9508.0MB reserved
2025-03-25 09:29:29,176 - training - INFO - Epoch: 325/200000, Batch: 30/45, Loss: 2.5351, Throughput: 71.23 samples/sec
2025-03-25 09:29:40,092 - training - INFO - Epoch 325 completed in 35.29s. Average loss: 2.5771
2025-03-25 09:29:40,096 - training - INFO - Starting epoch 326/200000
2025-03-25 09:29:40,829 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9646.0MB reserved
2025-03-25 09:29:40,829 - training - INFO - Epoch: 326/200000, Batch: 0/45, Loss: 2.1976, Throughput: 76.50 samples/sec
2025-03-25 09:29:52,832 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9502.0MB reserved
2025-03-25 09:29:52,833 - training - INFO - Epoch: 326/200000, Batch: 15/45, Loss: 2.4692, Throughput: 70.36 samples/sec
2025-03-25 09:30:04,452 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9502.0MB reserved
2025-03-25 09:30:04,452 - training - INFO - Epoch: 326/200000, Batch: 30/45, Loss: 2.4609, Throughput: 71.28 samples/sec
2025-03-25 09:30:15,400 - training - INFO - Epoch 326 completed in 35.30s. Average loss: 2.5337
2025-03-25 09:30:15,404 - training - INFO - Starting epoch 327/200000
2025-03-25 09:30:16,134 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9640.0MB reserved
2025-03-25 09:30:16,135 - training - INFO - Epoch: 327/200000, Batch: 0/45, Loss: 2.6605, Throughput: 76.79 samples/sec
2025-03-25 09:30:28,088 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9502.0MB reserved
2025-03-25 09:30:28,088 - training - INFO - Epoch: 327/200000, Batch: 15/45, Loss: 2.4398, Throughput: 70.65 samples/sec
2025-03-25 09:30:39,681 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9502.0MB reserved
2025-03-25 09:30:39,681 - training - INFO - Epoch: 327/200000, Batch: 30/45, Loss: 2.4670, Throughput: 71.51 samples/sec
2025-03-25 09:30:50,605 - training - INFO - Epoch 327 completed in 35.20s. Average loss: 2.5277
2025-03-25 09:30:50,609 - training - INFO - Starting epoch 328/200000
2025-03-25 09:30:51,368 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9640.0MB reserved
2025-03-25 09:30:51,368 - training - INFO - Epoch: 328/200000, Batch: 0/45, Loss: 2.6585, Throughput: 73.92 samples/sec
2025-03-25 09:31:03,211 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9504.0MB reserved
2025-03-25 09:31:03,211 - training - INFO - Epoch: 328/200000, Batch: 15/45, Loss: 2.6075, Throughput: 71.11 samples/sec
2025-03-25 09:31:14,754 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9504.0MB reserved
2025-03-25 09:31:14,755 - training - INFO - Epoch: 328/200000, Batch: 30/45, Loss: 2.5608, Throughput: 71.91 samples/sec
2025-03-25 09:31:25,676 - training - INFO - Epoch 328 completed in 35.07s. Average loss: 2.5489
2025-03-25 09:31:25,680 - training - INFO - Starting epoch 329/200000
2025-03-25 09:31:26,435 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9642.0MB reserved
2025-03-25 09:31:26,436 - training - INFO - Epoch: 329/200000, Batch: 0/45, Loss: 2.8702, Throughput: 74.29 samples/sec
2025-03-25 09:31:38,354 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9494.0MB reserved
2025-03-25 09:31:38,355 - training - INFO - Epoch: 329/200000, Batch: 15/45, Loss: 2.4926, Throughput: 70.70 samples/sec
2025-03-25 09:31:49,947 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9494.0MB reserved
2025-03-25 09:31:49,947 - training - INFO - Epoch: 329/200000, Batch: 30/45, Loss: 2.4439, Throughput: 71.54 samples/sec
2025-03-25 09:32:00,931 - training - INFO - Epoch 329 completed in 35.25s. Average loss: 2.5215
2025-03-25 09:32:00,934 - training - INFO - Starting epoch 330/200000
2025-03-25 09:32:01,679 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9632.0MB reserved
2025-03-25 09:32:01,679 - training - INFO - Epoch: 330/200000, Batch: 0/45, Loss: 2.6459, Throughput: 75.39 samples/sec
2025-03-25 09:32:13,639 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9492.0MB reserved
2025-03-25 09:32:13,640 - training - INFO - Epoch: 330/200000, Batch: 15/45, Loss: 2.6400, Throughput: 70.54 samples/sec
2025-03-25 09:32:25,310 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9492.0MB reserved
2025-03-25 09:32:25,311 - training - INFO - Epoch: 330/200000, Batch: 30/45, Loss: 2.5334, Throughput: 71.22 samples/sec
2025-03-25 09:32:36,321 - training - INFO - Epoch 330 completed in 35.39s. Average loss: 2.5167
2025-03-25 09:32:36,325 - training - INFO - Starting epoch 331/200000
2025-03-25 09:32:37,070 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9630.0MB reserved
2025-03-25 09:32:37,071 - training - INFO - Epoch: 331/200000, Batch: 0/45, Loss: 2.6089, Throughput: 75.19 samples/sec
2025-03-25 09:32:48,976 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9492.0MB reserved
2025-03-25 09:32:48,976 - training - INFO - Epoch: 331/200000, Batch: 15/45, Loss: 2.5394, Throughput: 70.83 samples/sec
2025-03-25 09:33:00,559 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9492.0MB reserved
2025-03-25 09:33:00,559 - training - INFO - Epoch: 331/200000, Batch: 30/45, Loss: 2.4930, Throughput: 71.64 samples/sec
2025-03-25 09:33:11,455 - training - INFO - Epoch 331 completed in 35.13s. Average loss: 2.5253
2025-03-25 09:33:11,458 - training - INFO - Starting epoch 332/200000
2025-03-25 09:33:12,197 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9630.0MB reserved
2025-03-25 09:33:12,198 - training - INFO - Epoch: 332/200000, Batch: 0/45, Loss: 2.3571, Throughput: 75.92 samples/sec
2025-03-25 09:33:24,110 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9482.0MB reserved
2025-03-25 09:33:24,110 - training - INFO - Epoch: 332/200000, Batch: 15/45, Loss: 2.5628, Throughput: 70.83 samples/sec
2025-03-25 09:33:35,770 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9482.0MB reserved
2025-03-25 09:33:35,771 - training - INFO - Epoch: 332/200000, Batch: 30/45, Loss: 2.5629, Throughput: 71.41 samples/sec
2025-03-25 09:33:46,665 - training - INFO - Epoch 332 completed in 35.21s. Average loss: 2.5535
2025-03-25 09:33:46,668 - training - INFO - Starting epoch 333/200000
2025-03-25 09:33:47,404 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9622.0MB reserved
2025-03-25 09:33:47,405 - training - INFO - Epoch: 333/200000, Batch: 0/45, Loss: 2.4774, Throughput: 76.28 samples/sec
2025-03-25 09:33:59,371 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9514.0MB reserved
2025-03-25 09:33:59,372 - training - INFO - Epoch: 333/200000, Batch: 15/45, Loss: 2.6102, Throughput: 70.54 samples/sec
2025-03-25 09:34:11,032 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9514.0MB reserved
2025-03-25 09:34:11,033 - training - INFO - Epoch: 333/200000, Batch: 30/45, Loss: 2.5154, Throughput: 71.26 samples/sec
2025-03-25 09:34:21,933 - training - INFO - Epoch 333 completed in 35.26s. Average loss: 2.5373
2025-03-25 09:34:21,937 - training - INFO - Starting epoch 334/200000
2025-03-25 09:34:22,675 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9652.0MB reserved
2025-03-25 09:34:22,676 - training - INFO - Epoch: 334/200000, Batch: 0/45, Loss: 3.0135, Throughput: 75.89 samples/sec
2025-03-25 09:34:34,667 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9506.0MB reserved
2025-03-25 09:34:34,667 - training - INFO - Epoch: 334/200000, Batch: 15/45, Loss: 2.5423, Throughput: 70.39 samples/sec
2025-03-25 09:34:46,240 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9508.0MB reserved
2025-03-25 09:34:46,240 - training - INFO - Epoch: 334/200000, Batch: 30/45, Loss: 2.5113, Throughput: 71.43 samples/sec
2025-03-25 09:34:57,183 - training - INFO - Epoch 334 completed in 35.25s. Average loss: 2.5898
2025-03-25 09:34:57,187 - training - INFO - Starting epoch 335/200000
2025-03-25 09:34:57,942 - training - INFO - Memory: GPU 0: 3556.7MB allocated, 9646.0MB reserved
2025-03-25 09:34:57,942 - training - INFO - Epoch: 335/200000, Batch: 0/45, Loss: 2.3901, Throughput: 74.31 samples/sec
2025-03-25 09:35:09,855 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9502.0MB reserved
2025-03-25 09:35:09,855 - training - INFO - Epoch: 335/200000, Batch: 15/45, Loss: 2.5244, Throughput: 70.73 samples/sec
2025-03-25 09:35:21,484 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9502.0MB reserved
2025-03-25 09:35:21,484 - training - INFO - Epoch: 335/200000, Batch: 30/45, Loss: 2.5390, Throughput: 71.45 samples/sec
2025-03-25 09:35:32,405 - training - INFO - Epoch 335 completed in 35.22s. Average loss: 2.5043
2025-03-25 09:35:32,408 - training - INFO - Starting epoch 336/200000
2025-03-25 09:35:33,150 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9640.0MB reserved
2025-03-25 09:35:33,150 - training - INFO - Epoch: 336/200000, Batch: 0/45, Loss: 2.1952, Throughput: 75.60 samples/sec
2025-03-25 09:35:44,965 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9394.0MB reserved
2025-03-25 09:35:44,966 - training - INFO - Epoch: 336/200000, Batch: 15/45, Loss: 2.4420, Throughput: 71.36 samples/sec
2025-03-25 09:35:56,555 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9500.0MB reserved
2025-03-25 09:35:56,556 - training - INFO - Epoch: 336/200000, Batch: 30/45, Loss: 2.5008, Throughput: 71.90 samples/sec
2025-03-25 09:36:07,460 - training - INFO - Epoch 336 completed in 35.05s. Average loss: 2.5193
2025-03-25 09:36:07,464 - training - INFO - Starting epoch 337/200000
2025-03-25 09:36:08,218 - training - INFO - Memory: GPU 0: 3556.9MB allocated, 9636.0MB reserved
2025-03-25 09:36:08,218 - training - INFO - Epoch: 337/200000, Batch: 0/45, Loss: 2.3529, Throughput: 74.40 samples/sec
2025-03-25 09:36:20,132 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9500.0MB reserved
2025-03-25 09:36:20,132 - training - INFO - Epoch: 337/200000, Batch: 15/45, Loss: 2.4652, Throughput: 70.74 samples/sec
2025-03-25 09:36:31,803 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9500.0MB reserved
2025-03-25 09:36:31,804 - training - INFO - Epoch: 337/200000, Batch: 30/45, Loss: 2.4883, Throughput: 71.33 samples/sec
2025-03-25 09:36:42,720 - training - INFO - Epoch 337 completed in 35.26s. Average loss: 2.5406
2025-03-25 09:36:42,723 - training - INFO - Starting epoch 338/200000
2025-03-25 09:36:43,469 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9638.0MB reserved
2025-03-25 09:36:43,470 - training - INFO - Epoch: 338/200000, Batch: 0/45, Loss: 2.6384, Throughput: 75.13 samples/sec
2025-03-25 09:36:55,324 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9504.0MB reserved
2025-03-25 09:36:55,325 - training - INFO - Epoch: 338/200000, Batch: 15/45, Loss: 2.4909, Throughput: 71.11 samples/sec
2025-03-25 09:37:06,880 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9506.0MB reserved
2025-03-25 09:37:06,880 - training - INFO - Epoch: 338/200000, Batch: 30/45, Loss: 2.4775, Throughput: 71.87 samples/sec
2025-03-25 09:37:17,779 - training - INFO - Epoch 338 completed in 35.06s. Average loss: 2.5223
2025-03-25 09:37:17,783 - training - INFO - Starting epoch 339/200000
2025-03-25 09:37:18,530 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9644.0MB reserved
2025-03-25 09:37:18,530 - training - INFO - Epoch: 339/200000, Batch: 0/45, Loss: 2.2785, Throughput: 75.11 samples/sec
2025-03-25 09:37:30,434 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9488.0MB reserved
2025-03-25 09:37:30,435 - training - INFO - Epoch: 339/200000, Batch: 15/45, Loss: 2.5472, Throughput: 70.84 samples/sec
2025-03-25 09:37:41,998 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9490.0MB reserved
2025-03-25 09:37:41,999 - training - INFO - Epoch: 339/200000, Batch: 30/45, Loss: 2.5997, Throughput: 71.69 samples/sec
2025-03-25 09:37:52,978 - training - INFO - Epoch 339 completed in 35.19s. Average loss: 2.5550
2025-03-25 09:37:52,982 - training - INFO - Starting epoch 340/200000
2025-03-25 09:37:53,730 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9628.0MB reserved
2025-03-25 09:37:53,730 - training - INFO - Epoch: 340/200000, Batch: 0/45, Loss: 2.6911, Throughput: 74.96 samples/sec
2025-03-25 09:38:05,644 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9510.0MB reserved
2025-03-25 09:38:05,645 - training - INFO - Epoch: 340/200000, Batch: 15/45, Loss: 2.4890, Throughput: 70.77 samples/sec
2025-03-25 09:38:17,279 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9510.0MB reserved
2025-03-25 09:38:17,396 - training - INFO - Epoch: 340/200000, Batch: 30/45, Loss: 2.4669, Throughput: 71.45 samples/sec
2025-03-25 09:38:28,343 - training - INFO - Epoch 340 completed in 35.36s. Average loss: 2.5119
2025-03-25 09:38:28,346 - training - INFO - Starting epoch 341/200000
2025-03-25 09:38:29,084 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9648.0MB reserved
2025-03-25 09:38:29,084 - training - INFO - Epoch: 341/200000, Batch: 0/45, Loss: 2.2609, Throughput: 76.25 samples/sec
2025-03-25 09:38:41,001 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9512.0MB reserved
2025-03-25 09:38:41,002 - training - INFO - Epoch: 341/200000, Batch: 15/45, Loss: 2.4538, Throughput: 70.81 samples/sec
2025-03-25 09:38:52,697 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9512.0MB reserved
2025-03-25 09:38:52,697 - training - INFO - Epoch: 341/200000, Batch: 30/45, Loss: 2.4762, Throughput: 71.29 samples/sec
2025-03-25 09:39:03,655 - training - INFO - Epoch 341 completed in 35.31s. Average loss: 2.5120
2025-03-25 09:39:03,659 - training - INFO - Starting epoch 342/200000
2025-03-25 09:39:04,393 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9650.0MB reserved
2025-03-25 09:39:04,393 - training - INFO - Epoch: 342/200000, Batch: 0/45, Loss: 1.9839, Throughput: 76.52 samples/sec
2025-03-25 09:39:16,347 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9496.0MB reserved
2025-03-25 09:39:16,348 - training - INFO - Epoch: 342/200000, Batch: 15/45, Loss: 2.4374, Throughput: 70.63 samples/sec
2025-03-25 09:39:28,047 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9496.0MB reserved
2025-03-25 09:39:28,048 - training - INFO - Epoch: 342/200000, Batch: 30/45, Loss: 2.4789, Throughput: 71.19 samples/sec
2025-03-25 09:39:39,026 - training - INFO - Epoch 342 completed in 35.37s. Average loss: 2.5558
2025-03-25 09:39:39,030 - training - INFO - Starting epoch 343/200000
2025-03-25 09:39:39,802 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9634.0MB reserved
2025-03-25 09:39:39,802 - training - INFO - Epoch: 343/200000, Batch: 0/45, Loss: 2.7132, Throughput: 72.73 samples/sec
2025-03-25 09:39:51,755 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9496.0MB reserved
2025-03-25 09:39:51,755 - training - INFO - Epoch: 343/200000, Batch: 15/45, Loss: 2.5823, Throughput: 70.42 samples/sec
2025-03-25 09:40:03,388 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9496.0MB reserved
2025-03-25 09:40:03,388 - training - INFO - Epoch: 343/200000, Batch: 30/45, Loss: 2.5634, Throughput: 71.27 samples/sec
2025-03-25 09:40:14,373 - training - INFO - Epoch 343 completed in 35.34s. Average loss: 2.4738
2025-03-25 09:40:14,377 - training - INFO - Starting epoch 344/200000
2025-03-25 09:40:15,118 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9634.0MB reserved
2025-03-25 09:40:15,118 - training - INFO - Epoch: 344/200000, Batch: 0/45, Loss: 2.6069, Throughput: 75.74 samples/sec
2025-03-25 09:40:26,992 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9502.0MB reserved
2025-03-25 09:40:26,993 - training - INFO - Epoch: 344/200000, Batch: 15/45, Loss: 2.5076, Throughput: 71.03 samples/sec
2025-03-25 09:40:38,584 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9502.0MB reserved
2025-03-25 09:40:38,584 - training - INFO - Epoch: 344/200000, Batch: 30/45, Loss: 2.5059, Throughput: 71.72 samples/sec
2025-03-25 09:40:49,514 - training - INFO - Epoch 344 completed in 35.14s. Average loss: 2.5513
2025-03-25 09:40:49,518 - training - INFO - Starting epoch 345/200000
2025-03-25 09:40:50,274 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9640.0MB reserved
2025-03-25 09:40:50,274 - training - INFO - Epoch: 345/200000, Batch: 0/45, Loss: 2.3453, Throughput: 74.22 samples/sec
2025-03-25 09:41:02,135 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9502.0MB reserved
2025-03-25 09:41:02,135 - training - INFO - Epoch: 345/200000, Batch: 15/45, Loss: 2.4884, Throughput: 71.02 samples/sec
2025-03-25 09:41:13,790 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9502.0MB reserved
2025-03-25 09:41:13,790 - training - INFO - Epoch: 345/200000, Batch: 30/45, Loss: 2.4778, Throughput: 71.53 samples/sec
2025-03-25 09:41:24,747 - training - INFO - Epoch 345 completed in 35.23s. Average loss: 2.5124
2025-03-25 09:41:24,750 - training - INFO - Starting epoch 346/200000
2025-03-25 09:41:25,523 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9640.0MB reserved
2025-03-25 09:41:25,524 - training - INFO - Epoch: 346/200000, Batch: 0/45, Loss: 2.6496, Throughput: 72.51 samples/sec
2025-03-25 09:41:37,474 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9506.0MB reserved
2025-03-25 09:41:37,474 - training - INFO - Epoch: 346/200000, Batch: 15/45, Loss: 2.6592, Throughput: 70.43 samples/sec
2025-03-25 09:41:49,144 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9506.0MB reserved
2025-03-25 09:41:49,144 - training - INFO - Epoch: 346/200000, Batch: 30/45, Loss: 2.5980, Throughput: 71.17 samples/sec
2025-03-25 09:42:00,049 - training - INFO - Epoch 346 completed in 35.30s. Average loss: 2.5361
2025-03-25 09:42:00,053 - training - INFO - Starting epoch 347/200000
2025-03-25 09:42:00,785 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9644.0MB reserved
2025-03-25 09:42:00,785 - training - INFO - Epoch: 347/200000, Batch: 0/45, Loss: 2.9285, Throughput: 76.69 samples/sec
2025-03-25 09:42:12,637 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9512.0MB reserved
2025-03-25 09:42:12,637 - training - INFO - Epoch: 347/200000, Batch: 15/45, Loss: 2.4729, Throughput: 71.21 samples/sec
2025-03-25 09:42:24,179 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9512.0MB reserved
2025-03-25 09:42:24,179 - training - INFO - Epoch: 347/200000, Batch: 30/45, Loss: 2.4611, Throughput: 71.96 samples/sec
2025-03-25 09:42:35,068 - training - INFO - Epoch 347 completed in 35.02s. Average loss: 2.5425
2025-03-25 09:42:35,071 - training - INFO - Starting epoch 348/200000
2025-03-25 09:42:35,802 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9650.0MB reserved
2025-03-25 09:42:35,802 - training - INFO - Epoch: 348/200000, Batch: 0/45, Loss: 2.7566, Throughput: 76.81 samples/sec
2025-03-25 09:42:47,659 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9496.0MB reserved
2025-03-25 09:42:47,659 - training - INFO - Epoch: 348/200000, Batch: 15/45, Loss: 2.5463, Throughput: 71.19 samples/sec
2025-03-25 09:42:59,248 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9498.0MB reserved
2025-03-25 09:42:59,248 - training - INFO - Epoch: 348/200000, Batch: 30/45, Loss: 2.4801, Throughput: 71.81 samples/sec
2025-03-25 09:43:10,252 - training - INFO - Epoch 348 completed in 35.18s. Average loss: 2.4924
2025-03-25 09:43:10,256 - training - INFO - Starting epoch 349/200000
2025-03-25 09:43:11,007 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9634.0MB reserved
2025-03-25 09:43:11,008 - training - INFO - Epoch: 349/200000, Batch: 0/45, Loss: 2.2637, Throughput: 74.63 samples/sec
2025-03-25 09:43:22,923 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9486.0MB reserved
2025-03-25 09:43:22,923 - training - INFO - Epoch: 349/200000, Batch: 15/45, Loss: 2.4880, Throughput: 70.74 samples/sec
2025-03-25 09:43:34,561 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9486.0MB reserved
2025-03-25 09:43:34,561 - training - INFO - Epoch: 349/200000, Batch: 30/45, Loss: 2.4626, Throughput: 71.43 samples/sec
2025-03-25 09:43:45,542 - training - INFO - Epoch 349 completed in 35.29s. Average loss: 2.5022
2025-03-25 09:43:45,547 - training - INFO - Starting epoch 350/200000
2025-03-25 09:43:46,301 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9624.0MB reserved
2025-03-25 09:43:46,301 - training - INFO - Epoch: 350/200000, Batch: 0/45, Loss: 2.9749, Throughput: 74.42 samples/sec
2025-03-25 09:43:58,149 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9502.0MB reserved
2025-03-25 09:43:58,149 - training - INFO - Epoch: 350/200000, Batch: 15/45, Loss: 2.5680, Throughput: 71.11 samples/sec
2025-03-25 09:44:09,719 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9502.0MB reserved
2025-03-25 09:44:09,719 - training - INFO - Epoch: 350/200000, Batch: 30/45, Loss: 2.5736, Throughput: 71.82 samples/sec
2025-03-25 09:44:20,628 - training - INFO - Epoch 350 completed in 35.08s. Average loss: 2.5393
2025-03-25 09:44:20,631 - training - INFO - Starting epoch 351/200000
2025-03-25 09:44:21,368 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9640.0MB reserved
2025-03-25 09:44:21,368 - training - INFO - Epoch: 351/200000, Batch: 0/45, Loss: 2.1561, Throughput: 76.16 samples/sec
2025-03-25 09:44:33,270 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9490.0MB reserved
2025-03-25 09:44:33,271 - training - INFO - Epoch: 351/200000, Batch: 15/45, Loss: 2.3940, Throughput: 70.91 samples/sec
2025-03-25 09:44:44,902 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9492.0MB reserved
2025-03-25 09:44:44,902 - training - INFO - Epoch: 351/200000, Batch: 30/45, Loss: 2.4330, Throughput: 71.53 samples/sec
2025-03-25 09:44:55,860 - training - INFO - Epoch 351 completed in 35.23s. Average loss: 2.5028
2025-03-25 09:44:55,864 - training - INFO - Starting epoch 352/200000
2025-03-25 09:44:56,616 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9630.0MB reserved
2025-03-25 09:44:56,743 - training - INFO - Epoch: 352/200000, Batch: 0/45, Loss: 2.8454, Throughput: 74.66 samples/sec
2025-03-25 09:45:08,547 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9500.0MB reserved
2025-03-25 09:45:08,547 - training - INFO - Epoch: 352/200000, Batch: 15/45, Loss: 2.5195, Throughput: 70.66 samples/sec
2025-03-25 09:45:20,105 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9500.0MB reserved
2025-03-25 09:45:20,105 - training - INFO - Epoch: 352/200000, Batch: 30/45, Loss: 2.4833, Throughput: 71.62 samples/sec
2025-03-25 09:45:31,002 - training - INFO - Epoch 352 completed in 35.14s. Average loss: 2.5443
2025-03-25 09:45:31,005 - training - INFO - Starting epoch 353/200000
2025-03-25 09:45:31,766 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9638.0MB reserved
2025-03-25 09:45:31,766 - training - INFO - Epoch: 353/200000, Batch: 0/45, Loss: 2.4782, Throughput: 73.84 samples/sec
2025-03-25 09:45:43,793 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9488.0MB reserved
2025-03-25 09:45:43,794 - training - INFO - Epoch: 353/200000, Batch: 15/45, Loss: 2.4964, Throughput: 70.09 samples/sec
2025-03-25 09:45:55,446 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9488.0MB reserved
2025-03-25 09:45:55,447 - training - INFO - Epoch: 353/200000, Batch: 30/45, Loss: 2.4907, Throughput: 71.04 samples/sec
2025-03-25 09:46:06,403 - training - INFO - Epoch 353 completed in 35.40s. Average loss: 2.5111
2025-03-25 09:46:06,407 - training - INFO - Starting epoch 354/200000
2025-03-25 09:46:07,142 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9626.0MB reserved
2025-03-25 09:46:07,142 - training - INFO - Epoch: 354/200000, Batch: 0/45, Loss: 2.7375, Throughput: 76.40 samples/sec
2025-03-25 09:46:19,155 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9482.0MB reserved
2025-03-25 09:46:19,156 - training - INFO - Epoch: 354/200000, Batch: 15/45, Loss: 2.4945, Throughput: 70.29 samples/sec
2025-03-25 09:46:30,704 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9482.0MB reserved
2025-03-25 09:46:30,704 - training - INFO - Epoch: 354/200000, Batch: 30/45, Loss: 2.4524, Throughput: 71.45 samples/sec
2025-03-25 09:46:41,626 - training - INFO - Epoch 354 completed in 35.22s. Average loss: 2.4451
2025-03-25 09:46:41,631 - training - INFO - Starting epoch 355/200000
2025-03-25 09:46:42,370 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9620.0MB reserved
2025-03-25 09:46:42,370 - training - INFO - Epoch: 355/200000, Batch: 0/45, Loss: 2.4733, Throughput: 75.96 samples/sec
2025-03-25 09:46:54,316 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9510.0MB reserved
2025-03-25 09:46:54,316 - training - INFO - Epoch: 355/200000, Batch: 15/45, Loss: 2.4504, Throughput: 70.64 samples/sec
2025-03-25 09:47:06,065 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9510.0MB reserved
2025-03-25 09:47:06,065 - training - INFO - Epoch: 355/200000, Batch: 30/45, Loss: 2.4206, Throughput: 71.05 samples/sec
2025-03-25 09:47:17,077 - training - INFO - Epoch 355 completed in 35.45s. Average loss: 2.4508
2025-03-25 09:47:17,081 - training - INFO - Starting epoch 356/200000
2025-03-25 09:47:17,847 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9648.0MB reserved
2025-03-25 09:47:17,847 - training - INFO - Epoch: 356/200000, Batch: 0/45, Loss: 2.6966, Throughput: 73.26 samples/sec
2025-03-25 09:47:29,794 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9496.0MB reserved
2025-03-25 09:47:29,794 - training - INFO - Epoch: 356/200000, Batch: 15/45, Loss: 2.4371, Throughput: 70.48 samples/sec
2025-03-25 09:47:41,417 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9498.0MB reserved
2025-03-25 09:47:41,417 - training - INFO - Epoch: 356/200000, Batch: 30/45, Loss: 2.4872, Throughput: 71.34 samples/sec
2025-03-25 09:47:52,343 - training - INFO - Epoch 356 completed in 35.26s. Average loss: 2.4398
2025-03-25 09:47:52,347 - training - INFO - Starting epoch 357/200000
2025-03-25 09:47:53,070 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9636.0MB reserved
2025-03-25 09:47:53,070 - training - INFO - Epoch: 357/200000, Batch: 0/45, Loss: 2.7849, Throughput: 77.58 samples/sec
2025-03-25 09:48:05,020 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9398.0MB reserved
2025-03-25 09:48:05,020 - training - INFO - Epoch: 357/200000, Batch: 15/45, Loss: 2.4518, Throughput: 70.71 samples/sec
2025-03-25 09:48:16,632 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9398.0MB reserved
2025-03-25 09:48:16,632 - training - INFO - Epoch: 357/200000, Batch: 30/45, Loss: 2.4504, Throughput: 71.49 samples/sec
2025-03-25 09:48:27,595 - training - INFO - Epoch 357 completed in 35.25s. Average loss: 2.5042
2025-03-25 09:48:27,598 - training - INFO - Starting epoch 358/200000
2025-03-25 09:48:28,344 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9536.0MB reserved
2025-03-25 09:48:28,345 - training - INFO - Epoch: 358/200000, Batch: 0/45, Loss: 2.5654, Throughput: 75.19 samples/sec
2025-03-25 09:48:40,261 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9504.0MB reserved
2025-03-25 09:48:40,262 - training - INFO - Epoch: 358/200000, Batch: 15/45, Loss: 2.4268, Throughput: 70.77 samples/sec
2025-03-25 09:48:51,908 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9504.0MB reserved
2025-03-25 09:48:51,908 - training - INFO - Epoch: 358/200000, Batch: 30/45, Loss: 2.4370, Throughput: 71.42 samples/sec
2025-03-25 09:49:02,842 - training - INFO - Epoch 358 completed in 35.24s. Average loss: 2.4688
2025-03-25 09:49:02,846 - training - INFO - Starting epoch 359/200000
2025-03-25 09:49:03,595 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9642.0MB reserved
2025-03-25 09:49:03,595 - training - INFO - Epoch: 359/200000, Batch: 0/45, Loss: 2.3272, Throughput: 74.77 samples/sec
2025-03-25 09:49:15,493 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9510.0MB reserved
2025-03-25 09:49:15,494 - training - INFO - Epoch: 359/200000, Batch: 15/45, Loss: 2.4454, Throughput: 70.85 samples/sec
2025-03-25 09:49:27,185 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9510.0MB reserved
2025-03-25 09:49:27,186 - training - INFO - Epoch: 359/200000, Batch: 30/45, Loss: 2.4329, Throughput: 71.33 samples/sec
2025-03-25 09:49:38,218 - training - INFO - Epoch 359 completed in 35.37s. Average loss: 2.4734
2025-03-25 09:49:38,222 - training - INFO - Starting epoch 360/200000
2025-03-25 09:49:38,973 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9646.0MB reserved
2025-03-25 09:49:38,974 - training - INFO - Epoch: 360/200000, Batch: 0/45, Loss: 2.0337, Throughput: 74.69 samples/sec
2025-03-25 09:49:50,872 - training - INFO - Memory: GPU 0: 3563.6MB allocated, 9502.0MB reserved
2025-03-25 09:49:50,872 - training - INFO - Epoch: 360/200000, Batch: 15/45, Loss: 2.3763, Throughput: 70.84 samples/sec
2025-03-25 09:50:02,453 - training - INFO - Memory: GPU 0: 3563.6MB allocated, 9502.0MB reserved
2025-03-25 09:50:02,454 - training - INFO - Epoch: 360/200000, Batch: 30/45, Loss: 2.4163, Throughput: 71.64 samples/sec
2025-03-25 09:50:13,370 - training - INFO - Epoch 360 completed in 35.15s. Average loss: 2.4839
2025-03-25 09:50:13,373 - training - INFO - Starting epoch 361/200000
2025-03-25 09:50:14,112 - training - INFO - Memory: GPU 0: 3562.5MB allocated, 9638.0MB reserved
2025-03-25 09:50:14,113 - training - INFO - Epoch: 361/200000, Batch: 0/45, Loss: 2.6304, Throughput: 75.89 samples/sec
2025-03-25 09:50:25,956 - training - INFO - Memory: GPU 0: 3563.9MB allocated, 9498.0MB reserved
2025-03-25 09:50:25,956 - training - INFO - Epoch: 361/200000, Batch: 15/45, Loss: 2.4231, Throughput: 71.21 samples/sec
2025-03-25 09:50:37,630 - training - INFO - Memory: GPU 0: 3563.9MB allocated, 9500.0MB reserved
2025-03-25 09:50:37,631 - training - INFO - Epoch: 361/200000, Batch: 30/45, Loss: 2.4517, Throughput: 71.57 samples/sec
2025-03-25 09:50:48,595 - training - INFO - Epoch 361 completed in 35.22s. Average loss: 2.4781
2025-03-25 09:50:48,599 - training - INFO - Starting epoch 362/200000
2025-03-25 09:50:49,326 - training - INFO - Memory: GPU 0: 3563.9MB allocated, 9638.0MB reserved
2025-03-25 09:50:49,326 - training - INFO - Epoch: 362/200000, Batch: 0/45, Loss: 2.3881, Throughput: 77.08 samples/sec
2025-03-25 09:51:01,221 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9506.0MB reserved
2025-03-25 09:51:01,221 - training - INFO - Epoch: 362/200000, Batch: 15/45, Loss: 2.3875, Throughput: 71.00 samples/sec
2025-03-25 09:51:12,838 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9506.0MB reserved
2025-03-25 09:51:12,839 - training - INFO - Epoch: 362/200000, Batch: 30/45, Loss: 2.4144, Throughput: 71.62 samples/sec
2025-03-25 09:51:23,750 - training - INFO - Epoch 362 completed in 35.15s. Average loss: 2.4709
2025-03-25 09:51:23,754 - training - INFO - Starting epoch 363/200000
2025-03-25 09:51:24,503 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9644.0MB reserved
2025-03-25 09:51:24,504 - training - INFO - Epoch: 363/200000, Batch: 0/45, Loss: 2.6403, Throughput: 74.82 samples/sec
2025-03-25 09:51:36,387 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9492.0MB reserved
2025-03-25 09:51:36,388 - training - INFO - Epoch: 363/200000, Batch: 15/45, Loss: 2.5217, Throughput: 70.94 samples/sec
2025-03-25 09:51:48,029 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9494.0MB reserved
2025-03-25 09:51:48,029 - training - INFO - Epoch: 363/200000, Batch: 30/45, Loss: 2.5306, Throughput: 71.52 samples/sec
2025-03-25 09:51:59,030 - training - INFO - Epoch 363 completed in 35.28s. Average loss: 2.4689
2025-03-25 09:51:59,034 - training - INFO - Starting epoch 364/200000
2025-03-25 09:51:59,781 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9630.0MB reserved
2025-03-25 09:51:59,782 - training - INFO - Epoch: 364/200000, Batch: 0/45, Loss: 2.2386, Throughput: 75.06 samples/sec
2025-03-25 09:52:11,692 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9488.0MB reserved
2025-03-25 09:52:11,692 - training - INFO - Epoch: 364/200000, Batch: 15/45, Loss: 2.5177, Throughput: 70.79 samples/sec
2025-03-25 09:52:23,320 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9488.0MB reserved
2025-03-25 09:52:23,321 - training - INFO - Epoch: 364/200000, Batch: 30/45, Loss: 2.4959, Throughput: 71.49 samples/sec
2025-03-25 09:52:34,261 - training - INFO - Epoch 364 completed in 35.23s. Average loss: 2.4739
2025-03-25 09:52:34,264 - training - INFO - Starting epoch 365/200000
2025-03-25 09:52:35,011 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9628.0MB reserved
2025-03-25 09:52:35,011 - training - INFO - Epoch: 365/200000, Batch: 0/45, Loss: 2.1278, Throughput: 75.13 samples/sec
2025-03-25 09:52:46,942 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9512.0MB reserved
2025-03-25 09:52:46,942 - training - INFO - Epoch: 365/200000, Batch: 15/45, Loss: 2.3021, Throughput: 70.68 samples/sec
2025-03-25 09:52:58,478 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9512.0MB reserved
2025-03-25 09:52:58,479 - training - INFO - Epoch: 365/200000, Batch: 30/45, Loss: 2.3068, Throughput: 71.70 samples/sec
2025-03-25 09:53:09,393 - training - INFO - Epoch 365 completed in 35.13s. Average loss: 2.3879
2025-03-25 09:53:09,396 - training - INFO - Starting epoch 366/200000
2025-03-25 09:53:10,126 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9652.0MB reserved
2025-03-25 09:53:10,126 - training - INFO - Epoch: 366/200000, Batch: 0/45, Loss: 3.0759, Throughput: 76.92 samples/sec
2025-03-25 09:53:22,043 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9398.0MB reserved
2025-03-25 09:53:22,043 - training - INFO - Epoch: 366/200000, Batch: 15/45, Loss: 2.5531, Throughput: 70.86 samples/sec
2025-03-25 09:53:33,623 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9398.0MB reserved
2025-03-25 09:53:33,623 - training - INFO - Epoch: 366/200000, Batch: 30/45, Loss: 2.4893, Throughput: 71.66 samples/sec
2025-03-25 09:53:44,571 - training - INFO - Epoch 366 completed in 35.17s. Average loss: 2.4675
2025-03-25 09:53:44,575 - training - INFO - Starting epoch 367/200000
2025-03-25 09:53:45,310 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9536.0MB reserved
2025-03-25 09:53:45,310 - training - INFO - Epoch: 367/200000, Batch: 0/45, Loss: 2.1863, Throughput: 76.31 samples/sec
2025-03-25 09:53:57,216 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9482.0MB reserved
2025-03-25 09:53:57,217 - training - INFO - Epoch: 367/200000, Batch: 15/45, Loss: 2.4959, Throughput: 70.88 samples/sec
2025-03-25 09:54:08,830 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9482.0MB reserved
2025-03-25 09:54:08,830 - training - INFO - Epoch: 367/200000, Batch: 30/45, Loss: 2.4856, Throughput: 71.58 samples/sec
2025-03-25 09:54:19,792 - training - INFO - Epoch 367 completed in 35.22s. Average loss: 2.4468
2025-03-25 09:54:19,796 - training - INFO - Starting epoch 368/200000
2025-03-25 09:54:20,547 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9620.0MB reserved
2025-03-25 09:54:20,547 - training - INFO - Epoch: 368/200000, Batch: 0/45, Loss: 2.1422, Throughput: 74.69 samples/sec
2025-03-25 09:54:32,572 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9502.0MB reserved
2025-03-25 09:54:32,573 - training - INFO - Epoch: 368/200000, Batch: 15/45, Loss: 2.3474, Throughput: 70.14 samples/sec
2025-03-25 09:54:44,266 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9502.0MB reserved
2025-03-25 09:54:44,267 - training - INFO - Epoch: 368/200000, Batch: 30/45, Loss: 2.3696, Throughput: 70.95 samples/sec
2025-03-25 09:54:55,194 - training - INFO - Epoch 368 completed in 35.40s. Average loss: 2.4595
2025-03-25 09:54:55,197 - training - INFO - Starting epoch 369/200000
2025-03-25 09:54:55,919 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9640.0MB reserved
2025-03-25 09:54:55,920 - training - INFO - Epoch: 369/200000, Batch: 0/45, Loss: 2.3878, Throughput: 77.75 samples/sec
2025-03-25 09:55:07,737 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9516.0MB reserved
2025-03-25 09:55:07,737 - training - INFO - Epoch: 369/200000, Batch: 15/45, Loss: 2.4474, Throughput: 71.46 samples/sec
2025-03-25 09:55:19,287 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9516.0MB reserved
2025-03-25 09:55:19,287 - training - INFO - Epoch: 369/200000, Batch: 30/45, Loss: 2.4142, Throughput: 72.07 samples/sec
2025-03-25 09:55:30,214 - training - INFO - Epoch 369 completed in 35.02s. Average loss: 2.4413
2025-03-25 09:55:30,217 - training - INFO - Starting epoch 370/200000
2025-03-25 09:55:30,954 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9654.0MB reserved
2025-03-25 09:55:30,954 - training - INFO - Epoch: 370/200000, Batch: 0/45, Loss: 2.7335, Throughput: 76.19 samples/sec
2025-03-25 09:55:42,874 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9490.0MB reserved
2025-03-25 09:55:42,874 - training - INFO - Epoch: 370/200000, Batch: 15/45, Loss: 2.3745, Throughput: 70.80 samples/sec
2025-03-25 09:55:54,564 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9490.0MB reserved
2025-03-25 09:55:54,565 - training - INFO - Epoch: 370/200000, Batch: 30/45, Loss: 2.3667, Throughput: 71.31 samples/sec
2025-03-25 09:56:05,526 - training - INFO - Epoch 370 completed in 35.31s. Average loss: 2.4104
2025-03-25 09:56:05,530 - training - INFO - Starting epoch 371/200000
2025-03-25 09:56:06,287 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9628.0MB reserved
2025-03-25 09:56:06,287 - training - INFO - Epoch: 371/200000, Batch: 0/45, Loss: 1.9246, Throughput: 74.14 samples/sec
2025-03-25 09:56:18,120 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9480.0MB reserved
2025-03-25 09:56:18,121 - training - INFO - Epoch: 371/200000, Batch: 15/45, Loss: 2.4806, Throughput: 71.17 samples/sec
2025-03-25 09:56:29,647 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9480.0MB reserved
2025-03-25 09:56:29,648 - training - INFO - Epoch: 371/200000, Batch: 30/45, Loss: 2.4760, Throughput: 71.98 samples/sec
2025-03-25 09:56:40,552 - training - INFO - Epoch 371 completed in 35.02s. Average loss: 2.4442
2025-03-25 09:56:40,556 - training - INFO - Starting epoch 372/200000
2025-03-25 09:56:41,296 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9620.0MB reserved
2025-03-25 09:56:41,296 - training - INFO - Epoch: 372/200000, Batch: 0/45, Loss: 2.2539, Throughput: 75.71 samples/sec
2025-03-25 09:56:53,199 - training - INFO - Memory: GPU 0: 3556.7MB allocated, 9512.0MB reserved
2025-03-25 09:56:53,199 - training - INFO - Epoch: 372/200000, Batch: 15/45, Loss: 2.4752, Throughput: 70.87 samples/sec
2025-03-25 09:57:04,859 - training - INFO - Memory: GPU 0: 3556.7MB allocated, 9512.0MB reserved
2025-03-25 09:57:04,859 - training - INFO - Epoch: 372/200000, Batch: 30/45, Loss: 2.4374, Throughput: 71.43 samples/sec
2025-03-25 09:57:15,859 - training - INFO - Epoch 372 completed in 35.30s. Average loss: 2.4663
2025-03-25 09:57:15,862 - training - INFO - Starting epoch 373/200000
2025-03-25 09:57:16,615 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9650.0MB reserved
2025-03-25 09:57:16,615 - training - INFO - Epoch: 373/200000, Batch: 0/45, Loss: 2.2825, Throughput: 74.54 samples/sec
2025-03-25 09:57:28,591 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9498.0MB reserved
2025-03-25 09:57:28,591 - training - INFO - Epoch: 373/200000, Batch: 15/45, Loss: 2.4528, Throughput: 70.39 samples/sec
2025-03-25 09:57:40,293 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9498.0MB reserved
2025-03-25 09:57:40,293 - training - INFO - Epoch: 373/200000, Batch: 30/45, Loss: 2.4914, Throughput: 71.06 samples/sec
2025-03-25 09:57:51,244 - training - INFO - Epoch 373 completed in 35.38s. Average loss: 2.4575
2025-03-25 09:57:51,247 - training - INFO - Starting epoch 374/200000
2025-03-25 09:57:52,003 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9636.0MB reserved
2025-03-25 09:57:52,003 - training - INFO - Epoch: 374/200000, Batch: 0/45, Loss: 2.8635, Throughput: 74.27 samples/sec
2025-03-25 09:58:03,825 - training - INFO - Memory: GPU 0: 3556.0MB allocated, 9500.0MB reserved
2025-03-25 09:58:03,825 - training - INFO - Epoch: 374/200000, Batch: 15/45, Loss: 2.6356, Throughput: 71.24 samples/sec
2025-03-25 09:58:15,368 - training - INFO - Memory: GPU 0: 3556.0MB allocated, 9500.0MB reserved
2025-03-25 09:58:15,525 - training - INFO - Epoch: 374/200000, Batch: 30/45, Loss: 2.5207, Throughput: 71.98 samples/sec
2025-03-25 09:58:26,427 - training - INFO - Epoch 374 completed in 35.18s. Average loss: 2.4972
2025-03-25 09:58:26,430 - training - INFO - Starting epoch 375/200000
2025-03-25 09:58:27,170 - training - INFO - Memory: GPU 0: 3556.0MB allocated, 9638.0MB reserved
2025-03-25 09:58:27,170 - training - INFO - Epoch: 375/200000, Batch: 0/45, Loss: 2.3395, Throughput: 75.89 samples/sec
2025-03-25 09:58:39,050 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9498.0MB reserved
2025-03-25 09:58:39,050 - training - INFO - Epoch: 375/200000, Batch: 15/45, Loss: 2.4910, Throughput: 71.01 samples/sec
2025-03-25 09:58:50,668 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9498.0MB reserved
2025-03-25 09:58:50,669 - training - INFO - Epoch: 375/200000, Batch: 30/45, Loss: 2.4725, Throughput: 71.63 samples/sec
2025-03-25 09:59:01,689 - training - INFO - Epoch 375 completed in 35.26s. Average loss: 2.4306
2025-03-25 09:59:01,693 - training - INFO - Starting epoch 376/200000
2025-03-25 09:59:02,439 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9636.0MB reserved
2025-03-25 09:59:02,439 - training - INFO - Epoch: 376/200000, Batch: 0/45, Loss: 2.9502, Throughput: 75.23 samples/sec
2025-03-25 09:59:14,355 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9494.0MB reserved
2025-03-25 09:59:14,355 - training - INFO - Epoch: 376/200000, Batch: 15/45, Loss: 2.5138, Throughput: 70.77 samples/sec
2025-03-25 09:59:25,931 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9494.0MB reserved
2025-03-25 09:59:25,931 - training - INFO - Epoch: 376/200000, Batch: 30/45, Loss: 2.3789, Throughput: 71.63 samples/sec
2025-03-25 09:59:36,907 - training - INFO - Epoch 376 completed in 35.21s. Average loss: 2.4702
2025-03-25 09:59:36,910 - training - INFO - Starting epoch 377/200000
2025-03-25 09:59:37,651 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9632.0MB reserved
2025-03-25 09:59:37,652 - training - INFO - Epoch: 377/200000, Batch: 0/45, Loss: 1.9123, Throughput: 75.75 samples/sec
2025-03-25 09:59:49,488 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9502.0MB reserved
2025-03-25 09:59:49,488 - training - INFO - Epoch: 377/200000, Batch: 15/45, Loss: 2.3690, Throughput: 71.25 samples/sec
2025-03-25 10:00:01,038 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9502.0MB reserved
2025-03-25 10:00:01,038 - training - INFO - Epoch: 377/200000, Batch: 30/45, Loss: 2.3991, Throughput: 71.95 samples/sec
2025-03-25 10:00:12,061 - training - INFO - Epoch 377 completed in 35.15s. Average loss: 2.4190
2025-03-25 10:00:12,065 - training - INFO - Starting epoch 378/200000
2025-03-25 10:00:12,798 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9640.0MB reserved
2025-03-25 10:00:12,798 - training - INFO - Epoch: 378/200000, Batch: 0/45, Loss: 2.8140, Throughput: 76.52 samples/sec
2025-03-25 10:00:24,650 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9496.0MB reserved
2025-03-25 10:00:24,650 - training - INFO - Epoch: 378/200000, Batch: 15/45, Loss: 2.4634, Throughput: 71.20 samples/sec
2025-03-25 10:00:36,242 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9498.0MB reserved
2025-03-25 10:00:36,242 - training - INFO - Epoch: 378/200000, Batch: 30/45, Loss: 2.4420, Throughput: 71.81 samples/sec
2025-03-25 10:00:47,193 - training - INFO - Epoch 378 completed in 35.13s. Average loss: 2.4072
2025-03-25 10:00:47,197 - training - INFO - Starting epoch 379/200000
2025-03-25 10:00:47,945 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9636.0MB reserved
2025-03-25 10:00:47,945 - training - INFO - Epoch: 379/200000, Batch: 0/45, Loss: 2.7820, Throughput: 75.08 samples/sec
2025-03-25 10:00:59,874 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9512.0MB reserved
2025-03-25 10:00:59,874 - training - INFO - Epoch: 379/200000, Batch: 15/45, Loss: 2.5190, Throughput: 70.69 samples/sec
2025-03-25 10:01:11,524 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9512.0MB reserved
2025-03-25 10:01:11,524 - training - INFO - Epoch: 379/200000, Batch: 30/45, Loss: 2.4283, Throughput: 71.36 samples/sec
2025-03-25 10:01:22,424 - training - INFO - Epoch 379 completed in 35.23s. Average loss: 2.4005
2025-03-25 10:01:22,428 - training - INFO - Starting epoch 380/200000
2025-03-25 10:01:23,154 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9650.0MB reserved
2025-03-25 10:01:23,154 - training - INFO - Epoch: 380/200000, Batch: 0/45, Loss: 2.3995, Throughput: 77.33 samples/sec
2025-03-25 10:01:35,039 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9498.0MB reserved
2025-03-25 10:01:35,039 - training - INFO - Epoch: 380/200000, Batch: 15/45, Loss: 2.3688, Throughput: 71.06 samples/sec
2025-03-25 10:01:46,681 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9500.0MB reserved
2025-03-25 10:01:46,681 - training - INFO - Epoch: 380/200000, Batch: 30/45, Loss: 2.4129, Throughput: 71.58 samples/sec
2025-03-25 10:01:57,597 - training - INFO - Epoch 380 completed in 35.17s. Average loss: 2.4082
2025-03-25 10:01:57,601 - training - INFO - Starting epoch 381/200000
2025-03-25 10:01:58,341 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9638.0MB reserved
2025-03-25 10:01:58,342 - training - INFO - Epoch: 381/200000, Batch: 0/45, Loss: 2.5150, Throughput: 75.71 samples/sec
2025-03-25 10:02:10,342 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9496.0MB reserved
2025-03-25 10:02:10,342 - training - INFO - Epoch: 381/200000, Batch: 15/45, Loss: 2.4045, Throughput: 70.34 samples/sec
2025-03-25 10:02:22,010 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9496.0MB reserved
2025-03-25 10:02:22,010 - training - INFO - Epoch: 381/200000, Batch: 30/45, Loss: 2.3770, Throughput: 71.13 samples/sec
2025-03-25 10:02:32,996 - training - INFO - Epoch 381 completed in 35.39s. Average loss: 2.4009
2025-03-25 10:02:33,000 - training - INFO - Starting epoch 382/200000
2025-03-25 10:02:33,725 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9634.0MB reserved
2025-03-25 10:02:33,726 - training - INFO - Epoch: 382/200000, Batch: 0/45, Loss: 2.4602, Throughput: 77.24 samples/sec
2025-03-25 10:02:45,710 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9500.0MB reserved
2025-03-25 10:02:45,710 - training - INFO - Epoch: 382/200000, Batch: 15/45, Loss: 2.3215, Throughput: 70.50 samples/sec
2025-03-25 10:02:57,318 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9500.0MB reserved
2025-03-25 10:02:57,318 - training - INFO - Epoch: 382/200000, Batch: 30/45, Loss: 2.3197, Throughput: 71.39 samples/sec
2025-03-25 10:03:08,236 - training - INFO - Epoch 382 completed in 35.24s. Average loss: 2.4142
2025-03-25 10:03:08,240 - training - INFO - Starting epoch 383/200000
2025-03-25 10:03:08,991 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9638.0MB reserved
2025-03-25 10:03:08,991 - training - INFO - Epoch: 383/200000, Batch: 0/45, Loss: 2.0654, Throughput: 74.76 samples/sec
2025-03-25 10:03:20,871 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9496.0MB reserved
2025-03-25 10:03:20,871 - training - INFO - Epoch: 383/200000, Batch: 15/45, Loss: 2.3461, Throughput: 70.94 samples/sec
2025-03-25 10:03:32,437 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9496.0MB reserved
2025-03-25 10:03:32,437 - training - INFO - Epoch: 383/200000, Batch: 30/45, Loss: 2.4011, Throughput: 71.75 samples/sec
2025-03-25 10:03:43,338 - training - INFO - Epoch 383 completed in 35.10s. Average loss: 2.4031
2025-03-25 10:03:43,342 - training - INFO - Starting epoch 384/200000
2025-03-25 10:03:44,102 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9634.0MB reserved
2025-03-25 10:03:44,102 - training - INFO - Epoch: 384/200000, Batch: 0/45, Loss: 2.5040, Throughput: 73.69 samples/sec
2025-03-25 10:03:55,946 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9504.0MB reserved
2025-03-25 10:03:55,946 - training - INFO - Epoch: 384/200000, Batch: 15/45, Loss: 2.4645, Throughput: 71.09 samples/sec
2025-03-25 10:04:07,631 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9504.0MB reserved
2025-03-25 10:04:07,632 - training - INFO - Epoch: 384/200000, Batch: 30/45, Loss: 2.4197, Throughput: 71.48 samples/sec
2025-03-25 10:04:18,604 - training - INFO - Epoch 384 completed in 35.26s. Average loss: 2.4120
2025-03-25 10:04:18,608 - training - INFO - Starting epoch 385/200000
2025-03-25 10:04:19,354 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9642.0MB reserved
2025-03-25 10:04:19,354 - training - INFO - Epoch: 385/200000, Batch: 0/45, Loss: 2.5247, Throughput: 75.17 samples/sec
2025-03-25 10:04:31,173 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9504.0MB reserved
2025-03-25 10:04:31,173 - training - INFO - Epoch: 385/200000, Batch: 15/45, Loss: 2.3976, Throughput: 71.31 samples/sec
2025-03-25 10:04:42,728 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9504.0MB reserved
2025-03-25 10:04:42,728 - training - INFO - Epoch: 385/200000, Batch: 30/45, Loss: 2.3720, Throughput: 71.98 samples/sec
2025-03-25 10:04:53,606 - training - INFO - Epoch 385 completed in 35.00s. Average loss: 2.3634
2025-03-25 10:04:53,611 - training - INFO - Starting epoch 386/200000
2025-03-25 10:04:54,357 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9642.0MB reserved
2025-03-25 10:04:54,358 - training - INFO - Epoch: 386/200000, Batch: 0/45, Loss: 2.1706, Throughput: 75.17 samples/sec
2025-03-25 10:05:06,229 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9508.0MB reserved
2025-03-25 10:05:06,229 - training - INFO - Epoch: 386/200000, Batch: 15/45, Loss: 2.4318, Throughput: 71.02 samples/sec
2025-03-25 10:05:17,827 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9508.0MB reserved
2025-03-25 10:05:17,827 - training - INFO - Epoch: 386/200000, Batch: 30/45, Loss: 2.3887, Throughput: 71.69 samples/sec
2025-03-25 10:05:28,800 - training - INFO - Epoch 386 completed in 35.19s. Average loss: 2.3656
2025-03-25 10:05:28,804 - training - INFO - Starting epoch 387/200000
2025-03-25 10:05:29,552 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9646.0MB reserved
2025-03-25 10:05:29,552 - training - INFO - Epoch: 387/200000, Batch: 0/45, Loss: 2.3112, Throughput: 74.94 samples/sec
2025-03-25 10:05:41,501 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9496.0MB reserved
2025-03-25 10:05:41,502 - training - INFO - Epoch: 387/200000, Batch: 15/45, Loss: 2.3350, Throughput: 70.57 samples/sec
2025-03-25 10:05:53,206 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9496.0MB reserved
2025-03-25 10:05:53,207 - training - INFO - Epoch: 387/200000, Batch: 30/45, Loss: 2.3519, Throughput: 71.15 samples/sec
2025-03-25 10:06:04,160 - training - INFO - Epoch 387 completed in 35.36s. Average loss: 2.3844
2025-03-25 10:06:04,165 - training - INFO - Starting epoch 388/200000
2025-03-25 10:06:04,916 - training - INFO - Memory: GPU 0: 3555.9MB allocated, 9634.0MB reserved
2025-03-25 10:06:04,916 - training - INFO - Epoch: 388/200000, Batch: 0/45, Loss: 2.5344, Throughput: 74.80 samples/sec
2025-03-25 10:06:16,859 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9498.0MB reserved
2025-03-25 10:06:16,860 - training - INFO - Epoch: 388/200000, Batch: 15/45, Loss: 2.5177, Throughput: 70.60 samples/sec
2025-03-25 10:06:28,427 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9498.0MB reserved
2025-03-25 10:06:28,427 - training - INFO - Epoch: 388/200000, Batch: 30/45, Loss: 2.4655, Throughput: 71.56 samples/sec
2025-03-25 10:06:39,346 - training - INFO - Epoch 388 completed in 35.18s. Average loss: 2.3960
2025-03-25 10:06:39,350 - training - INFO - Starting epoch 389/200000
2025-03-25 10:06:40,100 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9634.0MB reserved
2025-03-25 10:06:40,101 - training - INFO - Epoch: 389/200000, Batch: 0/45, Loss: 2.2417, Throughput: 74.80 samples/sec
2025-03-25 10:06:52,003 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9506.0MB reserved
2025-03-25 10:06:52,004 - training - INFO - Epoch: 389/200000, Batch: 15/45, Loss: 2.3587, Throughput: 70.82 samples/sec
2025-03-25 10:07:03,533 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9506.0MB reserved
2025-03-25 10:07:03,534 - training - INFO - Epoch: 389/200000, Batch: 30/45, Loss: 2.3268, Throughput: 71.79 samples/sec
2025-03-25 10:07:14,469 - training - INFO - Epoch 389 completed in 35.12s. Average loss: 2.3777
2025-03-25 10:07:14,473 - training - INFO - Starting epoch 390/200000
2025-03-25 10:07:15,213 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9644.0MB reserved
2025-03-25 10:07:15,214 - training - INFO - Epoch: 390/200000, Batch: 0/45, Loss: 2.3257, Throughput: 75.90 samples/sec
2025-03-25 10:07:27,155 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9512.0MB reserved
2025-03-25 10:07:27,155 - training - INFO - Epoch: 390/200000, Batch: 15/45, Loss: 2.3325, Throughput: 70.67 samples/sec
2025-03-25 10:07:38,807 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9512.0MB reserved
2025-03-25 10:07:38,807 - training - INFO - Epoch: 390/200000, Batch: 30/45, Loss: 2.3580, Throughput: 71.35 samples/sec
2025-03-25 10:07:49,826 - training - INFO - Epoch 390 completed in 35.35s. Average loss: 2.4087
2025-03-25 10:07:49,830 - training - INFO - Starting epoch 391/200000
2025-03-25 10:07:50,575 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9650.0MB reserved
2025-03-25 10:07:50,575 - training - INFO - Epoch: 391/200000, Batch: 0/45, Loss: 2.3555, Throughput: 75.35 samples/sec
2025-03-25 10:08:02,596 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9494.0MB reserved
2025-03-25 10:08:02,597 - training - INFO - Epoch: 391/200000, Batch: 15/45, Loss: 2.4148, Throughput: 70.19 samples/sec
2025-03-25 10:08:14,306 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9494.0MB reserved
2025-03-25 10:08:14,306 - training - INFO - Epoch: 391/200000, Batch: 30/45, Loss: 2.4495, Throughput: 70.93 samples/sec
2025-03-25 10:08:25,360 - training - INFO - Epoch 391 completed in 35.53s. Average loss: 2.4174
2025-03-25 10:08:25,364 - training - INFO - Starting epoch 392/200000
2025-03-25 10:08:26,107 - training - INFO - Memory: GPU 0: 3563.3MB allocated, 9630.0MB reserved
2025-03-25 10:08:26,108 - training - INFO - Epoch: 392/200000, Batch: 0/45, Loss: 2.3643, Throughput: 75.51 samples/sec
2025-03-25 10:08:38,032 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9496.0MB reserved
2025-03-25 10:08:38,033 - training - INFO - Epoch: 392/200000, Batch: 15/45, Loss: 2.3059, Throughput: 70.74 samples/sec
2025-03-25 10:08:49,642 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9496.0MB reserved
2025-03-25 10:08:49,642 - training - INFO - Epoch: 392/200000, Batch: 30/45, Loss: 2.3514, Throughput: 71.51 samples/sec
2025-03-25 10:09:00,594 - training - INFO - Epoch 392 completed in 35.23s. Average loss: 2.3953
2025-03-25 10:09:00,635 - training - INFO - Starting epoch 393/200000
2025-03-25 10:09:01,337 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9634.0MB reserved
2025-03-25 10:09:01,338 - training - INFO - Epoch: 393/200000, Batch: 0/45, Loss: 2.8231, Throughput: 79.91 samples/sec
2025-03-25 10:09:13,281 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9490.0MB reserved
2025-03-25 10:09:13,281 - training - INFO - Epoch: 393/200000, Batch: 15/45, Loss: 2.4657, Throughput: 70.87 samples/sec
2025-03-25 10:09:24,866 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9490.0MB reserved
2025-03-25 10:09:24,866 - training - INFO - Epoch: 393/200000, Batch: 30/45, Loss: 2.3781, Throughput: 71.65 samples/sec
2025-03-25 10:09:35,831 - training - INFO - Epoch 393 completed in 35.20s. Average loss: 2.4167
2025-03-25 10:09:35,835 - training - INFO - Starting epoch 394/200000
2025-03-25 10:09:36,586 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9628.0MB reserved
2025-03-25 10:09:36,586 - training - INFO - Epoch: 394/200000, Batch: 0/45, Loss: 2.0091, Throughput: 74.74 samples/sec
2025-03-25 10:09:48,426 - training - INFO - Memory: GPU 0: 3562.5MB allocated, 9494.0MB reserved
2025-03-25 10:09:48,427 - training - INFO - Epoch: 394/200000, Batch: 15/45, Loss: 2.3739, Throughput: 71.17 samples/sec
2025-03-25 10:09:59,987 - training - INFO - Memory: GPU 0: 3562.5MB allocated, 9494.0MB reserved
2025-03-25 10:09:59,987 - training - INFO - Epoch: 394/200000, Batch: 30/45, Loss: 2.3311, Throughput: 71.88 samples/sec
2025-03-25 10:10:10,911 - training - INFO - Epoch 394 completed in 35.08s. Average loss: 2.3511
2025-03-25 10:10:10,915 - training - INFO - Starting epoch 395/200000
2025-03-25 10:10:11,654 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9632.0MB reserved
2025-03-25 10:10:11,654 - training - INFO - Epoch: 395/200000, Batch: 0/45, Loss: 2.2150, Throughput: 76.03 samples/sec
2025-03-25 10:10:23,578 - training - INFO - Memory: GPU 0: 3556.7MB allocated, 9506.0MB reserved
2025-03-25 10:10:23,579 - training - INFO - Epoch: 395/200000, Batch: 15/45, Loss: 2.3651, Throughput: 70.77 samples/sec
2025-03-25 10:10:35,247 - training - INFO - Memory: GPU 0: 3556.7MB allocated, 9506.0MB reserved
2025-03-25 10:10:35,247 - training - INFO - Epoch: 395/200000, Batch: 30/45, Loss: 2.3701, Throughput: 71.35 samples/sec
2025-03-25 10:10:46,219 - training - INFO - Epoch 395 completed in 35.30s. Average loss: 2.3835
2025-03-25 10:10:46,223 - training - INFO - Starting epoch 396/200000
2025-03-25 10:10:46,963 - training - INFO - Memory: GPU 0: 3556.7MB allocated, 9644.0MB reserved
2025-03-25 10:10:46,963 - training - INFO - Epoch: 396/200000, Batch: 0/45, Loss: 2.5907, Throughput: 75.84 samples/sec
2025-03-25 10:10:58,874 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9498.0MB reserved
2025-03-25 10:10:58,874 - training - INFO - Epoch: 396/200000, Batch: 15/45, Loss: 2.3932, Throughput: 70.83 samples/sec
2025-03-25 10:11:10,453 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9498.0MB reserved
2025-03-25 10:11:10,453 - training - INFO - Epoch: 396/200000, Batch: 30/45, Loss: 2.3600, Throughput: 71.65 samples/sec
2025-03-25 10:11:21,419 - training - INFO - Epoch 396 completed in 35.20s. Average loss: 2.4257
2025-03-25 10:11:21,423 - training - INFO - Starting epoch 397/200000
2025-03-25 10:11:22,158 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9636.0MB reserved
2025-03-25 10:11:22,158 - training - INFO - Epoch: 397/200000, Batch: 0/45, Loss: 2.9220, Throughput: 76.20 samples/sec
2025-03-25 10:11:34,213 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9500.0MB reserved
2025-03-25 10:11:34,235 - training - INFO - Epoch: 397/200000, Batch: 15/45, Loss: 2.5927, Throughput: 70.06 samples/sec
2025-03-25 10:11:45,783 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9500.0MB reserved
2025-03-25 10:11:45,783 - training - INFO - Epoch: 397/200000, Batch: 30/45, Loss: 2.5630, Throughput: 71.27 samples/sec
2025-03-25 10:11:56,686 - training - INFO - Epoch 397 completed in 35.26s. Average loss: 2.5095
2025-03-25 10:11:56,690 - training - INFO - Starting epoch 398/200000
2025-03-25 10:11:57,432 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9636.0MB reserved
2025-03-25 10:11:57,432 - training - INFO - Epoch: 398/200000, Batch: 0/45, Loss: 2.3505, Throughput: 75.61 samples/sec
2025-03-25 10:12:09,374 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9512.0MB reserved
2025-03-25 10:12:09,375 - training - INFO - Epoch: 398/200000, Batch: 15/45, Loss: 2.4318, Throughput: 70.65 samples/sec
2025-03-25 10:12:21,057 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9512.0MB reserved
2025-03-25 10:12:21,057 - training - INFO - Epoch: 398/200000, Batch: 30/45, Loss: 2.3764, Throughput: 71.25 samples/sec
2025-03-25 10:12:32,104 - training - INFO - Epoch 398 completed in 35.41s. Average loss: 2.3649
2025-03-25 10:12:32,108 - training - INFO - Starting epoch 399/200000
2025-03-25 10:12:32,892 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9650.0MB reserved
2025-03-25 10:12:32,892 - training - INFO - Epoch: 399/200000, Batch: 0/45, Loss: 2.3015, Throughput: 71.52 samples/sec
2025-03-25 10:12:44,873 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9494.0MB reserved
2025-03-25 10:12:44,874 - training - INFO - Epoch: 399/200000, Batch: 15/45, Loss: 2.3512, Throughput: 70.21 samples/sec
2025-03-25 10:12:56,515 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9496.0MB reserved
2025-03-25 10:12:56,515 - training - INFO - Epoch: 399/200000, Batch: 30/45, Loss: 2.4126, Throughput: 71.13 samples/sec
2025-03-25 10:13:07,453 - training - INFO - Epoch 399 completed in 35.34s. Average loss: 2.3766
2025-03-25 10:13:07,457 - training - INFO - Starting epoch 400/200000
2025-03-25 10:13:08,208 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9634.0MB reserved
2025-03-25 10:13:08,208 - training - INFO - Epoch: 400/200000, Batch: 0/45, Loss: 2.7749, Throughput: 74.79 samples/sec
2025-03-25 10:13:20,137 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9510.0MB reserved
2025-03-25 10:13:20,137 - training - INFO - Epoch: 400/200000, Batch: 15/45, Loss: 2.3399, Throughput: 70.67 samples/sec
2025-03-25 10:13:31,847 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9510.0MB reserved
2025-03-25 10:13:31,847 - training - INFO - Epoch: 400/200000, Batch: 30/45, Loss: 2.3975, Throughput: 71.18 samples/sec
2025-03-25 10:13:42,797 - training - INFO - Epoch 400 completed in 35.34s. Average loss: 2.3979
2025-03-25 10:13:42,800 - training - INFO - Starting validation...
2025-03-25 10:13:43,122 - training - INFO - Validation Loss: 18.6164
2025-03-25 10:13:43,122 - training - INFO - Validation loss did not improve. Counter: 3/10
2025-03-25 10:18:27,707 - training - INFO - Checkpoint saved at epoch 400
2025-03-25 10:18:27,962 - training - INFO - Starting epoch 401/200000
2025-03-25 10:18:28,683 - training - INFO - Memory: GPU 0: 3566.8MB allocated, 8742.0MB reserved
2025-03-25 10:18:28,683 - training - INFO - Epoch: 401/200000, Batch: 0/45, Loss: 2.9440, Throughput: 77.80 samples/sec
2025-03-25 10:18:40,526 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9468.0MB reserved
2025-03-25 10:18:40,527 - training - INFO - Epoch: 401/200000, Batch: 15/45, Loss: 2.3532, Throughput: 71.33 samples/sec
2025-03-25 10:18:52,224 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9468.0MB reserved
2025-03-25 10:18:52,225 - training - INFO - Epoch: 401/200000, Batch: 30/45, Loss: 2.3338, Throughput: 71.56 samples/sec
2025-03-25 10:19:03,231 - training - INFO - Epoch 401 completed in 35.27s. Average loss: 2.3757
2025-03-25 10:19:03,235 - training - INFO - Starting epoch 402/200000
2025-03-25 10:19:03,974 - training - INFO - Memory: GPU 0: 3563.4MB allocated, 9604.0MB reserved
2025-03-25 10:19:03,975 - training - INFO - Epoch: 402/200000, Batch: 0/45, Loss: 1.9048, Throughput: 75.93 samples/sec
2025-03-25 10:19:15,900 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9492.0MB reserved
2025-03-25 10:19:15,901 - training - INFO - Epoch: 402/200000, Batch: 15/45, Loss: 2.3889, Throughput: 70.75 samples/sec
2025-03-25 10:19:27,625 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9492.0MB reserved
2025-03-25 10:19:27,626 - training - INFO - Epoch: 402/200000, Batch: 30/45, Loss: 2.3784, Throughput: 71.18 samples/sec
2025-03-25 10:19:38,675 - training - INFO - Epoch 402 completed in 35.44s. Average loss: 2.3875
2025-03-25 10:19:38,679 - training - INFO - Starting epoch 403/200000
2025-03-25 10:19:39,427 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9630.0MB reserved
2025-03-25 10:19:39,427 - training - INFO - Epoch: 403/200000, Batch: 0/45, Loss: 2.4206, Throughput: 75.11 samples/sec
2025-03-25 10:19:51,357 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9490.0MB reserved
2025-03-25 10:19:51,358 - training - INFO - Epoch: 403/200000, Batch: 15/45, Loss: 2.4671, Throughput: 70.68 samples/sec
2025-03-25 10:20:02,989 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9490.0MB reserved
2025-03-25 10:20:02,989 - training - INFO - Epoch: 403/200000, Batch: 30/45, Loss: 2.4428, Throughput: 71.42 samples/sec
2025-03-25 10:20:13,931 - training - INFO - Epoch 403 completed in 35.25s. Average loss: 2.4031
2025-03-25 10:20:13,935 - training - INFO - Starting epoch 404/200000
2025-03-25 10:20:14,674 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9626.0MB reserved
2025-03-25 10:20:14,674 - training - INFO - Epoch: 404/200000, Batch: 0/45, Loss: 2.4122, Throughput: 76.02 samples/sec
2025-03-25 10:20:26,518 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9488.0MB reserved
2025-03-25 10:20:26,519 - training - INFO - Epoch: 404/200000, Batch: 15/45, Loss: 2.4251, Throughput: 71.22 samples/sec
2025-03-25 10:20:38,085 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9490.0MB reserved
2025-03-25 10:20:38,085 - training - INFO - Epoch: 404/200000, Batch: 30/45, Loss: 2.4459, Throughput: 71.89 samples/sec
2025-03-25 10:20:49,032 - training - INFO - Epoch 404 completed in 35.10s. Average loss: 2.4195
2025-03-25 10:20:49,036 - training - INFO - Starting epoch 405/200000
2025-03-25 10:20:49,779 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9628.0MB reserved
2025-03-25 10:20:49,779 - training - INFO - Epoch: 405/200000, Batch: 0/45, Loss: 2.0882, Throughput: 75.59 samples/sec
2025-03-25 10:21:01,643 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9496.0MB reserved
2025-03-25 10:21:01,643 - training - INFO - Epoch: 405/200000, Batch: 15/45, Loss: 2.3507, Throughput: 71.08 samples/sec
2025-03-25 10:21:13,322 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9496.0MB reserved
2025-03-25 10:21:13,322 - training - INFO - Epoch: 405/200000, Batch: 30/45, Loss: 2.3855, Throughput: 71.49 samples/sec
2025-03-25 10:21:24,251 - training - INFO - Epoch 405 completed in 35.21s. Average loss: 2.3527
2025-03-25 10:21:24,254 - training - INFO - Starting epoch 406/200000
2025-03-25 10:21:25,006 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9636.0MB reserved
2025-03-25 10:21:25,006 - training - INFO - Epoch: 406/200000, Batch: 0/45, Loss: 2.3749, Throughput: 74.62 samples/sec
2025-03-25 10:21:36,859 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9400.0MB reserved
2025-03-25 10:21:36,859 - training - INFO - Epoch: 406/200000, Batch: 15/45, Loss: 2.3478, Throughput: 71.10 samples/sec
2025-03-25 10:21:48,443 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9400.0MB reserved
2025-03-25 10:21:48,443 - training - INFO - Epoch: 406/200000, Batch: 30/45, Loss: 2.3820, Throughput: 71.78 samples/sec
2025-03-25 10:21:59,367 - training - INFO - Epoch 406 completed in 35.11s. Average loss: 2.3660
2025-03-25 10:21:59,371 - training - INFO - Starting epoch 407/200000
2025-03-25 10:22:00,124 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9538.0MB reserved
2025-03-25 10:22:00,125 - training - INFO - Epoch: 407/200000, Batch: 0/45, Loss: 1.9717, Throughput: 74.57 samples/sec
2025-03-25 10:22:12,143 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9500.0MB reserved
2025-03-25 10:22:12,143 - training - INFO - Epoch: 407/200000, Batch: 15/45, Loss: 2.2723, Throughput: 70.17 samples/sec
2025-03-25 10:22:23,788 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9500.0MB reserved
2025-03-25 10:22:23,789 - training - INFO - Epoch: 407/200000, Batch: 30/45, Loss: 2.3019, Throughput: 71.10 samples/sec
2025-03-25 10:22:34,760 - training - INFO - Epoch 407 completed in 35.39s. Average loss: 2.3374
2025-03-25 10:22:34,764 - training - INFO - Starting epoch 408/200000
2025-03-25 10:22:35,510 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9638.0MB reserved
2025-03-25 10:22:35,511 - training - INFO - Epoch: 408/200000, Batch: 0/45, Loss: 2.6423, Throughput: 75.16 samples/sec
2025-03-25 10:22:47,464 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9506.0MB reserved
2025-03-25 10:22:47,464 - training - INFO - Epoch: 408/200000, Batch: 15/45, Loss: 2.1955, Throughput: 70.56 samples/sec
2025-03-25 10:22:59,024 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9506.0MB reserved
2025-03-25 10:22:59,024 - training - INFO - Epoch: 408/200000, Batch: 30/45, Loss: 2.2448, Throughput: 71.56 samples/sec
2025-03-25 10:23:09,958 - training - INFO - Epoch 408 completed in 35.19s. Average loss: 2.3388
2025-03-25 10:23:09,962 - training - INFO - Starting epoch 409/200000
2025-03-25 10:23:10,723 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9644.0MB reserved
2025-03-25 10:23:10,723 - training - INFO - Epoch: 409/200000, Batch: 0/45, Loss: 2.1411, Throughput: 73.79 samples/sec
2025-03-25 10:23:22,644 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9500.0MB reserved
2025-03-25 10:23:22,644 - training - INFO - Epoch: 409/200000, Batch: 15/45, Loss: 2.2794, Throughput: 70.66 samples/sec
2025-03-25 10:23:34,197 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9500.0MB reserved
2025-03-25 10:23:34,197 - training - INFO - Epoch: 409/200000, Batch: 30/45, Loss: 2.3078, Throughput: 71.64 samples/sec
2025-03-25 10:23:45,138 - training - INFO - Epoch 409 completed in 35.18s. Average loss: 2.3331
2025-03-25 10:23:45,141 - training - INFO - Starting epoch 410/200000
2025-03-25 10:23:45,885 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9638.0MB reserved
2025-03-25 10:23:45,885 - training - INFO - Epoch: 410/200000, Batch: 0/45, Loss: 2.9112, Throughput: 75.39 samples/sec
2025-03-25 10:23:57,753 - training - INFO - Memory: GPU 0: 3562.6MB allocated, 9512.0MB reserved
2025-03-25 10:23:57,753 - training - INFO - Epoch: 410/200000, Batch: 15/45, Loss: 2.4963, Throughput: 71.06 samples/sec
2025-03-25 10:24:09,349 - training - INFO - Memory: GPU 0: 3562.6MB allocated, 9512.0MB reserved
2025-03-25 10:24:09,349 - training - INFO - Epoch: 410/200000, Batch: 30/45, Loss: 2.4648, Throughput: 71.72 samples/sec
2025-03-25 10:24:20,264 - training - INFO - Epoch 410 completed in 35.12s. Average loss: 2.3847
2025-03-25 10:24:20,267 - training - INFO - Starting epoch 411/200000
2025-03-25 10:24:21,014 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9650.0MB reserved
2025-03-25 10:24:21,014 - training - INFO - Epoch: 411/200000, Batch: 0/45, Loss: 2.5160, Throughput: 75.10 samples/sec
2025-03-25 10:24:32,917 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9496.0MB reserved
2025-03-25 10:24:32,918 - training - INFO - Epoch: 411/200000, Batch: 15/45, Loss: 2.3007, Throughput: 70.83 samples/sec
2025-03-25 10:24:44,471 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9496.0MB reserved
2025-03-25 10:24:44,471 - training - INFO - Epoch: 411/200000, Batch: 30/45, Loss: 2.3498, Throughput: 71.73 samples/sec
2025-03-25 10:24:55,410 - training - INFO - Epoch 411 completed in 35.14s. Average loss: 2.3476
2025-03-25 10:24:55,414 - training - INFO - Starting epoch 412/200000
2025-03-25 10:24:56,163 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9634.0MB reserved
2025-03-25 10:24:56,163 - training - INFO - Epoch: 412/200000, Batch: 0/45, Loss: 2.2486, Throughput: 74.80 samples/sec
2025-03-25 10:25:07,994 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9490.0MB reserved
2025-03-25 10:25:07,995 - training - INFO - Epoch: 412/200000, Batch: 15/45, Loss: 2.3232, Throughput: 71.23 samples/sec
2025-03-25 10:25:19,581 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9490.0MB reserved
2025-03-25 10:25:19,582 - training - INFO - Epoch: 412/200000, Batch: 30/45, Loss: 2.3537, Throughput: 71.83 samples/sec
2025-03-25 10:25:30,508 - training - INFO - Epoch 412 completed in 35.09s. Average loss: 2.3965
2025-03-25 10:25:30,512 - training - INFO - Starting epoch 413/200000
2025-03-25 10:25:31,255 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9628.0MB reserved
2025-03-25 10:25:31,255 - training - INFO - Epoch: 413/200000, Batch: 0/45, Loss: 2.3871, Throughput: 75.49 samples/sec
2025-03-25 10:25:43,133 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9498.0MB reserved
2025-03-25 10:25:43,133 - training - INFO - Epoch: 413/200000, Batch: 15/45, Loss: 2.3305, Throughput: 71.00 samples/sec
2025-03-25 10:25:54,712 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9498.0MB reserved
2025-03-25 10:25:54,713 - training - INFO - Epoch: 413/200000, Batch: 30/45, Loss: 2.3060, Throughput: 71.74 samples/sec
2025-03-25 10:26:05,669 - training - INFO - Epoch 413 completed in 35.16s. Average loss: 2.3719
2025-03-25 10:26:05,672 - training - INFO - Starting epoch 414/200000
2025-03-25 10:26:06,426 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9636.0MB reserved
2025-03-25 10:26:06,426 - training - INFO - Epoch: 414/200000, Batch: 0/45, Loss: 2.4611, Throughput: 74.51 samples/sec
2025-03-25 10:26:18,340 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9510.0MB reserved
2025-03-25 10:26:18,340 - training - INFO - Epoch: 414/200000, Batch: 15/45, Loss: 2.3785, Throughput: 70.74 samples/sec
2025-03-25 10:26:29,966 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9510.0MB reserved
2025-03-25 10:26:29,966 - training - INFO - Epoch: 414/200000, Batch: 30/45, Loss: 2.3892, Throughput: 71.46 samples/sec
2025-03-25 10:26:40,886 - training - INFO - Epoch 414 completed in 35.21s. Average loss: 2.3745
2025-03-25 10:26:40,890 - training - INFO - Starting epoch 415/200000
2025-03-25 10:26:41,624 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9648.0MB reserved
2025-03-25 10:26:41,625 - training - INFO - Epoch: 415/200000, Batch: 0/45, Loss: 2.3827, Throughput: 76.39 samples/sec
2025-03-25 10:26:53,461 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9512.0MB reserved
2025-03-25 10:26:53,461 - training - INFO - Epoch: 415/200000, Batch: 15/45, Loss: 2.3833, Throughput: 71.28 samples/sec
2025-03-25 10:27:05,005 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9512.0MB reserved
2025-03-25 10:27:05,005 - training - INFO - Epoch: 415/200000, Batch: 30/45, Loss: 2.3902, Throughput: 71.99 samples/sec
2025-03-25 10:27:15,929 - training - INFO - Epoch 415 completed in 35.04s. Average loss: 2.4148
2025-03-25 10:27:15,932 - training - INFO - Starting epoch 416/200000
2025-03-25 10:27:16,662 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9650.0MB reserved
2025-03-25 10:27:16,662 - training - INFO - Epoch: 416/200000, Batch: 0/45, Loss: 1.6849, Throughput: 76.94 samples/sec
2025-03-25 10:27:28,566 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9510.0MB reserved
2025-03-25 10:27:28,567 - training - INFO - Epoch: 416/200000, Batch: 15/45, Loss: 2.3632, Throughput: 70.93 samples/sec
2025-03-25 10:27:40,156 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9510.0MB reserved
2025-03-25 10:27:40,157 - training - INFO - Epoch: 416/200000, Batch: 30/45, Loss: 2.3591, Throughput: 71.67 samples/sec
2025-03-25 10:27:51,062 - training - INFO - Epoch 416 completed in 35.13s. Average loss: 2.3426
2025-03-25 10:27:51,066 - training - INFO - Starting epoch 417/200000
2025-03-25 10:27:51,820 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9648.0MB reserved
2025-03-25 10:27:51,820 - training - INFO - Epoch: 417/200000, Batch: 0/45, Loss: 1.8386, Throughput: 74.33 samples/sec
2025-03-25 10:28:03,846 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9402.0MB reserved
2025-03-25 10:28:03,846 - training - INFO - Epoch: 417/200000, Batch: 15/45, Loss: 2.2399, Throughput: 70.12 samples/sec
2025-03-25 10:28:15,524 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9508.0MB reserved
2025-03-25 10:28:15,524 - training - INFO - Epoch: 417/200000, Batch: 30/45, Loss: 2.2747, Throughput: 70.98 samples/sec
2025-03-25 10:28:26,539 - training - INFO - Epoch 417 completed in 35.47s. Average loss: 2.3016
2025-03-25 10:28:26,543 - training - INFO - Starting epoch 418/200000
2025-03-25 10:28:27,303 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9646.0MB reserved
2025-03-25 10:28:27,303 - training - INFO - Epoch: 418/200000, Batch: 0/45, Loss: 2.4637, Throughput: 73.77 samples/sec
2025-03-25 10:28:39,184 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9496.0MB reserved
2025-03-25 10:28:39,184 - training - INFO - Epoch: 418/200000, Batch: 15/45, Loss: 2.3499, Throughput: 70.89 samples/sec
2025-03-25 10:28:50,801 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9498.0MB reserved
2025-03-25 10:28:50,801 - training - INFO - Epoch: 418/200000, Batch: 30/45, Loss: 2.3087, Throughput: 71.57 samples/sec
2025-03-25 10:29:01,763 - training - INFO - Epoch 418 completed in 35.22s. Average loss: 2.3872
2025-03-25 10:29:01,767 - training - INFO - Starting epoch 419/200000
2025-03-25 10:29:02,506 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9636.0MB reserved
2025-03-25 10:29:02,507 - training - INFO - Epoch: 419/200000, Batch: 0/45, Loss: 2.1663, Throughput: 75.90 samples/sec
2025-03-25 10:29:14,431 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9506.0MB reserved
2025-03-25 10:29:14,431 - training - INFO - Epoch: 419/200000, Batch: 15/45, Loss: 2.3566, Throughput: 70.76 samples/sec
2025-03-25 10:29:25,992 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9506.0MB reserved
2025-03-25 10:29:25,992 - training - INFO - Epoch: 419/200000, Batch: 30/45, Loss: 2.3306, Throughput: 71.66 samples/sec
2025-03-25 10:29:36,875 - training - INFO - Epoch 419 completed in 35.11s. Average loss: 2.3119
2025-03-25 10:29:36,879 - training - INFO - Starting epoch 420/200000
2025-03-25 10:29:37,603 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9644.0MB reserved
2025-03-25 10:29:37,604 - training - INFO - Epoch: 420/200000, Batch: 0/45, Loss: 2.6676, Throughput: 77.44 samples/sec
2025-03-25 10:29:49,594 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9492.0MB reserved
2025-03-25 10:29:49,594 - training - INFO - Epoch: 420/200000, Batch: 15/45, Loss: 2.2159, Throughput: 70.48 samples/sec
2025-03-25 10:30:01,230 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9492.0MB reserved
2025-03-25 10:30:01,230 - training - INFO - Epoch: 420/200000, Batch: 30/45, Loss: 2.2883, Throughput: 71.30 samples/sec
2025-03-25 10:30:12,214 - training - INFO - Epoch 420 completed in 35.33s. Average loss: 2.3267
2025-03-25 10:30:12,218 - training - INFO - Starting epoch 421/200000
2025-03-25 10:30:12,972 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9630.0MB reserved
2025-03-25 10:30:12,972 - training - INFO - Epoch: 421/200000, Batch: 0/45, Loss: 2.2802, Throughput: 74.39 samples/sec
2025-03-25 10:30:24,885 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9496.0MB reserved
2025-03-25 10:30:24,886 - training - INFO - Epoch: 421/200000, Batch: 15/45, Loss: 2.2938, Throughput: 70.74 samples/sec
2025-03-25 10:30:36,430 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9496.0MB reserved
2025-03-25 10:30:36,431 - training - INFO - Epoch: 421/200000, Batch: 30/45, Loss: 2.2521, Throughput: 71.70 samples/sec
2025-03-25 10:30:47,332 - training - INFO - Epoch 421 completed in 35.11s. Average loss: 2.3539
2025-03-25 10:30:47,336 - training - INFO - Starting epoch 422/200000
2025-03-25 10:30:48,068 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9632.0MB reserved
2025-03-25 10:30:48,068 - training - INFO - Epoch: 422/200000, Batch: 0/45, Loss: 2.1394, Throughput: 76.65 samples/sec
2025-03-25 10:30:59,961 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9494.0MB reserved
2025-03-25 10:30:59,962 - training - INFO - Epoch: 422/200000, Batch: 15/45, Loss: 2.3821, Throughput: 70.98 samples/sec
2025-03-25 10:31:11,680 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9494.0MB reserved
2025-03-25 10:31:11,680 - training - INFO - Epoch: 422/200000, Batch: 30/45, Loss: 2.3609, Throughput: 71.32 samples/sec
2025-03-25 10:31:22,648 - training - INFO - Epoch 422 completed in 35.31s. Average loss: 2.3565
2025-03-25 10:31:22,651 - training - INFO - Starting epoch 423/200000
2025-03-25 10:31:23,382 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9632.0MB reserved
2025-03-25 10:31:23,382 - training - INFO - Epoch: 423/200000, Batch: 0/45, Loss: 2.0508, Throughput: 76.78 samples/sec
2025-03-25 10:31:35,420 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9500.0MB reserved
2025-03-25 10:31:35,421 - training - INFO - Epoch: 423/200000, Batch: 15/45, Loss: 2.3086, Throughput: 70.18 samples/sec
2025-03-25 10:31:47,068 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9500.0MB reserved
2025-03-25 10:31:47,068 - training - INFO - Epoch: 423/200000, Batch: 30/45, Loss: 2.3009, Throughput: 71.11 samples/sec
2025-03-25 10:31:58,042 - training - INFO - Epoch 423 completed in 35.39s. Average loss: 2.3119
2025-03-25 10:31:58,045 - training - INFO - Starting epoch 424/200000
2025-03-25 10:31:58,786 - training - INFO - Memory: GPU 0: 3556.9MB allocated, 9638.0MB reserved
2025-03-25 10:31:58,786 - training - INFO - Epoch: 424/200000, Batch: 0/45, Loss: 2.4609, Throughput: 75.71 samples/sec
2025-03-25 10:32:10,652 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9498.0MB reserved
2025-03-25 10:32:10,652 - training - INFO - Epoch: 424/200000, Batch: 15/45, Loss: 2.3194, Throughput: 71.08 samples/sec
2025-03-25 10:32:22,201 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9500.0MB reserved
2025-03-25 10:32:22,202 - training - INFO - Epoch: 424/200000, Batch: 30/45, Loss: 2.3190, Throughput: 71.87 samples/sec
2025-03-25 10:32:33,134 - training - INFO - Epoch 424 completed in 35.09s. Average loss: 2.3321
2025-03-25 10:32:33,139 - training - INFO - Starting epoch 425/200000
2025-03-25 10:32:33,871 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9638.0MB reserved
2025-03-25 10:32:33,871 - training - INFO - Epoch: 425/200000, Batch: 0/45, Loss: 2.5308, Throughput: 76.74 samples/sec
2025-03-25 10:32:45,689 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9492.0MB reserved
2025-03-25 10:32:45,690 - training - INFO - Epoch: 425/200000, Batch: 15/45, Loss: 2.3649, Throughput: 71.40 samples/sec
2025-03-25 10:32:57,235 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9494.0MB reserved
2025-03-25 10:32:57,235 - training - INFO - Epoch: 425/200000, Batch: 30/45, Loss: 2.3001, Throughput: 72.05 samples/sec
2025-03-25 10:33:08,145 - training - INFO - Epoch 425 completed in 35.01s. Average loss: 2.3122
2025-03-25 10:33:08,149 - training - INFO - Starting epoch 426/200000
2025-03-25 10:33:08,894 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9632.0MB reserved
2025-03-25 10:33:08,894 - training - INFO - Epoch: 426/200000, Batch: 0/45, Loss: 2.3898, Throughput: 75.35 samples/sec
2025-03-25 10:33:20,743 - training - INFO - Memory: GPU 0: 3564.0MB allocated, 9508.0MB reserved
2025-03-25 10:33:20,743 - training - INFO - Epoch: 426/200000, Batch: 15/45, Loss: 2.2895, Throughput: 71.15 samples/sec
2025-03-25 10:33:32,324 - training - INFO - Memory: GPU 0: 3564.0MB allocated, 9508.0MB reserved
2025-03-25 10:33:32,325 - training - INFO - Epoch: 426/200000, Batch: 30/45, Loss: 2.3472, Throughput: 71.81 samples/sec
2025-03-25 10:33:43,243 - training - INFO - Epoch 426 completed in 35.09s. Average loss: 2.3315
2025-03-25 10:33:43,247 - training - INFO - Starting epoch 427/200000
2025-03-25 10:33:43,988 - training - INFO - Memory: GPU 0: 3564.0MB allocated, 9648.0MB reserved
2025-03-25 10:33:43,989 - training - INFO - Epoch: 427/200000, Batch: 0/45, Loss: 2.3700, Throughput: 75.66 samples/sec
2025-03-25 10:33:55,973 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9506.0MB reserved
2025-03-25 10:33:55,973 - training - INFO - Epoch: 427/200000, Batch: 15/45, Loss: 2.2665, Throughput: 70.41 samples/sec
2025-03-25 10:34:07,631 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9506.0MB reserved
2025-03-25 10:34:07,631 - training - INFO - Epoch: 427/200000, Batch: 30/45, Loss: 2.2985, Throughput: 71.20 samples/sec
2025-03-25 10:34:18,535 - training - INFO - Epoch 427 completed in 35.29s. Average loss: 2.3150
2025-03-25 10:34:18,539 - training - INFO - Starting epoch 428/200000
2025-03-25 10:34:19,268 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9644.0MB reserved
2025-03-25 10:34:19,269 - training - INFO - Epoch: 428/200000, Batch: 0/45, Loss: 2.4898, Throughput: 76.98 samples/sec
2025-03-25 10:34:31,213 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9508.0MB reserved
2025-03-25 10:34:31,213 - training - INFO - Epoch: 428/200000, Batch: 15/45, Loss: 2.4231, Throughput: 70.71 samples/sec
2025-03-25 10:34:42,882 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9508.0MB reserved
2025-03-25 10:34:42,883 - training - INFO - Epoch: 428/200000, Batch: 30/45, Loss: 2.3481, Throughput: 71.32 samples/sec
2025-03-25 10:34:53,851 - training - INFO - Epoch 428 completed in 35.31s. Average loss: 2.3585
2025-03-25 10:34:53,855 - training - INFO - Starting epoch 429/200000
2025-03-25 10:34:54,585 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9646.0MB reserved
2025-03-25 10:34:54,585 - training - INFO - Epoch: 429/200000, Batch: 0/45, Loss: 2.1035, Throughput: 76.82 samples/sec
2025-03-25 10:35:06,421 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9506.0MB reserved
2025-03-25 10:35:06,421 - training - INFO - Epoch: 429/200000, Batch: 15/45, Loss: 2.3197, Throughput: 71.31 samples/sec
2025-03-25 10:35:18,044 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9506.0MB reserved
2025-03-25 10:35:18,045 - training - INFO - Epoch: 429/200000, Batch: 30/45, Loss: 2.2880, Throughput: 71.77 samples/sec
2025-03-25 10:35:29,030 - training - INFO - Epoch 429 completed in 35.18s. Average loss: 2.3685
2025-03-25 10:35:29,034 - training - INFO - Starting epoch 430/200000
2025-03-25 10:35:29,772 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9644.0MB reserved
2025-03-25 10:35:29,772 - training - INFO - Epoch: 430/200000, Batch: 0/45, Loss: 1.9489, Throughput: 76.01 samples/sec
2025-03-25 10:35:41,746 - training - INFO - Memory: GPU 0: 3556.2MB allocated, 9510.0MB reserved
2025-03-25 10:35:41,746 - training - INFO - Epoch: 430/200000, Batch: 15/45, Loss: 2.2752, Throughput: 70.50 samples/sec
2025-03-25 10:35:53,358 - training - INFO - Memory: GPU 0: 3556.2MB allocated, 9512.0MB reserved
2025-03-25 10:35:53,358 - training - INFO - Epoch: 430/200000, Batch: 30/45, Loss: 2.3102, Throughput: 71.38 samples/sec
2025-03-25 10:36:04,275 - training - INFO - Epoch 430 completed in 35.24s. Average loss: 2.3426
2025-03-25 10:36:04,280 - training - INFO - Starting epoch 431/200000
2025-03-25 10:36:05,014 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9648.0MB reserved
2025-03-25 10:36:05,014 - training - INFO - Epoch: 431/200000, Batch: 0/45, Loss: 1.9460, Throughput: 76.52 samples/sec
2025-03-25 10:36:16,869 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9498.0MB reserved
2025-03-25 10:36:16,870 - training - INFO - Epoch: 431/200000, Batch: 15/45, Loss: 2.2287, Throughput: 71.18 samples/sec
2025-03-25 10:36:28,533 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9498.0MB reserved
2025-03-25 10:36:28,534 - training - INFO - Epoch: 431/200000, Batch: 30/45, Loss: 2.2659, Throughput: 71.58 samples/sec
2025-03-25 10:36:39,455 - training - INFO - Epoch 431 completed in 35.17s. Average loss: 2.3074
2025-03-25 10:36:39,459 - training - INFO - Starting epoch 432/200000
2025-03-25 10:36:40,217 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9636.0MB reserved
2025-03-25 10:36:40,217 - training - INFO - Epoch: 432/200000, Batch: 0/45, Loss: 1.9980, Throughput: 74.03 samples/sec
2025-03-25 10:36:52,198 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9500.0MB reserved
2025-03-25 10:36:52,198 - training - INFO - Epoch: 432/200000, Batch: 15/45, Loss: 2.3309, Throughput: 70.34 samples/sec
2025-03-25 10:37:03,775 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9500.0MB reserved
2025-03-25 10:37:03,775 - training - INFO - Epoch: 432/200000, Batch: 30/45, Loss: 2.3074, Throughput: 71.40 samples/sec
2025-03-25 10:37:14,699 - training - INFO - Epoch 432 completed in 35.24s. Average loss: 2.2871
2025-03-25 10:37:14,703 - training - INFO - Starting epoch 433/200000
2025-03-25 10:37:15,448 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9638.0MB reserved
2025-03-25 10:37:15,449 - training - INFO - Epoch: 433/200000, Batch: 0/45, Loss: 2.0001, Throughput: 75.30 samples/sec
2025-03-25 10:37:27,420 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9498.0MB reserved
2025-03-25 10:37:27,420 - training - INFO - Epoch: 433/200000, Batch: 15/45, Loss: 2.3295, Throughput: 70.47 samples/sec
2025-03-25 10:37:39,098 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9498.0MB reserved
2025-03-25 10:37:39,099 - training - INFO - Epoch: 433/200000, Batch: 30/45, Loss: 2.3092, Throughput: 71.16 samples/sec
2025-03-25 10:37:50,056 - training - INFO - Epoch 433 completed in 35.35s. Average loss: 2.3097
2025-03-25 10:37:50,060 - training - INFO - Starting epoch 434/200000
2025-03-25 10:37:50,809 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9634.0MB reserved
2025-03-25 10:37:50,809 - training - INFO - Epoch: 434/200000, Batch: 0/45, Loss: 2.7566, Throughput: 74.83 samples/sec
2025-03-25 10:38:02,729 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9496.0MB reserved
2025-03-25 10:38:02,730 - training - INFO - Epoch: 434/200000, Batch: 15/45, Loss: 2.2479, Throughput: 70.73 samples/sec
2025-03-25 10:38:14,356 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9496.0MB reserved
2025-03-25 10:38:14,357 - training - INFO - Epoch: 434/200000, Batch: 30/45, Loss: 2.2426, Throughput: 71.46 samples/sec
2025-03-25 10:38:25,309 - training - INFO - Epoch 434 completed in 35.25s. Average loss: 2.3014
2025-03-25 10:38:25,313 - training - INFO - Starting epoch 435/200000
2025-03-25 10:38:26,057 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9636.0MB reserved
2025-03-25 10:38:26,057 - training - INFO - Epoch: 435/200000, Batch: 0/45, Loss: 2.2824, Throughput: 75.45 samples/sec
2025-03-25 10:38:37,968 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9498.0MB reserved
2025-03-25 10:38:37,968 - training - INFO - Epoch: 435/200000, Batch: 15/45, Loss: 2.3488, Throughput: 70.81 samples/sec
2025-03-25 10:38:49,577 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9500.0MB reserved
2025-03-25 10:38:49,577 - training - INFO - Epoch: 435/200000, Batch: 30/45, Loss: 2.3271, Throughput: 71.55 samples/sec
2025-03-25 10:39:00,527 - training - INFO - Epoch 435 completed in 35.21s. Average loss: 2.3209
2025-03-25 10:39:00,531 - training - INFO - Starting epoch 436/200000
2025-03-25 10:39:01,259 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9638.0MB reserved
2025-03-25 10:39:01,259 - training - INFO - Epoch: 436/200000, Batch: 0/45, Loss: 2.5255, Throughput: 77.06 samples/sec
2025-03-25 10:39:13,221 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9508.0MB reserved
2025-03-25 10:39:13,222 - training - INFO - Epoch: 436/200000, Batch: 15/45, Loss: 2.2821, Throughput: 70.61 samples/sec
2025-03-25 10:39:24,793 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9508.0MB reserved
2025-03-25 10:39:24,794 - training - INFO - Epoch: 436/200000, Batch: 30/45, Loss: 2.3285, Throughput: 71.55 samples/sec
2025-03-25 10:39:35,779 - training - INFO - Epoch 436 completed in 35.25s. Average loss: 2.3434
2025-03-25 10:39:35,783 - training - INFO - Starting epoch 437/200000
2025-03-25 10:39:36,521 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9646.0MB reserved
2025-03-25 10:39:36,521 - training - INFO - Epoch: 437/200000, Batch: 0/45, Loss: 2.2184, Throughput: 75.94 samples/sec
2025-03-25 10:39:48,451 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9500.0MB reserved
2025-03-25 10:39:48,452 - training - INFO - Epoch: 437/200000, Batch: 15/45, Loss: 2.2416, Throughput: 70.74 samples/sec
2025-03-25 10:40:00,092 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9500.0MB reserved
2025-03-25 10:40:00,093 - training - INFO - Epoch: 437/200000, Batch: 30/45, Loss: 2.2881, Throughput: 71.42 samples/sec
2025-03-25 10:40:11,095 - training - INFO - Epoch 437 completed in 35.31s. Average loss: 2.2601
2025-03-25 10:40:11,099 - training - INFO - Starting epoch 438/200000
2025-03-25 10:40:11,833 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9638.0MB reserved
2025-03-25 10:40:11,833 - training - INFO - Epoch: 438/200000, Batch: 0/45, Loss: 1.6122, Throughput: 76.38 samples/sec
2025-03-25 10:40:23,814 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9502.0MB reserved
2025-03-25 10:40:23,815 - training - INFO - Epoch: 438/200000, Batch: 15/45, Loss: 2.2551, Throughput: 70.47 samples/sec
2025-03-25 10:40:35,458 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9502.0MB reserved
2025-03-25 10:40:35,458 - training - INFO - Epoch: 438/200000, Batch: 30/45, Loss: 2.3510, Throughput: 71.27 samples/sec
2025-03-25 10:40:46,447 - training - INFO - Epoch 438 completed in 35.35s. Average loss: 2.4143
2025-03-25 10:40:46,451 - training - INFO - Starting epoch 439/200000
2025-03-25 10:40:47,197 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9642.0MB reserved
2025-03-25 10:40:47,197 - training - INFO - Epoch: 439/200000, Batch: 0/45, Loss: 2.1844, Throughput: 75.14 samples/sec
2025-03-25 10:40:59,089 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9500.0MB reserved
2025-03-25 10:40:59,090 - training - INFO - Epoch: 439/200000, Batch: 15/45, Loss: 2.2659, Throughput: 70.90 samples/sec
2025-03-25 10:41:10,689 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9500.0MB reserved
2025-03-25 10:41:10,689 - training - INFO - Epoch: 439/200000, Batch: 30/45, Loss: 2.3439, Throughput: 71.63 samples/sec
2025-03-25 10:41:21,660 - training - INFO - Epoch 439 completed in 35.21s. Average loss: 2.3027
2025-03-25 10:41:21,664 - training - INFO - Starting epoch 440/200000
2025-03-25 10:41:22,399 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9638.0MB reserved
2025-03-25 10:41:22,399 - training - INFO - Epoch: 440/200000, Batch: 0/45, Loss: 2.6348, Throughput: 76.28 samples/sec
2025-03-25 10:41:34,258 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9506.0MB reserved
2025-03-25 10:41:34,258 - training - INFO - Epoch: 440/200000, Batch: 15/45, Loss: 2.1604, Throughput: 71.15 samples/sec
2025-03-25 10:41:45,936 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9506.0MB reserved
2025-03-25 10:41:45,936 - training - INFO - Epoch: 440/200000, Batch: 30/45, Loss: 2.2512, Throughput: 71.53 samples/sec
2025-03-25 10:41:56,834 - training - INFO - Epoch 440 completed in 35.17s. Average loss: 2.2437
2025-03-25 10:41:56,837 - training - INFO - Starting epoch 441/200000
2025-03-25 10:41:57,583 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9644.0MB reserved
2025-03-25 10:41:57,584 - training - INFO - Epoch: 441/200000, Batch: 0/45, Loss: 1.9955, Throughput: 75.20 samples/sec
2025-03-25 10:42:09,441 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9500.0MB reserved
2025-03-25 10:42:09,441 - training - INFO - Epoch: 441/200000, Batch: 15/45, Loss: 2.2241, Throughput: 71.11 samples/sec
2025-03-25 10:42:21,047 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9502.0MB reserved
2025-03-25 10:42:21,047 - training - INFO - Epoch: 441/200000, Batch: 30/45, Loss: 2.1803, Throughput: 71.71 samples/sec
2025-03-25 10:42:31,966 - training - INFO - Epoch 441 completed in 35.13s. Average loss: 2.3038
2025-03-25 10:42:31,970 - training - INFO - Starting epoch 442/200000
2025-03-25 10:42:32,721 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9638.0MB reserved
2025-03-25 10:42:32,722 - training - INFO - Epoch: 442/200000, Batch: 0/45, Loss: 2.0813, Throughput: 74.67 samples/sec
2025-03-25 10:42:44,607 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9508.0MB reserved
2025-03-25 10:42:44,608 - training - INFO - Epoch: 442/200000, Batch: 15/45, Loss: 2.2093, Throughput: 70.92 samples/sec
2025-03-25 10:42:56,176 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9508.0MB reserved
2025-03-25 10:42:56,176 - training - INFO - Epoch: 442/200000, Batch: 30/45, Loss: 2.2177, Throughput: 71.72 samples/sec
2025-03-25 10:43:07,076 - training - INFO - Epoch 442 completed in 35.11s. Average loss: 2.2828
2025-03-25 10:43:07,079 - training - INFO - Starting epoch 443/200000
2025-03-25 10:43:07,840 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9644.0MB reserved
2025-03-25 10:43:07,841 - training - INFO - Epoch: 443/200000, Batch: 0/45, Loss: 2.6579, Throughput: 73.76 samples/sec
2025-03-25 10:43:19,817 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9510.0MB reserved
2025-03-25 10:43:19,818 - training - INFO - Epoch: 443/200000, Batch: 15/45, Loss: 2.3588, Throughput: 70.35 samples/sec
2025-03-25 10:43:31,473 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9510.0MB reserved
2025-03-25 10:43:31,473 - training - INFO - Epoch: 443/200000, Batch: 30/45, Loss: 2.2711, Throughput: 71.17 samples/sec
2025-03-25 10:43:42,473 - training - INFO - Epoch 443 completed in 35.39s. Average loss: 2.3554
2025-03-25 10:43:42,477 - training - INFO - Starting epoch 444/200000
2025-03-25 10:43:43,234 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9648.0MB reserved
2025-03-25 10:43:43,234 - training - INFO - Epoch: 444/200000, Batch: 0/45, Loss: 2.0572, Throughput: 74.09 samples/sec
2025-03-25 10:43:55,088 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9504.0MB reserved
2025-03-25 10:43:55,088 - training - INFO - Epoch: 444/200000, Batch: 15/45, Loss: 2.2713, Throughput: 71.06 samples/sec
2025-03-25 10:44:06,653 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9504.0MB reserved
2025-03-25 10:44:06,653 - training - INFO - Epoch: 444/200000, Batch: 30/45, Loss: 2.3124, Throughput: 71.81 samples/sec
2025-03-25 10:44:17,616 - training - INFO - Epoch 444 completed in 35.14s. Average loss: 2.3301
2025-03-25 10:44:17,619 - training - INFO - Starting epoch 445/200000
2025-03-25 10:44:18,376 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9642.0MB reserved
2025-03-25 10:44:18,376 - training - INFO - Epoch: 445/200000, Batch: 0/45, Loss: 2.7127, Throughput: 74.15 samples/sec
2025-03-25 10:44:30,244 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9488.0MB reserved
2025-03-25 10:44:30,244 - training - INFO - Epoch: 445/200000, Batch: 15/45, Loss: 2.4995, Throughput: 70.98 samples/sec
2025-03-25 10:44:41,816 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9488.0MB reserved
2025-03-25 10:44:41,817 - training - INFO - Epoch: 445/200000, Batch: 30/45, Loss: 2.3684, Throughput: 71.75 samples/sec
2025-03-25 10:44:52,796 - training - INFO - Epoch 445 completed in 35.18s. Average loss: 2.3170
2025-03-25 10:44:52,799 - training - INFO - Starting epoch 446/200000
2025-03-25 10:44:53,530 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9626.0MB reserved
2025-03-25 10:44:53,531 - training - INFO - Epoch: 446/200000, Batch: 0/45, Loss: 2.5631, Throughput: 76.82 samples/sec
2025-03-25 10:45:05,437 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9490.0MB reserved
2025-03-25 10:45:05,438 - training - INFO - Epoch: 446/200000, Batch: 15/45, Loss: 2.2715, Throughput: 70.91 samples/sec
2025-03-25 10:45:17,020 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9492.0MB reserved
2025-03-25 10:45:17,020 - training - INFO - Epoch: 446/200000, Batch: 30/45, Loss: 2.3046, Throughput: 71.68 samples/sec
2025-03-25 10:45:27,949 - training - INFO - Epoch 446 completed in 35.15s. Average loss: 2.2828
2025-03-25 10:45:27,953 - training - INFO - Starting epoch 447/200000
2025-03-25 10:45:28,700 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9630.0MB reserved
2025-03-25 10:45:28,700 - training - INFO - Epoch: 447/200000, Batch: 0/45, Loss: 2.4139, Throughput: 75.21 samples/sec
2025-03-25 10:45:40,652 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9504.0MB reserved
2025-03-25 10:45:40,652 - training - INFO - Epoch: 447/200000, Batch: 15/45, Loss: 2.2128, Throughput: 70.57 samples/sec
2025-03-25 10:45:52,332 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9504.0MB reserved
2025-03-25 10:45:52,333 - training - INFO - Epoch: 447/200000, Batch: 30/45, Loss: 2.2032, Throughput: 71.21 samples/sec
2025-03-25 10:46:03,322 - training - INFO - Epoch 447 completed in 35.37s. Average loss: 2.2766
2025-03-25 10:46:03,326 - training - INFO - Starting epoch 448/200000
2025-03-25 10:46:04,074 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9642.0MB reserved
2025-03-25 10:46:04,075 - training - INFO - Epoch: 448/200000, Batch: 0/45, Loss: 1.9879, Throughput: 74.99 samples/sec
2025-03-25 10:46:16,075 - training - INFO - Memory: GPU 0: 3556.0MB allocated, 9508.0MB reserved
2025-03-25 10:46:16,075 - training - INFO - Epoch: 448/200000, Batch: 15/45, Loss: 2.2591, Throughput: 70.30 samples/sec
2025-03-25 10:46:27,703 - training - INFO - Memory: GPU 0: 3556.0MB allocated, 9508.0MB reserved
2025-03-25 10:46:27,703 - training - INFO - Epoch: 448/200000, Batch: 30/45, Loss: 2.2977, Throughput: 71.22 samples/sec
2025-03-25 10:46:38,646 - training - INFO - Epoch 448 completed in 35.32s. Average loss: 2.2954
2025-03-25 10:46:38,649 - training - INFO - Starting epoch 449/200000
2025-03-25 10:46:39,409 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9644.0MB reserved
2025-03-25 10:46:39,409 - training - INFO - Epoch: 449/200000, Batch: 0/45, Loss: 2.4006, Throughput: 73.93 samples/sec
2025-03-25 10:46:51,345 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9484.0MB reserved
2025-03-25 10:46:51,345 - training - INFO - Epoch: 449/200000, Batch: 15/45, Loss: 2.1775, Throughput: 70.58 samples/sec
2025-03-25 10:47:02,955 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9484.0MB reserved
2025-03-25 10:47:02,955 - training - INFO - Epoch: 449/200000, Batch: 30/45, Loss: 2.2473, Throughput: 71.43 samples/sec
2025-03-25 10:47:13,898 - training - INFO - Epoch 449 completed in 35.25s. Average loss: 2.3239
2025-03-25 10:47:13,902 - training - INFO - Starting epoch 450/200000
2025-03-25 10:47:14,630 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9622.0MB reserved
2025-03-25 10:47:14,630 - training - INFO - Epoch: 450/200000, Batch: 0/45, Loss: 2.0895, Throughput: 77.05 samples/sec
2025-03-25 10:47:26,498 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9502.0MB reserved
2025-03-25 10:47:26,499 - training - INFO - Epoch: 450/200000, Batch: 15/45, Loss: 2.3062, Throughput: 71.15 samples/sec
2025-03-25 10:47:38,091 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9504.0MB reserved
2025-03-25 10:47:38,092 - training - INFO - Epoch: 450/200000, Batch: 30/45, Loss: 2.3188, Throughput: 71.77 samples/sec
2025-03-25 10:47:49,049 - training - INFO - Epoch 450 completed in 35.15s. Average loss: 2.2906
2025-03-25 10:47:49,053 - training - INFO - Starting epoch 451/200000
2025-03-25 10:47:49,807 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9642.0MB reserved
2025-03-25 10:47:49,807 - training - INFO - Epoch: 451/200000, Batch: 0/45, Loss: 2.1445, Throughput: 74.44 samples/sec
2025-03-25 10:48:01,775 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9392.0MB reserved
2025-03-25 10:48:01,775 - training - INFO - Epoch: 451/200000, Batch: 15/45, Loss: 2.3568, Throughput: 70.44 samples/sec
2025-03-25 10:48:13,435 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9498.0MB reserved
2025-03-25 10:48:13,436 - training - INFO - Epoch: 451/200000, Batch: 30/45, Loss: 2.2642, Throughput: 71.21 samples/sec
2025-03-25 10:48:24,369 - training - INFO - Epoch 451 completed in 35.32s. Average loss: 2.2538
2025-03-25 10:48:24,373 - training - INFO - Starting epoch 452/200000
2025-03-25 10:48:25,130 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9636.0MB reserved
2025-03-25 10:48:25,130 - training - INFO - Epoch: 452/200000, Batch: 0/45, Loss: 2.5164, Throughput: 74.14 samples/sec
2025-03-25 10:48:37,089 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9492.0MB reserved
2025-03-25 10:48:37,090 - training - INFO - Epoch: 452/200000, Batch: 15/45, Loss: 2.2645, Throughput: 70.47 samples/sec
2025-03-25 10:48:48,780 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9492.0MB reserved
2025-03-25 10:48:48,781 - training - INFO - Epoch: 452/200000, Batch: 30/45, Loss: 2.2936, Throughput: 71.13 samples/sec
2025-03-25 10:48:59,774 - training - INFO - Epoch 452 completed in 35.40s. Average loss: 2.2817
2025-03-25 10:48:59,777 - training - INFO - Starting epoch 453/200000
2025-03-25 10:49:00,525 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9628.0MB reserved
2025-03-25 10:49:00,525 - training - INFO - Epoch: 453/200000, Batch: 0/45, Loss: 2.0505, Throughput: 75.12 samples/sec
2025-03-25 10:49:12,471 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9492.0MB reserved
2025-03-25 10:49:12,472 - training - INFO - Epoch: 453/200000, Batch: 15/45, Loss: 2.1640, Throughput: 70.59 samples/sec
2025-03-25 10:49:24,105 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9492.0MB reserved
2025-03-25 10:49:24,105 - training - INFO - Epoch: 453/200000, Batch: 30/45, Loss: 2.2594, Throughput: 71.37 samples/sec
2025-03-25 10:49:35,012 - training - INFO - Epoch 453 completed in 35.23s. Average loss: 2.2514
2025-03-25 10:49:35,016 - training - INFO - Starting epoch 454/200000
2025-03-25 10:49:35,735 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9628.0MB reserved
2025-03-25 10:49:35,735 - training - INFO - Epoch: 454/200000, Batch: 0/45, Loss: 2.2755, Throughput: 78.16 samples/sec
2025-03-25 10:49:47,544 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9492.0MB reserved
2025-03-25 10:49:47,545 - training - INFO - Epoch: 454/200000, Batch: 15/45, Loss: 2.1913, Throughput: 71.53 samples/sec
2025-03-25 10:49:59,211 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9492.0MB reserved
2025-03-25 10:49:59,212 - training - INFO - Epoch: 454/200000, Batch: 30/45, Loss: 2.2590, Throughput: 71.76 samples/sec
2025-03-25 10:50:10,165 - training - INFO - Epoch 454 completed in 35.15s. Average loss: 2.3330
2025-03-25 10:50:10,169 - training - INFO - Starting epoch 455/200000
2025-03-25 10:50:10,930 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9630.0MB reserved
2025-03-25 10:50:10,930 - training - INFO - Epoch: 455/200000, Batch: 0/45, Loss: 2.0530, Throughput: 73.72 samples/sec
2025-03-25 10:50:22,811 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9494.0MB reserved
2025-03-25 10:50:22,811 - training - INFO - Epoch: 455/200000, Batch: 15/45, Loss: 2.4052, Throughput: 70.89 samples/sec
2025-03-25 10:50:34,433 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9494.0MB reserved
2025-03-25 10:50:34,434 - training - INFO - Epoch: 455/200000, Batch: 30/45, Loss: 2.3147, Throughput: 71.55 samples/sec
2025-03-25 10:50:45,358 - training - INFO - Epoch 455 completed in 35.19s. Average loss: 2.2794
2025-03-25 10:50:45,361 - training - INFO - Starting epoch 456/200000
2025-03-25 10:50:46,112 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9632.0MB reserved
2025-03-25 10:50:46,113 - training - INFO - Epoch: 456/200000, Batch: 0/45, Loss: 2.6380, Throughput: 74.74 samples/sec
2025-03-25 10:50:58,037 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9510.0MB reserved
2025-03-25 10:50:58,037 - training - INFO - Epoch: 456/200000, Batch: 15/45, Loss: 2.3587, Throughput: 70.70 samples/sec
2025-03-25 10:51:09,617 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9512.0MB reserved
2025-03-25 10:51:09,617 - training - INFO - Epoch: 456/200000, Batch: 30/45, Loss: 2.2761, Throughput: 71.58 samples/sec
2025-03-25 10:51:20,535 - training - INFO - Epoch 456 completed in 35.17s. Average loss: 2.2940
2025-03-25 10:51:20,539 - training - INFO - Starting epoch 457/200000
2025-03-25 10:51:21,287 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9650.0MB reserved
2025-03-25 10:51:21,287 - training - INFO - Epoch: 457/200000, Batch: 0/45, Loss: 2.0544, Throughput: 75.05 samples/sec
2025-03-25 10:51:33,207 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9498.0MB reserved
2025-03-25 10:51:33,207 - training - INFO - Epoch: 457/200000, Batch: 15/45, Loss: 2.2296, Throughput: 70.74 samples/sec
2025-03-25 10:51:44,865 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9500.0MB reserved
2025-03-25 10:51:44,865 - training - INFO - Epoch: 457/200000, Batch: 30/45, Loss: 2.2187, Throughput: 71.37 samples/sec
2025-03-25 10:51:55,826 - training - INFO - Epoch 457 completed in 35.29s. Average loss: 2.2649
2025-03-25 10:51:55,830 - training - INFO - Starting epoch 458/200000
2025-03-25 10:51:56,568 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9638.0MB reserved
2025-03-25 10:51:56,569 - training - INFO - Epoch: 458/200000, Batch: 0/45, Loss: 2.0786, Throughput: 76.01 samples/sec
2025-03-25 10:52:08,537 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9498.0MB reserved
2025-03-25 10:52:08,537 - training - INFO - Epoch: 458/200000, Batch: 15/45, Loss: 2.2147, Throughput: 70.52 samples/sec
2025-03-25 10:52:20,194 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9500.0MB reserved
2025-03-25 10:52:20,194 - training - INFO - Epoch: 458/200000, Batch: 30/45, Loss: 2.1897, Throughput: 71.26 samples/sec
2025-03-25 10:52:31,170 - training - INFO - Epoch 458 completed in 35.34s. Average loss: 2.2352
2025-03-25 10:52:31,178 - training - INFO - Starting epoch 459/200000
2025-03-25 10:52:31,920 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9638.0MB reserved
2025-03-25 10:52:31,920 - training - INFO - Epoch: 459/200000, Batch: 0/45, Loss: 2.2704, Throughput: 75.62 samples/sec
2025-03-25 10:52:43,731 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9496.0MB reserved
2025-03-25 10:52:43,731 - training - INFO - Epoch: 459/200000, Batch: 15/45, Loss: 2.3368, Throughput: 71.38 samples/sec
2025-03-25 10:52:55,240 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9496.0MB reserved
2025-03-25 10:52:55,241 - training - INFO - Epoch: 459/200000, Batch: 30/45, Loss: 2.2969, Throughput: 72.15 samples/sec
2025-03-25 10:53:06,175 - training - INFO - Epoch 459 completed in 35.00s. Average loss: 2.2346
2025-03-25 10:53:06,179 - training - INFO - Starting epoch 460/200000
2025-03-25 10:53:06,934 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9634.0MB reserved
2025-03-25 10:53:06,934 - training - INFO - Epoch: 460/200000, Batch: 0/45, Loss: 2.7222, Throughput: 74.27 samples/sec
2025-03-25 10:53:18,840 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9498.0MB reserved
2025-03-25 10:53:18,841 - training - INFO - Epoch: 460/200000, Batch: 15/45, Loss: 2.3497, Throughput: 70.77 samples/sec
2025-03-25 10:53:30,437 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9498.0MB reserved
2025-03-25 10:53:30,437 - training - INFO - Epoch: 460/200000, Batch: 30/45, Loss: 2.3616, Throughput: 71.57 samples/sec
2025-03-25 10:53:41,363 - training - INFO - Epoch 460 completed in 35.18s. Average loss: 2.2803
2025-03-25 10:53:41,366 - training - INFO - Starting epoch 461/200000
2025-03-25 10:53:42,110 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9636.0MB reserved
2025-03-25 10:53:42,110 - training - INFO - Epoch: 461/200000, Batch: 0/45, Loss: 2.6109, Throughput: 75.51 samples/sec
2025-03-25 10:53:54,052 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9504.0MB reserved
2025-03-25 10:53:54,053 - training - INFO - Epoch: 461/200000, Batch: 15/45, Loss: 2.3671, Throughput: 70.65 samples/sec
2025-03-25 10:54:05,724 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9504.0MB reserved
2025-03-25 10:54:05,724 - training - INFO - Epoch: 461/200000, Batch: 30/45, Loss: 2.2844, Throughput: 71.28 samples/sec
2025-03-25 10:54:16,707 - training - INFO - Epoch 461 completed in 35.34s. Average loss: 2.2754
2025-03-25 10:54:16,711 - training - INFO - Starting epoch 462/200000
2025-03-25 10:54:17,445 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9640.0MB reserved
2025-03-25 10:54:17,446 - training - INFO - Epoch: 462/200000, Batch: 0/45, Loss: 2.3601, Throughput: 76.42 samples/sec
2025-03-25 10:54:29,302 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9500.0MB reserved
2025-03-25 10:54:29,303 - training - INFO - Epoch: 462/200000, Batch: 15/45, Loss: 2.2359, Throughput: 71.17 samples/sec
2025-03-25 10:54:40,975 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9500.0MB reserved
2025-03-25 10:54:40,976 - training - INFO - Epoch: 462/200000, Batch: 30/45, Loss: 2.2867, Throughput: 71.55 samples/sec
2025-03-25 10:54:51,892 - training - INFO - Epoch 462 completed in 35.18s. Average loss: 2.2518
2025-03-25 10:54:51,896 - training - INFO - Starting epoch 463/200000
2025-03-25 10:54:52,635 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9638.0MB reserved
2025-03-25 10:54:52,635 - training - INFO - Epoch: 463/200000, Batch: 0/45, Loss: 2.2319, Throughput: 75.84 samples/sec
2025-03-25 10:55:04,494 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9512.0MB reserved
2025-03-25 10:55:04,494 - training - INFO - Epoch: 463/200000, Batch: 15/45, Loss: 2.1701, Throughput: 71.13 samples/sec
2025-03-25 10:55:16,131 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9514.0MB reserved
2025-03-25 10:55:16,131 - training - INFO - Epoch: 463/200000, Batch: 30/45, Loss: 2.1898, Throughput: 71.64 samples/sec
2025-03-25 10:55:27,075 - training - INFO - Epoch 463 completed in 35.18s. Average loss: 2.2485
2025-03-25 10:55:27,079 - training - INFO - Starting epoch 464/200000
2025-03-25 10:55:27,826 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9652.0MB reserved
2025-03-25 10:55:27,826 - training - INFO - Epoch: 464/200000, Batch: 0/45, Loss: 2.1113, Throughput: 75.21 samples/sec
2025-03-25 10:55:39,729 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9496.0MB reserved
2025-03-25 10:55:39,729 - training - INFO - Epoch: 464/200000, Batch: 15/45, Loss: 2.2275, Throughput: 70.85 samples/sec
2025-03-25 10:55:51,372 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9498.0MB reserved
2025-03-25 10:55:51,491 - training - INFO - Epoch: 464/200000, Batch: 30/45, Loss: 2.2821, Throughput: 71.47 samples/sec
2025-03-25 10:56:02,368 - training - INFO - Epoch 464 completed in 35.29s. Average loss: 2.2467
2025-03-25 10:56:02,371 - training - INFO - Starting epoch 465/200000
2025-03-25 10:56:03,098 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9634.0MB reserved
2025-03-25 10:56:03,098 - training - INFO - Epoch: 465/200000, Batch: 0/45, Loss: 2.1622, Throughput: 77.28 samples/sec
2025-03-25 10:56:15,087 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9504.0MB reserved
2025-03-25 10:56:15,088 - training - INFO - Epoch: 465/200000, Batch: 15/45, Loss: 2.3555, Throughput: 70.47 samples/sec
2025-03-25 10:56:26,754 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9504.0MB reserved
2025-03-25 10:56:26,755 - training - INFO - Epoch: 465/200000, Batch: 30/45, Loss: 2.3042, Throughput: 71.20 samples/sec
2025-03-25 10:56:37,711 - training - INFO - Epoch 465 completed in 35.34s. Average loss: 2.2408
2025-03-25 10:56:37,714 - training - INFO - Starting epoch 466/200000
2025-03-25 10:56:38,471 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9642.0MB reserved
2025-03-25 10:56:38,471 - training - INFO - Epoch: 466/200000, Batch: 0/45, Loss: 2.3536, Throughput: 74.12 samples/sec
2025-03-25 10:56:50,280 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9494.0MB reserved
2025-03-25 10:56:50,280 - training - INFO - Epoch: 466/200000, Batch: 15/45, Loss: 2.1909, Throughput: 71.31 samples/sec
2025-03-25 10:57:01,842 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9494.0MB reserved
2025-03-25 10:57:01,842 - training - INFO - Epoch: 466/200000, Batch: 30/45, Loss: 2.2404, Throughput: 71.96 samples/sec
2025-03-25 10:57:12,751 - training - INFO - Epoch 466 completed in 35.04s. Average loss: 2.2300
2025-03-25 10:57:12,754 - training - INFO - Starting epoch 467/200000
2025-03-25 10:57:13,498 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9632.0MB reserved
2025-03-25 10:57:13,498 - training - INFO - Epoch: 467/200000, Batch: 0/45, Loss: 1.6953, Throughput: 75.56 samples/sec
2025-03-25 10:57:25,367 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9500.0MB reserved
2025-03-25 10:57:25,367 - training - INFO - Epoch: 467/200000, Batch: 15/45, Loss: 2.2979, Throughput: 71.05 samples/sec
2025-03-25 10:57:36,980 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9500.0MB reserved
2025-03-25 10:57:36,980 - training - INFO - Epoch: 467/200000, Batch: 30/45, Loss: 2.2496, Throughput: 71.66 samples/sec
2025-03-25 10:57:47,889 - training - INFO - Epoch 467 completed in 35.13s. Average loss: 2.2502
2025-03-25 10:57:47,893 - training - INFO - Starting epoch 468/200000
2025-03-25 10:57:48,637 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9638.0MB reserved
2025-03-25 10:57:48,637 - training - INFO - Epoch: 468/200000, Batch: 0/45, Loss: 2.0823, Throughput: 75.46 samples/sec
2025-03-25 10:58:00,531 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9502.0MB reserved
2025-03-25 10:58:00,531 - training - INFO - Epoch: 468/200000, Batch: 15/45, Loss: 2.1336, Throughput: 70.91 samples/sec
2025-03-25 10:58:12,217 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9502.0MB reserved
2025-03-25 10:58:12,218 - training - INFO - Epoch: 468/200000, Batch: 30/45, Loss: 2.1500, Throughput: 71.38 samples/sec
2025-03-25 10:58:23,253 - training - INFO - Epoch 468 completed in 35.36s. Average loss: 2.2365
2025-03-25 10:58:23,256 - training - INFO - Starting epoch 469/200000
2025-03-25 10:58:23,991 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9640.0MB reserved
2025-03-25 10:58:23,991 - training - INFO - Epoch: 469/200000, Batch: 0/45, Loss: 2.2920, Throughput: 76.44 samples/sec
2025-03-25 10:58:35,867 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9506.0MB reserved
2025-03-25 10:58:35,867 - training - INFO - Epoch: 469/200000, Batch: 15/45, Loss: 2.1961, Throughput: 71.07 samples/sec
2025-03-25 10:58:47,411 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9508.0MB reserved
2025-03-25 10:58:47,411 - training - INFO - Epoch: 469/200000, Batch: 30/45, Loss: 2.1551, Throughput: 71.87 samples/sec
2025-03-25 10:58:58,295 - training - INFO - Epoch 469 completed in 35.04s. Average loss: 2.2545
2025-03-25 10:58:58,299 - training - INFO - Starting epoch 470/200000
2025-03-25 10:58:59,062 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9644.0MB reserved
2025-03-25 10:58:59,063 - training - INFO - Epoch: 470/200000, Batch: 0/45, Loss: 2.8981, Throughput: 73.42 samples/sec
2025-03-25 10:59:10,961 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9502.0MB reserved
2025-03-25 10:59:10,962 - training - INFO - Epoch: 470/200000, Batch: 15/45, Loss: 2.1925, Throughput: 70.77 samples/sec
2025-03-25 10:59:22,591 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9502.0MB reserved
2025-03-25 10:59:22,591 - training - INFO - Epoch: 470/200000, Batch: 30/45, Loss: 2.2164, Throughput: 71.47 samples/sec
2025-03-25 10:59:33,588 - training - INFO - Epoch 470 completed in 35.29s. Average loss: 2.2173
2025-03-25 10:59:33,592 - training - INFO - Starting epoch 471/200000
2025-03-25 10:59:34,351 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9640.0MB reserved
2025-03-25 10:59:34,352 - training - INFO - Epoch: 471/200000, Batch: 0/45, Loss: 2.3435, Throughput: 73.92 samples/sec
2025-03-25 10:59:46,236 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9486.0MB reserved
2025-03-25 10:59:46,237 - training - INFO - Epoch: 471/200000, Batch: 15/45, Loss: 2.2278, Throughput: 70.87 samples/sec
2025-03-25 10:59:57,843 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9486.0MB reserved
2025-03-25 10:59:57,843 - training - INFO - Epoch: 471/200000, Batch: 30/45, Loss: 2.2685, Throughput: 71.59 samples/sec
2025-03-25 11:00:08,890 - training - INFO - Epoch 471 completed in 35.30s. Average loss: 2.2384
2025-03-25 11:00:08,894 - training - INFO - Starting epoch 472/200000
2025-03-25 11:00:09,638 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9624.0MB reserved
2025-03-25 11:00:09,639 - training - INFO - Epoch: 472/200000, Batch: 0/45, Loss: 2.3278, Throughput: 75.29 samples/sec
2025-03-25 11:00:21,597 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9508.0MB reserved
2025-03-25 11:00:21,597 - training - INFO - Epoch: 472/200000, Batch: 15/45, Loss: 2.1972, Throughput: 70.54 samples/sec
2025-03-25 11:00:33,209 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9508.0MB reserved
2025-03-25 11:00:33,210 - training - INFO - Epoch: 472/200000, Batch: 30/45, Loss: 2.1863, Throughput: 71.40 samples/sec
2025-03-25 11:00:44,123 - training - INFO - Epoch 472 completed in 35.23s. Average loss: 2.2591
2025-03-25 11:00:44,127 - training - INFO - Starting epoch 473/200000
2025-03-25 11:00:44,866 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9646.0MB reserved
2025-03-25 11:00:44,866 - training - INFO - Epoch: 473/200000, Batch: 0/45, Loss: 2.2919, Throughput: 75.99 samples/sec
2025-03-25 11:00:56,723 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9494.0MB reserved
2025-03-25 11:00:56,723 - training - INFO - Epoch: 473/200000, Batch: 15/45, Loss: 2.2001, Throughput: 71.14 samples/sec
2025-03-25 11:01:08,296 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9494.0MB reserved
2025-03-25 11:01:08,297 - training - INFO - Epoch: 473/200000, Batch: 30/45, Loss: 2.1457, Throughput: 71.83 samples/sec
2025-03-25 11:01:19,221 - training - INFO - Epoch 473 completed in 35.09s. Average loss: 2.2581
2025-03-25 11:01:19,225 - training - INFO - Starting epoch 474/200000
2025-03-25 11:01:19,961 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9632.0MB reserved
2025-03-25 11:01:19,961 - training - INFO - Epoch: 474/200000, Batch: 0/45, Loss: 2.0887, Throughput: 76.14 samples/sec
2025-03-25 11:01:31,943 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9496.0MB reserved
2025-03-25 11:01:31,944 - training - INFO - Epoch: 474/200000, Batch: 15/45, Loss: 2.1276, Throughput: 70.45 samples/sec
2025-03-25 11:01:43,663 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9496.0MB reserved
2025-03-25 11:01:43,663 - training - INFO - Epoch: 474/200000, Batch: 30/45, Loss: 2.1471, Throughput: 71.04 samples/sec
2025-03-25 11:01:54,675 - training - INFO - Epoch 474 completed in 35.45s. Average loss: 2.1975
2025-03-25 11:01:54,679 - training - INFO - Starting epoch 475/200000
2025-03-25 11:01:55,424 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9634.0MB reserved
2025-03-25 11:01:55,424 - training - INFO - Epoch: 475/200000, Batch: 0/45, Loss: 2.3058, Throughput: 75.24 samples/sec
2025-03-25 11:02:07,354 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9506.0MB reserved
2025-03-25 11:02:07,354 - training - INFO - Epoch: 475/200000, Batch: 15/45, Loss: 2.2225, Throughput: 70.70 samples/sec
2025-03-25 11:02:18,961 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9506.0MB reserved
2025-03-25 11:02:18,961 - training - INFO - Epoch: 475/200000, Batch: 30/45, Loss: 2.1650, Throughput: 71.50 samples/sec
2025-03-25 11:02:29,955 - training - INFO - Epoch 475 completed in 35.28s. Average loss: 2.2094
2025-03-25 11:02:29,959 - training - INFO - Starting epoch 476/200000
2025-03-25 11:02:30,722 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9644.0MB reserved
2025-03-25 11:02:30,747 - training - INFO - Epoch: 476/200000, Batch: 0/45, Loss: 2.0865, Throughput: 73.61 samples/sec
2025-03-25 11:02:42,720 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9500.0MB reserved
2025-03-25 11:02:42,720 - training - INFO - Epoch: 476/200000, Batch: 15/45, Loss: 2.2608, Throughput: 70.22 samples/sec
2025-03-25 11:02:54,509 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9500.0MB reserved
2025-03-25 11:02:54,509 - training - INFO - Epoch: 476/200000, Batch: 30/45, Loss: 2.2930, Throughput: 70.72 samples/sec
2025-03-25 11:03:05,553 - training - INFO - Epoch 476 completed in 35.59s. Average loss: 2.2327
2025-03-25 11:03:05,556 - training - INFO - Starting epoch 477/200000
2025-03-25 11:03:06,285 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9638.0MB reserved
2025-03-25 11:03:06,285 - training - INFO - Epoch: 477/200000, Batch: 0/45, Loss: 2.2537, Throughput: 77.06 samples/sec
2025-03-25 11:03:18,306 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9496.0MB reserved
2025-03-25 11:03:18,306 - training - INFO - Epoch: 477/200000, Batch: 15/45, Loss: 2.1856, Throughput: 70.29 samples/sec
2025-03-25 11:03:29,840 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9496.0MB reserved
2025-03-25 11:03:29,841 - training - INFO - Epoch: 477/200000, Batch: 30/45, Loss: 2.1747, Throughput: 71.49 samples/sec
2025-03-25 11:03:40,746 - training - INFO - Epoch 477 completed in 35.19s. Average loss: 2.2367
2025-03-25 11:03:40,749 - training - INFO - Starting epoch 478/200000
2025-03-25 11:03:41,511 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9634.0MB reserved
2025-03-25 11:03:41,512 - training - INFO - Epoch: 478/200000, Batch: 0/45, Loss: 1.9953, Throughput: 73.55 samples/sec
2025-03-25 11:03:53,372 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9506.0MB reserved
2025-03-25 11:03:53,372 - training - INFO - Epoch: 478/200000, Batch: 15/45, Loss: 2.2332, Throughput: 70.99 samples/sec
2025-03-25 11:04:04,985 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9506.0MB reserved
2025-03-25 11:04:04,985 - training - INFO - Epoch: 478/200000, Batch: 30/45, Loss: 2.1760, Throughput: 71.63 samples/sec
2025-03-25 11:04:15,991 - training - INFO - Epoch 478 completed in 35.24s. Average loss: 2.2086
2025-03-25 11:04:15,994 - training - INFO - Starting epoch 479/200000
2025-03-25 11:04:16,733 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9644.0MB reserved
2025-03-25 11:04:16,734 - training - INFO - Epoch: 479/200000, Batch: 0/45, Loss: 2.1671, Throughput: 75.88 samples/sec
2025-03-25 11:04:28,679 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9502.0MB reserved
2025-03-25 11:04:28,679 - training - INFO - Epoch: 479/200000, Batch: 15/45, Loss: 2.1362, Throughput: 70.65 samples/sec
2025-03-25 11:04:40,262 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9502.0MB reserved
2025-03-25 11:04:40,262 - training - INFO - Epoch: 479/200000, Batch: 30/45, Loss: 2.1536, Throughput: 71.54 samples/sec
2025-03-25 11:04:51,178 - training - INFO - Epoch 479 completed in 35.18s. Average loss: 2.1983
2025-03-25 11:04:51,181 - training - INFO - Starting epoch 480/200000
2025-03-25 11:04:51,905 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9640.0MB reserved
2025-03-25 11:04:51,905 - training - INFO - Epoch: 480/200000, Batch: 0/45, Loss: 2.1723, Throughput: 77.45 samples/sec
2025-03-25 11:05:03,831 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9488.0MB reserved
2025-03-25 11:05:03,831 - training - INFO - Epoch: 480/200000, Batch: 15/45, Loss: 2.1964, Throughput: 70.84 samples/sec
2025-03-25 11:05:15,443 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9488.0MB reserved
2025-03-25 11:05:15,443 - training - INFO - Epoch: 480/200000, Batch: 30/45, Loss: 2.1645, Throughput: 71.56 samples/sec
2025-03-25 11:05:26,374 - training - INFO - Epoch 480 completed in 35.19s. Average loss: 2.2307
2025-03-25 11:05:26,377 - training - INFO - Starting epoch 481/200000
2025-03-25 11:05:27,130 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9626.0MB reserved
2025-03-25 11:05:27,131 - training - INFO - Epoch: 481/200000, Batch: 0/45, Loss: 2.2386, Throughput: 74.54 samples/sec
2025-03-25 11:05:39,011 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9498.0MB reserved
2025-03-25 11:05:39,011 - training - INFO - Epoch: 481/200000, Batch: 15/45, Loss: 2.3174, Throughput: 70.93 samples/sec
2025-03-25 11:05:50,658 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9500.0MB reserved
2025-03-25 11:05:50,658 - training - INFO - Epoch: 481/200000, Batch: 30/45, Loss: 2.2831, Throughput: 71.50 samples/sec
2025-03-25 11:06:01,566 - training - INFO - Epoch 481 completed in 35.19s. Average loss: 2.2485
2025-03-25 11:06:01,569 - training - INFO - Starting epoch 482/200000
2025-03-25 11:06:02,318 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9638.0MB reserved
2025-03-25 11:06:02,318 - training - INFO - Epoch: 482/200000, Batch: 0/45, Loss: 2.8281, Throughput: 74.90 samples/sec
2025-03-25 11:06:14,263 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9482.0MB reserved
2025-03-25 11:06:14,264 - training - INFO - Epoch: 482/200000, Batch: 15/45, Loss: 2.2721, Throughput: 70.60 samples/sec
2025-03-25 11:06:25,795 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9482.0MB reserved
2025-03-25 11:06:25,796 - training - INFO - Epoch: 482/200000, Batch: 30/45, Loss: 2.1721, Throughput: 71.66 samples/sec
2025-03-25 11:06:36,730 - training - INFO - Epoch 482 completed in 35.16s. Average loss: 2.2218
2025-03-25 11:06:36,734 - training - INFO - Starting epoch 483/200000
2025-03-25 11:06:37,494 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9620.0MB reserved
2025-03-25 11:06:37,494 - training - INFO - Epoch: 483/200000, Batch: 0/45, Loss: 2.3883, Throughput: 73.84 samples/sec
2025-03-25 11:06:49,430 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9502.0MB reserved
2025-03-25 11:06:49,430 - training - INFO - Epoch: 483/200000, Batch: 15/45, Loss: 2.2056, Throughput: 70.59 samples/sec
2025-03-25 11:07:01,307 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9502.0MB reserved
2025-03-25 11:07:01,307 - training - INFO - Epoch: 483/200000, Batch: 30/45, Loss: 2.2280, Throughput: 70.65 samples/sec
2025-03-25 11:07:12,345 - training - INFO - Epoch 483 completed in 35.61s. Average loss: 2.2442
2025-03-25 11:07:12,349 - training - INFO - Starting epoch 484/200000
2025-03-25 11:07:13,099 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9640.0MB reserved
2025-03-25 11:07:13,099 - training - INFO - Epoch: 484/200000, Batch: 0/45, Loss: 1.6957, Throughput: 74.90 samples/sec
2025-03-25 11:07:25,101 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9494.0MB reserved
2025-03-25 11:07:25,101 - training - INFO - Epoch: 484/200000, Batch: 15/45, Loss: 2.1370, Throughput: 70.27 samples/sec
2025-03-25 11:07:36,796 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9496.0MB reserved
2025-03-25 11:07:36,797 - training - INFO - Epoch: 484/200000, Batch: 30/45, Loss: 2.1133, Throughput: 71.02 samples/sec
2025-03-25 11:07:47,768 - training - INFO - Epoch 484 completed in 35.42s. Average loss: 2.2264
2025-03-25 11:07:47,772 - training - INFO - Starting epoch 485/200000
2025-03-25 11:07:48,519 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9634.0MB reserved
2025-03-25 11:07:48,519 - training - INFO - Epoch: 485/200000, Batch: 0/45, Loss: 2.3055, Throughput: 75.17 samples/sec
2025-03-25 11:08:00,448 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9484.0MB reserved
2025-03-25 11:08:00,448 - training - INFO - Epoch: 485/200000, Batch: 15/45, Loss: 2.2305, Throughput: 70.70 samples/sec
2025-03-25 11:08:12,092 - training - INFO - Memory: GPU 0: 3556.7MB allocated, 9484.0MB reserved
2025-03-25 11:08:12,093 - training - INFO - Epoch: 485/200000, Batch: 30/45, Loss: 2.1912, Throughput: 71.39 samples/sec
2025-03-25 11:08:23,102 - training - INFO - Epoch 485 completed in 35.33s. Average loss: 2.2125
2025-03-25 11:08:23,107 - training - INFO - Starting epoch 486/200000
2025-03-25 11:08:23,851 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9620.0MB reserved
2025-03-25 11:08:23,851 - training - INFO - Epoch: 486/200000, Batch: 0/45, Loss: 2.6355, Throughput: 75.39 samples/sec
2025-03-25 11:08:35,786 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9498.0MB reserved
2025-03-25 11:08:35,787 - training - INFO - Epoch: 486/200000, Batch: 15/45, Loss: 2.3060, Throughput: 70.67 samples/sec
2025-03-25 11:08:47,387 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9498.0MB reserved
2025-03-25 11:08:47,387 - training - INFO - Epoch: 486/200000, Batch: 30/45, Loss: 2.3113, Throughput: 71.50 samples/sec
2025-03-25 11:08:58,328 - training - INFO - Epoch 486 completed in 35.22s. Average loss: 2.2585
2025-03-25 11:08:58,332 - training - INFO - Starting epoch 487/200000
2025-03-25 11:08:59,090 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9636.0MB reserved
2025-03-25 11:08:59,090 - training - INFO - Epoch: 487/200000, Batch: 0/45, Loss: 2.2357, Throughput: 73.97 samples/sec
2025-03-25 11:09:11,026 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9482.0MB reserved
2025-03-25 11:09:11,027 - training - INFO - Epoch: 487/200000, Batch: 15/45, Loss: 2.2397, Throughput: 70.59 samples/sec
2025-03-25 11:09:22,689 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9484.0MB reserved
2025-03-25 11:09:22,689 - training - INFO - Epoch: 487/200000, Batch: 30/45, Loss: 2.2093, Throughput: 71.28 samples/sec
2025-03-25 11:09:33,609 - training - INFO - Epoch 487 completed in 35.28s. Average loss: 2.2036
2025-03-25 11:09:33,613 - training - INFO - Starting epoch 488/200000
2025-03-25 11:09:34,346 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9622.0MB reserved
2025-03-25 11:09:34,346 - training - INFO - Epoch: 488/200000, Batch: 0/45, Loss: 2.0578, Throughput: 76.44 samples/sec
2025-03-25 11:09:46,353 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9506.0MB reserved
2025-03-25 11:09:46,353 - training - INFO - Epoch: 488/200000, Batch: 15/45, Loss: 2.1060, Throughput: 70.34 samples/sec
2025-03-25 11:09:57,937 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9506.0MB reserved
2025-03-25 11:09:57,937 - training - INFO - Epoch: 488/200000, Batch: 30/45, Loss: 2.1620, Throughput: 71.38 samples/sec
2025-03-25 11:10:08,941 - training - INFO - Epoch 488 completed in 35.33s. Average loss: 2.1915
2025-03-25 11:10:08,944 - training - INFO - Starting epoch 489/200000
2025-03-25 11:10:09,706 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9644.0MB reserved
2025-03-25 11:10:09,706 - training - INFO - Epoch: 489/200000, Batch: 0/45, Loss: 1.9337, Throughput: 73.75 samples/sec
2025-03-25 11:10:21,597 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9500.0MB reserved
2025-03-25 11:10:21,598 - training - INFO - Epoch: 489/200000, Batch: 15/45, Loss: 2.2046, Throughput: 70.83 samples/sec
2025-03-25 11:10:33,212 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9502.0MB reserved
2025-03-25 11:10:33,212 - training - INFO - Epoch: 489/200000, Batch: 30/45, Loss: 2.1930, Throughput: 71.54 samples/sec
2025-03-25 11:10:44,113 - training - INFO - Epoch 489 completed in 35.17s. Average loss: 2.2227
2025-03-25 11:10:44,117 - training - INFO - Starting epoch 490/200000
2025-03-25 11:10:44,864 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9640.0MB reserved
2025-03-25 11:10:44,865 - training - INFO - Epoch: 490/200000, Batch: 0/45, Loss: 2.0196, Throughput: 75.14 samples/sec
2025-03-25 11:10:56,873 - training - INFO - Memory: GPU 0: 3563.1MB allocated, 9498.0MB reserved
2025-03-25 11:10:56,873 - training - INFO - Epoch: 490/200000, Batch: 15/45, Loss: 2.0469, Throughput: 70.25 samples/sec
2025-03-25 11:11:08,399 - training - INFO - Memory: GPU 0: 3563.1MB allocated, 9498.0MB reserved
2025-03-25 11:11:08,400 - training - INFO - Epoch: 490/200000, Batch: 30/45, Loss: 2.1503, Throughput: 71.50 samples/sec
2025-03-25 11:11:19,346 - training - INFO - Epoch 490 completed in 35.23s. Average loss: 2.1952
2025-03-25 11:11:19,350 - training - INFO - Starting epoch 491/200000
2025-03-25 11:11:20,102 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9634.0MB reserved
2025-03-25 11:11:20,102 - training - INFO - Epoch: 491/200000, Batch: 0/45, Loss: 2.2763, Throughput: 74.54 samples/sec
2025-03-25 11:11:32,044 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9510.0MB reserved
2025-03-25 11:11:32,045 - training - INFO - Epoch: 491/200000, Batch: 15/45, Loss: 2.3065, Throughput: 70.60 samples/sec
2025-03-25 11:11:43,722 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9510.0MB reserved
2025-03-25 11:11:43,787 - training - INFO - Epoch: 491/200000, Batch: 30/45, Loss: 2.1131, Throughput: 71.24 samples/sec
2025-03-25 11:11:54,750 - training - INFO - Epoch 491 completed in 35.40s. Average loss: 2.2190
2025-03-25 11:11:54,753 - training - INFO - Starting epoch 492/200000
2025-03-25 11:11:55,485 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9648.0MB reserved
2025-03-25 11:11:55,485 - training - INFO - Epoch: 492/200000, Batch: 0/45, Loss: 2.8092, Throughput: 76.60 samples/sec
2025-03-25 11:12:07,365 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9512.0MB reserved
2025-03-25 11:12:07,365 - training - INFO - Epoch: 492/200000, Batch: 15/45, Loss: 2.2402, Throughput: 71.06 samples/sec
2025-03-25 11:12:18,904 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9512.0MB reserved
2025-03-25 11:12:18,905 - training - INFO - Epoch: 492/200000, Batch: 30/45, Loss: 2.2076, Throughput: 71.89 samples/sec
2025-03-25 11:12:29,825 - training - INFO - Epoch 492 completed in 35.07s. Average loss: 2.2352
2025-03-25 11:12:29,829 - training - INFO - Starting epoch 493/200000
2025-03-25 11:12:30,582 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9650.0MB reserved
2025-03-25 11:12:30,583 - training - INFO - Epoch: 493/200000, Batch: 0/45, Loss: 2.1916, Throughput: 74.74 samples/sec
2025-03-25 11:12:42,411 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9498.0MB reserved
2025-03-25 11:12:42,411 - training - INFO - Epoch: 493/200000, Batch: 15/45, Loss: 2.3420, Throughput: 71.22 samples/sec
2025-03-25 11:12:53,954 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9498.0MB reserved
2025-03-25 11:12:53,954 - training - INFO - Epoch: 493/200000, Batch: 30/45, Loss: 2.2665, Throughput: 71.96 samples/sec
2025-03-25 11:13:04,857 - training - INFO - Epoch 493 completed in 35.03s. Average loss: 2.2226
2025-03-25 11:13:04,861 - training - INFO - Starting epoch 494/200000
2025-03-25 11:13:05,593 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9636.0MB reserved
2025-03-25 11:13:05,593 - training - INFO - Epoch: 494/200000, Batch: 0/45, Loss: 1.8002, Throughput: 76.63 samples/sec
2025-03-25 11:13:17,484 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9486.0MB reserved
2025-03-25 11:13:17,484 - training - INFO - Epoch: 494/200000, Batch: 15/45, Loss: 2.0799, Throughput: 70.99 samples/sec
2025-03-25 11:13:29,153 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9486.0MB reserved
2025-03-25 11:13:29,153 - training - INFO - Epoch: 494/200000, Batch: 30/45, Loss: 2.1171, Throughput: 71.47 samples/sec
2025-03-25 11:13:40,117 - training - INFO - Epoch 494 completed in 35.26s. Average loss: 2.1721
2025-03-25 11:13:40,121 - training - INFO - Starting epoch 495/200000
2025-03-25 11:13:40,860 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9622.0MB reserved
2025-03-25 11:13:40,860 - training - INFO - Epoch: 495/200000, Batch: 0/45, Loss: 2.3369, Throughput: 75.90 samples/sec
2025-03-25 11:13:52,823 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9508.0MB reserved
2025-03-25 11:13:52,823 - training - INFO - Epoch: 495/200000, Batch: 15/45, Loss: 2.0690, Throughput: 70.54 samples/sec
2025-03-25 11:14:04,489 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9508.0MB reserved
2025-03-25 11:14:04,489 - training - INFO - Epoch: 495/200000, Batch: 30/45, Loss: 2.0602, Throughput: 71.25 samples/sec
2025-03-25 11:14:15,471 - training - INFO - Epoch 495 completed in 35.35s. Average loss: 2.1624
2025-03-25 11:14:15,474 - training - INFO - Starting epoch 496/200000
2025-03-25 11:14:16,222 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9646.0MB reserved
2025-03-25 11:14:16,222 - training - INFO - Epoch: 496/200000, Batch: 0/45, Loss: 1.8051, Throughput: 75.02 samples/sec
2025-03-25 11:14:28,133 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9492.0MB reserved
2025-03-25 11:14:28,133 - training - INFO - Epoch: 496/200000, Batch: 15/45, Loss: 2.1717, Throughput: 70.79 samples/sec
2025-03-25 11:14:39,740 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9492.0MB reserved
2025-03-25 11:14:39,741 - training - INFO - Epoch: 496/200000, Batch: 30/45, Loss: 2.1311, Throughput: 71.55 samples/sec
2025-03-25 11:14:50,624 - training - INFO - Epoch 496 completed in 35.15s. Average loss: 2.1902
2025-03-25 11:14:50,628 - training - INFO - Starting epoch 497/200000
2025-03-25 11:14:51,387 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9632.0MB reserved
2025-03-25 11:14:51,388 - training - INFO - Epoch: 497/200000, Batch: 0/45, Loss: 2.2438, Throughput: 73.93 samples/sec
2025-03-25 11:15:03,360 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9512.0MB reserved
2025-03-25 11:15:03,361 - training - INFO - Epoch: 497/200000, Batch: 15/45, Loss: 2.0691, Throughput: 70.38 samples/sec
2025-03-25 11:15:14,945 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9512.0MB reserved
2025-03-25 11:15:14,945 - training - INFO - Epoch: 497/200000, Batch: 30/45, Loss: 2.1485, Throughput: 71.40 samples/sec
2025-03-25 11:15:25,874 - training - INFO - Epoch 497 completed in 35.25s. Average loss: 2.2007
2025-03-25 11:15:25,877 - training - INFO - Starting epoch 498/200000
2025-03-25 11:15:26,609 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9650.0MB reserved
2025-03-25 11:15:26,610 - training - INFO - Epoch: 498/200000, Batch: 0/45, Loss: 2.5154, Throughput: 76.68 samples/sec
2025-03-25 11:15:38,500 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9496.0MB reserved
2025-03-25 11:15:38,500 - training - INFO - Epoch: 498/200000, Batch: 15/45, Loss: 2.2472, Throughput: 70.99 samples/sec
2025-03-25 11:15:50,243 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9498.0MB reserved
2025-03-25 11:15:50,243 - training - INFO - Epoch: 498/200000, Batch: 30/45, Loss: 2.2724, Throughput: 71.25 samples/sec
2025-03-25 11:16:01,252 - training - INFO - Epoch 498 completed in 35.37s. Average loss: 2.2035
2025-03-25 11:16:01,255 - training - INFO - Starting epoch 499/200000
2025-03-25 11:16:02,004 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9636.0MB reserved
2025-03-25 11:16:02,005 - training - INFO - Epoch: 499/200000, Batch: 0/45, Loss: 2.4735, Throughput: 75.03 samples/sec
2025-03-25 11:16:13,989 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9516.0MB reserved
2025-03-25 11:16:13,990 - training - INFO - Epoch: 499/200000, Batch: 15/45, Loss: 2.0283, Throughput: 70.38 samples/sec
2025-03-25 11:16:25,646 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9516.0MB reserved
2025-03-25 11:16:25,647 - training - INFO - Epoch: 499/200000, Batch: 30/45, Loss: 2.0793, Throughput: 71.18 samples/sec
2025-03-25 11:16:36,560 - training - INFO - Epoch 499 completed in 35.30s. Average loss: 2.1678
2025-03-25 11:16:36,563 - training - INFO - Starting epoch 500/200000
2025-03-25 11:16:37,289 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9654.0MB reserved
2025-03-25 11:16:37,289 - training - INFO - Epoch: 500/200000, Batch: 0/45, Loss: 2.8147, Throughput: 77.27 samples/sec
2025-03-25 11:16:49,116 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9478.0MB reserved
2025-03-25 11:16:49,116 - training - INFO - Epoch: 500/200000, Batch: 15/45, Loss: 2.2360, Throughput: 71.38 samples/sec
2025-03-25 11:17:00,661 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9478.0MB reserved
2025-03-25 11:17:00,662 - training - INFO - Epoch: 500/200000, Batch: 30/45, Loss: 2.2084, Throughput: 72.04 samples/sec
2025-03-25 11:17:11,604 - training - INFO - Epoch 500 completed in 35.04s. Average loss: 2.2145
2025-03-25 11:17:11,607 - training - INFO - Starting validation...
2025-03-25 11:17:11,925 - training - INFO - Validation Loss: 20.7660
2025-03-25 11:17:11,926 - training - INFO - Validation loss did not improve. Counter: 4/10
2025-03-25 11:17:12,240 - training - INFO - Starting epoch 501/200000
2025-03-25 11:17:13,026 - training - INFO - Memory: GPU 0: 3565.8MB allocated, 8742.0MB reserved
2025-03-25 11:17:13,026 - training - INFO - Epoch: 501/200000, Batch: 0/45, Loss: 2.1387, Throughput: 71.48 samples/sec
2025-03-25 11:17:24,813 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9470.0MB reserved
2025-03-25 11:17:24,813 - training - INFO - Epoch: 501/200000, Batch: 15/45, Loss: 2.1819, Throughput: 71.27 samples/sec
2025-03-25 11:17:36,397 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9470.0MB reserved
2025-03-25 11:17:36,397 - training - INFO - Epoch: 501/200000, Batch: 30/45, Loss: 2.1357, Throughput: 71.87 samples/sec
2025-03-25 11:17:47,317 - training - INFO - Epoch 501 completed in 35.08s. Average loss: 2.1928
2025-03-25 11:17:47,321 - training - INFO - Starting epoch 502/200000
2025-03-25 11:17:48,066 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9608.0MB reserved
2025-03-25 11:17:48,066 - training - INFO - Epoch: 502/200000, Batch: 0/45, Loss: 2.4576, Throughput: 75.33 samples/sec
2025-03-25 11:17:59,949 - training - INFO - Memory: GPU 0: 3563.2MB allocated, 9488.0MB reserved
2025-03-25 11:17:59,950 - training - INFO - Epoch: 502/200000, Batch: 15/45, Loss: 2.3865, Throughput: 70.96 samples/sec
2025-03-25 11:18:11,557 - training - INFO - Memory: GPU 0: 3563.2MB allocated, 9488.0MB reserved
2025-03-25 11:18:11,557 - training - INFO - Epoch: 502/200000, Batch: 30/45, Loss: 2.3394, Throughput: 71.63 samples/sec
2025-03-25 11:18:22,520 - training - INFO - Epoch 502 completed in 35.20s. Average loss: 2.2581
2025-03-25 11:18:22,523 - training - INFO - Starting epoch 503/200000
2025-03-25 11:18:23,288 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9626.0MB reserved
2025-03-25 11:18:23,289 - training - INFO - Epoch: 503/200000, Batch: 0/45, Loss: 1.6153, Throughput: 73.34 samples/sec
2025-03-25 11:18:35,297 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9500.0MB reserved
2025-03-25 11:18:35,297 - training - INFO - Epoch: 503/200000, Batch: 15/45, Loss: 2.1711, Throughput: 70.16 samples/sec
2025-03-25 11:18:46,993 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9500.0MB reserved
2025-03-25 11:18:46,994 - training - INFO - Epoch: 503/200000, Batch: 30/45, Loss: 2.1051, Throughput: 70.95 samples/sec
2025-03-25 11:18:58,054 - training - INFO - Epoch 503 completed in 35.53s. Average loss: 2.1874
2025-03-25 11:18:58,057 - training - INFO - Starting epoch 504/200000
2025-03-25 11:18:58,785 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9636.0MB reserved
2025-03-25 11:18:58,786 - training - INFO - Epoch: 504/200000, Batch: 0/45, Loss: 1.5631, Throughput: 77.11 samples/sec
2025-03-25 11:19:10,682 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9400.0MB reserved
2025-03-25 11:19:10,682 - training - INFO - Epoch: 504/200000, Batch: 15/45, Loss: 2.1087, Throughput: 70.99 samples/sec
2025-03-25 11:19:22,302 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9400.0MB reserved
2025-03-25 11:19:22,303 - training - INFO - Epoch: 504/200000, Batch: 30/45, Loss: 2.1064, Throughput: 71.61 samples/sec
2025-03-25 11:19:33,245 - training - INFO - Epoch 504 completed in 35.19s. Average loss: 2.1693
2025-03-25 11:19:33,249 - training - INFO - Starting epoch 505/200000
2025-03-25 11:19:33,982 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9538.0MB reserved
2025-03-25 11:19:33,982 - training - INFO - Epoch: 505/200000, Batch: 0/45, Loss: 2.2998, Throughput: 76.59 samples/sec
2025-03-25 11:19:45,870 - training - INFO - Memory: GPU 0: 3556.9MB allocated, 9404.0MB reserved
2025-03-25 11:19:45,871 - training - INFO - Epoch: 505/200000, Batch: 15/45, Loss: 2.1102, Throughput: 71.00 samples/sec
2025-03-25 11:19:57,491 - training - INFO - Memory: GPU 0: 3556.9MB allocated, 9406.0MB reserved
2025-03-25 11:19:57,491 - training - INFO - Epoch: 505/200000, Batch: 30/45, Loss: 2.1175, Throughput: 71.61 samples/sec
2025-03-25 11:20:08,455 - training - INFO - Epoch 505 completed in 35.21s. Average loss: 2.1727
2025-03-25 11:20:08,459 - training - INFO - Starting epoch 506/200000
2025-03-25 11:20:09,199 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9544.0MB reserved
2025-03-25 11:20:09,200 - training - INFO - Epoch: 506/200000, Batch: 0/45, Loss: 1.6942, Throughput: 75.70 samples/sec
2025-03-25 11:20:21,142 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9504.0MB reserved
2025-03-25 11:20:21,143 - training - INFO - Epoch: 506/200000, Batch: 15/45, Loss: 2.1855, Throughput: 70.65 samples/sec
2025-03-25 11:20:32,709 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9504.0MB reserved
2025-03-25 11:20:32,709 - training - INFO - Epoch: 506/200000, Batch: 30/45, Loss: 2.1673, Throughput: 71.59 samples/sec
2025-03-25 11:20:43,625 - training - INFO - Epoch 506 completed in 35.17s. Average loss: 2.2007
2025-03-25 11:20:43,629 - training - INFO - Starting epoch 507/200000
2025-03-25 11:20:44,374 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9642.0MB reserved
2025-03-25 11:20:44,374 - training - INFO - Epoch: 507/200000, Batch: 0/45, Loss: 2.2554, Throughput: 75.17 samples/sec
2025-03-25 11:20:56,216 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9498.0MB reserved
2025-03-25 11:20:56,216 - training - INFO - Epoch: 507/200000, Batch: 15/45, Loss: 2.1563, Throughput: 71.19 samples/sec
2025-03-25 11:21:07,721 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9498.0MB reserved
2025-03-25 11:21:07,721 - training - INFO - Epoch: 507/200000, Batch: 30/45, Loss: 2.1764, Throughput: 72.06 samples/sec
2025-03-25 11:21:18,614 - training - INFO - Epoch 507 completed in 34.99s. Average loss: 2.1813
2025-03-25 11:21:18,618 - training - INFO - Starting epoch 508/200000
2025-03-25 11:21:19,354 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9638.0MB reserved
2025-03-25 11:21:19,355 - training - INFO - Epoch: 508/200000, Batch: 0/45, Loss: 2.2865, Throughput: 76.20 samples/sec
2025-03-25 11:21:31,332 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9500.0MB reserved
2025-03-25 11:21:31,332 - training - INFO - Epoch: 508/200000, Batch: 15/45, Loss: 2.1624, Throughput: 70.48 samples/sec
2025-03-25 11:21:42,913 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9502.0MB reserved
2025-03-25 11:21:42,914 - training - INFO - Epoch: 508/200000, Batch: 30/45, Loss: 2.1470, Throughput: 71.46 samples/sec
2025-03-25 11:21:53,854 - training - INFO - Epoch 508 completed in 35.24s. Average loss: 2.1520
2025-03-25 11:21:53,857 - training - INFO - Starting epoch 509/200000
2025-03-25 11:21:54,594 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9638.0MB reserved
2025-03-25 11:21:54,595 - training - INFO - Epoch: 509/200000, Batch: 0/45, Loss: 1.7714, Throughput: 76.07 samples/sec
2025-03-25 11:22:06,519 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9498.0MB reserved
2025-03-25 11:22:06,519 - training - INFO - Epoch: 509/200000, Batch: 15/45, Loss: 2.0664, Throughput: 70.77 samples/sec
2025-03-25 11:22:18,194 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9500.0MB reserved
2025-03-25 11:22:18,195 - training - INFO - Epoch: 509/200000, Batch: 30/45, Loss: 2.0997, Throughput: 71.34 samples/sec
2025-03-25 11:22:29,165 - training - INFO - Epoch 509 completed in 35.31s. Average loss: 2.2003
2025-03-25 11:22:29,169 - training - INFO - Starting epoch 510/200000
2025-03-25 11:22:29,904 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9638.0MB reserved
2025-03-25 11:22:29,904 - training - INFO - Epoch: 510/200000, Batch: 0/45, Loss: 2.1898, Throughput: 76.28 samples/sec
2025-03-25 11:22:41,833 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9506.0MB reserved
2025-03-25 11:22:41,833 - training - INFO - Epoch: 510/200000, Batch: 15/45, Loss: 2.1457, Throughput: 70.76 samples/sec
2025-03-25 11:22:53,493 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9508.0MB reserved
2025-03-25 11:22:53,493 - training - INFO - Epoch: 510/200000, Batch: 30/45, Loss: 2.1634, Throughput: 71.37 samples/sec
2025-03-25 11:23:04,445 - training - INFO - Epoch 510 completed in 35.28s. Average loss: 2.2251
2025-03-25 11:23:04,449 - training - INFO - Starting epoch 511/200000
2025-03-25 11:23:05,191 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9646.0MB reserved
2025-03-25 11:23:05,191 - training - INFO - Epoch: 511/200000, Batch: 0/45, Loss: 1.9515, Throughput: 75.62 samples/sec
2025-03-25 11:23:17,108 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9494.0MB reserved
2025-03-25 11:23:17,109 - training - INFO - Epoch: 511/200000, Batch: 15/45, Loss: 2.1871, Throughput: 70.79 samples/sec
2025-03-25 11:23:28,743 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9494.0MB reserved
2025-03-25 11:23:28,743 - training - INFO - Epoch: 511/200000, Batch: 30/45, Loss: 2.1982, Throughput: 71.46 samples/sec
2025-03-25 11:23:39,656 - training - INFO - Epoch 511 completed in 35.21s. Average loss: 2.2251
2025-03-25 11:23:39,660 - training - INFO - Starting epoch 512/200000
2025-03-25 11:23:40,390 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9632.0MB reserved
2025-03-25 11:23:40,390 - training - INFO - Epoch: 512/200000, Batch: 0/45, Loss: 1.8550, Throughput: 76.76 samples/sec
2025-03-25 11:23:52,272 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9510.0MB reserved
2025-03-25 11:23:52,272 - training - INFO - Epoch: 512/200000, Batch: 15/45, Loss: 2.1345, Throughput: 71.05 samples/sec
2025-03-25 11:24:03,921 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9510.0MB reserved
2025-03-25 11:24:03,922 - training - INFO - Epoch: 512/200000, Batch: 30/45, Loss: 2.1821, Throughput: 71.56 samples/sec
2025-03-25 11:24:14,959 - training - INFO - Epoch 512 completed in 35.30s. Average loss: 2.1785
2025-03-25 11:24:14,962 - training - INFO - Starting epoch 513/200000
2025-03-25 11:24:15,699 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9650.0MB reserved
2025-03-25 11:24:15,699 - training - INFO - Epoch: 513/200000, Batch: 0/45, Loss: 2.5732, Throughput: 76.31 samples/sec
2025-03-25 11:24:27,520 - training - INFO - Memory: GPU 0: 3556.9MB allocated, 9498.0MB reserved
2025-03-25 11:24:27,520 - training - INFO - Epoch: 513/200000, Batch: 15/45, Loss: 2.1653, Throughput: 71.36 samples/sec
2025-03-25 11:24:39,111 - training - INFO - Memory: GPU 0: 3556.9MB allocated, 9498.0MB reserved
2025-03-25 11:24:39,112 - training - INFO - Epoch: 513/200000, Batch: 30/45, Loss: 2.1358, Throughput: 71.89 samples/sec
2025-03-25 11:24:50,027 - training - INFO - Epoch 513 completed in 35.06s. Average loss: 2.1783
2025-03-25 11:24:50,031 - training - INFO - Starting epoch 514/200000
2025-03-25 11:24:50,769 - training - INFO - Memory: GPU 0: 3556.9MB allocated, 9636.0MB reserved
2025-03-25 11:24:50,770 - training - INFO - Epoch: 514/200000, Batch: 0/45, Loss: 1.9546, Throughput: 75.99 samples/sec
2025-03-25 11:25:02,696 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9490.0MB reserved
2025-03-25 11:25:02,697 - training - INFO - Epoch: 514/200000, Batch: 15/45, Loss: 2.0692, Throughput: 70.76 samples/sec
2025-03-25 11:25:14,320 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9490.0MB reserved
2025-03-25 11:25:14,320 - training - INFO - Epoch: 514/200000, Batch: 30/45, Loss: 2.0808, Throughput: 71.48 samples/sec
2025-03-25 11:25:25,293 - training - INFO - Epoch 514 completed in 35.26s. Average loss: 2.1873
2025-03-25 11:25:25,297 - training - INFO - Starting epoch 515/200000
2025-03-25 11:25:26,030 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9630.0MB reserved
2025-03-25 11:25:26,031 - training - INFO - Epoch: 515/200000, Batch: 0/45, Loss: 2.2994, Throughput: 76.48 samples/sec
2025-03-25 11:25:37,847 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9472.0MB reserved
2025-03-25 11:25:37,848 - training - INFO - Epoch: 515/200000, Batch: 15/45, Loss: 2.1656, Throughput: 71.40 samples/sec
2025-03-25 11:25:49,467 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9474.0MB reserved
2025-03-25 11:25:49,468 - training - INFO - Epoch: 515/200000, Batch: 30/45, Loss: 2.1467, Throughput: 71.82 samples/sec
2025-03-25 11:26:00,484 - training - INFO - Epoch 515 completed in 35.19s. Average loss: 2.1675
2025-03-25 11:26:00,491 - training - INFO - Starting epoch 516/200000
2025-03-25 11:26:01,244 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9610.0MB reserved
2025-03-25 11:26:01,244 - training - INFO - Epoch: 516/200000, Batch: 0/45, Loss: 1.9163, Throughput: 74.55 samples/sec
2025-03-25 11:26:13,175 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9504.0MB reserved
2025-03-25 11:26:13,176 - training - INFO - Epoch: 516/200000, Batch: 15/45, Loss: 2.2318, Throughput: 70.65 samples/sec
2025-03-25 11:26:24,796 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9504.0MB reserved
2025-03-25 11:26:24,796 - training - INFO - Epoch: 516/200000, Batch: 30/45, Loss: 2.1988, Throughput: 71.43 samples/sec
2025-03-25 11:26:35,736 - training - INFO - Epoch 516 completed in 35.25s. Average loss: 2.1939
2025-03-25 11:26:35,740 - training - INFO - Starting epoch 517/200000
2025-03-25 11:26:36,502 - training - INFO - Memory: GPU 0: 3556.4MB allocated, 9642.0MB reserved
2025-03-25 11:26:36,502 - training - INFO - Epoch: 517/200000, Batch: 0/45, Loss: 2.3161, Throughput: 73.58 samples/sec
2025-03-25 11:26:48,540 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9494.0MB reserved
2025-03-25 11:26:48,540 - training - INFO - Epoch: 517/200000, Batch: 15/45, Loss: 2.1914, Throughput: 70.01 samples/sec
2025-03-25 11:27:00,210 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9494.0MB reserved
2025-03-25 11:27:00,211 - training - INFO - Epoch: 517/200000, Batch: 30/45, Loss: 2.1177, Throughput: 70.95 samples/sec
2025-03-25 11:27:11,167 - training - INFO - Epoch 517 completed in 35.43s. Average loss: 2.1743
2025-03-25 11:27:11,171 - training - INFO - Starting epoch 518/200000
2025-03-25 11:27:11,912 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9632.0MB reserved
2025-03-25 11:27:11,912 - training - INFO - Epoch: 518/200000, Batch: 0/45, Loss: 1.7582, Throughput: 75.83 samples/sec
2025-03-25 11:27:23,842 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9500.0MB reserved
2025-03-25 11:27:23,842 - training - INFO - Epoch: 518/200000, Batch: 15/45, Loss: 2.0558, Throughput: 70.73 samples/sec
2025-03-25 11:27:35,525 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9500.0MB reserved
2025-03-25 11:27:35,525 - training - INFO - Epoch: 518/200000, Batch: 30/45, Loss: 2.1075, Throughput: 71.29 samples/sec
2025-03-25 11:27:46,510 - training - INFO - Epoch 518 completed in 35.34s. Average loss: 2.1394
2025-03-25 11:27:46,515 - training - INFO - Starting epoch 519/200000
2025-03-25 11:27:47,261 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9638.0MB reserved
2025-03-25 11:27:47,261 - training - INFO - Epoch: 519/200000, Batch: 0/45, Loss: 1.8489, Throughput: 75.08 samples/sec
2025-03-25 11:27:59,198 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9494.0MB reserved
2025-03-25 11:27:59,198 - training - INFO - Epoch: 519/200000, Batch: 15/45, Loss: 2.1007, Throughput: 70.66 samples/sec
2025-03-25 11:28:10,811 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9494.0MB reserved
2025-03-25 11:28:10,812 - training - INFO - Epoch: 519/200000, Batch: 30/45, Loss: 2.1266, Throughput: 71.46 samples/sec
2025-03-25 11:28:21,748 - training - INFO - Epoch 519 completed in 35.23s. Average loss: 2.1871
2025-03-25 11:28:21,752 - training - INFO - Starting epoch 520/200000
2025-03-25 11:28:22,499 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9632.0MB reserved
2025-03-25 11:28:22,499 - training - INFO - Epoch: 520/200000, Batch: 0/45, Loss: 1.9331, Throughput: 75.11 samples/sec
2025-03-25 11:28:34,452 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9390.0MB reserved
2025-03-25 11:28:34,453 - training - INFO - Epoch: 520/200000, Batch: 15/45, Loss: 2.1182, Throughput: 70.56 samples/sec
2025-03-25 11:28:46,174 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9392.0MB reserved
2025-03-25 11:28:46,175 - training - INFO - Epoch: 520/200000, Batch: 30/45, Loss: 2.0994, Throughput: 71.09 samples/sec
2025-03-25 11:28:57,153 - training - INFO - Epoch 520 completed in 35.40s. Average loss: 2.1516
2025-03-25 11:28:57,157 - training - INFO - Starting epoch 521/200000
2025-03-25 11:28:57,914 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9636.0MB reserved
2025-03-25 11:28:57,915 - training - INFO - Epoch: 521/200000, Batch: 0/45, Loss: 2.3829, Throughput: 74.15 samples/sec
2025-03-25 11:29:09,780 - training - INFO - Memory: GPU 0: 3555.5MB allocated, 9488.0MB reserved
2025-03-25 11:29:09,780 - training - INFO - Epoch: 521/200000, Batch: 15/45, Loss: 2.0763, Throughput: 71.00 samples/sec
2025-03-25 11:29:21,324 - training - INFO - Memory: GPU 0: 3555.5MB allocated, 9490.0MB reserved
2025-03-25 11:29:21,324 - training - INFO - Epoch: 521/200000, Batch: 30/45, Loss: 2.0549, Throughput: 71.84 samples/sec
2025-03-25 11:29:32,248 - training - INFO - Epoch 521 completed in 35.09s. Average loss: 2.1735
2025-03-25 11:29:32,252 - training - INFO - Starting epoch 522/200000
2025-03-25 11:29:33,000 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9628.0MB reserved
2025-03-25 11:29:33,000 - training - INFO - Epoch: 522/200000, Batch: 0/45, Loss: 1.8670, Throughput: 75.10 samples/sec
2025-03-25 11:29:44,913 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9492.0MB reserved
2025-03-25 11:29:44,914 - training - INFO - Epoch: 522/200000, Batch: 15/45, Loss: 2.1004, Throughput: 70.79 samples/sec
2025-03-25 11:29:56,552 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9492.0MB reserved
2025-03-25 11:29:56,553 - training - INFO - Epoch: 522/200000, Batch: 30/45, Loss: 2.1858, Throughput: 71.45 samples/sec
2025-03-25 11:30:07,516 - training - INFO - Epoch 522 completed in 35.26s. Average loss: 2.1378
2025-03-25 11:30:07,520 - training - INFO - Starting epoch 523/200000
2025-03-25 11:30:08,289 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9630.0MB reserved
2025-03-25 11:30:08,289 - training - INFO - Epoch: 523/200000, Batch: 0/45, Loss: 1.9211, Throughput: 73.10 samples/sec
2025-03-25 11:30:20,187 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9504.0MB reserved
2025-03-25 11:30:20,187 - training - INFO - Epoch: 523/200000, Batch: 15/45, Loss: 2.0981, Throughput: 70.75 samples/sec
2025-03-25 11:30:31,748 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9504.0MB reserved
2025-03-25 11:30:31,748 - training - INFO - Epoch: 523/200000, Batch: 30/45, Loss: 2.1038, Throughput: 71.66 samples/sec
2025-03-25 11:30:42,770 - training - INFO - Epoch 523 completed in 35.25s. Average loss: 2.1406
2025-03-25 11:30:42,774 - training - INFO - Starting epoch 524/200000
2025-03-25 11:30:43,528 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9642.0MB reserved
2025-03-25 11:30:43,528 - training - INFO - Epoch: 524/200000, Batch: 0/45, Loss: 1.6970, Throughput: 74.58 samples/sec
2025-03-25 11:30:55,439 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9494.0MB reserved
2025-03-25 11:30:55,439 - training - INFO - Epoch: 524/200000, Batch: 15/45, Loss: 1.9685, Throughput: 70.76 samples/sec
2025-03-25 11:31:07,044 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9496.0MB reserved
2025-03-25 11:31:07,045 - training - INFO - Epoch: 524/200000, Batch: 30/45, Loss: 2.0794, Throughput: 71.54 samples/sec
2025-03-25 11:31:18,042 - training - INFO - Epoch 524 completed in 35.27s. Average loss: 2.1309
2025-03-25 11:31:18,046 - training - INFO - Starting epoch 525/200000
2025-03-25 11:31:18,782 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9634.0MB reserved
2025-03-25 11:31:18,782 - training - INFO - Epoch: 525/200000, Batch: 0/45, Loss: 1.9198, Throughput: 76.27 samples/sec
2025-03-25 11:31:30,705 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9492.0MB reserved
2025-03-25 11:31:30,706 - training - INFO - Epoch: 525/200000, Batch: 15/45, Loss: 2.0260, Throughput: 70.79 samples/sec
2025-03-25 11:31:42,388 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9492.0MB reserved
2025-03-25 11:31:42,389 - training - INFO - Epoch: 525/200000, Batch: 30/45, Loss: 2.1426, Throughput: 71.32 samples/sec
2025-03-25 11:31:53,406 - training - INFO - Epoch 525 completed in 35.36s. Average loss: 2.1734
2025-03-25 11:31:53,410 - training - INFO - Starting epoch 526/200000
2025-03-25 11:31:54,144 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9630.0MB reserved
2025-03-25 11:31:54,144 - training - INFO - Epoch: 526/200000, Batch: 0/45, Loss: 2.1262, Throughput: 76.53 samples/sec
2025-03-25 11:32:06,046 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9498.0MB reserved
2025-03-25 11:32:06,046 - training - INFO - Epoch: 526/200000, Batch: 15/45, Loss: 2.1947, Throughput: 70.92 samples/sec
2025-03-25 11:32:17,626 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9498.0MB reserved
2025-03-25 11:32:17,626 - training - INFO - Epoch: 526/200000, Batch: 30/45, Loss: 2.0867, Throughput: 71.69 samples/sec
2025-03-25 11:32:28,591 - training - INFO - Epoch 526 completed in 35.18s. Average loss: 2.1618
2025-03-25 11:32:28,595 - training - INFO - Starting epoch 527/200000
2025-03-25 11:32:29,324 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9636.0MB reserved
2025-03-25 11:32:29,324 - training - INFO - Epoch: 527/200000, Batch: 0/45, Loss: 1.4923, Throughput: 76.97 samples/sec
2025-03-25 11:32:41,192 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9500.0MB reserved
2025-03-25 11:32:41,192 - training - INFO - Epoch: 527/200000, Batch: 15/45, Loss: 2.2443, Throughput: 71.13 samples/sec
2025-03-25 11:32:52,791 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9500.0MB reserved
2025-03-25 11:32:52,791 - training - INFO - Epoch: 527/200000, Batch: 30/45, Loss: 2.1438, Throughput: 71.75 samples/sec
2025-03-25 11:33:03,749 - training - INFO - Epoch 527 completed in 35.15s. Average loss: 2.1967
2025-03-25 11:33:03,753 - training - INFO - Starting epoch 528/200000
2025-03-25 11:33:04,479 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9636.0MB reserved
2025-03-25 11:33:04,479 - training - INFO - Epoch: 528/200000, Batch: 0/45, Loss: 2.1273, Throughput: 77.40 samples/sec
2025-03-25 11:33:16,432 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9488.0MB reserved
2025-03-25 11:33:16,432 - training - INFO - Epoch: 528/200000, Batch: 15/45, Loss: 2.1494, Throughput: 70.67 samples/sec
2025-03-25 11:33:28,086 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9490.0MB reserved
2025-03-25 11:33:28,086 - training - INFO - Epoch: 528/200000, Batch: 30/45, Loss: 2.1633, Throughput: 71.35 samples/sec
2025-03-25 11:33:39,055 - training - INFO - Epoch 528 completed in 35.30s. Average loss: 2.1755
2025-03-25 11:33:39,059 - training - INFO - Starting epoch 529/200000
2025-03-25 11:33:39,786 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9628.0MB reserved
2025-03-25 11:33:39,786 - training - INFO - Epoch: 529/200000, Batch: 0/45, Loss: 1.4716, Throughput: 77.18 samples/sec
2025-03-25 11:33:51,731 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9500.0MB reserved
2025-03-25 11:33:51,731 - training - INFO - Epoch: 529/200000, Batch: 15/45, Loss: 2.2086, Throughput: 70.72 samples/sec
2025-03-25 11:34:03,336 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9500.0MB reserved
2025-03-25 11:34:03,336 - training - INFO - Epoch: 529/200000, Batch: 30/45, Loss: 2.1427, Throughput: 71.51 samples/sec
2025-03-25 11:34:14,263 - training - INFO - Epoch 529 completed in 35.20s. Average loss: 2.1570
2025-03-25 11:34:14,268 - training - INFO - Starting epoch 530/200000
2025-03-25 11:34:15,004 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9638.0MB reserved
2025-03-25 11:34:15,005 - training - INFO - Epoch: 530/200000, Batch: 0/45, Loss: 1.9493, Throughput: 76.34 samples/sec
2025-03-25 11:34:26,863 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9498.0MB reserved
2025-03-25 11:34:26,863 - training - INFO - Epoch: 530/200000, Batch: 15/45, Loss: 2.2111, Throughput: 71.14 samples/sec
2025-03-25 11:34:38,429 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9498.0MB reserved
2025-03-25 11:34:38,430 - training - INFO - Epoch: 530/200000, Batch: 30/45, Loss: 2.1391, Throughput: 71.86 samples/sec
2025-03-25 11:34:49,352 - training - INFO - Epoch 530 completed in 35.08s. Average loss: 2.1449
2025-03-25 11:34:49,355 - training - INFO - Starting epoch 531/200000
2025-03-25 11:34:50,114 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9638.0MB reserved
2025-03-25 11:34:50,114 - training - INFO - Epoch: 531/200000, Batch: 0/45, Loss: 2.3162, Throughput: 73.96 samples/sec
2025-03-25 11:35:01,982 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9498.0MB reserved
2025-03-25 11:35:01,982 - training - INFO - Epoch: 531/200000, Batch: 15/45, Loss: 2.1570, Throughput: 70.98 samples/sec
2025-03-25 11:35:13,551 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9498.0MB reserved
2025-03-25 11:35:13,551 - training - INFO - Epoch: 531/200000, Batch: 30/45, Loss: 2.1526, Throughput: 71.75 samples/sec
2025-03-25 11:35:24,502 - training - INFO - Epoch 531 completed in 35.15s. Average loss: 2.1533
2025-03-25 11:35:24,505 - training - INFO - Starting epoch 532/200000
2025-03-25 11:35:25,244 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9636.0MB reserved
2025-03-25 11:35:25,245 - training - INFO - Epoch: 532/200000, Batch: 0/45, Loss: 1.9386, Throughput: 75.94 samples/sec
2025-03-25 11:35:37,183 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9498.0MB reserved
2025-03-25 11:35:37,184 - training - INFO - Epoch: 532/200000, Batch: 15/45, Loss: 2.1278, Throughput: 70.68 samples/sec
2025-03-25 11:35:48,805 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9498.0MB reserved
2025-03-25 11:35:48,805 - training - INFO - Epoch: 532/200000, Batch: 30/45, Loss: 2.1100, Throughput: 71.44 samples/sec
2025-03-25 11:35:59,758 - training - INFO - Epoch 532 completed in 35.25s. Average loss: 2.1540
2025-03-25 11:35:59,762 - training - INFO - Starting epoch 533/200000
2025-03-25 11:36:00,508 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9636.0MB reserved
2025-03-25 11:36:00,508 - training - INFO - Epoch: 533/200000, Batch: 0/45, Loss: 2.0881, Throughput: 75.21 samples/sec
2025-03-25 11:36:12,492 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9510.0MB reserved
2025-03-25 11:36:12,493 - training - INFO - Epoch: 533/200000, Batch: 15/45, Loss: 2.1488, Throughput: 70.40 samples/sec
2025-03-25 11:36:24,084 - training - INFO - Memory: GPU 0: 3556.9MB allocated, 9510.0MB reserved
2025-03-25 11:36:24,085 - training - INFO - Epoch: 533/200000, Batch: 30/45, Loss: 2.1523, Throughput: 71.38 samples/sec
2025-03-25 11:36:35,066 - training - INFO - Epoch 533 completed in 35.30s. Average loss: 2.2547
2025-03-25 11:36:35,070 - training - INFO - Starting epoch 534/200000
2025-03-25 11:36:35,814 - training - INFO - Memory: GPU 0: 3556.9MB allocated, 9646.0MB reserved
2025-03-25 11:36:35,814 - training - INFO - Epoch: 534/200000, Batch: 0/45, Loss: 2.9501, Throughput: 75.46 samples/sec
2025-03-25 11:36:47,652 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9502.0MB reserved
2025-03-25 11:36:47,653 - training - INFO - Epoch: 534/200000, Batch: 15/45, Loss: 2.3043, Throughput: 71.23 samples/sec
2025-03-25 11:36:59,339 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9502.0MB reserved
2025-03-25 11:36:59,339 - training - INFO - Epoch: 534/200000, Batch: 30/45, Loss: 2.2211, Throughput: 71.54 samples/sec
2025-03-25 11:37:10,276 - training - INFO - Epoch 534 completed in 35.21s. Average loss: 2.2215
2025-03-25 11:37:10,280 - training - INFO - Starting epoch 535/200000
2025-03-25 11:37:11,033 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9640.0MB reserved
2025-03-25 11:37:11,034 - training - INFO - Epoch: 535/200000, Batch: 0/45, Loss: 1.8749, Throughput: 74.45 samples/sec
2025-03-25 11:37:22,909 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9504.0MB reserved
2025-03-25 11:37:22,909 - training - INFO - Epoch: 535/200000, Batch: 15/45, Loss: 2.0289, Throughput: 70.96 samples/sec
2025-03-25 11:37:34,463 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9504.0MB reserved
2025-03-25 11:37:34,463 - training - INFO - Epoch: 535/200000, Batch: 30/45, Loss: 2.0195, Throughput: 71.79 samples/sec
2025-03-25 11:37:45,370 - training - INFO - Epoch 535 completed in 35.09s. Average loss: 2.1349
2025-03-25 11:37:45,374 - training - INFO - Starting epoch 536/200000
2025-03-25 11:37:46,128 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9642.0MB reserved
2025-03-25 11:37:46,128 - training - INFO - Epoch: 536/200000, Batch: 0/45, Loss: 2.2470, Throughput: 74.33 samples/sec
2025-03-25 11:37:58,189 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9514.0MB reserved
2025-03-25 11:37:58,190 - training - INFO - Epoch: 536/200000, Batch: 15/45, Loss: 2.1878, Throughput: 69.93 samples/sec
2025-03-25 11:38:09,916 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9514.0MB reserved
2025-03-25 11:38:09,916 - training - INFO - Epoch: 536/200000, Batch: 30/45, Loss: 2.1211, Throughput: 70.74 samples/sec
2025-03-25 11:38:20,960 - training - INFO - Epoch 536 completed in 35.59s. Average loss: 2.1923
2025-03-25 11:38:20,964 - training - INFO - Starting epoch 537/200000
2025-03-25 11:38:21,712 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9652.0MB reserved
2025-03-25 11:38:21,712 - training - INFO - Epoch: 537/200000, Batch: 0/45, Loss: 2.1705, Throughput: 74.90 samples/sec
2025-03-25 11:38:33,590 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9510.0MB reserved
2025-03-25 11:38:33,590 - training - INFO - Epoch: 537/200000, Batch: 15/45, Loss: 2.0695, Throughput: 70.97 samples/sec
2025-03-25 11:38:45,231 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9510.0MB reserved
2025-03-25 11:38:45,231 - training - INFO - Epoch: 537/200000, Batch: 30/45, Loss: 2.0958, Throughput: 71.54 samples/sec
2025-03-25 11:38:56,173 - training - INFO - Epoch 537 completed in 35.21s. Average loss: 2.0959
2025-03-25 11:38:56,177 - training - INFO - Starting epoch 538/200000
2025-03-25 11:38:56,917 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9648.0MB reserved
2025-03-25 11:38:56,918 - training - INFO - Epoch: 538/200000, Batch: 0/45, Loss: 1.6872, Throughput: 75.86 samples/sec
2025-03-25 11:39:08,828 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9506.0MB reserved
2025-03-25 11:39:08,828 - training - INFO - Epoch: 538/200000, Batch: 15/45, Loss: 2.0796, Throughput: 70.84 samples/sec
2025-03-25 11:39:20,404 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9506.0MB reserved
2025-03-25 11:39:20,404 - training - INFO - Epoch: 538/200000, Batch: 30/45, Loss: 2.1224, Throughput: 71.66 samples/sec
2025-03-25 11:39:31,400 - training - INFO - Epoch 538 completed in 35.22s. Average loss: 2.1341
2025-03-25 11:39:31,403 - training - INFO - Starting epoch 539/200000
2025-03-25 11:39:32,139 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9644.0MB reserved
2025-03-25 11:39:32,140 - training - INFO - Epoch: 539/200000, Batch: 0/45, Loss: 2.1612, Throughput: 76.28 samples/sec
2025-03-25 11:39:43,972 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9506.0MB reserved
2025-03-25 11:39:43,972 - training - INFO - Epoch: 539/200000, Batch: 15/45, Loss: 2.0580, Throughput: 71.30 samples/sec
2025-03-25 11:39:55,586 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9506.0MB reserved
2025-03-25 11:39:55,586 - training - INFO - Epoch: 539/200000, Batch: 30/45, Loss: 2.1055, Throughput: 71.80 samples/sec
2025-03-25 11:40:06,502 - training - INFO - Epoch 539 completed in 35.10s. Average loss: 2.1376
2025-03-25 11:40:06,506 - training - INFO - Starting epoch 540/200000
2025-03-25 11:40:07,254 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9642.0MB reserved
2025-03-25 11:40:07,255 - training - INFO - Epoch: 540/200000, Batch: 0/45, Loss: 1.5475, Throughput: 75.00 samples/sec
2025-03-25 11:40:19,214 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9506.0MB reserved
2025-03-25 11:40:19,215 - training - INFO - Epoch: 540/200000, Batch: 15/45, Loss: 2.2122, Throughput: 70.51 samples/sec
2025-03-25 11:40:30,858 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9508.0MB reserved
2025-03-25 11:40:30,858 - training - INFO - Epoch: 540/200000, Batch: 30/45, Loss: 2.1230, Throughput: 71.29 samples/sec
2025-03-25 11:40:41,837 - training - INFO - Epoch 540 completed in 35.33s. Average loss: 2.1150
2025-03-25 11:40:41,841 - training - INFO - Starting epoch 541/200000
2025-03-25 11:40:42,581 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9646.0MB reserved
2025-03-25 11:40:42,581 - training - INFO - Epoch: 541/200000, Batch: 0/45, Loss: 1.9174, Throughput: 75.72 samples/sec
2025-03-25 11:40:54,578 - training - INFO - Memory: GPU 0: 3556.6MB allocated, 9502.0MB reserved
2025-03-25 11:40:54,583 - training - INFO - Epoch: 541/200000, Batch: 15/45, Loss: 2.1832, Throughput: 70.35 samples/sec
2025-03-25 11:41:06,255 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9502.0MB reserved
2025-03-25 11:41:06,255 - training - INFO - Epoch: 541/200000, Batch: 30/45, Loss: 2.1637, Throughput: 71.11 samples/sec
2025-03-25 11:41:17,220 - training - INFO - Epoch 541 completed in 35.38s. Average loss: 2.1281
2025-03-25 11:41:17,224 - training - INFO - Starting epoch 542/200000
2025-03-25 11:41:17,980 - training - INFO - Memory: GPU 0: 3556.6MB allocated, 9638.0MB reserved
2025-03-25 11:41:17,980 - training - INFO - Epoch: 542/200000, Batch: 0/45, Loss: 1.6149, Throughput: 74.17 samples/sec
2025-03-25 11:41:30,008 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9498.0MB reserved
2025-03-25 11:41:30,008 - training - INFO - Epoch: 542/200000, Batch: 15/45, Loss: 2.0516, Throughput: 70.10 samples/sec
2025-03-25 11:41:41,737 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9498.0MB reserved
2025-03-25 11:41:41,737 - training - INFO - Epoch: 542/200000, Batch: 30/45, Loss: 2.0708, Throughput: 70.82 samples/sec
2025-03-25 11:41:52,686 - training - INFO - Epoch 542 completed in 35.46s. Average loss: 2.1255
2025-03-25 11:41:52,690 - training - INFO - Starting epoch 543/200000
2025-03-25 11:41:53,439 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9636.0MB reserved
2025-03-25 11:41:53,439 - training - INFO - Epoch: 543/200000, Batch: 0/45, Loss: 2.1978, Throughput: 74.91 samples/sec
2025-03-25 11:42:05,307 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9494.0MB reserved
2025-03-25 11:42:05,307 - training - INFO - Epoch: 543/200000, Batch: 15/45, Loss: 2.2057, Throughput: 71.02 samples/sec
2025-03-25 11:42:16,873 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9494.0MB reserved
2025-03-25 11:42:16,873 - training - INFO - Epoch: 543/200000, Batch: 30/45, Loss: 2.1682, Throughput: 71.79 samples/sec
2025-03-25 11:42:27,862 - training - INFO - Epoch 543 completed in 35.17s. Average loss: 2.1829
2025-03-25 11:42:27,865 - training - INFO - Starting epoch 544/200000
2025-03-25 11:42:28,601 - training - INFO - Memory: GPU 0: 3557.2MB allocated, 9632.0MB reserved
2025-03-25 11:42:28,601 - training - INFO - Epoch: 544/200000, Batch: 0/45, Loss: 1.9293, Throughput: 76.29 samples/sec
2025-03-25 11:42:40,569 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9492.0MB reserved
2025-03-25 11:42:40,569 - training - INFO - Epoch: 544/200000, Batch: 15/45, Loss: 1.9672, Throughput: 70.55 samples/sec
2025-03-25 11:42:52,249 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9492.0MB reserved
2025-03-25 11:42:52,249 - training - INFO - Epoch: 544/200000, Batch: 30/45, Loss: 2.0217, Throughput: 71.20 samples/sec
2025-03-25 11:43:03,186 - training - INFO - Epoch 544 completed in 35.32s. Average loss: 2.1540
2025-03-25 11:43:03,190 - training - INFO - Starting epoch 545/200000
2025-03-25 11:43:03,936 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9628.0MB reserved
2025-03-25 11:43:03,936 - training - INFO - Epoch: 545/200000, Batch: 0/45, Loss: 2.1717, Throughput: 75.22 samples/sec
2025-03-25 11:43:15,828 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9508.0MB reserved
2025-03-25 11:43:15,828 - training - INFO - Epoch: 545/200000, Batch: 15/45, Loss: 2.2394, Throughput: 70.91 samples/sec
2025-03-25 11:43:27,469 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9510.0MB reserved
2025-03-25 11:43:27,469 - training - INFO - Epoch: 545/200000, Batch: 30/45, Loss: 2.2672, Throughput: 71.51 samples/sec
2025-03-25 11:43:38,436 - training - INFO - Epoch 545 completed in 35.25s. Average loss: 2.1449
2025-03-25 11:43:38,439 - training - INFO - Starting epoch 546/200000
2025-03-25 11:43:39,195 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9648.0MB reserved
2025-03-25 11:43:39,196 - training - INFO - Epoch: 546/200000, Batch: 0/45, Loss: 2.3313, Throughput: 74.27 samples/sec
2025-03-25 11:43:51,224 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9512.0MB reserved
2025-03-25 11:43:51,225 - training - INFO - Epoch: 546/200000, Batch: 15/45, Loss: 2.1252, Throughput: 70.09 samples/sec
2025-03-25 11:44:02,817 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9512.0MB reserved
2025-03-25 11:44:02,817 - training - INFO - Epoch: 546/200000, Batch: 30/45, Loss: 2.0779, Throughput: 71.21 samples/sec
2025-03-25 11:44:13,715 - training - INFO - Epoch 546 completed in 35.28s. Average loss: 2.1537
2025-03-25 11:44:13,718 - training - INFO - Starting epoch 547/200000
2025-03-25 11:44:14,466 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9650.0MB reserved
2025-03-25 11:44:14,466 - training - INFO - Epoch: 547/200000, Batch: 0/45, Loss: 2.0050, Throughput: 75.11 samples/sec
2025-03-25 11:44:26,365 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9496.0MB reserved
2025-03-25 11:44:26,365 - training - INFO - Epoch: 547/200000, Batch: 15/45, Loss: 2.0888, Throughput: 70.86 samples/sec
2025-03-25 11:44:37,913 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9496.0MB reserved
2025-03-25 11:44:37,913 - training - INFO - Epoch: 547/200000, Batch: 30/45, Loss: 2.1481, Throughput: 71.76 samples/sec
2025-03-25 11:44:48,834 - training - INFO - Epoch 547 completed in 35.12s. Average loss: 2.1712
2025-03-25 11:44:48,838 - training - INFO - Starting epoch 548/200000
2025-03-25 11:44:49,581 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9634.0MB reserved
2025-03-25 11:44:49,582 - training - INFO - Epoch: 548/200000, Batch: 0/45, Loss: 2.0773, Throughput: 75.40 samples/sec
2025-03-25 11:45:01,573 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9500.0MB reserved
2025-03-25 11:45:01,573 - training - INFO - Epoch: 548/200000, Batch: 15/45, Loss: 2.0861, Throughput: 70.37 samples/sec
2025-03-25 11:45:13,211 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9500.0MB reserved
2025-03-25 11:45:13,212 - training - INFO - Epoch: 548/200000, Batch: 30/45, Loss: 2.0773, Throughput: 71.23 samples/sec
2025-03-25 11:45:24,214 - training - INFO - Epoch 548 completed in 35.38s. Average loss: 2.1186
2025-03-25 11:45:24,218 - training - INFO - Starting epoch 549/200000
2025-03-25 11:45:24,966 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9638.0MB reserved
2025-03-25 11:45:24,966 - training - INFO - Epoch: 549/200000, Batch: 0/45, Loss: 2.1203, Throughput: 74.97 samples/sec
2025-03-25 11:45:36,920 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9510.0MB reserved
2025-03-25 11:45:36,920 - training - INFO - Epoch: 549/200000, Batch: 15/45, Loss: 2.2621, Throughput: 70.55 samples/sec
2025-03-25 11:45:48,561 - training - INFO - Memory: GPU 0: 3555.5MB allocated, 9510.0MB reserved
2025-03-25 11:45:48,561 - training - INFO - Epoch: 549/200000, Batch: 30/45, Loss: 2.2220, Throughput: 71.32 samples/sec
2025-03-25 11:45:59,517 - training - INFO - Epoch 549 completed in 35.30s. Average loss: 2.1631
2025-03-25 11:45:59,521 - training - INFO - Starting epoch 550/200000
2025-03-25 11:46:00,264 - training - INFO - Memory: GPU 0: 3555.5MB allocated, 9648.0MB reserved
2025-03-25 11:46:00,264 - training - INFO - Epoch: 550/200000, Batch: 0/45, Loss: 2.0834, Throughput: 75.54 samples/sec
2025-03-25 11:46:12,068 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9506.0MB reserved
2025-03-25 11:46:12,069 - training - INFO - Epoch: 550/200000, Batch: 15/45, Loss: 2.0335, Throughput: 71.42 samples/sec
2025-03-25 11:46:23,652 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9506.0MB reserved
2025-03-25 11:46:23,653 - training - INFO - Epoch: 550/200000, Batch: 30/45, Loss: 2.0561, Throughput: 71.94 samples/sec
2025-03-25 11:46:34,577 - training - INFO - Epoch 550 completed in 35.06s. Average loss: 2.1077
2025-03-25 11:46:34,581 - training - INFO - Starting epoch 551/200000
2025-03-25 11:46:35,313 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9644.0MB reserved
2025-03-25 11:46:35,314 - training - INFO - Epoch: 551/200000, Batch: 0/45, Loss: 1.8769, Throughput: 76.58 samples/sec
2025-03-25 11:46:47,201 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9508.0MB reserved
2025-03-25 11:46:47,201 - training - INFO - Epoch: 551/200000, Batch: 15/45, Loss: 2.1385, Throughput: 71.00 samples/sec
2025-03-25 11:46:58,811 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9508.0MB reserved
2025-03-25 11:46:58,812 - training - INFO - Epoch: 551/200000, Batch: 30/45, Loss: 2.0472, Throughput: 71.65 samples/sec
2025-03-25 11:47:09,720 - training - INFO - Epoch 551 completed in 35.14s. Average loss: 2.1072
2025-03-25 11:47:09,724 - training - INFO - Starting epoch 552/200000
2025-03-25 11:47:10,475 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9646.0MB reserved
2025-03-25 11:47:10,475 - training - INFO - Epoch: 552/200000, Batch: 0/45, Loss: 2.3106, Throughput: 74.69 samples/sec
2025-03-25 11:47:22,387 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9494.0MB reserved
2025-03-25 11:47:22,387 - training - INFO - Epoch: 552/200000, Batch: 15/45, Loss: 2.1073, Throughput: 70.77 samples/sec
2025-03-25 11:47:33,899 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9494.0MB reserved
2025-03-25 11:47:33,899 - training - INFO - Epoch: 552/200000, Batch: 30/45, Loss: 2.1031, Throughput: 71.81 samples/sec
2025-03-25 11:47:44,780 - training - INFO - Epoch 552 completed in 35.06s. Average loss: 2.1677
2025-03-25 11:47:44,783 - training - INFO - Starting epoch 553/200000
2025-03-25 11:47:45,532 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9632.0MB reserved
2025-03-25 11:47:45,533 - training - INFO - Epoch: 553/200000, Batch: 0/45, Loss: 2.4979, Throughput: 74.89 samples/sec
2025-03-25 11:47:57,445 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9506.0MB reserved
2025-03-25 11:47:57,445 - training - INFO - Epoch: 553/200000, Batch: 15/45, Loss: 1.9755, Throughput: 70.78 samples/sec
2025-03-25 11:48:09,054 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9506.0MB reserved
2025-03-25 11:48:09,054 - training - INFO - Epoch: 553/200000, Batch: 30/45, Loss: 2.0791, Throughput: 71.53 samples/sec
2025-03-25 11:48:19,988 - training - INFO - Epoch 553 completed in 35.20s. Average loss: 2.1211
2025-03-25 11:48:19,992 - training - INFO - Starting epoch 554/200000
2025-03-25 11:48:20,719 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9644.0MB reserved
2025-03-25 11:48:20,719 - training - INFO - Epoch: 554/200000, Batch: 0/45, Loss: 2.2920, Throughput: 77.11 samples/sec
2025-03-25 11:48:32,562 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9496.0MB reserved
2025-03-25 11:48:32,562 - training - INFO - Epoch: 554/200000, Batch: 15/45, Loss: 2.0362, Throughput: 71.29 samples/sec
2025-03-25 11:48:44,077 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9498.0MB reserved
2025-03-25 11:48:44,077 - training - INFO - Epoch: 554/200000, Batch: 30/45, Loss: 2.0514, Throughput: 72.08 samples/sec
2025-03-25 11:48:54,977 - training - INFO - Epoch 554 completed in 34.98s. Average loss: 2.1076
2025-03-25 11:48:54,980 - training - INFO - Starting epoch 555/200000
2025-03-25 11:48:55,711 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9636.0MB reserved
2025-03-25 11:48:55,711 - training - INFO - Epoch: 555/200000, Batch: 0/45, Loss: 1.7555, Throughput: 76.69 samples/sec
2025-03-25 11:49:07,624 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9502.0MB reserved
2025-03-25 11:49:07,624 - training - INFO - Epoch: 555/200000, Batch: 15/45, Loss: 1.9752, Throughput: 70.87 samples/sec
2025-03-25 11:49:19,259 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9502.0MB reserved
2025-03-25 11:49:19,259 - training - INFO - Epoch: 555/200000, Batch: 30/45, Loss: 2.0401, Throughput: 71.51 samples/sec
2025-03-25 11:49:30,219 - training - INFO - Epoch 555 completed in 35.24s. Average loss: 2.0878
2025-03-25 11:49:30,223 - training - INFO - Starting epoch 556/200000
2025-03-25 11:49:30,970 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9640.0MB reserved
2025-03-25 11:49:30,970 - training - INFO - Epoch: 556/200000, Batch: 0/45, Loss: 2.0904, Throughput: 75.18 samples/sec
2025-03-25 11:49:42,952 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9508.0MB reserved
2025-03-25 11:49:42,953 - training - INFO - Epoch: 556/200000, Batch: 15/45, Loss: 2.0949, Throughput: 70.40 samples/sec
2025-03-25 11:49:54,548 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9508.0MB reserved
2025-03-25 11:49:54,549 - training - INFO - Epoch: 556/200000, Batch: 30/45, Loss: 2.1097, Throughput: 71.38 samples/sec
2025-03-25 11:50:05,534 - training - INFO - Epoch 556 completed in 35.31s. Average loss: 2.1104
2025-03-25 11:50:05,538 - training - INFO - Starting epoch 557/200000
2025-03-25 11:50:06,298 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9646.0MB reserved
2025-03-25 11:50:06,298 - training - INFO - Epoch: 557/200000, Batch: 0/45, Loss: 2.6550, Throughput: 73.78 samples/sec
2025-03-25 11:50:18,249 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9488.0MB reserved
2025-03-25 11:50:18,250 - training - INFO - Epoch: 557/200000, Batch: 15/45, Loss: 2.1901, Throughput: 70.50 samples/sec
2025-03-25 11:50:29,837 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9488.0MB reserved
2025-03-25 11:50:29,837 - training - INFO - Epoch: 557/200000, Batch: 30/45, Loss: 2.1270, Throughput: 71.45 samples/sec
2025-03-25 11:50:40,797 - training - INFO - Epoch 557 completed in 35.26s. Average loss: 2.1302
2025-03-25 11:50:40,800 - training - INFO - Starting epoch 558/200000
2025-03-25 11:50:41,541 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9624.0MB reserved
2025-03-25 11:50:41,541 - training - INFO - Epoch: 558/200000, Batch: 0/45, Loss: 2.0274, Throughput: 75.71 samples/sec
2025-03-25 11:50:53,507 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9496.0MB reserved
2025-03-25 11:50:53,508 - training - INFO - Epoch: 558/200000, Batch: 15/45, Loss: 2.1396, Throughput: 70.52 samples/sec
2025-03-25 11:51:05,102 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9496.0MB reserved
2025-03-25 11:51:05,103 - training - INFO - Epoch: 558/200000, Batch: 30/45, Loss: 2.1074, Throughput: 71.44 samples/sec
2025-03-25 11:51:16,045 - training - INFO - Epoch 558 completed in 35.25s. Average loss: 2.1240
2025-03-25 11:51:16,049 - training - INFO - Starting epoch 559/200000
2025-03-25 11:51:16,786 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9634.0MB reserved
2025-03-25 11:51:16,786 - training - INFO - Epoch: 559/200000, Batch: 0/45, Loss: 2.3021, Throughput: 76.08 samples/sec
2025-03-25 11:51:28,675 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9498.0MB reserved
2025-03-25 11:51:28,676 - training - INFO - Epoch: 559/200000, Batch: 15/45, Loss: 2.1669, Throughput: 70.98 samples/sec
2025-03-25 11:51:40,230 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9498.0MB reserved
2025-03-25 11:51:40,230 - training - INFO - Epoch: 559/200000, Batch: 30/45, Loss: 2.1813, Throughput: 71.80 samples/sec
2025-03-25 11:51:51,159 - training - INFO - Epoch 559 completed in 35.11s. Average loss: 2.0951
2025-03-25 11:51:51,162 - training - INFO - Starting epoch 560/200000
2025-03-25 11:51:51,913 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9636.0MB reserved
2025-03-25 11:51:51,913 - training - INFO - Epoch: 560/200000, Batch: 0/45, Loss: 1.9355, Throughput: 74.77 samples/sec
2025-03-25 11:52:03,907 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9502.0MB reserved
2025-03-25 11:52:03,908 - training - INFO - Epoch: 560/200000, Batch: 15/45, Loss: 1.9552, Throughput: 70.32 samples/sec
2025-03-25 11:52:15,485 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9502.0MB reserved
2025-03-25 11:52:15,485 - training - INFO - Epoch: 560/200000, Batch: 30/45, Loss: 2.0577, Throughput: 71.38 samples/sec
2025-03-25 11:52:26,451 - training - INFO - Epoch 560 completed in 35.29s. Average loss: 2.0829
2025-03-25 11:52:26,455 - training - INFO - Starting epoch 561/200000
2025-03-25 11:52:27,207 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9640.0MB reserved
2025-03-25 11:52:27,207 - training - INFO - Epoch: 561/200000, Batch: 0/45, Loss: 2.0435, Throughput: 74.54 samples/sec
2025-03-25 11:52:39,118 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9504.0MB reserved
2025-03-25 11:52:39,118 - training - INFO - Epoch: 561/200000, Batch: 15/45, Loss: 1.9897, Throughput: 70.76 samples/sec
2025-03-25 11:52:50,755 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9504.0MB reserved
2025-03-25 11:52:50,755 - training - INFO - Epoch: 561/200000, Batch: 30/45, Loss: 2.0730, Throughput: 71.45 samples/sec
2025-03-25 11:53:01,695 - training - INFO - Epoch 561 completed in 35.24s. Average loss: 2.1503
2025-03-25 11:53:01,699 - training - INFO - Starting epoch 562/200000
2025-03-25 11:53:02,455 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9642.0MB reserved
2025-03-25 11:53:02,455 - training - INFO - Epoch: 562/200000, Batch: 0/45, Loss: 2.2183, Throughput: 74.12 samples/sec
2025-03-25 11:53:14,399 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9500.0MB reserved
2025-03-25 11:53:14,399 - training - INFO - Epoch: 562/200000, Batch: 15/45, Loss: 2.1139, Throughput: 70.55 samples/sec
2025-03-25 11:53:25,985 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9500.0MB reserved
2025-03-25 11:53:25,986 - training - INFO - Epoch: 562/200000, Batch: 30/45, Loss: 2.1843, Throughput: 71.48 samples/sec
2025-03-25 11:53:36,940 - training - INFO - Epoch 562 completed in 35.24s. Average loss: 2.1362
2025-03-25 11:53:36,944 - training - INFO - Starting epoch 563/200000
2025-03-25 11:53:37,704 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9638.0MB reserved
2025-03-25 11:53:37,704 - training - INFO - Epoch: 563/200000, Batch: 0/45, Loss: 1.8658, Throughput: 73.89 samples/sec
2025-03-25 11:53:49,646 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9502.0MB reserved
2025-03-25 11:53:49,647 - training - INFO - Epoch: 563/200000, Batch: 15/45, Loss: 2.0971, Throughput: 70.55 samples/sec
2025-03-25 11:54:01,218 - training - INFO - Memory: GPU 0: 3556.6MB allocated, 9502.0MB reserved
2025-03-25 11:54:01,218 - training - INFO - Epoch: 563/200000, Batch: 30/45, Loss: 2.1005, Throughput: 71.52 samples/sec
2025-03-25 11:54:12,175 - training - INFO - Epoch 563 completed in 35.23s. Average loss: 2.1168
2025-03-25 11:54:12,179 - training - INFO - Starting epoch 564/200000
2025-03-25 11:54:12,915 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9638.0MB reserved
2025-03-25 11:54:12,915 - training - INFO - Epoch: 564/200000, Batch: 0/45, Loss: 1.8558, Throughput: 76.17 samples/sec
2025-03-25 11:54:24,829 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9500.0MB reserved
2025-03-25 11:54:24,829 - training - INFO - Epoch: 564/200000, Batch: 15/45, Loss: 2.1371, Throughput: 70.84 samples/sec
2025-03-25 11:54:36,455 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9502.0MB reserved
2025-03-25 11:54:36,455 - training - INFO - Epoch: 564/200000, Batch: 30/45, Loss: 2.0724, Throughput: 71.52 samples/sec
2025-03-25 11:54:47,446 - training - INFO - Epoch 564 completed in 35.27s. Average loss: 2.1127
2025-03-25 11:54:47,450 - training - INFO - Starting epoch 565/200000
2025-03-25 11:54:48,211 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9640.0MB reserved
2025-03-25 11:54:48,211 - training - INFO - Epoch: 565/200000, Batch: 0/45, Loss: 2.3945, Throughput: 73.67 samples/sec
2025-03-25 11:55:00,265 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9500.0MB reserved
2025-03-25 11:55:00,265 - training - INFO - Epoch: 565/200000, Batch: 15/45, Loss: 2.1123, Throughput: 69.93 samples/sec
2025-03-25 11:55:11,955 - training - INFO - Memory: GPU 0: 3564.9MB allocated, 9500.0MB reserved
2025-03-25 11:55:11,955 - training - INFO - Epoch: 565/200000, Batch: 30/45, Loss: 2.1196, Throughput: 70.85 samples/sec
2025-03-25 11:55:22,977 - training - INFO - Epoch 565 completed in 35.53s. Average loss: 2.1475
2025-03-25 11:55:22,980 - training - INFO - Starting epoch 566/200000
2025-03-25 11:55:23,708 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9638.0MB reserved
2025-03-25 11:55:23,708 - training - INFO - Epoch: 566/200000, Batch: 0/45, Loss: 1.9278, Throughput: 77.11 samples/sec
2025-03-25 11:55:35,683 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9500.0MB reserved
2025-03-25 11:55:35,684 - training - INFO - Epoch: 566/200000, Batch: 15/45, Loss: 2.0761, Throughput: 70.54 samples/sec
2025-03-25 11:55:47,380 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9500.0MB reserved
2025-03-25 11:55:47,381 - training - INFO - Epoch: 566/200000, Batch: 30/45, Loss: 2.0896, Throughput: 71.15 samples/sec
2025-03-25 11:55:58,347 - training - INFO - Epoch 566 completed in 35.37s. Average loss: 2.0625
2025-03-25 11:55:58,350 - training - INFO - Starting epoch 567/200000
2025-03-25 11:55:59,077 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9638.0MB reserved
2025-03-25 11:55:59,077 - training - INFO - Epoch: 567/200000, Batch: 0/45, Loss: 2.0148, Throughput: 77.24 samples/sec
2025-03-25 11:56:11,035 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9502.0MB reserved
2025-03-25 11:56:11,035 - training - INFO - Epoch: 567/200000, Batch: 15/45, Loss: 2.0843, Throughput: 70.65 samples/sec
2025-03-25 11:56:22,625 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9502.0MB reserved
2025-03-25 11:56:22,626 - training - INFO - Epoch: 567/200000, Batch: 30/45, Loss: 2.0599, Throughput: 71.52 samples/sec
2025-03-25 11:56:33,519 - training - INFO - Epoch 567 completed in 35.17s. Average loss: 2.0966
2025-03-25 11:56:33,523 - training - INFO - Starting epoch 568/200000
2025-03-25 11:56:34,274 - training - INFO - Memory: GPU 0: 3557.2MB allocated, 9640.0MB reserved
2025-03-25 11:56:34,274 - training - INFO - Epoch: 568/200000, Batch: 0/45, Loss: 1.6609, Throughput: 74.81 samples/sec
2025-03-25 11:56:46,182 - training - INFO - Memory: GPU 0: 3562.5MB allocated, 9508.0MB reserved
2025-03-25 11:56:46,183 - training - INFO - Epoch: 568/200000, Batch: 15/45, Loss: 2.1253, Throughput: 70.79 samples/sec
2025-03-25 11:56:57,829 - training - INFO - Memory: GPU 0: 3562.5MB allocated, 9508.0MB reserved
2025-03-25 11:56:57,829 - training - INFO - Epoch: 568/200000, Batch: 30/45, Loss: 2.1194, Throughput: 71.43 samples/sec
2025-03-25 11:57:08,783 - training - INFO - Epoch 568 completed in 35.26s. Average loss: 2.1637
2025-03-25 11:57:08,786 - training - INFO - Starting epoch 569/200000
2025-03-25 11:57:09,531 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9644.0MB reserved
2025-03-25 11:57:09,531 - training - INFO - Epoch: 569/200000, Batch: 0/45, Loss: 2.3165, Throughput: 75.28 samples/sec
2025-03-25 11:57:21,419 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9514.0MB reserved
2025-03-25 11:57:21,420 - training - INFO - Epoch: 569/200000, Batch: 15/45, Loss: 2.1446, Throughput: 70.94 samples/sec
2025-03-25 11:57:33,045 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9514.0MB reserved
2025-03-25 11:57:33,045 - training - INFO - Epoch: 569/200000, Batch: 30/45, Loss: 2.0961, Throughput: 71.57 samples/sec
2025-03-25 11:57:43,977 - training - INFO - Epoch 569 completed in 35.19s. Average loss: 2.1052
2025-03-25 11:57:43,981 - training - INFO - Starting epoch 570/200000
2025-03-25 11:57:44,734 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9652.0MB reserved
2025-03-25 11:57:44,734 - training - INFO - Epoch: 570/200000, Batch: 0/45, Loss: 2.1741, Throughput: 74.39 samples/sec
2025-03-25 11:57:56,705 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9498.0MB reserved
2025-03-25 11:57:56,706 - training - INFO - Epoch: 570/200000, Batch: 15/45, Loss: 2.2540, Throughput: 70.43 samples/sec
2025-03-25 11:58:08,383 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9498.0MB reserved
2025-03-25 11:58:08,383 - training - INFO - Epoch: 570/200000, Batch: 30/45, Loss: 2.2229, Throughput: 71.15 samples/sec
2025-03-25 11:58:19,368 - training - INFO - Epoch 570 completed in 35.39s. Average loss: 2.1901
2025-03-25 11:58:19,371 - training - INFO - Starting epoch 571/200000
2025-03-25 11:58:20,140 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9636.0MB reserved
2025-03-25 11:58:20,140 - training - INFO - Epoch: 571/200000, Batch: 0/45, Loss: 2.0459, Throughput: 73.05 samples/sec
2025-03-25 11:58:32,019 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9502.0MB reserved
2025-03-25 11:58:32,020 - training - INFO - Epoch: 571/200000, Batch: 15/45, Loss: 2.1474, Throughput: 70.86 samples/sec
2025-03-25 11:58:43,540 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9502.0MB reserved
2025-03-25 11:58:43,541 - training - INFO - Epoch: 571/200000, Batch: 30/45, Loss: 2.0291, Throughput: 71.83 samples/sec
2025-03-25 11:58:54,457 - training - INFO - Epoch 571 completed in 35.09s. Average loss: 2.0997
2025-03-25 11:58:54,461 - training - INFO - Starting epoch 572/200000
2025-03-25 11:58:55,213 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9640.0MB reserved
2025-03-25 11:58:55,213 - training - INFO - Epoch: 572/200000, Batch: 0/45, Loss: 1.9083, Throughput: 74.62 samples/sec
2025-03-25 11:59:07,121 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9498.0MB reserved
2025-03-25 11:59:07,122 - training - INFO - Epoch: 572/200000, Batch: 15/45, Loss: 1.9865, Throughput: 70.78 samples/sec
2025-03-25 11:59:18,834 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9498.0MB reserved
2025-03-25 11:59:18,834 - training - INFO - Epoch: 572/200000, Batch: 30/45, Loss: 2.0099, Throughput: 71.23 samples/sec
2025-03-25 11:59:29,838 - training - INFO - Epoch 572 completed in 35.38s. Average loss: 2.1403
2025-03-25 11:59:29,842 - training - INFO - Starting epoch 573/200000
2025-03-25 11:59:30,574 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9636.0MB reserved
2025-03-25 11:59:30,574 - training - INFO - Epoch: 573/200000, Batch: 0/45, Loss: 2.1054, Throughput: 76.69 samples/sec
2025-03-25 11:59:42,505 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9496.0MB reserved
2025-03-25 11:59:42,506 - training - INFO - Epoch: 573/200000, Batch: 15/45, Loss: 2.0936, Throughput: 70.77 samples/sec
2025-03-25 11:59:54,151 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9496.0MB reserved
2025-03-25 11:59:54,151 - training - INFO - Epoch: 573/200000, Batch: 30/45, Loss: 2.0411, Throughput: 71.42 samples/sec
2025-03-25 12:00:05,085 - training - INFO - Epoch 573 completed in 35.24s. Average loss: 2.0634
2025-03-25 12:00:05,089 - training - INFO - Starting epoch 574/200000
2025-03-25 12:00:05,847 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9632.0MB reserved
2025-03-25 12:00:05,848 - training - INFO - Epoch: 574/200000, Batch: 0/45, Loss: 2.4041, Throughput: 74.07 samples/sec
2025-03-25 12:00:17,788 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9510.0MB reserved
2025-03-25 12:00:17,788 - training - INFO - Epoch: 574/200000, Batch: 15/45, Loss: 2.0993, Throughput: 70.56 samples/sec
2025-03-25 12:00:29,473 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9512.0MB reserved
2025-03-25 12:00:29,473 - training - INFO - Epoch: 574/200000, Batch: 30/45, Loss: 2.0758, Throughput: 71.20 samples/sec
2025-03-25 12:00:40,472 - training - INFO - Epoch 574 completed in 35.38s. Average loss: 2.1097
2025-03-25 12:00:40,476 - training - INFO - Starting epoch 575/200000
2025-03-25 12:00:41,213 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9650.0MB reserved
2025-03-25 12:00:41,214 - training - INFO - Epoch: 575/200000, Batch: 0/45, Loss: 1.4313, Throughput: 76.08 samples/sec
2025-03-25 12:00:53,065 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9396.0MB reserved
2025-03-25 12:00:53,065 - training - INFO - Epoch: 575/200000, Batch: 15/45, Loss: 2.0899, Throughput: 71.19 samples/sec
2025-03-25 12:01:04,571 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9502.0MB reserved
2025-03-25 12:01:04,572 - training - INFO - Epoch: 575/200000, Batch: 30/45, Loss: 2.0077, Throughput: 72.05 samples/sec
2025-03-25 12:01:15,457 - training - INFO - Epoch 575 completed in 34.98s. Average loss: 2.1162
2025-03-25 12:01:15,461 - training - INFO - Starting epoch 576/200000
2025-03-25 12:01:16,217 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9642.0MB reserved
2025-03-25 12:01:16,217 - training - INFO - Epoch: 576/200000, Batch: 0/45, Loss: 2.0909, Throughput: 74.19 samples/sec
2025-03-25 12:01:28,073 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9496.0MB reserved
2025-03-25 12:01:28,073 - training - INFO - Epoch: 576/200000, Batch: 15/45, Loss: 2.0781, Throughput: 71.05 samples/sec
2025-03-25 12:01:39,734 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9498.0MB reserved
2025-03-25 12:01:39,734 - training - INFO - Epoch: 576/200000, Batch: 30/45, Loss: 2.0555, Throughput: 71.53 samples/sec
2025-03-25 12:01:50,671 - training - INFO - Epoch 576 completed in 35.21s. Average loss: 2.0798
2025-03-25 12:01:50,675 - training - INFO - Starting epoch 577/200000
2025-03-25 12:01:51,406 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9636.0MB reserved
2025-03-25 12:01:51,406 - training - INFO - Epoch: 577/200000, Batch: 0/45, Loss: 2.2874, Throughput: 76.70 samples/sec
2025-03-25 12:02:03,267 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9512.0MB reserved
2025-03-25 12:02:03,268 - training - INFO - Epoch: 577/200000, Batch: 15/45, Loss: 2.0062, Throughput: 71.16 samples/sec
2025-03-25 12:02:14,865 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9514.0MB reserved
2025-03-25 12:02:14,865 - training - INFO - Epoch: 577/200000, Batch: 30/45, Loss: 2.0048, Throughput: 71.77 samples/sec
2025-03-25 12:02:25,766 - training - INFO - Epoch 577 completed in 35.09s. Average loss: 2.0987
2025-03-25 12:02:25,769 - training - INFO - Starting epoch 578/200000
2025-03-25 12:02:26,506 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9652.0MB reserved
2025-03-25 12:02:26,507 - training - INFO - Epoch: 578/200000, Batch: 0/45, Loss: 1.9130, Throughput: 76.18 samples/sec
2025-03-25 12:02:38,342 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9504.0MB reserved
2025-03-25 12:02:38,342 - training - INFO - Epoch: 578/200000, Batch: 15/45, Loss: 2.2042, Throughput: 71.27 samples/sec
2025-03-25 12:02:49,892 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9504.0MB reserved
2025-03-25 12:02:49,893 - training - INFO - Epoch: 578/200000, Batch: 30/45, Loss: 2.0811, Throughput: 71.97 samples/sec
2025-03-25 12:03:00,785 - training - INFO - Epoch 578 completed in 35.02s. Average loss: 2.0732
2025-03-25 12:03:00,789 - training - INFO - Starting epoch 579/200000
2025-03-25 12:03:01,512 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9642.0MB reserved
2025-03-25 12:03:01,512 - training - INFO - Epoch: 579/200000, Batch: 0/45, Loss: 2.3341, Throughput: 77.59 samples/sec
2025-03-25 12:03:13,422 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9508.0MB reserved
2025-03-25 12:03:13,423 - training - INFO - Epoch: 579/200000, Batch: 15/45, Loss: 2.1473, Throughput: 70.94 samples/sec
2025-03-25 12:03:25,049 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9508.0MB reserved
2025-03-25 12:03:25,049 - training - INFO - Epoch: 579/200000, Batch: 30/45, Loss: 2.1610, Throughput: 71.56 samples/sec
2025-03-25 12:03:35,996 - training - INFO - Epoch 579 completed in 35.21s. Average loss: 2.0724
2025-03-25 12:03:36,000 - training - INFO - Starting epoch 580/200000
2025-03-25 12:03:36,749 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9644.0MB reserved
2025-03-25 12:03:36,750 - training - INFO - Epoch: 580/200000, Batch: 0/45, Loss: 2.1143, Throughput: 74.98 samples/sec
2025-03-25 12:03:48,727 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9508.0MB reserved
2025-03-25 12:03:48,728 - training - INFO - Epoch: 580/200000, Batch: 15/45, Loss: 1.9972, Throughput: 70.41 samples/sec
2025-03-25 12:04:00,304 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9508.0MB reserved
2025-03-25 12:04:00,304 - training - INFO - Epoch: 580/200000, Batch: 30/45, Loss: 2.0334, Throughput: 71.43 samples/sec
2025-03-25 12:04:11,223 - training - INFO - Epoch 580 completed in 35.22s. Average loss: 2.0625
2025-03-25 12:04:11,227 - training - INFO - Starting epoch 581/200000
2025-03-25 12:04:11,995 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9646.0MB reserved
2025-03-25 12:04:11,995 - training - INFO - Epoch: 581/200000, Batch: 0/45, Loss: 1.7717, Throughput: 73.01 samples/sec
2025-03-25 12:04:23,925 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9486.0MB reserved
2025-03-25 12:04:23,925 - training - INFO - Epoch: 581/200000, Batch: 15/45, Loss: 2.0135, Throughput: 70.57 samples/sec
2025-03-25 12:04:35,646 - training - INFO - Memory: GPU 0: 3556.4MB allocated, 9488.0MB reserved
2025-03-25 12:04:35,646 - training - INFO - Epoch: 581/200000, Batch: 30/45, Loss: 2.0792, Throughput: 71.10 samples/sec
2025-03-25 12:04:46,651 - training - INFO - Epoch 581 completed in 35.42s. Average loss: 2.1017
2025-03-25 12:04:46,655 - training - INFO - Starting epoch 582/200000
2025-03-25 12:04:47,390 - training - INFO - Memory: GPU 0: 3556.4MB allocated, 9626.0MB reserved
2025-03-25 12:04:47,390 - training - INFO - Epoch: 582/200000, Batch: 0/45, Loss: 1.6261, Throughput: 76.26 samples/sec
2025-03-25 12:04:59,355 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9502.0MB reserved
2025-03-25 12:04:59,355 - training - INFO - Epoch: 582/200000, Batch: 15/45, Loss: 2.0189, Throughput: 70.56 samples/sec
2025-03-25 12:05:11,077 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9502.0MB reserved
2025-03-25 12:05:11,077 - training - INFO - Epoch: 582/200000, Batch: 30/45, Loss: 2.0115, Throughput: 71.09 samples/sec
2025-03-25 12:05:22,041 - training - INFO - Epoch 582 completed in 35.39s. Average loss: 2.0995
2025-03-25 12:05:22,045 - training - INFO - Starting epoch 583/200000
2025-03-25 12:05:22,779 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9640.0MB reserved
2025-03-25 12:05:22,779 - training - INFO - Epoch: 583/200000, Batch: 0/45, Loss: 2.4470, Throughput: 76.31 samples/sec
2025-03-25 12:05:34,756 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9394.0MB reserved
2025-03-25 12:05:34,756 - training - INFO - Epoch: 583/200000, Batch: 15/45, Loss: 2.0454, Throughput: 70.50 samples/sec
2025-03-25 12:05:46,450 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9396.0MB reserved
2025-03-25 12:05:46,450 - training - INFO - Epoch: 583/200000, Batch: 30/45, Loss: 2.0946, Throughput: 71.14 samples/sec
2025-03-25 12:05:57,438 - training - INFO - Epoch 583 completed in 35.39s. Average loss: 2.0774
2025-03-25 12:05:57,442 - training - INFO - Starting epoch 584/200000
2025-03-25 12:05:58,196 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9534.0MB reserved
2025-03-25 12:05:58,196 - training - INFO - Epoch: 584/200000, Batch: 0/45, Loss: 1.6297, Throughput: 74.43 samples/sec
2025-03-25 12:06:10,039 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9502.0MB reserved
2025-03-25 12:06:10,040 - training - INFO - Epoch: 584/200000, Batch: 15/45, Loss: 2.0511, Throughput: 71.13 samples/sec
2025-03-25 12:06:21,622 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9502.0MB reserved
2025-03-25 12:06:21,622 - training - INFO - Epoch: 584/200000, Batch: 30/45, Loss: 2.1202, Throughput: 71.80 samples/sec
2025-03-25 12:06:32,534 - training - INFO - Epoch 584 completed in 35.09s. Average loss: 2.1345
2025-03-25 12:06:32,538 - training - INFO - Starting epoch 585/200000
2025-03-25 12:06:33,271 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9640.0MB reserved
2025-03-25 12:06:33,271 - training - INFO - Epoch: 585/200000, Batch: 0/45, Loss: 2.0058, Throughput: 76.42 samples/sec
2025-03-25 12:06:45,165 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9400.0MB reserved
2025-03-25 12:06:45,165 - training - INFO - Epoch: 585/200000, Batch: 15/45, Loss: 1.9779, Throughput: 70.97 samples/sec
2025-03-25 12:06:56,785 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9400.0MB reserved
2025-03-25 12:06:56,786 - training - INFO - Epoch: 585/200000, Batch: 30/45, Loss: 2.0075, Throughput: 71.60 samples/sec
2025-03-25 12:07:07,721 - training - INFO - Epoch 585 completed in 35.18s. Average loss: 2.0715
2025-03-25 12:07:07,725 - training - INFO - Starting epoch 586/200000
2025-03-25 12:07:08,477 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9538.0MB reserved
2025-03-25 12:07:08,478 - training - INFO - Epoch: 586/200000, Batch: 0/45, Loss: 2.3013, Throughput: 74.50 samples/sec
2025-03-25 12:07:20,379 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9406.0MB reserved
2025-03-25 12:07:20,380 - training - INFO - Epoch: 586/200000, Batch: 15/45, Loss: 2.0606, Throughput: 70.81 samples/sec
2025-03-25 12:07:32,004 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9512.0MB reserved
2025-03-25 12:07:32,005 - training - INFO - Epoch: 586/200000, Batch: 30/45, Loss: 2.0342, Throughput: 71.51 samples/sec
2025-03-25 12:07:42,968 - training - INFO - Epoch 586 completed in 35.24s. Average loss: 2.0915
2025-03-25 12:07:42,971 - training - INFO - Starting epoch 587/200000
2025-03-25 12:07:43,721 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9648.0MB reserved
2025-03-25 12:07:43,722 - training - INFO - Epoch: 587/200000, Batch: 0/45, Loss: 2.0514, Throughput: 74.80 samples/sec
2025-03-25 12:07:55,547 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9504.0MB reserved
2025-03-25 12:07:55,548 - training - INFO - Epoch: 587/200000, Batch: 15/45, Loss: 2.0176, Throughput: 71.26 samples/sec
2025-03-25 12:08:07,119 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9504.0MB reserved
2025-03-25 12:08:07,120 - training - INFO - Epoch: 587/200000, Batch: 30/45, Loss: 1.9919, Throughput: 71.90 samples/sec
2025-03-25 12:08:18,007 - training - INFO - Epoch 587 completed in 35.04s. Average loss: 2.1036
2025-03-25 12:08:18,011 - training - INFO - Starting epoch 588/200000
2025-03-25 12:08:18,744 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9642.0MB reserved
2025-03-25 12:08:18,745 - training - INFO - Epoch: 588/200000, Batch: 0/45, Loss: 2.1227, Throughput: 76.45 samples/sec
2025-03-25 12:08:30,565 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9494.0MB reserved
2025-03-25 12:08:30,565 - training - INFO - Epoch: 588/200000, Batch: 15/45, Loss: 2.1734, Throughput: 71.38 samples/sec
2025-03-25 12:08:42,284 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9494.0MB reserved
2025-03-25 12:08:42,284 - training - INFO - Epoch: 588/200000, Batch: 30/45, Loss: 2.1022, Throughput: 71.52 samples/sec
2025-03-25 12:08:53,286 - training - INFO - Epoch 588 completed in 35.27s. Average loss: 2.0784
2025-03-25 12:08:53,290 - training - INFO - Starting epoch 589/200000
2025-03-25 12:08:54,043 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9634.0MB reserved
2025-03-25 12:08:54,043 - training - INFO - Epoch: 589/200000, Batch: 0/45, Loss: 2.2692, Throughput: 74.53 samples/sec
2025-03-25 12:09:05,939 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9510.0MB reserved
2025-03-25 12:09:05,939 - training - INFO - Epoch: 589/200000, Batch: 15/45, Loss: 2.0197, Throughput: 70.84 samples/sec
2025-03-25 12:09:17,536 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9510.0MB reserved
2025-03-25 12:09:17,536 - training - INFO - Epoch: 589/200000, Batch: 30/45, Loss: 2.0403, Throughput: 71.60 samples/sec
2025-03-25 12:09:28,567 - training - INFO - Epoch 589 completed in 35.28s. Average loss: 2.0353
2025-03-25 12:09:28,570 - training - INFO - Starting epoch 590/200000
2025-03-25 12:09:29,333 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9648.0MB reserved
2025-03-25 12:09:29,334 - training - INFO - Epoch: 590/200000, Batch: 0/45, Loss: 1.7817, Throughput: 73.70 samples/sec
2025-03-25 12:09:41,186 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9506.0MB reserved
2025-03-25 12:09:41,186 - training - INFO - Epoch: 590/200000, Batch: 15/45, Loss: 2.0340, Throughput: 71.04 samples/sec
2025-03-25 12:09:52,746 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9508.0MB reserved
2025-03-25 12:09:52,746 - training - INFO - Epoch: 590/200000, Batch: 30/45, Loss: 2.0521, Throughput: 71.82 samples/sec
2025-03-25 12:10:03,645 - training - INFO - Epoch 590 completed in 35.07s. Average loss: 2.0581
2025-03-25 12:10:03,649 - training - INFO - Starting epoch 591/200000
2025-03-25 12:10:04,370 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9646.0MB reserved
2025-03-25 12:10:04,370 - training - INFO - Epoch: 591/200000, Batch: 0/45, Loss: 2.0159, Throughput: 77.78 samples/sec
2025-03-25 12:10:16,155 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9494.0MB reserved
2025-03-25 12:10:16,155 - training - INFO - Epoch: 591/200000, Batch: 15/45, Loss: 2.1315, Throughput: 71.66 samples/sec
2025-03-25 12:10:27,790 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9494.0MB reserved
2025-03-25 12:10:27,790 - training - INFO - Epoch: 591/200000, Batch: 30/45, Loss: 2.0976, Throughput: 71.91 samples/sec
2025-03-25 12:10:38,798 - training - INFO - Epoch 591 completed in 35.15s. Average loss: 2.0665
2025-03-25 12:10:38,802 - training - INFO - Starting epoch 592/200000
2025-03-25 12:10:39,550 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9632.0MB reserved
2025-03-25 12:10:39,551 - training - INFO - Epoch: 592/200000, Batch: 0/45, Loss: 1.8198, Throughput: 75.01 samples/sec
2025-03-25 12:10:51,433 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9504.0MB reserved
2025-03-25 12:10:51,433 - training - INFO - Epoch: 592/200000, Batch: 15/45, Loss: 2.0031, Throughput: 70.95 samples/sec
2025-03-25 12:11:02,986 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9504.0MB reserved
2025-03-25 12:11:02,986 - training - INFO - Epoch: 592/200000, Batch: 30/45, Loss: 2.0534, Throughput: 71.79 samples/sec
2025-03-25 12:11:13,870 - training - INFO - Epoch 592 completed in 35.07s. Average loss: 2.0805
2025-03-25 12:11:13,874 - training - INFO - Starting epoch 593/200000
2025-03-25 12:11:14,613 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9642.0MB reserved
2025-03-25 12:11:14,614 - training - INFO - Epoch: 593/200000, Batch: 0/45, Loss: 1.8788, Throughput: 75.93 samples/sec
2025-03-25 12:11:26,565 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9512.0MB reserved
2025-03-25 12:11:26,566 - training - INFO - Epoch: 593/200000, Batch: 15/45, Loss: 1.9656, Throughput: 70.62 samples/sec
2025-03-25 12:11:38,206 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9512.0MB reserved
2025-03-25 12:11:38,207 - training - INFO - Epoch: 593/200000, Batch: 30/45, Loss: 2.0178, Throughput: 71.35 samples/sec
2025-03-25 12:11:49,164 - training - INFO - Epoch 593 completed in 35.29s. Average loss: 2.1066
2025-03-25 12:11:49,168 - training - INFO - Starting epoch 594/200000
2025-03-25 12:11:49,907 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9648.0MB reserved
2025-03-25 12:11:49,907 - training - INFO - Epoch: 594/200000, Batch: 0/45, Loss: 2.0781, Throughput: 75.91 samples/sec
2025-03-25 12:12:01,816 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9494.0MB reserved
2025-03-25 12:12:01,817 - training - INFO - Epoch: 594/200000, Batch: 15/45, Loss: 1.9479, Throughput: 70.85 samples/sec
2025-03-25 12:12:13,384 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9496.0MB reserved
2025-03-25 12:12:13,385 - training - INFO - Epoch: 594/200000, Batch: 30/45, Loss: 2.0044, Throughput: 71.69 samples/sec
2025-03-25 12:12:24,315 - training - INFO - Epoch 594 completed in 35.15s. Average loss: 2.0161
2025-03-25 12:12:24,319 - training - INFO - Starting epoch 595/200000
2025-03-25 12:12:25,041 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9632.0MB reserved
2025-03-25 12:12:25,042 - training - INFO - Epoch: 595/200000, Batch: 0/45, Loss: 2.2148, Throughput: 77.71 samples/sec
2025-03-25 12:12:36,943 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9512.0MB reserved
2025-03-25 12:12:36,943 - training - INFO - Epoch: 595/200000, Batch: 15/45, Loss: 1.9557, Throughput: 70.98 samples/sec
2025-03-25 12:12:48,522 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9514.0MB reserved
2025-03-25 12:12:48,523 - training - INFO - Epoch: 595/200000, Batch: 30/45, Loss: 1.9655, Throughput: 71.73 samples/sec
2025-03-25 12:12:59,437 - training - INFO - Epoch 595 completed in 35.12s. Average loss: 2.0372
2025-03-25 12:12:59,441 - training - INFO - Starting epoch 596/200000
2025-03-25 12:13:00,186 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9652.0MB reserved
2025-03-25 12:13:00,186 - training - INFO - Epoch: 596/200000, Batch: 0/45, Loss: 2.3909, Throughput: 75.35 samples/sec
2025-03-25 12:13:12,128 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9504.0MB reserved
2025-03-25 12:13:12,129 - training - INFO - Epoch: 596/200000, Batch: 15/45, Loss: 2.0353, Throughput: 70.64 samples/sec
2025-03-25 12:13:23,726 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9504.0MB reserved
2025-03-25 12:13:23,727 - training - INFO - Epoch: 596/200000, Batch: 30/45, Loss: 2.0140, Throughput: 71.49 samples/sec
2025-03-25 12:13:34,626 - training - INFO - Epoch 596 completed in 35.18s. Average loss: 2.0224
2025-03-25 12:13:34,630 - training - INFO - Starting epoch 597/200000
2025-03-25 12:13:35,361 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9640.0MB reserved
2025-03-25 12:13:35,361 - training - INFO - Epoch: 597/200000, Batch: 0/45, Loss: 2.5033, Throughput: 76.64 samples/sec
2025-03-25 12:13:47,261 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9492.0MB reserved
2025-03-25 12:13:47,261 - training - INFO - Epoch: 597/200000, Batch: 15/45, Loss: 2.1646, Throughput: 70.95 samples/sec
2025-03-25 12:13:58,906 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9492.0MB reserved
2025-03-25 12:13:58,907 - training - INFO - Epoch: 597/200000, Batch: 30/45, Loss: 2.0707, Throughput: 71.51 samples/sec
2025-03-25 12:14:09,865 - training - INFO - Epoch 597 completed in 35.24s. Average loss: 2.0258
2025-03-25 12:14:09,869 - training - INFO - Starting epoch 598/200000
2025-03-25 12:14:10,616 - training - INFO - Memory: GPU 0: 3556.2MB allocated, 9630.0MB reserved
2025-03-25 12:14:10,616 - training - INFO - Epoch: 598/200000, Batch: 0/45, Loss: 1.8335, Throughput: 75.17 samples/sec
2025-03-25 12:14:22,584 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9494.0MB reserved
2025-03-25 12:14:22,584 - training - INFO - Epoch: 598/200000, Batch: 15/45, Loss: 2.0312, Throughput: 70.48 samples/sec
2025-03-25 12:14:34,322 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9496.0MB reserved
2025-03-25 12:14:34,323 - training - INFO - Epoch: 598/200000, Batch: 30/45, Loss: 2.0243, Throughput: 71.00 samples/sec
2025-03-25 12:14:45,307 - training - INFO - Epoch 598 completed in 35.44s. Average loss: 2.0552
2025-03-25 12:14:45,311 - training - INFO - Starting epoch 599/200000
2025-03-25 12:14:46,066 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9632.0MB reserved
2025-03-25 12:14:46,066 - training - INFO - Epoch: 599/200000, Batch: 0/45, Loss: 1.8645, Throughput: 74.31 samples/sec
2025-03-25 12:14:58,042 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9500.0MB reserved
2025-03-25 12:14:58,042 - training - INFO - Epoch: 599/200000, Batch: 15/45, Loss: 2.1276, Throughput: 70.39 samples/sec
2025-03-25 12:15:09,709 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9502.0MB reserved
2025-03-25 12:15:09,709 - training - INFO - Epoch: 599/200000, Batch: 30/45, Loss: 2.0828, Throughput: 71.16 samples/sec
2025-03-25 12:15:20,631 - training - INFO - Epoch 599 completed in 35.32s. Average loss: 2.0813
2025-03-25 12:15:20,635 - training - INFO - Starting epoch 600/200000
2025-03-25 12:15:21,382 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9638.0MB reserved
2025-03-25 12:15:21,382 - training - INFO - Epoch: 600/200000, Batch: 0/45, Loss: 2.1712, Throughput: 75.11 samples/sec
2025-03-25 12:15:33,269 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9506.0MB reserved
2025-03-25 12:15:33,269 - training - INFO - Epoch: 600/200000, Batch: 15/45, Loss: 2.1289, Throughput: 70.94 samples/sec
2025-03-25 12:15:45,003 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9506.0MB reserved
2025-03-25 12:15:45,004 - training - INFO - Epoch: 600/200000, Batch: 30/45, Loss: 2.0280, Throughput: 71.24 samples/sec
2025-03-25 12:15:56,011 - training - INFO - Epoch 600 completed in 35.38s. Average loss: 2.0724
2025-03-25 12:15:56,014 - training - INFO - Starting validation...
2025-03-25 12:15:56,327 - training - INFO - Validation Loss: 20.6639
2025-03-25 12:15:56,327 - training - INFO - Validation loss did not improve. Counter: 5/10
2025-03-25 12:15:56,646 - training - INFO - Starting epoch 601/200000
2025-03-25 12:15:57,416 - training - INFO - Memory: GPU 0: 3565.6MB allocated, 8742.0MB reserved
2025-03-25 12:15:57,417 - training - INFO - Epoch: 601/200000, Batch: 0/45, Loss: 2.2183, Throughput: 72.80 samples/sec
2025-03-25 12:16:09,213 - training - INFO - Memory: GPU 0: 3564.3MB allocated, 9478.0MB reserved
2025-03-25 12:16:09,214 - training - INFO - Epoch: 601/200000, Batch: 15/45, Loss: 2.1373, Throughput: 71.31 samples/sec
2025-03-25 12:16:20,916 - training - INFO - Memory: GPU 0: 3564.3MB allocated, 9480.0MB reserved
2025-03-25 12:16:20,917 - training - INFO - Epoch: 601/200000, Batch: 30/45, Loss: 2.1758, Throughput: 71.53 samples/sec
2025-03-25 12:16:31,948 - training - INFO - Epoch 601 completed in 35.30s. Average loss: 2.0816
2025-03-25 12:16:31,952 - training - INFO - Starting epoch 602/200000
2025-03-25 12:16:32,685 - training - INFO - Memory: GPU 0: 3564.6MB allocated, 9618.0MB reserved
2025-03-25 12:16:32,685 - training - INFO - Epoch: 602/200000, Batch: 0/45, Loss: 1.9050, Throughput: 76.61 samples/sec
2025-03-25 12:16:44,548 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9490.0MB reserved
2025-03-25 12:16:44,549 - training - INFO - Epoch: 602/200000, Batch: 15/45, Loss: 2.1071, Throughput: 71.15 samples/sec
2025-03-25 12:16:56,082 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9490.0MB reserved
2025-03-25 12:16:56,083 - training - INFO - Epoch: 602/200000, Batch: 30/45, Loss: 2.1319, Throughput: 71.95 samples/sec
2025-03-25 12:17:07,008 - training - INFO - Epoch 602 completed in 35.06s. Average loss: 2.0853
2025-03-25 12:17:07,012 - training - INFO - Starting epoch 603/200000
2025-03-25 12:17:07,754 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9628.0MB reserved
2025-03-25 12:17:07,755 - training - INFO - Epoch: 603/200000, Batch: 0/45, Loss: 2.2484, Throughput: 75.67 samples/sec
2025-03-25 12:17:19,645 - training - INFO - Memory: GPU 0: 3563.5MB allocated, 9498.0MB reserved
2025-03-25 12:17:19,645 - training - INFO - Epoch: 603/200000, Batch: 15/45, Loss: 1.9524, Throughput: 70.94 samples/sec
2025-03-25 12:17:31,257 - training - INFO - Memory: GPU 0: 3563.5MB allocated, 9498.0MB reserved
2025-03-25 12:17:31,257 - training - INFO - Epoch: 603/200000, Batch: 30/45, Loss: 1.9701, Throughput: 71.61 samples/sec
2025-03-25 12:17:42,225 - training - INFO - Epoch 603 completed in 35.21s. Average loss: 2.0412
2025-03-25 12:17:42,229 - training - INFO - Starting epoch 604/200000
2025-03-25 12:17:42,963 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9636.0MB reserved
2025-03-25 12:17:42,964 - training - INFO - Epoch: 604/200000, Batch: 0/45, Loss: 1.7953, Throughput: 76.28 samples/sec
2025-03-25 12:17:54,768 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9490.0MB reserved
2025-03-25 12:17:54,768 - training - INFO - Epoch: 604/200000, Batch: 15/45, Loss: 2.0226, Throughput: 71.46 samples/sec
2025-03-25 12:18:06,367 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9490.0MB reserved
2025-03-25 12:18:06,367 - training - INFO - Epoch: 604/200000, Batch: 30/45, Loss: 1.9916, Throughput: 71.92 samples/sec
2025-03-25 12:18:17,311 - training - INFO - Epoch 604 completed in 35.08s. Average loss: 2.0942
2025-03-25 12:18:17,336 - training - INFO - Starting epoch 605/200000
2025-03-25 12:18:18,037 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9628.0MB reserved
2025-03-25 12:18:18,038 - training - INFO - Epoch: 605/200000, Batch: 0/45, Loss: 1.6865, Throughput: 79.98 samples/sec
2025-03-25 12:18:29,914 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9500.0MB reserved
2025-03-25 12:18:29,914 - training - INFO - Epoch: 605/200000, Batch: 15/45, Loss: 2.0166, Throughput: 71.25 samples/sec
2025-03-25 12:18:41,473 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9500.0MB reserved
2025-03-25 12:18:41,473 - training - INFO - Epoch: 605/200000, Batch: 30/45, Loss: 2.0317, Throughput: 71.93 samples/sec
2025-03-25 12:18:52,409 - training - INFO - Epoch 605 completed in 35.07s. Average loss: 2.0466
2025-03-25 12:18:52,413 - training - INFO - Starting epoch 606/200000
2025-03-25 12:18:53,172 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9636.0MB reserved
2025-03-25 12:18:53,173 - training - INFO - Epoch: 606/200000, Batch: 0/45, Loss: 2.8353, Throughput: 73.90 samples/sec
2025-03-25 12:19:05,152 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9512.0MB reserved
2025-03-25 12:19:05,153 - training - INFO - Epoch: 606/200000, Batch: 15/45, Loss: 2.1866, Throughput: 70.34 samples/sec
2025-03-25 12:19:16,800 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9512.0MB reserved
2025-03-25 12:19:16,800 - training - INFO - Epoch: 606/200000, Batch: 30/45, Loss: 2.0936, Throughput: 71.19 samples/sec
2025-03-25 12:19:27,716 - training - INFO - Epoch 606 completed in 35.30s. Average loss: 2.0364
2025-03-25 12:19:27,720 - training - INFO - Starting epoch 607/200000
2025-03-25 12:19:28,458 - training - INFO - Memory: GPU 0: 3557.2MB allocated, 9650.0MB reserved
2025-03-25 12:19:28,459 - training - INFO - Epoch: 607/200000, Batch: 0/45, Loss: 2.4071, Throughput: 76.02 samples/sec
2025-03-25 12:19:40,306 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9504.0MB reserved
2025-03-25 12:19:40,307 - training - INFO - Epoch: 607/200000, Batch: 15/45, Loss: 2.1536, Throughput: 71.20 samples/sec
2025-03-25 12:19:51,860 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9504.0MB reserved
2025-03-25 12:19:51,861 - training - INFO - Epoch: 607/200000, Batch: 30/45, Loss: 2.1071, Throughput: 71.92 samples/sec
2025-03-25 12:20:02,813 - training - INFO - Epoch 607 completed in 35.09s. Average loss: 2.0614
2025-03-25 12:20:02,817 - training - INFO - Starting epoch 608/200000
2025-03-25 12:20:03,564 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9642.0MB reserved
2025-03-25 12:20:03,564 - training - INFO - Epoch: 608/200000, Batch: 0/45, Loss: 2.0772, Throughput: 75.18 samples/sec
2025-03-25 12:20:15,398 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9510.0MB reserved
2025-03-25 12:20:15,398 - training - INFO - Epoch: 608/200000, Batch: 15/45, Loss: 2.0682, Throughput: 71.23 samples/sec
2025-03-25 12:20:27,022 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9510.0MB reserved
2025-03-25 12:20:27,023 - training - INFO - Epoch: 608/200000, Batch: 30/45, Loss: 2.1019, Throughput: 71.73 samples/sec
2025-03-25 12:20:37,966 - training - INFO - Epoch 608 completed in 35.15s. Average loss: 2.1308
2025-03-25 12:20:37,970 - training - INFO - Starting epoch 609/200000
2025-03-25 12:20:38,710 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9648.0MB reserved
2025-03-25 12:20:38,710 - training - INFO - Epoch: 609/200000, Batch: 0/45, Loss: 1.6956, Throughput: 75.73 samples/sec
2025-03-25 12:20:50,542 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9488.0MB reserved
2025-03-25 12:20:50,542 - training - INFO - Epoch: 609/200000, Batch: 15/45, Loss: 2.0932, Throughput: 71.27 samples/sec
2025-03-25 12:21:02,202 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9490.0MB reserved
2025-03-25 12:21:02,202 - training - INFO - Epoch: 609/200000, Batch: 30/45, Loss: 2.1496, Throughput: 71.65 samples/sec
2025-03-25 12:21:13,169 - training - INFO - Epoch 609 completed in 35.20s. Average loss: 2.0788
2025-03-25 12:21:13,172 - training - INFO - Starting epoch 610/200000
2025-03-25 12:21:13,919 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9628.0MB reserved
2025-03-25 12:21:13,919 - training - INFO - Epoch: 610/200000, Batch: 0/45, Loss: 2.4768, Throughput: 75.22 samples/sec
2025-03-25 12:21:25,767 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9510.0MB reserved
2025-03-25 12:21:25,768 - training - INFO - Epoch: 610/200000, Batch: 15/45, Loss: 2.0797, Throughput: 71.15 samples/sec
2025-03-25 12:21:37,373 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9510.0MB reserved
2025-03-25 12:21:37,374 - training - INFO - Epoch: 610/200000, Batch: 30/45, Loss: 2.0091, Throughput: 71.74 samples/sec
2025-03-25 12:21:48,368 - training - INFO - Epoch 610 completed in 35.20s. Average loss: 2.0522
2025-03-25 12:21:48,371 - training - INFO - Starting epoch 611/200000
2025-03-25 12:21:49,126 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9648.0MB reserved
2025-03-25 12:21:49,126 - training - INFO - Epoch: 611/200000, Batch: 0/45, Loss: 1.8556, Throughput: 74.33 samples/sec
2025-03-25 12:22:01,044 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9400.0MB reserved
2025-03-25 12:22:01,044 - training - INFO - Epoch: 611/200000, Batch: 15/45, Loss: 1.9326, Throughput: 70.71 samples/sec
2025-03-25 12:22:12,713 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9400.0MB reserved
2025-03-25 12:22:12,714 - training - INFO - Epoch: 611/200000, Batch: 30/45, Loss: 1.9856, Throughput: 71.32 samples/sec
2025-03-25 12:22:23,774 - training - INFO - Epoch 611 completed in 35.40s. Average loss: 2.0235
2025-03-25 12:22:23,778 - training - INFO - Starting epoch 612/200000
2025-03-25 12:22:24,522 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9538.0MB reserved
2025-03-25 12:22:24,523 - training - INFO - Epoch: 612/200000, Batch: 0/45, Loss: 2.0848, Throughput: 75.41 samples/sec
2025-03-25 12:22:36,366 - training - INFO - Memory: GPU 0: 3556.7MB allocated, 9492.0MB reserved
2025-03-25 12:22:36,366 - training - INFO - Epoch: 612/200000, Batch: 15/45, Loss: 2.0369, Throughput: 71.19 samples/sec
2025-03-25 12:22:48,009 - training - INFO - Memory: GPU 0: 3556.7MB allocated, 9492.0MB reserved
2025-03-25 12:22:48,009 - training - INFO - Epoch: 612/200000, Batch: 30/45, Loss: 2.0653, Throughput: 71.65 samples/sec
2025-03-25 12:22:58,921 - training - INFO - Epoch 612 completed in 35.14s. Average loss: 2.0436
2025-03-25 12:22:58,925 - training - INFO - Starting epoch 613/200000
2025-03-25 12:22:59,672 - training - INFO - Memory: GPU 0: 3556.7MB allocated, 9630.0MB reserved
2025-03-25 12:22:59,672 - training - INFO - Epoch: 613/200000, Batch: 0/45, Loss: 1.5140, Throughput: 75.20 samples/sec
2025-03-25 12:23:11,568 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9496.0MB reserved
2025-03-25 12:23:11,569 - training - INFO - Epoch: 613/200000, Batch: 15/45, Loss: 1.8776, Throughput: 70.88 samples/sec
2025-03-25 12:23:23,187 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9496.0MB reserved
2025-03-25 12:23:23,187 - training - INFO - Epoch: 613/200000, Batch: 30/45, Loss: 1.9764, Throughput: 71.56 samples/sec
2025-03-25 12:23:34,129 - training - INFO - Epoch 613 completed in 35.20s. Average loss: 2.0436
2025-03-25 12:23:34,133 - training - INFO - Starting epoch 614/200000
2025-03-25 12:23:34,872 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9634.0MB reserved
2025-03-25 12:23:34,872 - training - INFO - Epoch: 614/200000, Batch: 0/45, Loss: 2.2471, Throughput: 76.03 samples/sec
2025-03-25 12:23:46,726 - training - INFO - Memory: GPU 0: 3557.3MB allocated, 9500.0MB reserved
2025-03-25 12:23:46,727 - training - INFO - Epoch: 614/200000, Batch: 15/45, Loss: 2.1114, Throughput: 71.16 samples/sec
2025-03-25 12:23:58,318 - training - INFO - Memory: GPU 0: 3557.3MB allocated, 9500.0MB reserved
2025-03-25 12:23:58,319 - training - INFO - Epoch: 614/200000, Batch: 30/45, Loss: 2.1420, Throughput: 71.79 samples/sec
2025-03-25 12:24:09,223 - training - INFO - Epoch 614 completed in 35.09s. Average loss: 2.1572
2025-03-25 12:24:09,226 - training - INFO - Starting epoch 615/200000
2025-03-25 12:24:09,977 - training - INFO - Memory: GPU 0: 3557.3MB allocated, 9638.0MB reserved
2025-03-25 12:24:09,977 - training - INFO - Epoch: 615/200000, Batch: 0/45, Loss: 2.4225, Throughput: 74.75 samples/sec
2025-03-25 12:24:21,900 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9506.0MB reserved
2025-03-25 12:24:21,900 - training - INFO - Epoch: 615/200000, Batch: 15/45, Loss: 2.0644, Throughput: 70.72 samples/sec
2025-03-25 12:24:33,449 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9506.0MB reserved
2025-03-25 12:24:33,450 - training - INFO - Epoch: 615/200000, Batch: 30/45, Loss: 2.0228, Throughput: 71.67 samples/sec
2025-03-25 12:24:44,359 - training - INFO - Epoch 615 completed in 35.13s. Average loss: 2.0327
2025-03-25 12:24:44,363 - training - INFO - Starting epoch 616/200000
2025-03-25 12:24:45,102 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9644.0MB reserved
2025-03-25 12:24:45,103 - training - INFO - Epoch: 616/200000, Batch: 0/45, Loss: 2.2641, Throughput: 75.89 samples/sec
2025-03-25 12:24:57,011 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9498.0MB reserved
2025-03-25 12:24:57,012 - training - INFO - Epoch: 616/200000, Batch: 15/45, Loss: 2.0402, Throughput: 70.85 samples/sec
2025-03-25 12:25:08,648 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9498.0MB reserved
2025-03-25 12:25:08,648 - training - INFO - Epoch: 616/200000, Batch: 30/45, Loss: 1.9802, Throughput: 71.49 samples/sec
2025-03-25 12:25:19,530 - training - INFO - Epoch 616 completed in 35.17s. Average loss: 2.0147
2025-03-25 12:25:19,534 - training - INFO - Starting epoch 617/200000
2025-03-25 12:25:20,282 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9636.0MB reserved
2025-03-25 12:25:20,282 - training - INFO - Epoch: 617/200000, Batch: 0/45, Loss: 1.7334, Throughput: 75.04 samples/sec
2025-03-25 12:25:32,110 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9502.0MB reserved
2025-03-25 12:25:32,111 - training - INFO - Epoch: 617/200000, Batch: 15/45, Loss: 1.9786, Throughput: 71.25 samples/sec
2025-03-25 12:25:43,667 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9504.0MB reserved
2025-03-25 12:25:43,668 - training - INFO - Epoch: 617/200000, Batch: 30/45, Loss: 1.9932, Throughput: 71.94 samples/sec
2025-03-25 12:25:54,627 - training - INFO - Epoch 617 completed in 35.09s. Average loss: 2.0456
2025-03-25 12:25:54,631 - training - INFO - Starting epoch 618/200000
2025-03-25 12:25:55,366 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9642.0MB reserved
2025-03-25 12:25:55,366 - training - INFO - Epoch: 618/200000, Batch: 0/45, Loss: 2.2874, Throughput: 76.43 samples/sec
2025-03-25 12:26:07,195 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9502.0MB reserved
2025-03-25 12:26:07,195 - training - INFO - Epoch: 618/200000, Batch: 15/45, Loss: 1.9244, Throughput: 71.32 samples/sec
2025-03-25 12:26:18,723 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9502.0MB reserved
2025-03-25 12:26:18,724 - training - INFO - Epoch: 618/200000, Batch: 30/45, Loss: 2.0207, Throughput: 72.06 samples/sec
2025-03-25 12:26:29,602 - training - INFO - Epoch 618 completed in 34.97s. Average loss: 2.0514
2025-03-25 12:26:29,605 - training - INFO - Starting epoch 619/200000
2025-03-25 12:26:30,357 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9642.0MB reserved
2025-03-25 12:26:30,358 - training - INFO - Epoch: 619/200000, Batch: 0/45, Loss: 2.2502, Throughput: 74.65 samples/sec
2025-03-25 12:26:42,217 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9494.0MB reserved
2025-03-25 12:26:42,218 - training - INFO - Epoch: 619/200000, Batch: 15/45, Loss: 2.0111, Throughput: 71.06 samples/sec
2025-03-25 12:26:53,865 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9494.0MB reserved
2025-03-25 12:26:53,865 - training - INFO - Epoch: 619/200000, Batch: 30/45, Loss: 2.0335, Throughput: 71.56 samples/sec
2025-03-25 12:27:04,829 - training - INFO - Epoch 619 completed in 35.22s. Average loss: 1.9917
2025-03-25 12:27:04,832 - training - INFO - Starting epoch 620/200000
2025-03-25 12:27:05,585 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9632.0MB reserved
2025-03-25 12:27:05,586 - training - INFO - Epoch: 620/200000, Batch: 0/45, Loss: 1.9403, Throughput: 74.55 samples/sec
2025-03-25 12:27:17,504 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9498.0MB reserved
2025-03-25 12:27:17,504 - training - INFO - Epoch: 620/200000, Batch: 15/45, Loss: 2.0080, Throughput: 70.72 samples/sec
2025-03-25 12:27:29,157 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9498.0MB reserved
2025-03-25 12:27:29,157 - training - INFO - Epoch: 620/200000, Batch: 30/45, Loss: 1.9646, Throughput: 71.37 samples/sec
2025-03-25 12:27:40,197 - training - INFO - Epoch 620 completed in 35.36s. Average loss: 1.9987
2025-03-25 12:27:40,201 - training - INFO - Starting epoch 621/200000
2025-03-25 12:27:40,966 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9636.0MB reserved
2025-03-25 12:27:40,966 - training - INFO - Epoch: 621/200000, Batch: 0/45, Loss: 2.6474, Throughput: 73.33 samples/sec
2025-03-25 12:27:52,951 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9500.0MB reserved
2025-03-25 12:27:52,951 - training - INFO - Epoch: 621/200000, Batch: 15/45, Loss: 2.0859, Throughput: 70.28 samples/sec
2025-03-25 12:28:04,607 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9500.0MB reserved
2025-03-25 12:28:04,608 - training - INFO - Epoch: 621/200000, Batch: 30/45, Loss: 2.0846, Throughput: 71.14 samples/sec
2025-03-25 12:28:15,552 - training - INFO - Epoch 621 completed in 35.35s. Average loss: 2.0360
2025-03-25 12:28:15,556 - training - INFO - Starting epoch 622/200000
2025-03-25 12:28:16,292 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9638.0MB reserved
2025-03-25 12:28:16,293 - training - INFO - Epoch: 622/200000, Batch: 0/45, Loss: 1.9528, Throughput: 76.17 samples/sec
2025-03-25 12:28:28,237 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9494.0MB reserved
2025-03-25 12:28:28,237 - training - INFO - Epoch: 622/200000, Batch: 15/45, Loss: 2.0389, Throughput: 70.67 samples/sec
2025-03-25 12:28:39,940 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9494.0MB reserved
2025-03-25 12:28:39,941 - training - INFO - Epoch: 622/200000, Batch: 30/45, Loss: 1.9976, Throughput: 71.20 samples/sec
2025-03-25 12:28:50,911 - training - INFO - Epoch 622 completed in 35.35s. Average loss: 2.0647
2025-03-25 12:28:50,914 - training - INFO - Starting epoch 623/200000
2025-03-25 12:28:51,652 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9632.0MB reserved
2025-03-25 12:28:51,652 - training - INFO - Epoch: 623/200000, Batch: 0/45, Loss: 2.1295, Throughput: 76.10 samples/sec
2025-03-25 12:29:03,540 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9500.0MB reserved
2025-03-25 12:29:03,540 - training - INFO - Epoch: 623/200000, Batch: 15/45, Loss: 1.9161, Throughput: 70.97 samples/sec
2025-03-25 12:29:15,188 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9500.0MB reserved
2025-03-25 12:29:15,189 - training - INFO - Epoch: 623/200000, Batch: 30/45, Loss: 1.9496, Throughput: 71.52 samples/sec
2025-03-25 12:29:26,160 - training - INFO - Epoch 623 completed in 35.25s. Average loss: 1.9864
2025-03-25 12:29:26,163 - training - INFO - Starting epoch 624/200000
2025-03-25 12:29:26,897 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9640.0MB reserved
2025-03-25 12:29:26,897 - training - INFO - Epoch: 624/200000, Batch: 0/45, Loss: 2.2675, Throughput: 76.56 samples/sec
2025-03-25 12:29:38,926 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9490.0MB reserved
2025-03-25 12:29:38,926 - training - INFO - Epoch: 624/200000, Batch: 15/45, Loss: 2.0371, Throughput: 70.21 samples/sec
2025-03-25 12:29:50,599 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9492.0MB reserved
2025-03-25 12:29:50,599 - training - INFO - Epoch: 624/200000, Batch: 30/45, Loss: 2.0174, Throughput: 71.05 samples/sec
2025-03-25 12:30:01,593 - training - INFO - Epoch 624 completed in 35.43s. Average loss: 2.0324
2025-03-25 12:30:01,596 - training - INFO - Starting epoch 625/200000
2025-03-25 12:30:02,336 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9630.0MB reserved
2025-03-25 12:30:02,336 - training - INFO - Epoch: 625/200000, Batch: 0/45, Loss: 1.7783, Throughput: 75.92 samples/sec
2025-03-25 12:30:14,301 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9400.0MB reserved
2025-03-25 12:30:14,301 - training - INFO - Epoch: 625/200000, Batch: 15/45, Loss: 1.9479, Throughput: 70.53 samples/sec
2025-03-25 12:30:25,944 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9402.0MB reserved
2025-03-25 12:30:25,944 - training - INFO - Epoch: 625/200000, Batch: 30/45, Loss: 1.9533, Throughput: 71.31 samples/sec
2025-03-25 12:30:36,900 - training - INFO - Epoch 625 completed in 35.30s. Average loss: 2.0603
2025-03-25 12:30:36,905 - training - INFO - Starting epoch 626/200000
2025-03-25 12:30:37,653 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9646.0MB reserved
2025-03-25 12:30:37,653 - training - INFO - Epoch: 626/200000, Batch: 0/45, Loss: 2.2308, Throughput: 75.13 samples/sec
2025-03-25 12:30:49,627 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9508.0MB reserved
2025-03-25 12:30:49,627 - training - INFO - Epoch: 626/200000, Batch: 15/45, Loss: 2.0022, Throughput: 70.44 samples/sec
2025-03-25 12:31:01,198 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9508.0MB reserved
2025-03-25 12:31:01,199 - training - INFO - Epoch: 626/200000, Batch: 30/45, Loss: 2.0367, Throughput: 71.46 samples/sec
2025-03-25 12:31:12,161 - training - INFO - Epoch 626 completed in 35.26s. Average loss: 2.1011
2025-03-25 12:31:12,165 - training - INFO - Starting epoch 627/200000
2025-03-25 12:31:12,921 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9646.0MB reserved
2025-03-25 12:31:12,922 - training - INFO - Epoch: 627/200000, Batch: 0/45, Loss: 1.9070, Throughput: 74.17 samples/sec
2025-03-25 12:31:24,874 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9498.0MB reserved
2025-03-25 12:31:24,874 - training - INFO - Epoch: 627/200000, Batch: 15/45, Loss: 1.9564, Throughput: 70.51 samples/sec
2025-03-25 12:31:36,581 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9498.0MB reserved
2025-03-25 12:31:36,581 - training - INFO - Epoch: 627/200000, Batch: 30/45, Loss: 1.9323, Throughput: 71.10 samples/sec
2025-03-25 12:31:47,570 - training - INFO - Epoch 627 completed in 35.41s. Average loss: 2.0716
2025-03-25 12:31:47,573 - training - INFO - Starting epoch 628/200000
2025-03-25 12:31:48,326 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9636.0MB reserved
2025-03-25 12:31:48,327 - training - INFO - Epoch: 628/200000, Batch: 0/45, Loss: 1.7868, Throughput: 74.53 samples/sec
2025-03-25 12:32:00,210 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9486.0MB reserved
2025-03-25 12:32:00,210 - training - INFO - Epoch: 628/200000, Batch: 15/45, Loss: 2.0781, Throughput: 70.92 samples/sec
2025-03-25 12:32:11,827 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9486.0MB reserved
2025-03-25 12:32:11,828 - training - INFO - Epoch: 628/200000, Batch: 30/45, Loss: 2.1058, Throughput: 71.59 samples/sec
2025-03-25 12:32:22,731 - training - INFO - Epoch 628 completed in 35.16s. Average loss: 2.0610
2025-03-25 12:32:22,735 - training - INFO - Starting epoch 629/200000
2025-03-25 12:32:23,480 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9622.0MB reserved
2025-03-25 12:32:23,480 - training - INFO - Epoch: 629/200000, Batch: 0/45, Loss: 1.9809, Throughput: 75.42 samples/sec
2025-03-25 12:32:35,387 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9492.0MB reserved
2025-03-25 12:32:35,387 - training - INFO - Epoch: 629/200000, Batch: 15/45, Loss: 2.0741, Throughput: 70.83 samples/sec
2025-03-25 12:32:47,051 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9492.0MB reserved
2025-03-25 12:32:47,051 - training - INFO - Epoch: 629/200000, Batch: 30/45, Loss: 2.0351, Throughput: 71.40 samples/sec
2025-03-25 12:32:58,025 - training - INFO - Epoch 629 completed in 35.29s. Average loss: 2.0684
2025-03-25 12:32:58,028 - training - INFO - Starting epoch 630/200000
2025-03-25 12:32:58,766 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9630.0MB reserved
2025-03-25 12:32:58,766 - training - INFO - Epoch: 630/200000, Batch: 0/45, Loss: 2.2465, Throughput: 76.17 samples/sec
2025-03-25 12:33:10,722 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9506.0MB reserved
2025-03-25 12:33:10,722 - training - INFO - Epoch: 630/200000, Batch: 15/45, Loss: 1.9985, Throughput: 70.59 samples/sec
2025-03-25 12:33:22,461 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9506.0MB reserved
2025-03-25 12:33:22,461 - training - INFO - Epoch: 630/200000, Batch: 30/45, Loss: 1.9234, Throughput: 71.06 samples/sec
2025-03-25 12:33:33,427 - training - INFO - Epoch 630 completed in 35.40s. Average loss: 2.0264
2025-03-25 12:33:33,430 - training - INFO - Starting epoch 631/200000
2025-03-25 12:33:34,202 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9644.0MB reserved
2025-03-25 12:33:34,202 - training - INFO - Epoch: 631/200000, Batch: 0/45, Loss: 1.8835, Throughput: 72.74 samples/sec
2025-03-25 12:33:46,082 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9490.0MB reserved
2025-03-25 12:33:46,082 - training - INFO - Epoch: 631/200000, Batch: 15/45, Loss: 2.0185, Throughput: 70.84 samples/sec
2025-03-25 12:33:57,715 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9490.0MB reserved
2025-03-25 12:33:57,716 - training - INFO - Epoch: 631/200000, Batch: 30/45, Loss: 2.0381, Throughput: 71.49 samples/sec
2025-03-25 12:34:08,653 - training - INFO - Epoch 631 completed in 35.22s. Average loss: 2.0256
2025-03-25 12:34:08,657 - training - INFO - Starting epoch 632/200000
2025-03-25 12:34:09,394 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9628.0MB reserved
2025-03-25 12:34:09,395 - training - INFO - Epoch: 632/200000, Batch: 0/45, Loss: 1.9796, Throughput: 76.09 samples/sec
2025-03-25 12:34:21,293 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9494.0MB reserved
2025-03-25 12:34:21,294 - training - INFO - Epoch: 632/200000, Batch: 15/45, Loss: 2.0442, Throughput: 70.92 samples/sec
2025-03-25 12:34:32,843 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9494.0MB reserved
2025-03-25 12:34:32,844 - training - INFO - Epoch: 632/200000, Batch: 30/45, Loss: 2.0171, Throughput: 71.78 samples/sec
2025-03-25 12:34:43,756 - training - INFO - Epoch 632 completed in 35.10s. Average loss: 2.0145
2025-03-25 12:34:43,760 - training - INFO - Starting epoch 633/200000
2025-03-25 12:34:44,488 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9632.0MB reserved
2025-03-25 12:34:44,489 - training - INFO - Epoch: 633/200000, Batch: 0/45, Loss: 1.6539, Throughput: 76.92 samples/sec
2025-03-25 12:34:56,297 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9514.0MB reserved
2025-03-25 12:34:56,297 - training - INFO - Epoch: 633/200000, Batch: 15/45, Loss: 1.9978, Throughput: 71.47 samples/sec
2025-03-25 12:35:07,933 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9514.0MB reserved
2025-03-25 12:35:07,934 - training - INFO - Epoch: 633/200000, Batch: 30/45, Loss: 2.0198, Throughput: 71.82 samples/sec
2025-03-25 12:35:18,906 - training - INFO - Epoch 633 completed in 35.15s. Average loss: 1.9910
2025-03-25 12:35:18,909 - training - INFO - Starting epoch 634/200000
2025-03-25 12:35:19,670 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9650.0MB reserved
2025-03-25 12:35:19,670 - training - INFO - Epoch: 634/200000, Batch: 0/45, Loss: 2.1254, Throughput: 73.72 samples/sec
2025-03-25 12:35:31,527 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9502.0MB reserved
2025-03-25 12:35:31,527 - training - INFO - Epoch: 634/200000, Batch: 15/45, Loss: 2.0676, Throughput: 71.02 samples/sec
2025-03-25 12:35:43,182 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9502.0MB reserved
2025-03-25 12:35:43,182 - training - INFO - Epoch: 634/200000, Batch: 30/45, Loss: 2.0648, Throughput: 71.53 samples/sec
2025-03-25 12:35:54,167 - training - INFO - Epoch 634 completed in 35.26s. Average loss: 2.0482
2025-03-25 12:35:54,170 - training - INFO - Starting epoch 635/200000
2025-03-25 12:35:54,926 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9638.0MB reserved
2025-03-25 12:35:54,927 - training - INFO - Epoch: 635/200000, Batch: 0/45, Loss: 2.0709, Throughput: 74.27 samples/sec
2025-03-25 12:36:06,792 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9504.0MB reserved
2025-03-25 12:36:06,793 - training - INFO - Epoch: 635/200000, Batch: 15/45, Loss: 1.9666, Throughput: 71.00 samples/sec
2025-03-25 12:36:18,337 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9504.0MB reserved
2025-03-25 12:36:18,337 - training - INFO - Epoch: 635/200000, Batch: 30/45, Loss: 2.0214, Throughput: 71.84 samples/sec
2025-03-25 12:36:29,250 - training - INFO - Epoch 635 completed in 35.08s. Average loss: 1.9930
2025-03-25 12:36:29,253 - training - INFO - Starting epoch 636/200000
2025-03-25 12:36:30,005 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9644.0MB reserved
2025-03-25 12:36:30,005 - training - INFO - Epoch: 636/200000, Batch: 0/45, Loss: 1.7038, Throughput: 74.68 samples/sec
2025-03-25 12:36:41,945 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9516.0MB reserved
2025-03-25 12:36:41,946 - training - INFO - Epoch: 636/200000, Batch: 15/45, Loss: 1.9229, Throughput: 70.61 samples/sec
2025-03-25 12:36:53,629 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9516.0MB reserved
2025-03-25 12:36:53,630 - training - INFO - Epoch: 636/200000, Batch: 30/45, Loss: 2.0310, Throughput: 71.22 samples/sec
2025-03-25 12:37:04,633 - training - INFO - Epoch 636 completed in 35.38s. Average loss: 2.0933
2025-03-25 12:37:04,637 - training - INFO - Starting epoch 637/200000
2025-03-25 12:37:05,378 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9652.0MB reserved
2025-03-25 12:37:05,378 - training - INFO - Epoch: 637/200000, Batch: 0/45, Loss: 2.2053, Throughput: 75.72 samples/sec
2025-03-25 12:37:17,271 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9504.0MB reserved
2025-03-25 12:37:17,272 - training - INFO - Epoch: 637/200000, Batch: 15/45, Loss: 2.1340, Throughput: 70.93 samples/sec
2025-03-25 12:37:28,824 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9504.0MB reserved
2025-03-25 12:37:28,824 - training - INFO - Epoch: 637/200000, Batch: 30/45, Loss: 2.0903, Throughput: 71.78 samples/sec
2025-03-25 12:37:39,733 - training - INFO - Epoch 637 completed in 35.10s. Average loss: 2.0879
2025-03-25 12:37:39,737 - training - INFO - Starting epoch 638/200000
2025-03-25 12:37:40,490 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9642.0MB reserved
2025-03-25 12:37:40,490 - training - INFO - Epoch: 638/200000, Batch: 0/45, Loss: 1.7655, Throughput: 74.48 samples/sec
2025-03-25 12:37:52,458 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9492.0MB reserved
2025-03-25 12:37:52,458 - training - INFO - Epoch: 638/200000, Batch: 15/45, Loss: 1.9758, Throughput: 70.45 samples/sec
2025-03-25 12:38:04,061 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9492.0MB reserved
2025-03-25 12:38:04,062 - training - INFO - Epoch: 638/200000, Batch: 30/45, Loss: 1.9903, Throughput: 71.38 samples/sec
2025-03-25 12:38:15,085 - training - INFO - Epoch 638 completed in 35.35s. Average loss: 2.0071
2025-03-25 12:38:15,088 - training - INFO - Starting epoch 639/200000
2025-03-25 12:38:15,841 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9630.0MB reserved
2025-03-25 12:38:15,842 - training - INFO - Epoch: 639/200000, Batch: 0/45, Loss: 1.8367, Throughput: 74.52 samples/sec
2025-03-25 12:38:27,863 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9498.0MB reserved
2025-03-25 12:38:27,863 - training - INFO - Epoch: 639/200000, Batch: 15/45, Loss: 2.0394, Throughput: 70.15 samples/sec
2025-03-25 12:38:39,543 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9498.0MB reserved
2025-03-25 12:38:39,543 - training - INFO - Epoch: 639/200000, Batch: 30/45, Loss: 1.9950, Throughput: 70.99 samples/sec
2025-03-25 12:38:50,508 - training - INFO - Epoch 639 completed in 35.42s. Average loss: 2.0307
2025-03-25 12:38:50,512 - training - INFO - Starting epoch 640/200000
2025-03-25 12:38:51,248 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9638.0MB reserved
2025-03-25 12:38:51,248 - training - INFO - Epoch: 640/200000, Batch: 0/45, Loss: 1.6075, Throughput: 76.24 samples/sec
2025-03-25 12:39:03,225 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9498.0MB reserved
2025-03-25 12:39:03,225 - training - INFO - Epoch: 640/200000, Batch: 15/45, Loss: 1.9884, Throughput: 70.49 samples/sec
2025-03-25 12:39:14,892 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9498.0MB reserved
2025-03-25 12:39:14,893 - training - INFO - Epoch: 640/200000, Batch: 30/45, Loss: 2.0015, Throughput: 71.21 samples/sec
2025-03-25 12:39:25,876 - training - INFO - Epoch 640 completed in 35.36s. Average loss: 2.0173
2025-03-25 12:39:25,880 - training - INFO - Starting epoch 641/200000
2025-03-25 12:39:26,627 - training - INFO - Memory: GPU 0: 3562.5MB allocated, 9636.0MB reserved
2025-03-25 12:39:26,628 - training - INFO - Epoch: 641/200000, Batch: 0/45, Loss: 2.1892, Throughput: 75.15 samples/sec
2025-03-25 12:39:38,523 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9498.0MB reserved
2025-03-25 12:39:38,523 - training - INFO - Epoch: 641/200000, Batch: 15/45, Loss: 1.9701, Throughput: 70.89 samples/sec
2025-03-25 12:39:50,260 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9498.0MB reserved
2025-03-25 12:39:50,261 - training - INFO - Epoch: 641/200000, Batch: 30/45, Loss: 1.9811, Throughput: 71.21 samples/sec
2025-03-25 12:40:01,252 - training - INFO - Epoch 641 completed in 35.37s. Average loss: 2.0357
2025-03-25 12:40:01,256 - training - INFO - Starting epoch 642/200000
2025-03-25 12:40:02,006 - training - INFO - Memory: GPU 0: 3557.3MB allocated, 9636.0MB reserved
2025-03-25 12:40:02,007 - training - INFO - Epoch: 642/200000, Batch: 0/45, Loss: 2.0660, Throughput: 74.86 samples/sec
2025-03-25 12:40:13,844 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9502.0MB reserved
2025-03-25 12:40:13,845 - training - INFO - Epoch: 642/200000, Batch: 15/45, Loss: 1.9794, Throughput: 71.19 samples/sec
2025-03-25 12:40:25,448 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9502.0MB reserved
2025-03-25 12:40:25,448 - training - INFO - Epoch: 642/200000, Batch: 30/45, Loss: 2.0234, Throughput: 71.77 samples/sec
2025-03-25 12:40:36,406 - training - INFO - Epoch 642 completed in 35.15s. Average loss: 2.0352
2025-03-25 12:40:36,410 - training - INFO - Starting epoch 643/200000
2025-03-25 12:40:37,165 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9640.0MB reserved
2025-03-25 12:40:37,165 - training - INFO - Epoch: 643/200000, Batch: 0/45, Loss: 2.0555, Throughput: 74.36 samples/sec
2025-03-25 12:40:49,153 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9484.0MB reserved
2025-03-25 12:40:49,153 - training - INFO - Epoch: 643/200000, Batch: 15/45, Loss: 2.0178, Throughput: 70.33 samples/sec
2025-03-25 12:41:00,781 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9484.0MB reserved
2025-03-25 12:41:00,782 - training - INFO - Epoch: 643/200000, Batch: 30/45, Loss: 2.0066, Throughput: 71.24 samples/sec
2025-03-25 12:41:11,740 - training - INFO - Epoch 643 completed in 35.33s. Average loss: 1.9841
2025-03-25 12:41:11,743 - training - INFO - Starting epoch 644/200000
2025-03-25 12:41:12,503 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9620.0MB reserved
2025-03-25 12:41:12,503 - training - INFO - Epoch: 644/200000, Batch: 0/45, Loss: 1.9350, Throughput: 73.99 samples/sec
2025-03-25 12:41:24,415 - training - INFO - Memory: GPU 0: 3562.9MB allocated, 9510.0MB reserved
2025-03-25 12:41:24,415 - training - INFO - Epoch: 644/200000, Batch: 15/45, Loss: 1.8259, Throughput: 70.71 samples/sec
2025-03-25 12:41:36,007 - training - INFO - Memory: GPU 0: 3562.9MB allocated, 9510.0MB reserved
2025-03-25 12:41:36,008 - training - INFO - Epoch: 644/200000, Batch: 30/45, Loss: 1.9261, Throughput: 71.55 samples/sec
2025-03-25 12:41:46,912 - training - INFO - Epoch 644 completed in 35.17s. Average loss: 2.0160
2025-03-25 12:41:46,915 - training - INFO - Starting epoch 645/200000
2025-03-25 12:41:47,666 - training - INFO - Memory: GPU 0: 3562.9MB allocated, 9650.0MB reserved
2025-03-25 12:41:47,666 - training - INFO - Epoch: 645/200000, Batch: 0/45, Loss: 2.1297, Throughput: 74.68 samples/sec
2025-03-25 12:41:59,606 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9398.0MB reserved
2025-03-25 12:41:59,606 - training - INFO - Epoch: 645/200000, Batch: 15/45, Loss: 2.1746, Throughput: 70.61 samples/sec
2025-03-25 12:42:11,166 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9400.0MB reserved
2025-03-25 12:42:11,167 - training - INFO - Epoch: 645/200000, Batch: 30/45, Loss: 2.0683, Throughput: 71.59 samples/sec
2025-03-25 12:42:22,076 - training - INFO - Epoch 645 completed in 35.16s. Average loss: 2.0258
2025-03-25 12:42:22,080 - training - INFO - Starting epoch 646/200000
2025-03-25 12:42:22,825 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9538.0MB reserved
2025-03-25 12:42:22,825 - training - INFO - Epoch: 646/200000, Batch: 0/45, Loss: 1.9185, Throughput: 75.36 samples/sec
2025-03-25 12:42:34,670 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9498.0MB reserved
2025-03-25 12:42:34,671 - training - INFO - Epoch: 646/200000, Batch: 15/45, Loss: 1.8953, Throughput: 71.17 samples/sec
2025-03-25 12:42:46,202 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9500.0MB reserved
2025-03-25 12:42:46,203 - training - INFO - Epoch: 646/200000, Batch: 30/45, Loss: 1.8902, Throughput: 71.97 samples/sec
2025-03-25 12:42:57,120 - training - INFO - Epoch 646 completed in 35.04s. Average loss: 1.9863
2025-03-25 12:42:57,124 - training - INFO - Starting epoch 647/200000
2025-03-25 12:42:57,879 - training - INFO - Memory: GPU 0: 3556.1MB allocated, 9638.0MB reserved
2025-03-25 12:42:57,879 - training - INFO - Epoch: 647/200000, Batch: 0/45, Loss: 2.0957, Throughput: 74.25 samples/sec
2025-03-25 12:43:09,800 - training - INFO - Memory: GPU 0: 3562.9MB allocated, 9482.0MB reserved
2025-03-25 12:43:09,800 - training - INFO - Epoch: 647/200000, Batch: 15/45, Loss: 1.9162, Throughput: 70.69 samples/sec
2025-03-25 12:43:21,489 - training - INFO - Memory: GPU 0: 3562.9MB allocated, 9482.0MB reserved
2025-03-25 12:43:21,490 - training - INFO - Epoch: 647/200000, Batch: 30/45, Loss: 1.9114, Throughput: 71.25 samples/sec
2025-03-25 12:43:32,455 - training - INFO - Epoch 647 completed in 35.33s. Average loss: 2.0160
2025-03-25 12:43:32,459 - training - INFO - Starting epoch 648/200000
2025-03-25 12:43:33,201 - training - INFO - Memory: GPU 0: 3562.9MB allocated, 9622.0MB reserved
2025-03-25 12:43:33,201 - training - INFO - Epoch: 648/200000, Batch: 0/45, Loss: 2.0209, Throughput: 75.54 samples/sec
2025-03-25 12:43:45,230 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9502.0MB reserved
2025-03-25 12:43:45,231 - training - INFO - Epoch: 648/200000, Batch: 15/45, Loss: 1.9769, Throughput: 70.17 samples/sec
2025-03-25 12:43:56,928 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9502.0MB reserved
2025-03-25 12:43:56,928 - training - INFO - Epoch: 648/200000, Batch: 30/45, Loss: 2.0754, Throughput: 70.95 samples/sec
2025-03-25 12:44:07,888 - training - INFO - Epoch 648 completed in 35.43s. Average loss: 2.0885
2025-03-25 12:44:07,892 - training - INFO - Starting epoch 649/200000
2025-03-25 12:44:08,638 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9640.0MB reserved
2025-03-25 12:44:08,638 - training - INFO - Epoch: 649/200000, Batch: 0/45, Loss: 2.2368, Throughput: 75.12 samples/sec
2025-03-25 12:44:20,456 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9502.0MB reserved
2025-03-25 12:44:20,456 - training - INFO - Epoch: 649/200000, Batch: 15/45, Loss: 2.0764, Throughput: 71.32 samples/sec
2025-03-25 12:44:32,123 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9502.0MB reserved
2025-03-25 12:44:32,123 - training - INFO - Epoch: 649/200000, Batch: 30/45, Loss: 2.0664, Throughput: 71.64 samples/sec
2025-03-25 12:44:43,113 - training - INFO - Epoch 649 completed in 35.22s. Average loss: 1.9826
2025-03-25 12:44:43,119 - training - INFO - Starting epoch 650/200000
2025-03-25 12:44:43,868 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9640.0MB reserved
2025-03-25 12:44:43,868 - training - INFO - Epoch: 650/200000, Batch: 0/45, Loss: 1.6481, Throughput: 74.93 samples/sec
2025-03-25 12:44:55,815 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9488.0MB reserved
2025-03-25 12:44:55,815 - training - INFO - Epoch: 650/200000, Batch: 15/45, Loss: 2.1016, Throughput: 70.58 samples/sec
2025-03-25 12:45:07,435 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9488.0MB reserved
2025-03-25 12:45:07,435 - training - INFO - Epoch: 650/200000, Batch: 30/45, Loss: 2.0670, Throughput: 71.40 samples/sec
2025-03-25 12:45:18,428 - training - INFO - Epoch 650 completed in 35.31s. Average loss: 1.9884
2025-03-25 12:45:18,432 - training - INFO - Starting epoch 651/200000
2025-03-25 12:45:19,172 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9624.0MB reserved
2025-03-25 12:45:19,173 - training - INFO - Epoch: 651/200000, Batch: 0/45, Loss: 2.3146, Throughput: 75.77 samples/sec
2025-03-25 12:45:31,056 - training - INFO - Memory: GPU 0: 3562.7MB allocated, 9494.0MB reserved
2025-03-25 12:45:31,057 - training - INFO - Epoch: 651/200000, Batch: 15/45, Loss: 2.0349, Throughput: 70.99 samples/sec
2025-03-25 12:45:42,745 - training - INFO - Memory: GPU 0: 3562.7MB allocated, 9498.0MB reserved
2025-03-25 12:45:42,745 - training - INFO - Epoch: 651/200000, Batch: 30/45, Loss: 1.9741, Throughput: 71.41 samples/sec
2025-03-25 12:45:53,742 - training - INFO - Epoch 651 completed in 35.31s. Average loss: 1.9709
2025-03-25 12:45:53,745 - training - INFO - Starting epoch 652/200000
2025-03-25 12:45:54,490 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9634.0MB reserved
2025-03-25 12:45:54,490 - training - INFO - Epoch: 652/200000, Batch: 0/45, Loss: 1.8412, Throughput: 75.39 samples/sec
2025-03-25 12:46:06,453 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9496.0MB reserved
2025-03-25 12:46:06,453 - training - INFO - Epoch: 652/200000, Batch: 15/45, Loss: 1.9193, Throughput: 70.52 samples/sec
2025-03-25 12:46:18,148 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9496.0MB reserved
2025-03-25 12:46:18,148 - training - INFO - Epoch: 652/200000, Batch: 30/45, Loss: 1.9123, Throughput: 71.14 samples/sec
2025-03-25 12:46:29,142 - training - INFO - Epoch 652 completed in 35.40s. Average loss: 1.9608
2025-03-25 12:46:29,147 - training - INFO - Starting epoch 653/200000
2025-03-25 12:46:29,884 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9634.0MB reserved
2025-03-25 12:46:29,884 - training - INFO - Epoch: 653/200000, Batch: 0/45, Loss: 1.8119, Throughput: 76.08 samples/sec
2025-03-25 12:46:41,751 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9512.0MB reserved
2025-03-25 12:46:41,751 - training - INFO - Epoch: 653/200000, Batch: 15/45, Loss: 2.0303, Throughput: 71.11 samples/sec
2025-03-25 12:46:53,403 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9514.0MB reserved
2025-03-25 12:46:53,403 - training - INFO - Epoch: 653/200000, Batch: 30/45, Loss: 1.9928, Throughput: 71.58 samples/sec
2025-03-25 12:47:04,409 - training - INFO - Epoch 653 completed in 35.26s. Average loss: 2.0054
2025-03-25 12:47:04,413 - training - INFO - Starting epoch 654/200000
2025-03-25 12:47:05,171 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9652.0MB reserved
2025-03-25 12:47:05,171 - training - INFO - Epoch: 654/200000, Batch: 0/45, Loss: 1.8338, Throughput: 73.97 samples/sec
2025-03-25 12:47:17,073 - training - INFO - Memory: GPU 0: 3562.8MB allocated, 9398.0MB reserved
2025-03-25 12:47:17,073 - training - INFO - Epoch: 654/200000, Batch: 15/45, Loss: 2.0247, Throughput: 70.78 samples/sec
2025-03-25 12:47:28,796 - training - INFO - Memory: GPU 0: 3562.8MB allocated, 9504.0MB reserved
2025-03-25 12:47:28,797 - training - INFO - Epoch: 654/200000, Batch: 30/45, Loss: 2.0343, Throughput: 71.20 samples/sec
2025-03-25 12:47:39,745 - training - INFO - Epoch 654 completed in 35.33s. Average loss: 1.9679
2025-03-25 12:47:39,749 - training - INFO - Starting epoch 655/200000
2025-03-25 12:47:40,499 - training - INFO - Memory: GPU 0: 3562.8MB allocated, 9640.0MB reserved
2025-03-25 12:47:40,499 - training - INFO - Epoch: 655/200000, Batch: 0/45, Loss: 1.5657, Throughput: 74.70 samples/sec
2025-03-25 12:47:52,441 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9508.0MB reserved
2025-03-25 12:47:52,441 - training - INFO - Epoch: 655/200000, Batch: 15/45, Loss: 2.0224, Throughput: 70.60 samples/sec
2025-03-25 12:48:04,106 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9508.0MB reserved
2025-03-25 12:48:04,106 - training - INFO - Epoch: 655/200000, Batch: 30/45, Loss: 1.9815, Throughput: 71.28 samples/sec
2025-03-25 12:48:15,018 - training - INFO - Epoch 655 completed in 35.27s. Average loss: 1.9843
2025-03-25 12:48:15,022 - training - INFO - Starting epoch 656/200000
2025-03-25 12:48:15,765 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9644.0MB reserved
2025-03-25 12:48:15,766 - training - INFO - Epoch: 656/200000, Batch: 0/45, Loss: 2.1430, Throughput: 75.46 samples/sec
2025-03-25 12:48:27,679 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9492.0MB reserved
2025-03-25 12:48:27,679 - training - INFO - Epoch: 656/200000, Batch: 15/45, Loss: 1.9650, Throughput: 70.80 samples/sec
2025-03-25 12:48:39,240 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9492.0MB reserved
2025-03-25 12:48:39,240 - training - INFO - Epoch: 656/200000, Batch: 30/45, Loss: 1.9558, Throughput: 71.68 samples/sec
2025-03-25 12:48:50,198 - training - INFO - Epoch 656 completed in 35.18s. Average loss: 2.0135
2025-03-25 12:48:50,202 - training - INFO - Starting epoch 657/200000
2025-03-25 12:48:50,931 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9630.0MB reserved
2025-03-25 12:48:50,931 - training - INFO - Epoch: 657/200000, Batch: 0/45, Loss: 1.6184, Throughput: 76.93 samples/sec
2025-03-25 12:49:02,780 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9488.0MB reserved
2025-03-25 12:49:02,780 - training - INFO - Epoch: 657/200000, Batch: 15/45, Loss: 1.9597, Throughput: 71.24 samples/sec
2025-03-25 12:49:14,271 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9488.0MB reserved
2025-03-25 12:49:14,271 - training - INFO - Epoch: 657/200000, Batch: 30/45, Loss: 1.9673, Throughput: 72.13 samples/sec
2025-03-25 12:49:25,207 - training - INFO - Epoch 657 completed in 35.01s. Average loss: 1.9884
2025-03-25 12:49:25,210 - training - INFO - Starting epoch 658/200000
2025-03-25 12:49:25,940 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9626.0MB reserved
2025-03-25 12:49:25,940 - training - INFO - Epoch: 658/200000, Batch: 0/45, Loss: 1.9271, Throughput: 76.85 samples/sec
2025-03-25 12:49:37,922 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9492.0MB reserved
2025-03-25 12:49:37,922 - training - INFO - Epoch: 658/200000, Batch: 15/45, Loss: 1.9453, Throughput: 70.49 samples/sec
2025-03-25 12:49:49,636 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9492.0MB reserved
2025-03-25 12:49:49,636 - training - INFO - Epoch: 658/200000, Batch: 30/45, Loss: 1.9601, Throughput: 71.08 samples/sec
2025-03-25 12:50:00,637 - training - INFO - Epoch 658 completed in 35.43s. Average loss: 1.9617
2025-03-25 12:50:00,640 - training - INFO - Starting epoch 659/200000
2025-03-25 12:50:01,393 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9630.0MB reserved
2025-03-25 12:50:01,393 - training - INFO - Epoch: 659/200000, Batch: 0/45, Loss: 2.0733, Throughput: 74.60 samples/sec
2025-03-25 12:50:13,363 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9508.0MB reserved
2025-03-25 12:50:13,363 - training - INFO - Epoch: 659/200000, Batch: 15/45, Loss: 1.8272, Throughput: 70.44 samples/sec
2025-03-25 12:50:24,982 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9508.0MB reserved
2025-03-25 12:50:24,982 - training - INFO - Epoch: 659/200000, Batch: 30/45, Loss: 1.8696, Throughput: 71.32 samples/sec
2025-03-25 12:50:35,942 - training - INFO - Epoch 659 completed in 35.30s. Average loss: 2.0153
2025-03-25 12:50:35,946 - training - INFO - Starting epoch 660/200000
2025-03-25 12:50:36,688 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9646.0MB reserved
2025-03-25 12:50:36,689 - training - INFO - Epoch: 660/200000, Batch: 0/45, Loss: 2.4695, Throughput: 75.66 samples/sec
2025-03-25 12:50:48,558 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9490.0MB reserved
2025-03-25 12:50:48,559 - training - INFO - Epoch: 660/200000, Batch: 15/45, Loss: 1.9692, Throughput: 71.05 samples/sec
2025-03-25 12:51:00,173 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9490.0MB reserved
2025-03-25 12:51:00,173 - training - INFO - Epoch: 660/200000, Batch: 30/45, Loss: 2.0021, Throughput: 71.66 samples/sec
2025-03-25 12:51:11,130 - training - INFO - Epoch 660 completed in 35.18s. Average loss: 2.0046
2025-03-25 12:51:11,134 - training - INFO - Starting epoch 661/200000
2025-03-25 12:51:11,872 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9628.0MB reserved
2025-03-25 12:51:11,872 - training - INFO - Epoch: 661/200000, Batch: 0/45, Loss: 2.0949, Throughput: 76.07 samples/sec
2025-03-25 12:51:23,750 - training - INFO - Memory: GPU 0: 3557.2MB allocated, 9506.0MB reserved
2025-03-25 12:51:23,750 - training - INFO - Epoch: 661/200000, Batch: 15/45, Loss: 1.8811, Throughput: 71.03 samples/sec
2025-03-25 12:51:35,309 - training - INFO - Memory: GPU 0: 3557.2MB allocated, 9506.0MB reserved
2025-03-25 12:51:35,309 - training - INFO - Epoch: 661/200000, Batch: 30/45, Loss: 1.9042, Throughput: 71.81 samples/sec
2025-03-25 12:51:46,214 - training - INFO - Epoch 661 completed in 35.08s. Average loss: 2.0273
2025-03-25 12:51:46,218 - training - INFO - Starting epoch 662/200000
2025-03-25 12:51:46,979 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9644.0MB reserved
2025-03-25 12:51:46,980 - training - INFO - Epoch: 662/200000, Batch: 0/45, Loss: 1.9649, Throughput: 73.74 samples/sec
2025-03-25 12:51:58,880 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9482.0MB reserved
2025-03-25 12:51:58,880 - training - INFO - Epoch: 662/200000, Batch: 15/45, Loss: 1.8827, Throughput: 70.77 samples/sec
2025-03-25 12:52:10,497 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9484.0MB reserved
2025-03-25 12:52:10,497 - training - INFO - Epoch: 662/200000, Batch: 30/45, Loss: 1.9213, Throughput: 71.51 samples/sec
2025-03-25 12:52:21,425 - training - INFO - Epoch 662 completed in 35.21s. Average loss: 1.9678
2025-03-25 12:52:21,429 - training - INFO - Starting epoch 663/200000
2025-03-25 12:52:22,166 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9622.0MB reserved
2025-03-25 12:52:22,166 - training - INFO - Epoch: 663/200000, Batch: 0/45, Loss: 2.1564, Throughput: 76.04 samples/sec
2025-03-25 12:52:34,124 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9504.0MB reserved
2025-03-25 12:52:34,124 - training - INFO - Epoch: 663/200000, Batch: 15/45, Loss: 1.9226, Throughput: 70.59 samples/sec
2025-03-25 12:52:45,804 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9504.0MB reserved
2025-03-25 12:52:45,805 - training - INFO - Epoch: 663/200000, Batch: 30/45, Loss: 1.9553, Throughput: 71.22 samples/sec
2025-03-25 12:52:56,818 - training - INFO - Epoch 663 completed in 35.39s. Average loss: 1.9974
2025-03-25 12:52:56,822 - training - INFO - Starting epoch 664/200000
2025-03-25 12:52:57,579 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9642.0MB reserved
2025-03-25 12:52:57,579 - training - INFO - Epoch: 664/200000, Batch: 0/45, Loss: 2.4032, Throughput: 74.13 samples/sec
2025-03-25 12:53:09,589 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9518.0MB reserved
2025-03-25 12:53:09,590 - training - INFO - Epoch: 664/200000, Batch: 15/45, Loss: 2.0753, Throughput: 70.18 samples/sec
2025-03-25 12:53:21,323 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9518.0MB reserved
2025-03-25 12:53:21,324 - training - INFO - Epoch: 664/200000, Batch: 30/45, Loss: 2.0875, Throughput: 70.86 samples/sec
2025-03-25 12:53:32,344 - training - INFO - Epoch 664 completed in 35.52s. Average loss: 1.9820
2025-03-25 12:53:32,347 - training - INFO - Starting epoch 665/200000
2025-03-25 12:53:33,085 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9656.0MB reserved
2025-03-25 12:53:33,085 - training - INFO - Epoch: 665/200000, Batch: 0/45, Loss: 2.4538, Throughput: 76.11 samples/sec
2025-03-25 12:53:45,060 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9510.0MB reserved
2025-03-25 12:53:45,061 - training - INFO - Epoch: 665/200000, Batch: 15/45, Loss: 2.1831, Throughput: 70.49 samples/sec
2025-03-25 12:53:56,746 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9510.0MB reserved
2025-03-25 12:53:56,746 - training - INFO - Epoch: 665/200000, Batch: 30/45, Loss: 2.1528, Throughput: 71.16 samples/sec
2025-03-25 12:54:07,718 - training - INFO - Epoch 665 completed in 35.37s. Average loss: 2.0184
2025-03-25 12:54:07,749 - training - INFO - Starting epoch 666/200000
2025-03-25 12:54:08,480 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9648.0MB reserved
2025-03-25 12:54:08,480 - training - INFO - Epoch: 666/200000, Batch: 0/45, Loss: 1.7935, Throughput: 76.78 samples/sec
2025-03-25 12:54:20,427 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9502.0MB reserved
2025-03-25 12:54:20,427 - training - INFO - Epoch: 666/200000, Batch: 15/45, Loss: 1.8836, Throughput: 70.68 samples/sec
2025-03-25 12:54:32,108 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9502.0MB reserved
2025-03-25 12:54:32,109 - training - INFO - Epoch: 666/200000, Batch: 30/45, Loss: 1.9157, Throughput: 71.27 samples/sec
2025-03-25 12:54:43,036 - training - INFO - Epoch 666 completed in 35.29s. Average loss: 1.9468
2025-03-25 12:54:43,042 - training - INFO - Starting epoch 667/200000
2025-03-25 12:54:43,796 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9640.0MB reserved
2025-03-25 12:54:43,796 - training - INFO - Epoch: 667/200000, Batch: 0/45, Loss: 1.4724, Throughput: 74.43 samples/sec
2025-03-25 12:54:55,794 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9518.0MB reserved
2025-03-25 12:54:55,794 - training - INFO - Epoch: 667/200000, Batch: 15/45, Loss: 1.9624, Throughput: 70.28 samples/sec
2025-03-25 12:55:07,493 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9518.0MB reserved
2025-03-25 12:55:07,494 - training - INFO - Epoch: 667/200000, Batch: 30/45, Loss: 1.9812, Throughput: 71.00 samples/sec
2025-03-25 12:55:18,474 - training - INFO - Epoch 667 completed in 35.43s. Average loss: 1.9539
2025-03-25 12:55:18,478 - training - INFO - Starting epoch 668/200000
2025-03-25 12:55:19,219 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9654.0MB reserved
2025-03-25 12:55:19,219 - training - INFO - Epoch: 668/200000, Batch: 0/45, Loss: 1.6937, Throughput: 75.74 samples/sec
2025-03-25 12:55:31,166 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9514.0MB reserved
2025-03-25 12:55:31,167 - training - INFO - Epoch: 668/200000, Batch: 15/45, Loss: 1.9576, Throughput: 70.63 samples/sec
2025-03-25 12:55:42,778 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9514.0MB reserved
2025-03-25 12:55:42,779 - training - INFO - Epoch: 668/200000, Batch: 30/45, Loss: 1.9796, Throughput: 71.45 samples/sec
2025-03-25 12:55:53,729 - training - INFO - Epoch 668 completed in 35.25s. Average loss: 2.0013
2025-03-25 12:55:53,732 - training - INFO - Starting epoch 669/200000
2025-03-25 12:55:54,487 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9652.0MB reserved
2025-03-25 12:55:54,487 - training - INFO - Epoch: 669/200000, Batch: 0/45, Loss: 1.9919, Throughput: 74.36 samples/sec
2025-03-25 12:56:06,503 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9502.0MB reserved
2025-03-25 12:56:06,503 - training - INFO - Epoch: 669/200000, Batch: 15/45, Loss: 2.0419, Throughput: 70.18 samples/sec
2025-03-25 12:56:18,179 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9504.0MB reserved
2025-03-25 12:56:18,180 - training - INFO - Epoch: 669/200000, Batch: 30/45, Loss: 2.0216, Throughput: 71.02 samples/sec
2025-03-25 12:56:29,158 - training - INFO - Epoch 669 completed in 35.43s. Average loss: 1.9603
2025-03-25 12:56:29,162 - training - INFO - Starting epoch 670/200000
2025-03-25 12:56:29,918 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9640.0MB reserved
2025-03-25 12:56:29,918 - training - INFO - Epoch: 670/200000, Batch: 0/45, Loss: 1.9469, Throughput: 74.16 samples/sec
2025-03-25 12:56:41,869 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9502.0MB reserved
2025-03-25 12:56:41,869 - training - INFO - Epoch: 670/200000, Batch: 15/45, Loss: 2.0938, Throughput: 70.52 samples/sec
2025-03-25 12:56:53,443 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9502.0MB reserved
2025-03-25 12:56:53,444 - training - INFO - Epoch: 670/200000, Batch: 30/45, Loss: 2.0217, Throughput: 71.50 samples/sec
2025-03-25 12:57:04,368 - training - INFO - Epoch 670 completed in 35.21s. Average loss: 1.9904
2025-03-25 12:57:04,371 - training - INFO - Starting epoch 671/200000
2025-03-25 12:57:05,111 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9640.0MB reserved
2025-03-25 12:57:05,111 - training - INFO - Epoch: 671/200000, Batch: 0/45, Loss: 1.9402, Throughput: 75.89 samples/sec
2025-03-25 12:57:17,121 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9490.0MB reserved
2025-03-25 12:57:17,121 - training - INFO - Epoch: 671/200000, Batch: 15/45, Loss: 2.0233, Throughput: 70.30 samples/sec
2025-03-25 12:57:28,685 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9490.0MB reserved
2025-03-25 12:57:28,685 - training - INFO - Epoch: 671/200000, Batch: 30/45, Loss: 1.9987, Throughput: 71.41 samples/sec
2025-03-25 12:57:39,662 - training - INFO - Epoch 671 completed in 35.29s. Average loss: 1.9763
2025-03-25 12:57:39,666 - training - INFO - Starting epoch 672/200000
2025-03-25 12:57:40,407 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9628.0MB reserved
2025-03-25 12:57:40,408 - training - INFO - Epoch: 672/200000, Batch: 0/45, Loss: 3.0841, Throughput: 75.68 samples/sec
2025-03-25 12:57:52,297 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9496.0MB reserved
2025-03-25 12:57:52,297 - training - INFO - Epoch: 672/200000, Batch: 15/45, Loss: 2.0885, Throughput: 70.94 samples/sec
2025-03-25 12:58:03,855 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9496.0MB reserved
2025-03-25 12:58:03,855 - training - INFO - Epoch: 672/200000, Batch: 30/45, Loss: 1.9934, Throughput: 71.77 samples/sec
2025-03-25 12:58:14,779 - training - INFO - Epoch 672 completed in 35.11s. Average loss: 2.0542
2025-03-25 12:58:14,783 - training - INFO - Starting epoch 673/200000
2025-03-25 12:58:15,540 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9634.0MB reserved
2025-03-25 12:58:15,540 - training - INFO - Epoch: 673/200000, Batch: 0/45, Loss: 1.9818, Throughput: 74.13 samples/sec
2025-03-25 12:58:27,403 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9496.0MB reserved
2025-03-25 12:58:27,404 - training - INFO - Epoch: 673/200000, Batch: 15/45, Loss: 1.9521, Throughput: 71.00 samples/sec
2025-03-25 12:58:39,033 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9496.0MB reserved
2025-03-25 12:58:39,033 - training - INFO - Epoch: 673/200000, Batch: 30/45, Loss: 1.9816, Throughput: 71.59 samples/sec
2025-03-25 12:58:49,935 - training - INFO - Epoch 673 completed in 35.15s. Average loss: 1.9165
2025-03-25 12:58:49,938 - training - INFO - Starting epoch 674/200000
2025-03-25 12:58:50,672 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9634.0MB reserved
2025-03-25 12:58:50,673 - training - INFO - Epoch: 674/200000, Batch: 0/45, Loss: 1.7413, Throughput: 76.52 samples/sec
2025-03-25 12:59:02,572 - training - INFO - Memory: GPU 0: 3556.0MB allocated, 9502.0MB reserved
2025-03-25 12:59:02,572 - training - INFO - Epoch: 674/200000, Batch: 15/45, Loss: 1.8464, Throughput: 70.94 samples/sec
2025-03-25 12:59:14,320 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9502.0MB reserved
2025-03-25 12:59:14,320 - training - INFO - Epoch: 674/200000, Batch: 30/45, Loss: 1.8811, Throughput: 71.21 samples/sec
2025-03-25 12:59:25,269 - training - INFO - Epoch 674 completed in 35.33s. Average loss: 1.9315
2025-03-25 12:59:25,273 - training - INFO - Starting epoch 675/200000
2025-03-25 12:59:26,017 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9640.0MB reserved
2025-03-25 12:59:26,017 - training - INFO - Epoch: 675/200000, Batch: 0/45, Loss: 2.0370, Throughput: 75.43 samples/sec
2025-03-25 12:59:37,892 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9498.0MB reserved
2025-03-25 12:59:37,892 - training - INFO - Epoch: 675/200000, Batch: 15/45, Loss: 1.7923, Throughput: 71.01 samples/sec
2025-03-25 12:59:49,435 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9498.0MB reserved
2025-03-25 12:59:49,436 - training - INFO - Epoch: 675/200000, Batch: 30/45, Loss: 1.8799, Throughput: 71.85 samples/sec
2025-03-25 13:00:00,368 - training - INFO - Epoch 675 completed in 35.10s. Average loss: 1.9568
2025-03-25 13:00:00,372 - training - INFO - Starting epoch 676/200000
2025-03-25 13:00:01,113 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9634.0MB reserved
2025-03-25 13:00:01,113 - training - INFO - Epoch: 676/200000, Batch: 0/45, Loss: 1.9639, Throughput: 75.73 samples/sec
2025-03-25 13:00:13,018 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9496.0MB reserved
2025-03-25 13:00:13,019 - training - INFO - Epoch: 676/200000, Batch: 15/45, Loss: 1.9575, Throughput: 70.86 samples/sec
2025-03-25 13:00:24,641 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9496.0MB reserved
2025-03-25 13:00:24,641 - training - INFO - Epoch: 676/200000, Batch: 30/45, Loss: 1.8946, Throughput: 71.54 samples/sec
2025-03-25 13:00:35,578 - training - INFO - Epoch 676 completed in 35.21s. Average loss: 1.9620
2025-03-25 13:00:35,582 - training - INFO - Starting epoch 677/200000
2025-03-25 13:00:36,319 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9634.0MB reserved
2025-03-25 13:00:36,319 - training - INFO - Epoch: 677/200000, Batch: 0/45, Loss: 1.6218, Throughput: 76.09 samples/sec
2025-03-25 13:00:48,178 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9496.0MB reserved
2025-03-25 13:00:48,179 - training - INFO - Epoch: 677/200000, Batch: 15/45, Loss: 1.9131, Throughput: 71.14 samples/sec
2025-03-25 13:00:59,783 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9496.0MB reserved
2025-03-25 13:00:59,783 - training - INFO - Epoch: 677/200000, Batch: 30/45, Loss: 1.9331, Throughput: 71.74 samples/sec
2025-03-25 13:01:10,685 - training - INFO - Epoch 677 completed in 35.10s. Average loss: 2.0130
2025-03-25 13:01:10,689 - training - INFO - Starting epoch 678/200000
2025-03-25 13:01:11,427 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9632.0MB reserved
2025-03-25 13:01:11,427 - training - INFO - Epoch: 678/200000, Batch: 0/45, Loss: 1.9931, Throughput: 75.93 samples/sec
2025-03-25 13:01:23,289 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9500.0MB reserved
2025-03-25 13:01:23,289 - training - INFO - Epoch: 678/200000, Batch: 15/45, Loss: 1.9944, Throughput: 71.12 samples/sec
2025-03-25 13:01:34,938 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9500.0MB reserved
2025-03-25 13:01:34,939 - training - INFO - Epoch: 678/200000, Batch: 30/45, Loss: 1.9543, Throughput: 71.60 samples/sec
2025-03-25 13:01:45,899 - training - INFO - Epoch 678 completed in 35.21s. Average loss: 2.0101
2025-03-25 13:01:45,903 - training - INFO - Starting epoch 679/200000
2025-03-25 13:01:46,657 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9636.0MB reserved
2025-03-25 13:01:46,657 - training - INFO - Epoch: 679/200000, Batch: 0/45, Loss: 2.2573, Throughput: 74.48 samples/sec
2025-03-25 13:01:58,563 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9502.0MB reserved
2025-03-25 13:01:58,563 - training - INFO - Epoch: 679/200000, Batch: 15/45, Loss: 1.9811, Throughput: 70.79 samples/sec
2025-03-25 13:02:10,116 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9502.0MB reserved
2025-03-25 13:02:10,116 - training - INFO - Epoch: 679/200000, Batch: 30/45, Loss: 1.9855, Throughput: 71.70 samples/sec
2025-03-25 13:02:21,037 - training - INFO - Epoch 679 completed in 35.13s. Average loss: 2.0065
2025-03-25 13:02:21,042 - training - INFO - Starting epoch 680/200000
2025-03-25 13:02:21,793 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9640.0MB reserved
2025-03-25 13:02:21,794 - training - INFO - Epoch: 680/200000, Batch: 0/45, Loss: 2.3336, Throughput: 74.75 samples/sec
2025-03-25 13:02:33,667 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9510.0MB reserved
2025-03-25 13:02:33,668 - training - INFO - Epoch: 680/200000, Batch: 15/45, Loss: 1.9828, Throughput: 70.98 samples/sec
2025-03-25 13:02:45,186 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9512.0MB reserved
2025-03-25 13:02:45,187 - training - INFO - Epoch: 680/200000, Batch: 30/45, Loss: 1.9610, Throughput: 71.91 samples/sec
2025-03-25 13:02:56,101 - training - INFO - Epoch 680 completed in 35.06s. Average loss: 1.9789
2025-03-25 13:02:56,105 - training - INFO - Starting epoch 681/200000
2025-03-25 13:02:56,842 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9650.0MB reserved
2025-03-25 13:02:56,842 - training - INFO - Epoch: 681/200000, Batch: 0/45, Loss: 1.9610, Throughput: 76.18 samples/sec
2025-03-25 13:03:08,772 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9506.0MB reserved
2025-03-25 13:03:08,772 - training - INFO - Epoch: 681/200000, Batch: 15/45, Loss: 1.8386, Throughput: 70.74 samples/sec
2025-03-25 13:03:20,359 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9506.0MB reserved
2025-03-25 13:03:20,359 - training - INFO - Epoch: 681/200000, Batch: 30/45, Loss: 1.9540, Throughput: 71.58 samples/sec
2025-03-25 13:03:31,317 - training - INFO - Epoch 681 completed in 35.21s. Average loss: 2.0319
2025-03-25 13:03:31,321 - training - INFO - Starting epoch 682/200000
2025-03-25 13:03:32,064 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9644.0MB reserved
2025-03-25 13:03:32,064 - training - INFO - Epoch: 682/200000, Batch: 0/45, Loss: 1.9427, Throughput: 75.42 samples/sec
2025-03-25 13:03:44,006 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9500.0MB reserved
2025-03-25 13:03:44,007 - training - INFO - Epoch: 682/200000, Batch: 15/45, Loss: 1.9370, Throughput: 70.64 samples/sec
2025-03-25 13:03:55,625 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9500.0MB reserved
2025-03-25 13:03:55,625 - training - INFO - Epoch: 682/200000, Batch: 30/45, Loss: 1.9851, Throughput: 71.44 samples/sec
2025-03-25 13:04:06,587 - training - INFO - Epoch 682 completed in 35.27s. Average loss: 1.9477
2025-03-25 13:04:06,591 - training - INFO - Starting epoch 683/200000
2025-03-25 13:04:07,327 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9638.0MB reserved
2025-03-25 13:04:07,327 - training - INFO - Epoch: 683/200000, Batch: 0/45, Loss: 2.2315, Throughput: 76.28 samples/sec
2025-03-25 13:04:19,318 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9492.0MB reserved
2025-03-25 13:04:19,318 - training - INFO - Epoch: 683/200000, Batch: 15/45, Loss: 1.9647, Throughput: 70.41 samples/sec
2025-03-25 13:04:30,899 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9492.0MB reserved
2025-03-25 13:04:30,899 - training - INFO - Epoch: 683/200000, Batch: 30/45, Loss: 1.9007, Throughput: 71.42 samples/sec
2025-03-25 13:04:41,822 - training - INFO - Epoch 683 completed in 35.23s. Average loss: 1.9477
2025-03-25 13:04:41,827 - training - INFO - Starting epoch 684/200000
2025-03-25 13:04:42,569 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9630.0MB reserved
2025-03-25 13:04:42,569 - training - INFO - Epoch: 684/200000, Batch: 0/45, Loss: 1.5586, Throughput: 75.57 samples/sec
2025-03-25 13:04:54,407 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9508.0MB reserved
2025-03-25 13:04:54,407 - training - INFO - Epoch: 684/200000, Batch: 15/45, Loss: 1.9965, Throughput: 71.23 samples/sec
2025-03-25 13:05:05,984 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9510.0MB reserved
2025-03-25 13:05:05,984 - training - INFO - Epoch: 684/200000, Batch: 30/45, Loss: 1.9920, Throughput: 71.87 samples/sec
2025-03-25 13:05:16,991 - training - INFO - Epoch 684 completed in 35.16s. Average loss: 2.0286
2025-03-25 13:05:16,994 - training - INFO - Starting epoch 685/200000
2025-03-25 13:05:17,749 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9648.0MB reserved
2025-03-25 13:05:17,749 - training - INFO - Epoch: 685/200000, Batch: 0/45, Loss: 2.3246, Throughput: 74.39 samples/sec
2025-03-25 13:05:29,737 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9494.0MB reserved
2025-03-25 13:05:29,737 - training - INFO - Epoch: 685/200000, Batch: 15/45, Loss: 1.9753, Throughput: 70.32 samples/sec
2025-03-25 13:05:41,457 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9494.0MB reserved
2025-03-25 13:05:41,457 - training - INFO - Epoch: 685/200000, Batch: 30/45, Loss: 1.9115, Throughput: 70.97 samples/sec
2025-03-25 13:05:52,457 - training - INFO - Epoch 685 completed in 35.46s. Average loss: 1.9658
2025-03-25 13:05:52,460 - training - INFO - Starting epoch 686/200000
2025-03-25 13:05:53,203 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9632.0MB reserved
2025-03-25 13:05:53,203 - training - INFO - Epoch: 686/200000, Batch: 0/45, Loss: 1.9384, Throughput: 75.62 samples/sec
2025-03-25 13:06:05,209 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9498.0MB reserved
2025-03-25 13:06:05,210 - training - INFO - Epoch: 686/200000, Batch: 15/45, Loss: 1.9622, Throughput: 70.29 samples/sec
2025-03-25 13:06:16,903 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9498.0MB reserved
2025-03-25 13:06:16,903 - training - INFO - Epoch: 686/200000, Batch: 30/45, Loss: 1.8677, Throughput: 71.03 samples/sec
2025-03-25 13:06:27,952 - training - INFO - Epoch 686 completed in 35.49s. Average loss: 1.9678
2025-03-25 13:06:27,955 - training - INFO - Starting epoch 687/200000
2025-03-25 13:06:28,713 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9636.0MB reserved
2025-03-25 13:06:28,713 - training - INFO - Epoch: 687/200000, Batch: 0/45, Loss: 1.7788, Throughput: 74.08 samples/sec
2025-03-25 13:06:40,645 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9506.0MB reserved
2025-03-25 13:06:40,645 - training - INFO - Epoch: 687/200000, Batch: 15/45, Loss: 1.9807, Throughput: 70.62 samples/sec
2025-03-25 13:06:52,186 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9506.0MB reserved
2025-03-25 13:06:52,187 - training - INFO - Epoch: 687/200000, Batch: 30/45, Loss: 1.9409, Throughput: 71.65 samples/sec
2025-03-25 13:07:03,111 - training - INFO - Epoch 687 completed in 35.16s. Average loss: 1.9244
2025-03-25 13:07:03,114 - training - INFO - Starting epoch 688/200000
2025-03-25 13:07:03,845 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9644.0MB reserved
2025-03-25 13:07:03,845 - training - INFO - Epoch: 688/200000, Batch: 0/45, Loss: 1.9871, Throughput: 76.72 samples/sec
2025-03-25 13:07:15,694 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9506.0MB reserved
2025-03-25 13:07:15,694 - training - INFO - Epoch: 688/200000, Batch: 15/45, Loss: 1.9771, Throughput: 71.23 samples/sec
2025-03-25 13:07:27,239 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9506.0MB reserved
2025-03-25 13:07:27,240 - training - INFO - Epoch: 688/200000, Batch: 30/45, Loss: 1.9518, Throughput: 71.96 samples/sec
2025-03-25 13:07:38,152 - training - INFO - Epoch 688 completed in 35.04s. Average loss: 1.9501
2025-03-25 13:07:38,156 - training - INFO - Starting epoch 689/200000
2025-03-25 13:07:38,902 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9644.0MB reserved
2025-03-25 13:07:38,902 - training - INFO - Epoch: 689/200000, Batch: 0/45, Loss: 2.2671, Throughput: 75.11 samples/sec
2025-03-25 13:07:50,873 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9490.0MB reserved
2025-03-25 13:07:50,874 - training - INFO - Epoch: 689/200000, Batch: 15/45, Loss: 1.9732, Throughput: 70.47 samples/sec
2025-03-25 13:08:02,519 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9490.0MB reserved
2025-03-25 13:08:02,520 - training - INFO - Epoch: 689/200000, Batch: 30/45, Loss: 1.9625, Throughput: 71.26 samples/sec
2025-03-25 13:08:13,531 - training - INFO - Epoch 689 completed in 35.38s. Average loss: 1.9895
2025-03-25 13:08:13,535 - training - INFO - Starting epoch 690/200000
2025-03-25 13:08:14,283 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9626.0MB reserved
2025-03-25 13:08:14,284 - training - INFO - Epoch: 690/200000, Batch: 0/45, Loss: 1.7103, Throughput: 75.01 samples/sec
2025-03-25 13:08:26,244 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9496.0MB reserved
2025-03-25 13:08:26,244 - training - INFO - Epoch: 690/200000, Batch: 15/45, Loss: 1.9423, Throughput: 70.51 samples/sec
2025-03-25 13:08:37,872 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9496.0MB reserved
2025-03-25 13:08:37,872 - training - INFO - Epoch: 690/200000, Batch: 30/45, Loss: 1.9393, Throughput: 71.33 samples/sec
2025-03-25 13:08:48,847 - training - INFO - Epoch 690 completed in 35.31s. Average loss: 2.0265
2025-03-25 13:08:48,851 - training - INFO - Starting epoch 691/200000
2025-03-25 13:08:49,613 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9632.0MB reserved
2025-03-25 13:08:49,613 - training - INFO - Epoch: 691/200000, Batch: 0/45, Loss: 2.1948, Throughput: 73.72 samples/sec
2025-03-25 13:09:01,573 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9510.0MB reserved
2025-03-25 13:09:01,574 - training - INFO - Epoch: 691/200000, Batch: 15/45, Loss: 2.1195, Throughput: 70.44 samples/sec
2025-03-25 13:09:13,270 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9512.0MB reserved
2025-03-25 13:09:13,271 - training - INFO - Epoch: 691/200000, Batch: 30/45, Loss: 2.1354, Throughput: 71.10 samples/sec
2025-03-25 13:09:24,264 - training - INFO - Epoch 691 completed in 35.41s. Average loss: 2.0929
2025-03-25 13:09:24,269 - training - INFO - Starting epoch 692/200000
2025-03-25 13:09:24,999 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9650.0MB reserved
2025-03-25 13:09:24,999 - training - INFO - Epoch: 692/200000, Batch: 0/45, Loss: 1.6051, Throughput: 76.81 samples/sec
2025-03-25 13:09:36,837 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9516.0MB reserved
2025-03-25 13:09:36,838 - training - INFO - Epoch: 692/200000, Batch: 15/45, Loss: 1.8624, Throughput: 71.30 samples/sec
2025-03-25 13:09:48,374 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9516.0MB reserved
2025-03-25 13:09:48,375 - training - INFO - Epoch: 692/200000, Batch: 30/45, Loss: 1.8502, Throughput: 72.02 samples/sec
2025-03-25 13:09:59,296 - training - INFO - Epoch 692 completed in 35.03s. Average loss: 1.9559
2025-03-25 13:09:59,299 - training - INFO - Starting epoch 693/200000
2025-03-25 13:10:00,054 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9654.0MB reserved
2025-03-25 13:10:00,054 - training - INFO - Epoch: 693/200000, Batch: 0/45, Loss: 2.1398, Throughput: 74.39 samples/sec
2025-03-25 13:10:11,889 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9498.0MB reserved
2025-03-25 13:10:11,889 - training - INFO - Epoch: 693/200000, Batch: 15/45, Loss: 1.9324, Throughput: 71.17 samples/sec
2025-03-25 13:10:23,419 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9498.0MB reserved
2025-03-25 13:10:23,419 - training - INFO - Epoch: 693/200000, Batch: 30/45, Loss: 1.9109, Throughput: 71.98 samples/sec
2025-03-25 13:10:34,343 - training - INFO - Epoch 693 completed in 35.04s. Average loss: 1.9691
2025-03-25 13:10:34,347 - training - INFO - Starting epoch 694/200000
2025-03-25 13:10:35,103 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9636.0MB reserved
2025-03-25 13:10:35,104 - training - INFO - Epoch: 694/200000, Batch: 0/45, Loss: 2.0073, Throughput: 74.18 samples/sec
2025-03-25 13:10:46,971 - training - INFO - Memory: GPU 0: 3563.1MB allocated, 9506.0MB reserved
2025-03-25 13:10:46,971 - training - INFO - Epoch: 694/200000, Batch: 15/45, Loss: 1.9035, Throughput: 70.98 samples/sec
2025-03-25 13:10:58,629 - training - INFO - Memory: GPU 0: 3563.1MB allocated, 9508.0MB reserved
2025-03-25 13:10:58,630 - training - INFO - Epoch: 694/200000, Batch: 30/45, Loss: 1.9036, Throughput: 71.50 samples/sec
2025-03-25 13:11:09,646 - training - INFO - Epoch 694 completed in 35.30s. Average loss: 1.9580
2025-03-25 13:11:09,650 - training - INFO - Starting epoch 695/200000
2025-03-25 13:11:10,399 - training - INFO - Memory: GPU 0: 3563.1MB allocated, 9646.0MB reserved
2025-03-25 13:11:10,399 - training - INFO - Epoch: 695/200000, Batch: 0/45, Loss: 2.2643, Throughput: 74.89 samples/sec
2025-03-25 13:11:22,320 - training - INFO - Memory: GPU 0: 3562.9MB allocated, 9492.0MB reserved
2025-03-25 13:11:22,320 - training - INFO - Epoch: 695/200000, Batch: 15/45, Loss: 2.0347, Throughput: 70.73 samples/sec
2025-03-25 13:11:33,905 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9492.0MB reserved
2025-03-25 13:11:33,905 - training - INFO - Epoch: 695/200000, Batch: 30/45, Loss: 1.9367, Throughput: 71.58 samples/sec
2025-03-25 13:11:44,849 - training - INFO - Epoch 695 completed in 35.20s. Average loss: 1.9562
2025-03-25 13:11:44,852 - training - INFO - Starting epoch 696/200000
2025-03-25 13:11:45,604 - training - INFO - Memory: GPU 0: 3562.9MB allocated, 9632.0MB reserved
2025-03-25 13:11:45,604 - training - INFO - Epoch: 696/200000, Batch: 0/45, Loss: 1.9908, Throughput: 74.65 samples/sec
2025-03-25 13:11:57,565 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9398.0MB reserved
2025-03-25 13:11:57,565 - training - INFO - Epoch: 696/200000, Batch: 15/45, Loss: 1.9133, Throughput: 70.49 samples/sec
2025-03-25 13:12:09,159 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9504.0MB reserved
2025-03-25 13:12:09,159 - training - INFO - Epoch: 696/200000, Batch: 30/45, Loss: 1.8828, Throughput: 71.42 samples/sec
2025-03-25 13:12:20,080 - training - INFO - Epoch 696 completed in 35.23s. Average loss: 1.9760
2025-03-25 13:12:20,084 - training - INFO - Starting epoch 697/200000
2025-03-25 13:12:20,837 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9642.0MB reserved
2025-03-25 13:12:20,837 - training - INFO - Epoch: 697/200000, Batch: 0/45, Loss: 2.0004, Throughput: 74.59 samples/sec
2025-03-25 13:12:32,825 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9498.0MB reserved
2025-03-25 13:12:32,825 - training - INFO - Epoch: 697/200000, Batch: 15/45, Loss: 1.9130, Throughput: 70.34 samples/sec
2025-03-25 13:12:44,401 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9498.0MB reserved
2025-03-25 13:12:44,401 - training - INFO - Epoch: 697/200000, Batch: 30/45, Loss: 1.9263, Throughput: 71.40 samples/sec
2025-03-25 13:12:55,348 - training - INFO - Epoch 697 completed in 35.26s. Average loss: 1.9772
2025-03-25 13:12:55,352 - training - INFO - Starting epoch 698/200000
2025-03-25 13:12:56,088 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9636.0MB reserved
2025-03-25 13:12:56,089 - training - INFO - Epoch: 698/200000, Batch: 0/45, Loss: 1.3261, Throughput: 76.34 samples/sec
2025-03-25 13:13:08,018 - training - INFO - Memory: GPU 0: 3563.6MB allocated, 9494.0MB reserved
2025-03-25 13:13:08,018 - training - INFO - Epoch: 698/200000, Batch: 15/45, Loss: 1.8269, Throughput: 70.75 samples/sec
2025-03-25 13:13:19,677 - training - INFO - Memory: GPU 0: 3563.6MB allocated, 9494.0MB reserved
2025-03-25 13:13:19,677 - training - INFO - Epoch: 698/200000, Batch: 30/45, Loss: 1.8537, Throughput: 71.37 samples/sec
2025-03-25 13:13:30,624 - training - INFO - Epoch 698 completed in 35.27s. Average loss: 1.9542
2025-03-25 13:13:30,628 - training - INFO - Starting epoch 699/200000
2025-03-25 13:13:31,377 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9632.0MB reserved
2025-03-25 13:13:31,378 - training - INFO - Epoch: 699/200000, Batch: 0/45, Loss: 2.2839, Throughput: 74.89 samples/sec
2025-03-25 13:13:43,360 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9496.0MB reserved
2025-03-25 13:13:43,360 - training - INFO - Epoch: 699/200000, Batch: 15/45, Loss: 1.9361, Throughput: 70.39 samples/sec
2025-03-25 13:13:55,049 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9496.0MB reserved
2025-03-25 13:13:55,050 - training - INFO - Epoch: 699/200000, Batch: 30/45, Loss: 1.9178, Throughput: 71.09 samples/sec
2025-03-25 13:14:06,066 - training - INFO - Epoch 699 completed in 35.44s. Average loss: 1.9102
2025-03-25 13:14:06,070 - training - INFO - Starting epoch 700/200000
2025-03-25 13:14:06,826 - training - INFO - Memory: GPU 0: 3562.7MB allocated, 9632.0MB reserved
2025-03-25 13:14:06,827 - training - INFO - Epoch: 700/200000, Batch: 0/45, Loss: 1.7297, Throughput: 74.15 samples/sec
2025-03-25 13:14:18,702 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9392.0MB reserved
2025-03-25 13:14:18,703 - training - INFO - Epoch: 700/200000, Batch: 15/45, Loss: 1.9305, Throughput: 70.93 samples/sec
2025-03-25 13:14:30,253 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9392.0MB reserved
2025-03-25 13:14:30,254 - training - INFO - Epoch: 700/200000, Batch: 30/45, Loss: 1.8946, Throughput: 71.79 samples/sec
2025-03-25 13:14:41,161 - training - INFO - Epoch 700 completed in 35.09s. Average loss: 1.9566
2025-03-25 13:14:41,164 - training - INFO - Starting validation...
2025-03-25 13:14:41,495 - training - INFO - Validation Loss: 21.6847
2025-03-25 13:14:41,495 - training - INFO - Validation loss did not improve. Counter: 6/10
2025-03-25 13:14:41,816 - training - INFO - Starting epoch 701/200000
2025-03-25 13:14:42,597 - training - INFO - Memory: GPU 0: 3565.8MB allocated, 8742.0MB reserved
2025-03-25 13:14:42,597 - training - INFO - Epoch: 701/200000, Batch: 0/45, Loss: 2.5225, Throughput: 71.88 samples/sec
2025-03-25 13:14:54,305 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9468.0MB reserved
2025-03-25 13:14:54,306 - training - INFO - Epoch: 701/200000, Batch: 15/45, Loss: 1.9025, Throughput: 71.75 samples/sec
2025-03-25 13:15:06,008 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9468.0MB reserved
2025-03-25 13:15:06,008 - training - INFO - Epoch: 701/200000, Batch: 30/45, Loss: 1.9827, Throughput: 71.77 samples/sec
2025-03-25 13:15:16,952 - training - INFO - Epoch 701 completed in 35.14s. Average loss: 1.9889
2025-03-25 13:15:16,956 - training - INFO - Starting epoch 702/200000
2025-03-25 13:15:17,704 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9608.0MB reserved
2025-03-25 13:15:17,704 - training - INFO - Epoch: 702/200000, Batch: 0/45, Loss: 1.5328, Throughput: 75.05 samples/sec
2025-03-25 13:15:29,670 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9486.0MB reserved
2025-03-25 13:15:29,670 - training - INFO - Epoch: 702/200000, Batch: 15/45, Loss: 1.9989, Throughput: 70.49 samples/sec
2025-03-25 13:15:41,402 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9486.0MB reserved
2025-03-25 13:15:41,403 - training - INFO - Epoch: 702/200000, Batch: 30/45, Loss: 1.9248, Throughput: 71.02 samples/sec
2025-03-25 13:15:52,389 - training - INFO - Epoch 702 completed in 35.43s. Average loss: 1.9986
2025-03-25 13:15:52,393 - training - INFO - Starting epoch 703/200000
2025-03-25 13:15:53,142 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9622.0MB reserved
2025-03-25 13:15:53,142 - training - INFO - Epoch: 703/200000, Batch: 0/45, Loss: 1.7216, Throughput: 74.88 samples/sec
2025-03-25 13:16:04,983 - training - INFO - Memory: GPU 0: 3563.1MB allocated, 9506.0MB reserved
2025-03-25 13:16:04,984 - training - INFO - Epoch: 703/200000, Batch: 15/45, Loss: 1.8831, Throughput: 71.17 samples/sec
2025-03-25 13:16:16,565 - training - INFO - Memory: GPU 0: 3563.1MB allocated, 9506.0MB reserved
2025-03-25 13:16:16,566 - training - INFO - Epoch: 703/200000, Batch: 30/45, Loss: 1.9203, Throughput: 71.83 samples/sec
2025-03-25 13:16:27,466 - training - INFO - Epoch 703 completed in 35.07s. Average loss: 1.9417
2025-03-25 13:16:27,470 - training - INFO - Starting epoch 704/200000
2025-03-25 13:16:28,239 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9644.0MB reserved
2025-03-25 13:16:28,239 - training - INFO - Epoch: 704/200000, Batch: 0/45, Loss: 1.7727, Throughput: 72.91 samples/sec
2025-03-25 13:16:40,061 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9502.0MB reserved
2025-03-25 13:16:40,061 - training - INFO - Epoch: 704/200000, Batch: 15/45, Loss: 1.8228, Throughput: 71.17 samples/sec
2025-03-25 13:16:51,608 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9502.0MB reserved
2025-03-25 13:16:51,609 - training - INFO - Epoch: 704/200000, Batch: 30/45, Loss: 1.8179, Throughput: 71.93 samples/sec
2025-03-25 13:17:02,543 - training - INFO - Epoch 704 completed in 35.07s. Average loss: 1.9024
2025-03-25 13:17:02,547 - training - INFO - Starting epoch 705/200000
2025-03-25 13:17:03,298 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9640.0MB reserved
2025-03-25 13:17:03,299 - training - INFO - Epoch: 705/200000, Batch: 0/45, Loss: 2.3918, Throughput: 74.73 samples/sec
2025-03-25 13:17:15,296 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9500.0MB reserved
2025-03-25 13:17:15,296 - training - INFO - Epoch: 705/200000, Batch: 15/45, Loss: 1.9221, Throughput: 70.29 samples/sec
2025-03-25 13:17:27,022 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9500.0MB reserved
2025-03-25 13:17:27,022 - training - INFO - Epoch: 705/200000, Batch: 30/45, Loss: 1.8962, Throughput: 70.94 samples/sec
2025-03-25 13:17:37,980 - training - INFO - Epoch 705 completed in 35.43s. Average loss: 1.9544
2025-03-25 13:17:37,984 - training - INFO - Starting epoch 706/200000
2025-03-25 13:17:38,733 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9638.0MB reserved
2025-03-25 13:17:38,734 - training - INFO - Epoch: 706/200000, Batch: 0/45, Loss: 1.9360, Throughput: 74.89 samples/sec
2025-03-25 13:17:50,695 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9496.0MB reserved
2025-03-25 13:17:50,696 - training - INFO - Epoch: 706/200000, Batch: 15/45, Loss: 1.9239, Throughput: 70.50 samples/sec
2025-03-25 13:18:02,259 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9496.0MB reserved
2025-03-25 13:18:02,259 - training - INFO - Epoch: 706/200000, Batch: 30/45, Loss: 1.9431, Throughput: 71.52 samples/sec
2025-03-25 13:18:13,169 - training - INFO - Epoch 706 completed in 35.18s. Average loss: 2.1284
2025-03-25 13:18:13,173 - training - INFO - Starting epoch 707/200000
2025-03-25 13:18:13,920 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9632.0MB reserved
2025-03-25 13:18:13,921 - training - INFO - Epoch: 707/200000, Batch: 0/45, Loss: 2.3754, Throughput: 75.10 samples/sec
2025-03-25 13:18:25,935 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9492.0MB reserved
2025-03-25 13:18:25,935 - training - INFO - Epoch: 707/200000, Batch: 15/45, Loss: 1.9672, Throughput: 70.22 samples/sec
2025-03-25 13:18:37,573 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9492.0MB reserved
2025-03-25 13:18:37,574 - training - INFO - Epoch: 707/200000, Batch: 30/45, Loss: 1.9032, Throughput: 71.15 samples/sec
2025-03-25 13:18:48,570 - training - INFO - Epoch 707 completed in 35.40s. Average loss: 1.9572
2025-03-25 13:18:48,574 - training - INFO - Starting epoch 708/200000
2025-03-25 13:18:49,310 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9630.0MB reserved
2025-03-25 13:18:49,311 - training - INFO - Epoch: 708/200000, Batch: 0/45, Loss: 1.5859, Throughput: 76.19 samples/sec
2025-03-25 13:19:01,182 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9484.0MB reserved
2025-03-25 13:19:01,183 - training - INFO - Epoch: 708/200000, Batch: 15/45, Loss: 1.8400, Throughput: 71.07 samples/sec
2025-03-25 13:19:12,714 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9484.0MB reserved
2025-03-25 13:19:12,715 - training - INFO - Epoch: 708/200000, Batch: 30/45, Loss: 1.8681, Throughput: 71.92 samples/sec
2025-03-25 13:19:23,620 - training - INFO - Epoch 708 completed in 35.05s. Average loss: 1.9638
2025-03-25 13:19:23,624 - training - INFO - Starting epoch 709/200000
2025-03-25 13:19:24,365 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9624.0MB reserved
2025-03-25 13:19:24,366 - training - INFO - Epoch: 709/200000, Batch: 0/45, Loss: 1.8102, Throughput: 75.71 samples/sec
2025-03-25 13:19:36,217 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9502.0MB reserved
2025-03-25 13:19:36,217 - training - INFO - Epoch: 709/200000, Batch: 15/45, Loss: 1.8627, Throughput: 71.16 samples/sec
2025-03-25 13:19:47,761 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9502.0MB reserved
2025-03-25 13:19:47,761 - training - INFO - Epoch: 709/200000, Batch: 30/45, Loss: 1.8482, Throughput: 71.93 samples/sec
2025-03-25 13:19:58,713 - training - INFO - Epoch 709 completed in 35.09s. Average loss: 1.9464
2025-03-25 13:19:58,717 - training - INFO - Starting epoch 710/200000
2025-03-25 13:19:59,449 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9640.0MB reserved
2025-03-25 13:19:59,450 - training - INFO - Epoch: 710/200000, Batch: 0/45, Loss: 1.9145, Throughput: 76.52 samples/sec
2025-03-25 13:20:11,415 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9486.0MB reserved
2025-03-25 13:20:11,415 - training - INFO - Epoch: 710/200000, Batch: 15/45, Loss: 1.9648, Throughput: 70.56 samples/sec
2025-03-25 13:20:23,128 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9486.0MB reserved
2025-03-25 13:20:23,128 - training - INFO - Epoch: 710/200000, Batch: 30/45, Loss: 1.9324, Throughput: 71.12 samples/sec
2025-03-25 13:20:34,126 - training - INFO - Epoch 710 completed in 35.41s. Average loss: 2.0338
2025-03-25 13:20:34,130 - training - INFO - Starting epoch 711/200000
2025-03-25 13:20:34,882 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9622.0MB reserved
2025-03-25 13:20:34,882 - training - INFO - Epoch: 711/200000, Batch: 0/45, Loss: 1.7807, Throughput: 74.57 samples/sec
2025-03-25 13:20:46,749 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9496.0MB reserved
2025-03-25 13:20:46,750 - training - INFO - Epoch: 711/200000, Batch: 15/45, Loss: 1.8705, Throughput: 71.01 samples/sec
2025-03-25 13:20:58,410 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9496.0MB reserved
2025-03-25 13:20:58,410 - training - INFO - Epoch: 711/200000, Batch: 30/45, Loss: 1.8442, Throughput: 71.50 samples/sec
2025-03-25 13:21:09,367 - training - INFO - Epoch 711 completed in 35.24s. Average loss: 1.9897
2025-03-25 13:21:09,371 - training - INFO - Starting epoch 712/200000
2025-03-25 13:21:10,116 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9632.0MB reserved
2025-03-25 13:21:10,117 - training - INFO - Epoch: 712/200000, Batch: 0/45, Loss: 2.0877, Throughput: 75.39 samples/sec
2025-03-25 13:21:22,069 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9494.0MB reserved
2025-03-25 13:21:22,070 - training - INFO - Epoch: 712/200000, Batch: 15/45, Loss: 1.8999, Throughput: 70.57 samples/sec
2025-03-25 13:21:33,668 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9496.0MB reserved
2025-03-25 13:21:33,668 - training - INFO - Epoch: 712/200000, Batch: 30/45, Loss: 1.9016, Throughput: 71.46 samples/sec
2025-03-25 13:21:44,621 - training - INFO - Epoch 712 completed in 35.25s. Average loss: 1.9094
2025-03-25 13:21:44,625 - training - INFO - Starting epoch 713/200000
2025-03-25 13:21:45,360 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9634.0MB reserved
2025-03-25 13:21:45,360 - training - INFO - Epoch: 713/200000, Batch: 0/45, Loss: 1.4815, Throughput: 76.21 samples/sec
2025-03-25 13:21:57,192 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9490.0MB reserved
2025-03-25 13:21:57,192 - training - INFO - Epoch: 713/200000, Batch: 15/45, Loss: 1.8972, Throughput: 71.30 samples/sec
2025-03-25 13:22:08,816 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9490.0MB reserved
2025-03-25 13:22:08,816 - training - INFO - Epoch: 713/200000, Batch: 30/45, Loss: 1.8579, Throughput: 71.76 samples/sec
2025-03-25 13:22:19,740 - training - INFO - Epoch 713 completed in 35.12s. Average loss: 1.9072
2025-03-25 13:22:19,744 - training - INFO - Starting epoch 714/200000
2025-03-25 13:22:20,483 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9628.0MB reserved
2025-03-25 13:22:20,483 - training - INFO - Epoch: 714/200000, Batch: 0/45, Loss: 1.8124, Throughput: 75.89 samples/sec
2025-03-25 13:22:32,443 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9494.0MB reserved
2025-03-25 13:22:32,443 - training - INFO - Epoch: 714/200000, Batch: 15/45, Loss: 1.9254, Throughput: 70.56 samples/sec
2025-03-25 13:22:44,142 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9496.0MB reserved
2025-03-25 13:22:44,142 - training - INFO - Epoch: 714/200000, Batch: 30/45, Loss: 1.9042, Throughput: 71.16 samples/sec
2025-03-25 13:22:55,083 - training - INFO - Epoch 714 completed in 35.34s. Average loss: 1.9626
2025-03-25 13:22:55,086 - training - INFO - Starting epoch 715/200000
2025-03-25 13:22:55,844 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9634.0MB reserved
2025-03-25 13:22:55,845 - training - INFO - Epoch: 715/200000, Batch: 0/45, Loss: 1.5302, Throughput: 73.97 samples/sec
2025-03-25 13:23:07,673 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9484.0MB reserved
2025-03-25 13:23:07,674 - training - INFO - Epoch: 715/200000, Batch: 15/45, Loss: 1.9496, Throughput: 71.20 samples/sec
2025-03-25 13:23:19,254 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9484.0MB reserved
2025-03-25 13:23:19,254 - training - INFO - Epoch: 715/200000, Batch: 30/45, Loss: 1.9276, Throughput: 71.83 samples/sec
2025-03-25 13:23:30,148 - training - INFO - Epoch 715 completed in 35.06s. Average loss: 1.9151
2025-03-25 13:23:30,152 - training - INFO - Starting epoch 716/200000
2025-03-25 13:23:30,889 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9624.0MB reserved
2025-03-25 13:23:30,890 - training - INFO - Epoch: 716/200000, Batch: 0/45, Loss: 2.0547, Throughput: 76.15 samples/sec
2025-03-25 13:23:42,781 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9502.0MB reserved
2025-03-25 13:23:42,781 - training - INFO - Epoch: 716/200000, Batch: 15/45, Loss: 1.8181, Throughput: 70.96 samples/sec
2025-03-25 13:23:54,388 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9502.0MB reserved
2025-03-25 13:23:54,389 - training - INFO - Epoch: 716/200000, Batch: 30/45, Loss: 1.8350, Throughput: 71.63 samples/sec
2025-03-25 13:24:05,329 - training - INFO - Epoch 716 completed in 35.18s. Average loss: 1.9803
2025-03-25 13:24:05,333 - training - INFO - Starting epoch 717/200000
2025-03-25 13:24:06,088 - training - INFO - Memory: GPU 0: 3556.6MB allocated, 9640.0MB reserved
2025-03-25 13:24:06,088 - training - INFO - Epoch: 717/200000, Batch: 0/45, Loss: 1.6504, Throughput: 74.41 samples/sec
2025-03-25 13:24:18,120 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9488.0MB reserved
2025-03-25 13:24:18,121 - training - INFO - Epoch: 717/200000, Batch: 15/45, Loss: 1.8821, Throughput: 70.08 samples/sec
2025-03-25 13:24:29,753 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9488.0MB reserved
2025-03-25 13:24:29,754 - training - INFO - Epoch: 717/200000, Batch: 30/45, Loss: 1.8560, Throughput: 71.09 samples/sec
2025-03-25 13:24:40,778 - training - INFO - Epoch 717 completed in 35.44s. Average loss: 1.9957
2025-03-25 13:24:40,782 - training - INFO - Starting epoch 718/200000
2025-03-25 13:24:41,515 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9624.0MB reserved
2025-03-25 13:24:41,515 - training - INFO - Epoch: 718/200000, Batch: 0/45, Loss: 2.0873, Throughput: 76.47 samples/sec
2025-03-25 13:24:53,534 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9498.0MB reserved
2025-03-25 13:24:53,535 - training - INFO - Epoch: 718/200000, Batch: 15/45, Loss: 1.8573, Throughput: 70.27 samples/sec
2025-03-25 13:25:05,333 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9500.0MB reserved
2025-03-25 13:25:05,333 - training - INFO - Epoch: 718/200000, Batch: 30/45, Loss: 1.9062, Throughput: 70.71 samples/sec
2025-03-25 13:25:16,433 - training - INFO - Epoch 718 completed in 35.65s. Average loss: 1.9475
2025-03-25 13:25:16,437 - training - INFO - Starting epoch 719/200000
2025-03-25 13:25:17,195 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9638.0MB reserved
2025-03-25 13:25:17,195 - training - INFO - Epoch: 719/200000, Batch: 0/45, Loss: 1.8311, Throughput: 74.11 samples/sec
2025-03-25 13:25:29,233 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9504.0MB reserved
2025-03-25 13:25:29,233 - training - INFO - Epoch: 719/200000, Batch: 15/45, Loss: 1.8556, Throughput: 70.03 samples/sec
2025-03-25 13:25:40,912 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9504.0MB reserved
2025-03-25 13:25:40,912 - training - INFO - Epoch: 719/200000, Batch: 30/45, Loss: 1.8978, Throughput: 70.93 samples/sec
2025-03-25 13:25:51,908 - training - INFO - Epoch 719 completed in 35.47s. Average loss: 1.9731
2025-03-25 13:25:51,911 - training - INFO - Starting epoch 720/200000
2025-03-25 13:25:52,654 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9642.0MB reserved
2025-03-25 13:25:52,654 - training - INFO - Epoch: 720/200000, Batch: 0/45, Loss: 1.7635, Throughput: 75.51 samples/sec
2025-03-25 13:26:04,473 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9484.0MB reserved
2025-03-25 13:26:04,473 - training - INFO - Epoch: 720/200000, Batch: 15/45, Loss: 1.8242, Throughput: 71.34 samples/sec
2025-03-25 13:26:16,036 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9484.0MB reserved
2025-03-25 13:26:16,037 - training - INFO - Epoch: 720/200000, Batch: 30/45, Loss: 1.8618, Throughput: 71.97 samples/sec
2025-03-25 13:26:27,021 - training - INFO - Epoch 720 completed in 35.11s. Average loss: 1.9361
2025-03-25 13:26:27,025 - training - INFO - Starting epoch 721/200000
2025-03-25 13:26:27,762 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9622.0MB reserved
2025-03-25 13:26:27,763 - training - INFO - Epoch: 721/200000, Batch: 0/45, Loss: 1.9927, Throughput: 76.08 samples/sec
2025-03-25 13:26:39,748 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9508.0MB reserved
2025-03-25 13:26:39,749 - training - INFO - Epoch: 721/200000, Batch: 15/45, Loss: 1.8973, Throughput: 70.44 samples/sec
2025-03-25 13:26:51,373 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9508.0MB reserved
2025-03-25 13:26:51,373 - training - INFO - Epoch: 721/200000, Batch: 30/45, Loss: 1.8270, Throughput: 71.30 samples/sec
2025-03-25 13:27:02,286 - training - INFO - Epoch 721 completed in 35.26s. Average loss: 1.9702
2025-03-25 13:27:02,289 - training - INFO - Starting epoch 722/200000
2025-03-25 13:27:03,045 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9646.0MB reserved
2025-03-25 13:27:03,045 - training - INFO - Epoch: 722/200000, Batch: 0/45, Loss: 1.7560, Throughput: 74.16 samples/sec
2025-03-25 13:27:15,043 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9502.0MB reserved
2025-03-25 13:27:15,043 - training - INFO - Epoch: 722/200000, Batch: 15/45, Loss: 1.8519, Throughput: 70.26 samples/sec
2025-03-25 13:27:26,758 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9502.0MB reserved
2025-03-25 13:27:26,758 - training - INFO - Epoch: 722/200000, Batch: 30/45, Loss: 1.8521, Throughput: 70.95 samples/sec
2025-03-25 13:27:37,791 - training - INFO - Epoch 722 completed in 35.50s. Average loss: 1.9003
2025-03-25 13:27:37,795 - training - INFO - Starting epoch 723/200000
2025-03-25 13:27:38,531 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9640.0MB reserved
2025-03-25 13:27:38,531 - training - INFO - Epoch: 723/200000, Batch: 0/45, Loss: 1.8438, Throughput: 76.30 samples/sec
2025-03-25 13:27:50,397 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9498.0MB reserved
2025-03-25 13:27:50,397 - training - INFO - Epoch: 723/200000, Batch: 15/45, Loss: 1.9173, Throughput: 71.11 samples/sec
2025-03-25 13:28:02,009 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9498.0MB reserved
2025-03-25 13:28:02,009 - training - INFO - Epoch: 723/200000, Batch: 30/45, Loss: 1.8825, Throughput: 71.70 samples/sec
2025-03-25 13:28:12,930 - training - INFO - Epoch 723 completed in 35.14s. Average loss: 1.9587
2025-03-25 13:28:12,934 - training - INFO - Starting epoch 724/200000
2025-03-25 13:28:13,690 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9636.0MB reserved
2025-03-25 13:28:13,690 - training - INFO - Epoch: 724/200000, Batch: 0/45, Loss: 2.2199, Throughput: 74.19 samples/sec
2025-03-25 13:28:25,555 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9504.0MB reserved
2025-03-25 13:28:25,555 - training - INFO - Epoch: 724/200000, Batch: 15/45, Loss: 1.8802, Throughput: 71.00 samples/sec
2025-03-25 13:28:37,177 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9506.0MB reserved
2025-03-25 13:28:37,177 - training - INFO - Epoch: 724/200000, Batch: 30/45, Loss: 1.8333, Throughput: 71.62 samples/sec
2025-03-25 13:28:48,104 - training - INFO - Epoch 724 completed in 35.17s. Average loss: 1.9313
2025-03-25 13:28:48,107 - training - INFO - Starting epoch 725/200000
2025-03-25 13:28:48,843 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9644.0MB reserved
2025-03-25 13:28:48,843 - training - INFO - Epoch: 725/200000, Batch: 0/45, Loss: 1.6826, Throughput: 76.33 samples/sec
2025-03-25 13:29:00,750 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9506.0MB reserved
2025-03-25 13:29:00,750 - training - INFO - Epoch: 725/200000, Batch: 15/45, Loss: 1.8487, Throughput: 70.88 samples/sec
2025-03-25 13:29:12,431 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9506.0MB reserved
2025-03-25 13:29:12,431 - training - INFO - Epoch: 725/200000, Batch: 30/45, Loss: 1.9332, Throughput: 71.37 samples/sec
2025-03-25 13:29:23,489 - training - INFO - Epoch 725 completed in 35.38s. Average loss: 1.9862
2025-03-25 13:29:23,492 - training - INFO - Starting epoch 726/200000
2025-03-25 13:29:24,222 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9644.0MB reserved
2025-03-25 13:29:24,222 - training - INFO - Epoch: 726/200000, Batch: 0/45, Loss: 1.7429, Throughput: 76.87 samples/sec
2025-03-25 13:29:36,113 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9496.0MB reserved
2025-03-25 13:29:36,113 - training - INFO - Epoch: 726/200000, Batch: 15/45, Loss: 1.8626, Throughput: 71.01 samples/sec
2025-03-25 13:29:47,777 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9498.0MB reserved
2025-03-25 13:29:47,778 - training - INFO - Epoch: 726/200000, Batch: 30/45, Loss: 1.8066, Throughput: 71.49 samples/sec
2025-03-25 13:29:58,746 - training - INFO - Epoch 726 completed in 35.25s. Average loss: 1.9403
2025-03-25 13:29:58,752 - training - INFO - Starting epoch 727/200000
2025-03-25 13:29:59,510 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9636.0MB reserved
2025-03-25 13:29:59,510 - training - INFO - Epoch: 727/200000, Batch: 0/45, Loss: 2.1106, Throughput: 73.95 samples/sec
2025-03-25 13:30:11,486 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9506.0MB reserved
2025-03-25 13:30:11,486 - training - INFO - Epoch: 727/200000, Batch: 15/45, Loss: 1.8921, Throughput: 70.37 samples/sec
2025-03-25 13:30:23,156 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9506.0MB reserved
2025-03-25 13:30:23,156 - training - INFO - Epoch: 727/200000, Batch: 30/45, Loss: 1.9830, Throughput: 71.14 samples/sec
2025-03-25 13:30:34,119 - training - INFO - Epoch 727 completed in 35.37s. Average loss: 1.9863
2025-03-25 13:30:34,123 - training - INFO - Starting epoch 728/200000
2025-03-25 13:30:34,895 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9644.0MB reserved
2025-03-25 13:30:34,895 - training - INFO - Epoch: 728/200000, Batch: 0/45, Loss: 2.3612, Throughput: 72.58 samples/sec
2025-03-25 13:30:46,774 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9496.0MB reserved
2025-03-25 13:30:46,774 - training - INFO - Epoch: 728/200000, Batch: 15/45, Loss: 1.9086, Throughput: 70.83 samples/sec
2025-03-25 13:30:58,312 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9496.0MB reserved
2025-03-25 13:30:58,313 - training - INFO - Epoch: 728/200000, Batch: 30/45, Loss: 1.8069, Throughput: 71.77 samples/sec
2025-03-25 13:31:09,239 - training - INFO - Epoch 728 completed in 35.12s. Average loss: 1.9183
2025-03-25 13:31:09,243 - training - INFO - Starting epoch 729/200000
2025-03-25 13:31:09,996 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9632.0MB reserved
2025-03-25 13:31:09,996 - training - INFO - Epoch: 729/200000, Batch: 0/45, Loss: 1.9769, Throughput: 74.54 samples/sec
2025-03-25 13:31:21,935 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9510.0MB reserved
2025-03-25 13:31:21,936 - training - INFO - Epoch: 729/200000, Batch: 15/45, Loss: 1.8432, Throughput: 70.60 samples/sec
2025-03-25 13:31:33,506 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9510.0MB reserved
2025-03-25 13:31:33,506 - training - INFO - Epoch: 729/200000, Batch: 30/45, Loss: 1.8450, Throughput: 71.56 samples/sec
2025-03-25 13:31:44,407 - training - INFO - Epoch 729 completed in 35.16s. Average loss: 1.9138
2025-03-25 13:31:44,411 - training - INFO - Starting epoch 730/200000
2025-03-25 13:31:45,151 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9646.0MB reserved
2025-03-25 13:31:45,151 - training - INFO - Epoch: 730/200000, Batch: 0/45, Loss: 1.5870, Throughput: 75.85 samples/sec
2025-03-25 13:31:57,022 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9512.0MB reserved
2025-03-25 13:31:57,023 - training - INFO - Epoch: 730/200000, Batch: 15/45, Loss: 1.7792, Throughput: 71.06 samples/sec
2025-03-25 13:32:08,616 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9512.0MB reserved
2025-03-25 13:32:08,617 - training - INFO - Epoch: 730/200000, Batch: 30/45, Loss: 1.7324, Throughput: 71.72 samples/sec
2025-03-25 13:32:19,549 - training - INFO - Epoch 730 completed in 35.14s. Average loss: 1.9169
2025-03-25 13:32:19,553 - training - INFO - Starting epoch 731/200000
2025-03-25 13:32:20,279 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9650.0MB reserved
2025-03-25 13:32:20,279 - training - INFO - Epoch: 731/200000, Batch: 0/45, Loss: 1.7954, Throughput: 77.19 samples/sec
2025-03-25 13:32:32,182 - training - INFO - Memory: GPU 0: 3564.2MB allocated, 9502.0MB reserved
2025-03-25 13:32:32,183 - training - INFO - Epoch: 731/200000, Batch: 15/45, Loss: 1.8760, Throughput: 70.95 samples/sec
2025-03-25 13:32:43,791 - training - INFO - Memory: GPU 0: 3564.2MB allocated, 9502.0MB reserved
2025-03-25 13:32:43,791 - training - INFO - Epoch: 731/200000, Batch: 30/45, Loss: 1.9033, Throughput: 71.63 samples/sec
2025-03-25 13:32:54,701 - training - INFO - Epoch 731 completed in 35.15s. Average loss: 1.9539
2025-03-25 13:32:54,705 - training - INFO - Starting epoch 732/200000
2025-03-25 13:32:55,442 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9640.0MB reserved
2025-03-25 13:32:55,443 - training - INFO - Epoch: 732/200000, Batch: 0/45, Loss: 1.8619, Throughput: 75.96 samples/sec
2025-03-25 13:33:07,289 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9488.0MB reserved
2025-03-25 13:33:07,290 - training - INFO - Epoch: 732/200000, Batch: 15/45, Loss: 1.8253, Throughput: 71.20 samples/sec
2025-03-25 13:33:18,852 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9490.0MB reserved
2025-03-25 13:33:18,853 - training - INFO - Epoch: 732/200000, Batch: 30/45, Loss: 1.8316, Throughput: 71.90 samples/sec
2025-03-25 13:33:29,812 - training - INFO - Epoch 732 completed in 35.11s. Average loss: 1.9247
2025-03-25 13:33:29,816 - training - INFO - Starting epoch 733/200000
2025-03-25 13:33:30,569 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9628.0MB reserved
2025-03-25 13:33:30,570 - training - INFO - Epoch: 733/200000, Batch: 0/45, Loss: 1.6954, Throughput: 74.43 samples/sec
2025-03-25 13:33:42,391 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9496.0MB reserved
2025-03-25 13:33:42,391 - training - INFO - Epoch: 733/200000, Batch: 15/45, Loss: 1.8195, Throughput: 71.26 samples/sec
2025-03-25 13:33:53,954 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9498.0MB reserved
2025-03-25 13:33:53,954 - training - INFO - Epoch: 733/200000, Batch: 30/45, Loss: 1.8495, Throughput: 71.92 samples/sec
2025-03-25 13:34:04,876 - training - INFO - Epoch 733 completed in 35.06s. Average loss: 1.8976
2025-03-25 13:34:04,879 - training - INFO - Starting epoch 734/200000
2025-03-25 13:34:05,605 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9636.0MB reserved
2025-03-25 13:34:05,606 - training - INFO - Epoch: 734/200000, Batch: 0/45, Loss: 1.9001, Throughput: 77.36 samples/sec
2025-03-25 13:34:17,539 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9492.0MB reserved
2025-03-25 13:34:17,539 - training - INFO - Epoch: 734/200000, Batch: 15/45, Loss: 1.9307, Throughput: 70.79 samples/sec
2025-03-25 13:34:29,151 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9492.0MB reserved
2025-03-25 13:34:29,152 - training - INFO - Epoch: 734/200000, Batch: 30/45, Loss: 1.9170, Throughput: 71.53 samples/sec
2025-03-25 13:34:40,068 - training - INFO - Epoch 734 completed in 35.19s. Average loss: 1.9664
2025-03-25 13:34:40,072 - training - INFO - Starting epoch 735/200000
2025-03-25 13:34:40,814 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9630.0MB reserved
2025-03-25 13:34:40,815 - training - INFO - Epoch: 735/200000, Batch: 0/45, Loss: 1.6973, Throughput: 75.66 samples/sec
2025-03-25 13:34:52,771 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9494.0MB reserved
2025-03-25 13:34:52,771 - training - INFO - Epoch: 735/200000, Batch: 15/45, Loss: 1.8618, Throughput: 70.56 samples/sec
2025-03-25 13:35:04,426 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9494.0MB reserved
2025-03-25 13:35:04,426 - training - INFO - Epoch: 735/200000, Batch: 30/45, Loss: 1.8522, Throughput: 71.28 samples/sec
2025-03-25 13:35:15,346 - training - INFO - Epoch 735 completed in 35.27s. Average loss: 1.9110
2025-03-25 13:35:15,350 - training - INFO - Starting epoch 736/200000
2025-03-25 13:35:16,101 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9632.0MB reserved
2025-03-25 13:35:16,102 - training - INFO - Epoch: 736/200000, Batch: 0/45, Loss: 2.1541, Throughput: 74.57 samples/sec
2025-03-25 13:35:28,004 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9506.0MB reserved
2025-03-25 13:35:28,004 - training - INFO - Epoch: 736/200000, Batch: 15/45, Loss: 1.9089, Throughput: 70.81 samples/sec
2025-03-25 13:35:39,589 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9506.0MB reserved
2025-03-25 13:35:39,589 - training - INFO - Epoch: 736/200000, Batch: 30/45, Loss: 1.8508, Throughput: 71.62 samples/sec
2025-03-25 13:35:50,505 - training - INFO - Epoch 736 completed in 35.16s. Average loss: 1.9188
2025-03-25 13:35:50,509 - training - INFO - Starting epoch 737/200000
2025-03-25 13:35:51,278 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9644.0MB reserved
2025-03-25 13:35:51,278 - training - INFO - Epoch: 737/200000, Batch: 0/45, Loss: 1.7016, Throughput: 72.91 samples/sec
2025-03-25 13:36:03,189 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9502.0MB reserved
2025-03-25 13:36:03,189 - training - INFO - Epoch: 737/200000, Batch: 15/45, Loss: 1.8721, Throughput: 70.67 samples/sec
2025-03-25 13:36:14,898 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9504.0MB reserved
2025-03-25 13:36:14,898 - training - INFO - Epoch: 737/200000, Batch: 30/45, Loss: 1.8943, Throughput: 71.18 samples/sec
2025-03-25 13:36:25,891 - training - INFO - Epoch 737 completed in 35.38s. Average loss: 1.9401
2025-03-25 13:36:25,894 - training - INFO - Starting epoch 738/200000
2025-03-25 13:36:26,641 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9642.0MB reserved
2025-03-25 13:36:26,641 - training - INFO - Epoch: 738/200000, Batch: 0/45, Loss: 1.7552, Throughput: 75.21 samples/sec
2025-03-25 13:36:38,481 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9506.0MB reserved
2025-03-25 13:36:38,481 - training - INFO - Epoch: 738/200000, Batch: 15/45, Loss: 1.8670, Throughput: 71.20 samples/sec
2025-03-25 13:36:50,030 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9506.0MB reserved
2025-03-25 13:36:50,031 - training - INFO - Epoch: 738/200000, Batch: 30/45, Loss: 1.9039, Throughput: 71.93 samples/sec
2025-03-25 13:37:00,999 - training - INFO - Epoch 738 completed in 35.10s. Average loss: 1.9086
2025-03-25 13:37:01,003 - training - INFO - Starting epoch 739/200000
2025-03-25 13:37:01,763 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9644.0MB reserved
2025-03-25 13:37:01,763 - training - INFO - Epoch: 739/200000, Batch: 0/45, Loss: 2.1469, Throughput: 73.85 samples/sec
2025-03-25 13:37:13,716 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9496.0MB reserved
2025-03-25 13:37:13,717 - training - INFO - Epoch: 739/200000, Batch: 15/45, Loss: 1.8887, Throughput: 70.49 samples/sec
2025-03-25 13:37:25,329 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9496.0MB reserved
2025-03-25 13:37:25,330 - training - INFO - Epoch: 739/200000, Batch: 30/45, Loss: 1.8087, Throughput: 71.37 samples/sec
2025-03-25 13:37:36,267 - training - INFO - Epoch 739 completed in 35.26s. Average loss: 1.8969
2025-03-25 13:37:36,271 - training - INFO - Starting epoch 740/200000
2025-03-25 13:37:37,003 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9634.0MB reserved
2025-03-25 13:37:37,003 - training - INFO - Epoch: 740/200000, Batch: 0/45, Loss: 1.4894, Throughput: 76.68 samples/sec
2025-03-25 13:37:48,963 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9494.0MB reserved
2025-03-25 13:37:48,964 - training - INFO - Epoch: 740/200000, Batch: 15/45, Loss: 1.8160, Throughput: 70.60 samples/sec
2025-03-25 13:38:00,485 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9494.0MB reserved
2025-03-25 13:38:00,485 - training - INFO - Epoch: 740/200000, Batch: 30/45, Loss: 1.8102, Throughput: 71.70 samples/sec
2025-03-25 13:38:11,381 - training - INFO - Epoch 740 completed in 35.11s. Average loss: 1.8860
2025-03-25 13:38:11,385 - training - INFO - Starting epoch 741/200000
2025-03-25 13:38:12,137 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9632.0MB reserved
2025-03-25 13:38:12,138 - training - INFO - Epoch: 741/200000, Batch: 0/45, Loss: 1.7176, Throughput: 74.62 samples/sec
2025-03-25 13:38:24,009 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9504.0MB reserved
2025-03-25 13:38:24,010 - training - INFO - Epoch: 741/200000, Batch: 15/45, Loss: 1.9217, Throughput: 70.99 samples/sec
2025-03-25 13:38:35,699 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9504.0MB reserved
2025-03-25 13:38:35,700 - training - INFO - Epoch: 741/200000, Batch: 30/45, Loss: 1.8756, Throughput: 71.40 samples/sec
2025-03-25 13:38:46,741 - training - INFO - Epoch 741 completed in 35.36s. Average loss: 1.9049
2025-03-25 13:38:46,746 - training - INFO - Starting epoch 742/200000
2025-03-25 13:38:47,483 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9640.0MB reserved
2025-03-25 13:38:47,484 - training - INFO - Epoch: 742/200000, Batch: 0/45, Loss: 2.4435, Throughput: 76.16 samples/sec
2025-03-25 13:38:59,533 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9504.0MB reserved
2025-03-25 13:38:59,533 - training - INFO - Epoch: 742/200000, Batch: 15/45, Loss: 1.9841, Throughput: 70.09 samples/sec
2025-03-25 13:39:11,254 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9504.0MB reserved
2025-03-25 13:39:11,254 - training - INFO - Epoch: 742/200000, Batch: 30/45, Loss: 1.9150, Throughput: 70.84 samples/sec
2025-03-25 13:39:22,334 - training - INFO - Epoch 742 completed in 35.59s. Average loss: 1.9270
2025-03-25 13:39:22,338 - training - INFO - Starting epoch 743/200000
2025-03-25 13:39:23,095 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9642.0MB reserved
2025-03-25 13:39:23,096 - training - INFO - Epoch: 743/200000, Batch: 0/45, Loss: 2.2814, Throughput: 74.10 samples/sec
2025-03-25 13:39:35,186 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9488.0MB reserved
2025-03-25 13:39:35,187 - training - INFO - Epoch: 743/200000, Batch: 15/45, Loss: 1.8518, Throughput: 69.75 samples/sec
2025-03-25 13:39:46,819 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9490.0MB reserved
2025-03-25 13:39:46,820 - training - INFO - Epoch: 743/200000, Batch: 30/45, Loss: 1.8218, Throughput: 70.92 samples/sec
2025-03-25 13:39:57,707 - training - INFO - Epoch 743 completed in 35.37s. Average loss: 1.9222
2025-03-25 13:39:57,711 - training - INFO - Starting epoch 744/200000
2025-03-25 13:39:58,447 - training - INFO - Memory: GPU 0: 3556.1MB allocated, 9628.0MB reserved
2025-03-25 13:39:58,447 - training - INFO - Epoch: 744/200000, Batch: 0/45, Loss: 1.8791, Throughput: 76.15 samples/sec
2025-03-25 13:40:10,349 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9520.0MB reserved
2025-03-25 13:40:10,349 - training - INFO - Epoch: 744/200000, Batch: 15/45, Loss: 1.9129, Throughput: 70.91 samples/sec
2025-03-25 13:40:21,894 - training - INFO - Memory: GPU 0: 3557.2MB allocated, 9520.0MB reserved
2025-03-25 13:40:21,895 - training - INFO - Epoch: 744/200000, Batch: 30/45, Loss: 1.9311, Throughput: 71.79 samples/sec
2025-03-25 13:40:32,814 - training - INFO - Epoch 744 completed in 35.10s. Average loss: 1.9301
2025-03-25 13:40:32,818 - training - INFO - Starting epoch 745/200000
2025-03-25 13:40:33,570 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9658.0MB reserved
2025-03-25 13:40:33,570 - training - INFO - Epoch: 745/200000, Batch: 0/45, Loss: 1.6881, Throughput: 74.52 samples/sec
2025-03-25 13:40:45,549 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9508.0MB reserved
2025-03-25 13:40:45,550 - training - INFO - Epoch: 745/200000, Batch: 15/45, Loss: 1.8738, Throughput: 70.38 samples/sec
2025-03-25 13:40:57,265 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9510.0MB reserved
2025-03-25 13:40:57,266 - training - INFO - Epoch: 745/200000, Batch: 30/45, Loss: 1.8300, Throughput: 71.01 samples/sec
2025-03-25 13:41:08,253 - training - INFO - Epoch 745 completed in 35.44s. Average loss: 1.9247
2025-03-25 13:41:08,256 - training - INFO - Starting epoch 746/200000
2025-03-25 13:41:08,992 - training - INFO - Memory: GPU 0: 3556.6MB allocated, 9648.0MB reserved
2025-03-25 13:41:08,992 - training - INFO - Epoch: 746/200000, Batch: 0/45, Loss: 1.8535, Throughput: 76.25 samples/sec
2025-03-25 13:41:20,840 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9482.0MB reserved
2025-03-25 13:41:20,840 - training - INFO - Epoch: 746/200000, Batch: 15/45, Loss: 2.0017, Throughput: 71.21 samples/sec
2025-03-25 13:41:32,421 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9482.0MB reserved
2025-03-25 13:41:32,421 - training - INFO - Epoch: 746/200000, Batch: 30/45, Loss: 1.9592, Throughput: 71.84 samples/sec
2025-03-25 13:41:43,375 - training - INFO - Epoch 746 completed in 35.12s. Average loss: 1.9136
2025-03-25 13:41:43,379 - training - INFO - Starting epoch 747/200000
2025-03-25 13:41:44,152 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9620.0MB reserved
2025-03-25 13:41:44,152 - training - INFO - Epoch: 747/200000, Batch: 0/45, Loss: 2.0594, Throughput: 72.67 samples/sec
2025-03-25 13:41:56,028 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9510.0MB reserved
2025-03-25 13:41:56,029 - training - INFO - Epoch: 747/200000, Batch: 15/45, Loss: 1.8047, Throughput: 70.85 samples/sec
2025-03-25 13:42:07,778 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9510.0MB reserved
2025-03-25 13:42:07,778 - training - INFO - Epoch: 747/200000, Batch: 30/45, Loss: 1.8261, Throughput: 71.16 samples/sec
2025-03-25 13:42:18,812 - training - INFO - Epoch 747 completed in 35.43s. Average loss: 1.8860
2025-03-25 13:42:18,815 - training - INFO - Starting epoch 748/200000
2025-03-25 13:42:19,582 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9648.0MB reserved
2025-03-25 13:42:19,582 - training - INFO - Epoch: 748/200000, Batch: 0/45, Loss: 1.8414, Throughput: 73.18 samples/sec
2025-03-25 13:42:31,479 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9500.0MB reserved
2025-03-25 13:42:31,479 - training - INFO - Epoch: 748/200000, Batch: 15/45, Loss: 1.9746, Throughput: 70.76 samples/sec
2025-03-25 13:42:43,082 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9500.0MB reserved
2025-03-25 13:42:43,083 - training - INFO - Epoch: 748/200000, Batch: 30/45, Loss: 1.9596, Throughput: 71.55 samples/sec
2025-03-25 13:42:54,059 - training - INFO - Epoch 748 completed in 35.24s. Average loss: 1.9158
2025-03-25 13:42:54,063 - training - INFO - Starting epoch 749/200000
2025-03-25 13:42:54,794 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9638.0MB reserved
2025-03-25 13:42:54,794 - training - INFO - Epoch: 749/200000, Batch: 0/45, Loss: 2.1702, Throughput: 76.88 samples/sec
2025-03-25 13:43:06,727 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9514.0MB reserved
2025-03-25 13:43:06,728 - training - INFO - Epoch: 749/200000, Batch: 15/45, Loss: 1.7730, Throughput: 70.77 samples/sec
2025-03-25 13:43:18,423 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9514.0MB reserved
2025-03-25 13:43:18,424 - training - INFO - Epoch: 749/200000, Batch: 30/45, Loss: 1.7911, Throughput: 71.27 samples/sec
2025-03-25 13:43:29,393 - training - INFO - Epoch 749 completed in 35.33s. Average loss: 1.9659
2025-03-25 13:43:29,397 - training - INFO - Starting epoch 750/200000
2025-03-25 13:43:30,133 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9652.0MB reserved
2025-03-25 13:43:30,133 - training - INFO - Epoch: 750/200000, Batch: 0/45, Loss: 2.1545, Throughput: 76.15 samples/sec
2025-03-25 13:43:42,106 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9514.0MB reserved
2025-03-25 13:43:42,106 - training - INFO - Epoch: 750/200000, Batch: 15/45, Loss: 1.8762, Throughput: 70.51 samples/sec
2025-03-25 13:43:53,731 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9514.0MB reserved
2025-03-25 13:43:53,731 - training - INFO - Epoch: 750/200000, Batch: 30/45, Loss: 1.8806, Throughput: 71.34 samples/sec
2025-03-25 13:44:04,671 - training - INFO - Epoch 750 completed in 35.27s. Average loss: 1.9416
2025-03-25 13:44:04,675 - training - INFO - Starting epoch 751/200000
2025-03-25 13:44:05,408 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9652.0MB reserved
2025-03-25 13:44:05,409 - training - INFO - Epoch: 751/200000, Batch: 0/45, Loss: 1.8121, Throughput: 76.49 samples/sec
2025-03-25 13:44:17,337 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9506.0MB reserved
2025-03-25 13:44:17,337 - training - INFO - Epoch: 751/200000, Batch: 15/45, Loss: 1.8023, Throughput: 70.77 samples/sec
2025-03-25 13:44:28,929 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9506.0MB reserved
2025-03-25 13:44:28,930 - training - INFO - Epoch: 751/200000, Batch: 30/45, Loss: 1.8580, Throughput: 71.58 samples/sec
2025-03-25 13:44:39,865 - training - INFO - Epoch 751 completed in 35.19s. Average loss: 1.8920
2025-03-25 13:44:39,869 - training - INFO - Starting epoch 752/200000
2025-03-25 13:44:40,617 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9644.0MB reserved
2025-03-25 13:44:40,618 - training - INFO - Epoch: 752/200000, Batch: 0/45, Loss: 1.9981, Throughput: 75.06 samples/sec
2025-03-25 13:44:52,543 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9502.0MB reserved
2025-03-25 13:44:52,543 - training - INFO - Epoch: 752/200000, Batch: 15/45, Loss: 1.8194, Throughput: 70.71 samples/sec
2025-03-25 13:45:04,115 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9504.0MB reserved
2025-03-25 13:45:04,115 - training - INFO - Epoch: 752/200000, Batch: 30/45, Loss: 1.8187, Throughput: 71.61 samples/sec
2025-03-25 13:45:15,024 - training - INFO - Epoch 752 completed in 35.15s. Average loss: 1.8835
2025-03-25 13:45:15,028 - training - INFO - Starting epoch 753/200000
2025-03-25 13:45:15,784 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9642.0MB reserved
2025-03-25 13:45:15,784 - training - INFO - Epoch: 753/200000, Batch: 0/45, Loss: 2.0990, Throughput: 74.30 samples/sec
2025-03-25 13:45:27,692 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9514.0MB reserved
2025-03-25 13:45:27,692 - training - INFO - Epoch: 753/200000, Batch: 15/45, Loss: 1.8266, Throughput: 70.75 samples/sec
2025-03-25 13:45:39,278 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9514.0MB reserved
2025-03-25 13:45:39,279 - training - INFO - Epoch: 753/200000, Batch: 30/45, Loss: 1.8371, Throughput: 71.59 samples/sec
2025-03-25 13:45:50,252 - training - INFO - Epoch 753 completed in 35.22s. Average loss: 1.9383
2025-03-25 13:45:50,256 - training - INFO - Starting epoch 754/200000
2025-03-25 13:45:50,997 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9650.0MB reserved
2025-03-25 13:45:50,998 - training - INFO - Epoch: 754/200000, Batch: 0/45, Loss: 2.2947, Throughput: 75.63 samples/sec
2025-03-25 13:46:02,940 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9498.0MB reserved
2025-03-25 13:46:02,940 - training - INFO - Epoch: 754/200000, Batch: 15/45, Loss: 1.7879, Throughput: 70.65 samples/sec
2025-03-25 13:46:14,612 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9498.0MB reserved
2025-03-25 13:46:14,612 - training - INFO - Epoch: 754/200000, Batch: 30/45, Loss: 1.8678, Throughput: 71.28 samples/sec
2025-03-25 13:46:25,642 - training - INFO - Epoch 754 completed in 35.39s. Average loss: 1.9423
2025-03-25 13:46:25,646 - training - INFO - Starting epoch 755/200000
2025-03-25 13:46:26,399 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9636.0MB reserved
2025-03-25 13:46:26,399 - training - INFO - Epoch: 755/200000, Batch: 0/45, Loss: 2.0179, Throughput: 74.59 samples/sec
2025-03-25 13:46:38,459 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9494.0MB reserved
2025-03-25 13:46:38,459 - training - INFO - Epoch: 755/200000, Batch: 15/45, Loss: 1.8487, Throughput: 69.94 samples/sec
2025-03-25 13:46:50,251 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9494.0MB reserved
2025-03-25 13:46:50,251 - training - INFO - Epoch: 755/200000, Batch: 30/45, Loss: 1.9039, Throughput: 70.56 samples/sec
2025-03-25 13:47:01,275 - training - INFO - Epoch 755 completed in 35.63s. Average loss: 1.9891
2025-03-25 13:47:01,279 - training - INFO - Starting epoch 756/200000
2025-03-25 13:47:02,043 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9634.0MB reserved
2025-03-25 13:47:02,044 - training - INFO - Epoch: 756/200000, Batch: 0/45, Loss: 1.5559, Throughput: 73.37 samples/sec
2025-03-25 13:47:13,942 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9492.0MB reserved
2025-03-25 13:47:13,942 - training - INFO - Epoch: 756/200000, Batch: 15/45, Loss: 1.8178, Throughput: 70.77 samples/sec
2025-03-25 13:47:25,608 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9492.0MB reserved
2025-03-25 13:47:25,608 - training - INFO - Epoch: 756/200000, Batch: 30/45, Loss: 1.8470, Throughput: 71.36 samples/sec
2025-03-25 13:47:36,585 - training - INFO - Epoch 756 completed in 35.31s. Average loss: 1.8649
2025-03-25 13:47:36,590 - training - INFO - Starting epoch 757/200000
2025-03-25 13:47:37,323 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9630.0MB reserved
2025-03-25 13:47:37,323 - training - INFO - Epoch: 757/200000, Batch: 0/45, Loss: 2.4088, Throughput: 76.51 samples/sec
2025-03-25 13:47:49,319 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9512.0MB reserved
2025-03-25 13:47:49,319 - training - INFO - Epoch: 757/200000, Batch: 15/45, Loss: 1.7861, Throughput: 70.40 samples/sec
2025-03-25 13:48:01,001 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9512.0MB reserved
2025-03-25 13:48:01,002 - training - INFO - Epoch: 757/200000, Batch: 30/45, Loss: 1.8049, Throughput: 71.12 samples/sec
2025-03-25 13:48:11,975 - training - INFO - Epoch 757 completed in 35.39s. Average loss: 1.9191
2025-03-25 13:48:11,979 - training - INFO - Starting epoch 758/200000
2025-03-25 13:48:12,731 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9648.0MB reserved
2025-03-25 13:48:12,731 - training - INFO - Epoch: 758/200000, Batch: 0/45, Loss: 1.5643, Throughput: 74.60 samples/sec
2025-03-25 13:48:24,677 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9490.0MB reserved
2025-03-25 13:48:24,677 - training - INFO - Epoch: 758/200000, Batch: 15/45, Loss: 1.8144, Throughput: 70.57 samples/sec
2025-03-25 13:48:36,254 - training - INFO - Memory: GPU 0: 3563.1MB allocated, 9490.0MB reserved
2025-03-25 13:48:36,255 - training - INFO - Epoch: 758/200000, Batch: 30/45, Loss: 1.8559, Throughput: 71.51 samples/sec
2025-03-25 13:48:47,272 - training - INFO - Epoch 758 completed in 35.29s. Average loss: 1.9480
2025-03-25 13:48:47,280 - training - INFO - Starting epoch 759/200000
2025-03-25 13:48:48,035 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9626.0MB reserved
2025-03-25 13:48:48,035 - training - INFO - Epoch: 759/200000, Batch: 0/45, Loss: 1.6638, Throughput: 74.42 samples/sec
2025-03-25 13:48:59,934 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9490.0MB reserved
2025-03-25 13:48:59,935 - training - INFO - Epoch: 759/200000, Batch: 15/45, Loss: 1.8014, Throughput: 70.81 samples/sec
2025-03-25 13:49:11,555 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9492.0MB reserved
2025-03-25 13:49:11,555 - training - INFO - Epoch: 759/200000, Batch: 30/45, Loss: 1.7761, Throughput: 71.52 samples/sec
2025-03-25 13:49:22,507 - training - INFO - Epoch 759 completed in 35.23s. Average loss: 1.8813
2025-03-25 13:49:22,511 - training - INFO - Starting epoch 760/200000
2025-03-25 13:49:23,265 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9630.0MB reserved
2025-03-25 13:49:23,265 - training - INFO - Epoch: 760/200000, Batch: 0/45, Loss: 1.9407, Throughput: 74.47 samples/sec
2025-03-25 13:49:35,211 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9490.0MB reserved
2025-03-25 13:49:35,211 - training - INFO - Epoch: 760/200000, Batch: 15/45, Loss: 1.8386, Throughput: 70.56 samples/sec
2025-03-25 13:49:46,870 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9490.0MB reserved
2025-03-25 13:49:46,871 - training - INFO - Epoch: 760/200000, Batch: 30/45, Loss: 1.8129, Throughput: 71.28 samples/sec
2025-03-25 13:49:57,880 - training - INFO - Epoch 760 completed in 35.37s. Average loss: 1.8543
2025-03-25 13:49:57,884 - training - INFO - Starting epoch 761/200000
2025-03-25 13:49:58,636 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9628.0MB reserved
2025-03-25 13:49:58,636 - training - INFO - Epoch: 761/200000, Batch: 0/45, Loss: 1.5418, Throughput: 74.54 samples/sec
2025-03-25 13:50:10,614 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9504.0MB reserved
2025-03-25 13:50:10,614 - training - INFO - Epoch: 761/200000, Batch: 15/45, Loss: 1.7281, Throughput: 70.39 samples/sec
2025-03-25 13:50:22,346 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9504.0MB reserved
2025-03-25 13:50:22,347 - training - INFO - Epoch: 761/200000, Batch: 30/45, Loss: 1.8048, Throughput: 70.97 samples/sec
2025-03-25 13:50:33,375 - training - INFO - Epoch 761 completed in 35.49s. Average loss: 1.9064
2025-03-25 13:50:33,378 - training - INFO - Starting epoch 762/200000
2025-03-25 13:50:34,150 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9642.0MB reserved
2025-03-25 13:50:34,150 - training - INFO - Epoch: 762/200000, Batch: 0/45, Loss: 2.1514, Throughput: 72.78 samples/sec
2025-03-25 13:50:45,983 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9504.0MB reserved
2025-03-25 13:50:45,984 - training - INFO - Epoch: 762/200000, Batch: 15/45, Loss: 1.8082, Throughput: 71.09 samples/sec
2025-03-25 13:50:57,615 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9506.0MB reserved
2025-03-25 13:50:57,637 - training - INFO - Epoch: 762/200000, Batch: 30/45, Loss: 1.8485, Throughput: 71.63 samples/sec
2025-03-25 13:51:08,576 - training - INFO - Epoch 762 completed in 35.20s. Average loss: 1.9131
2025-03-25 13:51:08,580 - training - INFO - Starting epoch 763/200000
2025-03-25 13:51:09,345 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9644.0MB reserved
2025-03-25 13:51:09,346 - training - INFO - Epoch: 763/200000, Batch: 0/45, Loss: 1.3886, Throughput: 73.33 samples/sec
2025-03-25 13:51:21,280 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9496.0MB reserved
2025-03-25 13:51:21,281 - training - INFO - Epoch: 763/200000, Batch: 15/45, Loss: 1.7653, Throughput: 70.56 samples/sec
2025-03-25 13:51:32,895 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9496.0MB reserved
2025-03-25 13:51:32,896 - training - INFO - Epoch: 763/200000, Batch: 30/45, Loss: 1.7703, Throughput: 71.40 samples/sec
2025-03-25 13:51:43,826 - training - INFO - Epoch 763 completed in 35.25s. Average loss: 1.9042
2025-03-25 13:51:43,830 - training - INFO - Starting epoch 764/200000
2025-03-25 13:51:44,570 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9634.0MB reserved
2025-03-25 13:51:44,570 - training - INFO - Epoch: 764/200000, Batch: 0/45, Loss: 2.1602, Throughput: 75.86 samples/sec
2025-03-25 13:51:56,486 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9486.0MB reserved
2025-03-25 13:51:56,486 - training - INFO - Epoch: 764/200000, Batch: 15/45, Loss: 1.8308, Throughput: 70.81 samples/sec
2025-03-25 13:52:08,056 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9486.0MB reserved
2025-03-25 13:52:08,056 - training - INFO - Epoch: 764/200000, Batch: 30/45, Loss: 1.8428, Throughput: 71.67 samples/sec
2025-03-25 13:52:19,006 - training - INFO - Epoch 764 completed in 35.18s. Average loss: 1.8872
2025-03-25 13:52:19,010 - training - INFO - Starting epoch 765/200000
2025-03-25 13:52:19,745 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9624.0MB reserved
2025-03-25 13:52:19,746 - training - INFO - Epoch: 765/200000, Batch: 0/45, Loss: 1.6796, Throughput: 76.40 samples/sec
2025-03-25 13:52:31,644 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9510.0MB reserved
2025-03-25 13:52:31,645 - training - INFO - Epoch: 765/200000, Batch: 15/45, Loss: 1.8407, Throughput: 70.94 samples/sec
2025-03-25 13:52:43,338 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9510.0MB reserved
2025-03-25 13:52:43,339 - training - INFO - Epoch: 765/200000, Batch: 30/45, Loss: 1.8576, Throughput: 71.36 samples/sec
2025-03-25 13:52:54,290 - training - INFO - Epoch 765 completed in 35.28s. Average loss: 1.8941
2025-03-25 13:52:54,294 - training - INFO - Starting epoch 766/200000
2025-03-25 13:52:55,057 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9648.0MB reserved
2025-03-25 13:52:55,057 - training - INFO - Epoch: 766/200000, Batch: 0/45, Loss: 1.8011, Throughput: 73.59 samples/sec
2025-03-25 13:53:07,022 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9504.0MB reserved
2025-03-25 13:53:07,023 - training - INFO - Epoch: 766/200000, Batch: 15/45, Loss: 1.8072, Throughput: 70.40 samples/sec
2025-03-25 13:53:18,673 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9504.0MB reserved
2025-03-25 13:53:18,674 - training - INFO - Epoch: 766/200000, Batch: 30/45, Loss: 1.8009, Throughput: 71.21 samples/sec
2025-03-25 13:53:29,636 - training - INFO - Epoch 766 completed in 35.34s. Average loss: 1.9039
2025-03-25 13:53:29,640 - training - INFO - Starting epoch 767/200000
2025-03-25 13:53:30,364 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9642.0MB reserved
2025-03-25 13:53:30,364 - training - INFO - Epoch: 767/200000, Batch: 0/45, Loss: 1.8667, Throughput: 77.52 samples/sec
2025-03-25 13:53:42,270 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9490.0MB reserved
2025-03-25 13:53:42,271 - training - INFO - Epoch: 767/200000, Batch: 15/45, Loss: 1.8163, Throughput: 70.95 samples/sec
2025-03-25 13:53:53,950 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9492.0MB reserved
2025-03-25 13:53:53,951 - training - INFO - Epoch: 767/200000, Batch: 30/45, Loss: 1.8382, Throughput: 71.42 samples/sec
2025-03-25 13:54:04,930 - training - INFO - Epoch 767 completed in 35.29s. Average loss: 1.8789
2025-03-25 13:54:04,934 - training - INFO - Starting epoch 768/200000
2025-03-25 13:54:05,686 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9630.0MB reserved
2025-03-25 13:54:05,686 - training - INFO - Epoch: 768/200000, Batch: 0/45, Loss: 1.5481, Throughput: 74.63 samples/sec
2025-03-25 13:54:17,523 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9484.0MB reserved
2025-03-25 13:54:17,523 - training - INFO - Epoch: 768/200000, Batch: 15/45, Loss: 1.8615, Throughput: 71.18 samples/sec
2025-03-25 13:54:29,149 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9484.0MB reserved
2025-03-25 13:54:29,149 - training - INFO - Epoch: 768/200000, Batch: 30/45, Loss: 1.8762, Throughput: 71.70 samples/sec
2025-03-25 13:54:40,067 - training - INFO - Epoch 768 completed in 35.13s. Average loss: 1.9600
2025-03-25 13:54:40,070 - training - INFO - Starting epoch 769/200000
2025-03-25 13:54:40,796 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9622.0MB reserved
2025-03-25 13:54:40,796 - training - INFO - Epoch: 769/200000, Batch: 0/45, Loss: 1.5055, Throughput: 77.31 samples/sec
2025-03-25 13:54:52,631 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9506.0MB reserved
2025-03-25 13:54:52,631 - training - INFO - Epoch: 769/200000, Batch: 15/45, Loss: 1.9069, Throughput: 71.35 samples/sec
2025-03-25 13:55:04,138 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9506.0MB reserved
2025-03-25 13:55:04,138 - training - INFO - Epoch: 769/200000, Batch: 30/45, Loss: 1.8787, Throughput: 72.13 samples/sec
2025-03-25 13:55:15,035 - training - INFO - Epoch 769 completed in 34.96s. Average loss: 1.8961
2025-03-25 13:55:15,039 - training - INFO - Starting epoch 770/200000
2025-03-25 13:55:15,767 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9644.0MB reserved
2025-03-25 13:55:15,767 - training - INFO - Epoch: 770/200000, Batch: 0/45, Loss: 1.6796, Throughput: 76.99 samples/sec
2025-03-25 13:55:27,621 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9510.0MB reserved
2025-03-25 13:55:27,621 - training - INFO - Epoch: 770/200000, Batch: 15/45, Loss: 1.8240, Throughput: 71.22 samples/sec
2025-03-25 13:55:39,180 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9510.0MB reserved
2025-03-25 13:55:39,180 - training - INFO - Epoch: 770/200000, Batch: 30/45, Loss: 1.8620, Throughput: 71.91 samples/sec
2025-03-25 13:55:50,086 - training - INFO - Epoch 770 completed in 35.05s. Average loss: 1.8785
2025-03-25 13:55:50,089 - training - INFO - Starting epoch 771/200000
2025-03-25 13:55:50,854 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9648.0MB reserved
2025-03-25 13:55:50,854 - training - INFO - Epoch: 771/200000, Batch: 0/45, Loss: 1.5762, Throughput: 73.39 samples/sec
2025-03-25 13:56:02,698 - training - INFO - Memory: GPU 0: 3562.6MB allocated, 9494.0MB reserved
2025-03-25 13:56:02,698 - training - INFO - Epoch: 771/200000, Batch: 15/45, Loss: 1.9043, Throughput: 71.07 samples/sec
2025-03-25 13:56:14,251 - training - INFO - Memory: GPU 0: 3562.6MB allocated, 9494.0MB reserved
2025-03-25 13:56:14,251 - training - INFO - Epoch: 771/200000, Batch: 30/45, Loss: 1.8982, Throughput: 71.86 samples/sec
2025-03-25 13:56:25,151 - training - INFO - Epoch 771 completed in 35.06s. Average loss: 1.8888
2025-03-25 13:56:25,155 - training - INFO - Starting epoch 772/200000
2025-03-25 13:56:25,901 - training - INFO - Memory: GPU 0: 3562.6MB allocated, 9632.0MB reserved
2025-03-25 13:56:25,901 - training - INFO - Epoch: 772/200000, Batch: 0/45, Loss: 1.3642, Throughput: 75.27 samples/sec
2025-03-25 13:56:37,767 - training - INFO - Memory: GPU 0: 3562.8MB allocated, 9514.0MB reserved
2025-03-25 13:56:37,767 - training - INFO - Epoch: 772/200000, Batch: 15/45, Loss: 1.9115, Throughput: 71.06 samples/sec
2025-03-25 13:56:49,391 - training - INFO - Memory: GPU 0: 3562.8MB allocated, 9514.0MB reserved
2025-03-25 13:56:49,392 - training - INFO - Epoch: 772/200000, Batch: 30/45, Loss: 1.8503, Throughput: 71.63 samples/sec
2025-03-25 13:57:00,290 - training - INFO - Epoch 772 completed in 35.14s. Average loss: 1.9173
2025-03-25 13:57:00,294 - training - INFO - Starting epoch 773/200000
2025-03-25 13:57:01,028 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9652.0MB reserved
2025-03-25 13:57:01,028 - training - INFO - Epoch: 773/200000, Batch: 0/45, Loss: 2.1634, Throughput: 76.51 samples/sec
2025-03-25 13:57:13,046 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9482.0MB reserved
2025-03-25 13:57:13,046 - training - INFO - Epoch: 773/200000, Batch: 15/45, Loss: 1.8326, Throughput: 70.28 samples/sec
2025-03-25 13:57:24,687 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9484.0MB reserved
2025-03-25 13:57:24,687 - training - INFO - Epoch: 773/200000, Batch: 30/45, Loss: 1.7939, Throughput: 71.17 samples/sec
2025-03-25 13:57:35,642 - training - INFO - Epoch 773 completed in 35.35s. Average loss: 1.8906
2025-03-25 13:57:35,645 - training - INFO - Starting epoch 774/200000
2025-03-25 13:57:36,389 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9622.0MB reserved
2025-03-25 13:57:36,389 - training - INFO - Epoch: 774/200000, Batch: 0/45, Loss: 1.4632, Throughput: 75.43 samples/sec
2025-03-25 13:57:48,280 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9480.0MB reserved
2025-03-25 13:57:48,280 - training - INFO - Epoch: 774/200000, Batch: 15/45, Loss: 1.7420, Throughput: 70.92 samples/sec
2025-03-25 13:57:59,910 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9480.0MB reserved
2025-03-25 13:57:59,911 - training - INFO - Epoch: 774/200000, Batch: 30/45, Loss: 1.7695, Throughput: 71.55 samples/sec
2025-03-25 13:58:10,856 - training - INFO - Epoch 774 completed in 35.21s. Average loss: 1.8460
2025-03-25 13:58:10,860 - training - INFO - Starting epoch 775/200000
2025-03-25 13:58:11,595 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9620.0MB reserved
2025-03-25 13:58:11,596 - training - INFO - Epoch: 775/200000, Batch: 0/45, Loss: 2.0910, Throughput: 76.27 samples/sec
2025-03-25 13:58:23,442 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9500.0MB reserved
2025-03-25 13:58:23,442 - training - INFO - Epoch: 775/200000, Batch: 15/45, Loss: 1.9437, Throughput: 71.22 samples/sec
2025-03-25 13:58:34,924 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9500.0MB reserved
2025-03-25 13:58:34,924 - training - INFO - Epoch: 775/200000, Batch: 30/45, Loss: 1.8590, Throughput: 72.15 samples/sec
2025-03-25 13:58:45,811 - training - INFO - Epoch 775 completed in 34.95s. Average loss: 1.8815
2025-03-25 13:58:45,815 - training - INFO - Starting epoch 776/200000
2025-03-25 13:58:46,564 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9640.0MB reserved
2025-03-25 13:58:46,564 - training - INFO - Epoch: 776/200000, Batch: 0/45, Loss: 2.3677, Throughput: 74.98 samples/sec
2025-03-25 13:58:58,447 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9512.0MB reserved
2025-03-25 13:58:58,447 - training - INFO - Epoch: 776/200000, Batch: 15/45, Loss: 1.9072, Throughput: 70.94 samples/sec
2025-03-25 13:59:10,071 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9512.0MB reserved
2025-03-25 13:59:10,071 - training - INFO - Epoch: 776/200000, Batch: 30/45, Loss: 1.8575, Throughput: 71.58 samples/sec
2025-03-25 13:59:21,008 - training - INFO - Epoch 776 completed in 35.19s. Average loss: 1.8604
2025-03-25 13:59:21,011 - training - INFO - Starting epoch 777/200000
2025-03-25 13:59:21,753 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9650.0MB reserved
2025-03-25 13:59:21,753 - training - INFO - Epoch: 777/200000, Batch: 0/45, Loss: 2.1165, Throughput: 75.67 samples/sec
2025-03-25 13:59:33,572 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9512.0MB reserved
2025-03-25 13:59:33,572 - training - INFO - Epoch: 777/200000, Batch: 15/45, Loss: 1.8780, Throughput: 71.34 samples/sec
2025-03-25 13:59:45,103 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9512.0MB reserved
2025-03-25 13:59:45,104 - training - INFO - Epoch: 777/200000, Batch: 30/45, Loss: 1.8510, Throughput: 72.06 samples/sec
2025-03-25 13:59:56,018 - training - INFO - Epoch 777 completed in 35.01s. Average loss: 1.9290
2025-03-25 13:59:56,022 - training - INFO - Starting epoch 778/200000
2025-03-25 13:59:56,774 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9650.0MB reserved
2025-03-25 13:59:56,774 - training - INFO - Epoch: 778/200000, Batch: 0/45, Loss: 1.7434, Throughput: 74.63 samples/sec
2025-03-25 14:00:08,675 - training - INFO - Memory: GPU 0: 3555.9MB allocated, 9498.0MB reserved
2025-03-25 14:00:08,675 - training - INFO - Epoch: 778/200000, Batch: 15/45, Loss: 1.8111, Throughput: 70.82 samples/sec
2025-03-25 14:00:20,378 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9498.0MB reserved
2025-03-25 14:00:20,378 - training - INFO - Epoch: 778/200000, Batch: 30/45, Loss: 1.7825, Throughput: 71.28 samples/sec
2025-03-25 14:00:31,376 - training - INFO - Epoch 778 completed in 35.35s. Average loss: 1.8615
2025-03-25 14:00:31,379 - training - INFO - Starting epoch 779/200000
2025-03-25 14:00:32,120 - training - INFO - Memory: GPU 0: 3555.9MB allocated, 9636.0MB reserved
2025-03-25 14:00:32,120 - training - INFO - Epoch: 779/200000, Batch: 0/45, Loss: 1.6710, Throughput: 75.83 samples/sec
2025-03-25 14:00:44,059 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9502.0MB reserved
2025-03-25 14:00:44,060 - training - INFO - Epoch: 779/200000, Batch: 15/45, Loss: 1.7588, Throughput: 70.67 samples/sec
2025-03-25 14:00:55,653 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9502.0MB reserved
2025-03-25 14:00:55,653 - training - INFO - Epoch: 779/200000, Batch: 30/45, Loss: 1.7957, Throughput: 71.53 samples/sec
2025-03-25 14:01:06,577 - training - INFO - Epoch 779 completed in 35.20s. Average loss: 1.9274
2025-03-25 14:01:06,580 - training - INFO - Starting epoch 780/200000
2025-03-25 14:01:07,333 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9640.0MB reserved
2025-03-25 14:01:07,333 - training - INFO - Epoch: 780/200000, Batch: 0/45, Loss: 1.9874, Throughput: 74.50 samples/sec
2025-03-25 14:01:19,235 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9498.0MB reserved
2025-03-25 14:01:19,235 - training - INFO - Epoch: 780/200000, Batch: 15/45, Loss: 1.8974, Throughput: 70.81 samples/sec
2025-03-25 14:01:30,800 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9498.0MB reserved
2025-03-25 14:01:30,801 - training - INFO - Epoch: 780/200000, Batch: 30/45, Loss: 1.9080, Throughput: 71.68 samples/sec
2025-03-25 14:01:41,730 - training - INFO - Epoch 780 completed in 35.15s. Average loss: 1.9075
2025-03-25 14:01:41,733 - training - INFO - Starting epoch 781/200000
2025-03-25 14:01:42,469 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9638.0MB reserved
2025-03-25 14:01:42,469 - training - INFO - Epoch: 781/200000, Batch: 0/45, Loss: 1.9092, Throughput: 76.22 samples/sec
2025-03-25 14:01:54,445 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9498.0MB reserved
2025-03-25 14:01:54,445 - training - INFO - Epoch: 781/200000, Batch: 15/45, Loss: 1.7380, Throughput: 70.50 samples/sec
2025-03-25 14:02:06,095 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9498.0MB reserved
2025-03-25 14:02:06,095 - training - INFO - Epoch: 781/200000, Batch: 30/45, Loss: 1.7524, Throughput: 71.27 samples/sec
2025-03-25 14:02:17,053 - training - INFO - Epoch 781 completed in 35.32s. Average loss: 1.8377
2025-03-25 14:02:17,057 - training - INFO - Starting epoch 782/200000
2025-03-25 14:02:17,783 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9636.0MB reserved
2025-03-25 14:02:17,783 - training - INFO - Epoch: 782/200000, Batch: 0/45, Loss: 2.2343, Throughput: 77.27 samples/sec
2025-03-25 14:02:29,711 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9508.0MB reserved
2025-03-25 14:02:29,712 - training - INFO - Epoch: 782/200000, Batch: 15/45, Loss: 1.8452, Throughput: 70.82 samples/sec
2025-03-25 14:02:41,361 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9510.0MB reserved
2025-03-25 14:02:41,361 - training - INFO - Epoch: 782/200000, Batch: 30/45, Loss: 1.8784, Throughput: 71.43 samples/sec
2025-03-25 14:02:52,344 - training - INFO - Epoch 782 completed in 35.29s. Average loss: 1.9019
2025-03-25 14:02:52,348 - training - INFO - Starting epoch 783/200000
2025-03-25 14:02:53,091 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9646.0MB reserved
2025-03-25 14:02:53,092 - training - INFO - Epoch: 783/200000, Batch: 0/45, Loss: 1.8213, Throughput: 75.51 samples/sec
2025-03-25 14:03:05,003 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9482.0MB reserved
2025-03-25 14:03:05,003 - training - INFO - Epoch: 783/200000, Batch: 15/45, Loss: 1.7915, Throughput: 70.81 samples/sec
2025-03-25 14:03:16,604 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9482.0MB reserved
2025-03-25 14:03:16,605 - training - INFO - Epoch: 783/200000, Batch: 30/45, Loss: 1.8598, Throughput: 71.57 samples/sec
2025-03-25 14:03:27,539 - training - INFO - Epoch 783 completed in 35.19s. Average loss: 1.8739
2025-03-25 14:03:27,543 - training - INFO - Starting epoch 784/200000
2025-03-25 14:03:28,286 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9622.0MB reserved
2025-03-25 14:03:28,286 - training - INFO - Epoch: 784/200000, Batch: 0/45, Loss: 1.8663, Throughput: 75.43 samples/sec
2025-03-25 14:03:40,157 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9510.0MB reserved
2025-03-25 14:03:40,157 - training - INFO - Epoch: 784/200000, Batch: 15/45, Loss: 1.9668, Throughput: 71.04 samples/sec
2025-03-25 14:03:51,741 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9510.0MB reserved
2025-03-25 14:03:51,741 - training - INFO - Epoch: 784/200000, Batch: 30/45, Loss: 1.8736, Throughput: 71.75 samples/sec
2025-03-25 14:04:02,653 - training - INFO - Epoch 784 completed in 35.11s. Average loss: 1.8973
2025-03-25 14:04:02,658 - training - INFO - Starting epoch 785/200000
2025-03-25 14:04:03,387 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9648.0MB reserved
2025-03-25 14:04:03,387 - training - INFO - Epoch: 785/200000, Batch: 0/45, Loss: 1.9149, Throughput: 76.91 samples/sec
2025-03-25 14:04:15,427 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9512.0MB reserved
2025-03-25 14:04:15,427 - training - INFO - Epoch: 785/200000, Batch: 15/45, Loss: 1.8011, Throughput: 70.18 samples/sec
2025-03-25 14:04:27,143 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9512.0MB reserved
2025-03-25 14:04:27,143 - training - INFO - Epoch: 785/200000, Batch: 30/45, Loss: 1.7821, Throughput: 70.91 samples/sec
2025-03-25 14:04:38,180 - training - INFO - Epoch 785 completed in 35.52s. Average loss: 1.9008
2025-03-25 14:04:38,184 - training - INFO - Starting epoch 786/200000
2025-03-25 14:04:38,937 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9650.0MB reserved
2025-03-25 14:04:38,937 - training - INFO - Epoch: 786/200000, Batch: 0/45, Loss: 1.3559, Throughput: 74.47 samples/sec
2025-03-25 14:04:50,893 - training - INFO - Memory: GPU 0: 3556.6MB allocated, 9498.0MB reserved
2025-03-25 14:04:50,893 - training - INFO - Epoch: 786/200000, Batch: 15/45, Loss: 1.8512, Throughput: 70.50 samples/sec
2025-03-25 14:05:02,560 - training - INFO - Memory: GPU 0: 3556.6MB allocated, 9498.0MB reserved
2025-03-25 14:05:02,560 - training - INFO - Epoch: 786/200000, Batch: 30/45, Loss: 1.8100, Throughput: 71.22 samples/sec
2025-03-25 14:05:13,562 - training - INFO - Epoch 786 completed in 35.38s. Average loss: 1.8194
2025-03-25 14:05:13,565 - training - INFO - Starting epoch 787/200000
2025-03-25 14:05:14,301 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9636.0MB reserved
2025-03-25 14:05:14,301 - training - INFO - Epoch: 787/200000, Batch: 0/45, Loss: 1.9657, Throughput: 76.27 samples/sec
2025-03-25 14:05:26,113 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9486.0MB reserved
2025-03-25 14:05:26,113 - training - INFO - Epoch: 787/200000, Batch: 15/45, Loss: 1.7694, Throughput: 71.41 samples/sec
2025-03-25 14:05:37,614 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9486.0MB reserved
2025-03-25 14:05:37,614 - training - INFO - Epoch: 787/200000, Batch: 30/45, Loss: 1.7817, Throughput: 72.19 samples/sec
2025-03-25 14:05:48,513 - training - INFO - Epoch 787 completed in 34.95s. Average loss: 1.8370
2025-03-25 14:05:48,576 - training - INFO - Starting epoch 788/200000
2025-03-25 14:05:49,256 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9624.0MB reserved
2025-03-25 14:05:49,256 - training - INFO - Epoch: 788/200000, Batch: 0/45, Loss: 1.5717, Throughput: 82.57 samples/sec
2025-03-25 14:06:01,178 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9502.0MB reserved
2025-03-25 14:06:01,179 - training - INFO - Epoch: 788/200000, Batch: 15/45, Loss: 1.8297, Throughput: 71.11 samples/sec
2025-03-25 14:06:12,766 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9502.0MB reserved
2025-03-25 14:06:12,767 - training - INFO - Epoch: 788/200000, Batch: 30/45, Loss: 1.8359, Throughput: 71.77 samples/sec
2025-03-25 14:06:23,730 - training - INFO - Epoch 788 completed in 35.15s. Average loss: 1.8812
2025-03-25 14:06:23,734 - training - INFO - Starting epoch 789/200000
2025-03-25 14:06:24,469 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9638.0MB reserved
2025-03-25 14:06:24,470 - training - INFO - Epoch: 789/200000, Batch: 0/45, Loss: 1.4051, Throughput: 76.29 samples/sec
2025-03-25 14:06:36,412 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9512.0MB reserved
2025-03-25 14:06:36,412 - training - INFO - Epoch: 789/200000, Batch: 15/45, Loss: 1.8338, Throughput: 70.69 samples/sec
2025-03-25 14:06:48,085 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9512.0MB reserved
2025-03-25 14:06:48,086 - training - INFO - Epoch: 789/200000, Batch: 30/45, Loss: 1.8628, Throughput: 71.30 samples/sec
2025-03-25 14:06:58,986 - training - INFO - Epoch 789 completed in 35.25s. Average loss: 1.9523
2025-03-25 14:06:58,989 - training - INFO - Starting epoch 790/200000
2025-03-25 14:06:59,721 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9650.0MB reserved
2025-03-25 14:06:59,722 - training - INFO - Epoch: 790/200000, Batch: 0/45, Loss: 2.2862, Throughput: 76.82 samples/sec
2025-03-25 14:07:11,577 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9500.0MB reserved
2025-03-25 14:07:11,577 - training - INFO - Epoch: 790/200000, Batch: 15/45, Loss: 1.8122, Throughput: 71.20 samples/sec
2025-03-25 14:07:23,152 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9502.0MB reserved
2025-03-25 14:07:23,152 - training - INFO - Epoch: 790/200000, Batch: 30/45, Loss: 1.8562, Throughput: 71.85 samples/sec
2025-03-25 14:07:34,072 - training - INFO - Epoch 790 completed in 35.08s. Average loss: 1.9663
2025-03-25 14:07:34,076 - training - INFO - Starting epoch 791/200000
2025-03-25 14:07:34,821 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9640.0MB reserved
2025-03-25 14:07:34,822 - training - INFO - Epoch: 791/200000, Batch: 0/45, Loss: 1.7640, Throughput: 75.49 samples/sec
2025-03-25 14:07:46,801 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9492.0MB reserved
2025-03-25 14:07:46,802 - training - INFO - Epoch: 791/200000, Batch: 15/45, Loss: 1.7597, Throughput: 70.42 samples/sec
2025-03-25 14:07:58,476 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9492.0MB reserved
2025-03-25 14:07:58,476 - training - INFO - Epoch: 791/200000, Batch: 30/45, Loss: 1.7992, Throughput: 71.15 samples/sec
2025-03-25 14:08:09,447 - training - INFO - Epoch 791 completed in 35.37s. Average loss: 1.9560
2025-03-25 14:08:09,450 - training - INFO - Starting epoch 792/200000
2025-03-25 14:08:10,189 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9628.0MB reserved
2025-03-25 14:08:10,190 - training - INFO - Epoch: 792/200000, Batch: 0/45, Loss: 2.0074, Throughput: 75.97 samples/sec
2025-03-25 14:08:22,072 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9490.0MB reserved
2025-03-25 14:08:22,088 - training - INFO - Epoch: 792/200000, Batch: 15/45, Loss: 1.8987, Throughput: 71.00 samples/sec
2025-03-25 14:08:33,694 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9490.0MB reserved
2025-03-25 14:08:33,694 - training - INFO - Epoch: 792/200000, Batch: 30/45, Loss: 1.8235, Throughput: 71.61 samples/sec
2025-03-25 14:08:44,627 - training - INFO - Epoch 792 completed in 35.18s. Average loss: 1.8463
2025-03-25 14:08:44,630 - training - INFO - Starting epoch 793/200000
2025-03-25 14:08:45,361 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9628.0MB reserved
2025-03-25 14:08:45,361 - training - INFO - Epoch: 793/200000, Batch: 0/45, Loss: 1.9080, Throughput: 76.77 samples/sec
2025-03-25 14:08:57,336 - training - INFO - Memory: GPU 0: 3557.2MB allocated, 9492.0MB reserved
2025-03-25 14:08:57,336 - training - INFO - Epoch: 793/200000, Batch: 15/45, Loss: 1.9103, Throughput: 70.53 samples/sec
2025-03-25 14:09:08,931 - training - INFO - Memory: GPU 0: 3557.2MB allocated, 9492.0MB reserved
2025-03-25 14:09:08,931 - training - INFO - Epoch: 793/200000, Batch: 30/45, Loss: 1.8396, Throughput: 71.44 samples/sec
2025-03-25 14:09:19,899 - training - INFO - Epoch 793 completed in 35.27s. Average loss: 1.8769
2025-03-25 14:09:19,902 - training - INFO - Starting epoch 794/200000
2025-03-25 14:09:20,630 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9630.0MB reserved
2025-03-25 14:09:20,631 - training - INFO - Epoch: 794/200000, Batch: 0/45, Loss: 2.0007, Throughput: 76.99 samples/sec
2025-03-25 14:09:32,507 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9500.0MB reserved
2025-03-25 14:09:32,507 - training - INFO - Epoch: 794/200000, Batch: 15/45, Loss: 1.6832, Throughput: 71.09 samples/sec
2025-03-25 14:09:44,053 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9500.0MB reserved
2025-03-25 14:09:44,053 - training - INFO - Epoch: 794/200000, Batch: 30/45, Loss: 1.7681, Throughput: 71.89 samples/sec
2025-03-25 14:09:54,948 - training - INFO - Epoch 794 completed in 35.05s. Average loss: 1.8468
2025-03-25 14:09:54,951 - training - INFO - Starting epoch 795/200000
2025-03-25 14:09:55,695 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9636.0MB reserved
2025-03-25 14:09:55,695 - training - INFO - Epoch: 795/200000, Batch: 0/45, Loss: 1.6759, Throughput: 75.40 samples/sec
2025-03-25 14:10:07,664 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9510.0MB reserved
2025-03-25 14:10:07,665 - training - INFO - Epoch: 795/200000, Batch: 15/45, Loss: 1.8484, Throughput: 70.49 samples/sec
2025-03-25 14:10:19,387 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9510.0MB reserved
2025-03-25 14:10:19,387 - training - INFO - Epoch: 795/200000, Batch: 30/45, Loss: 1.8514, Throughput: 71.05 samples/sec
2025-03-25 14:10:30,418 - training - INFO - Epoch 795 completed in 35.47s. Average loss: 1.8831
2025-03-25 14:10:30,421 - training - INFO - Starting epoch 796/200000
2025-03-25 14:10:31,151 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9648.0MB reserved
2025-03-25 14:10:31,152 - training - INFO - Epoch: 796/200000, Batch: 0/45, Loss: 2.1164, Throughput: 76.92 samples/sec
2025-03-25 14:10:43,040 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9500.0MB reserved
2025-03-25 14:10:43,040 - training - INFO - Epoch: 796/200000, Batch: 15/45, Loss: 1.8730, Throughput: 71.01 samples/sec
2025-03-25 14:10:54,611 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9500.0MB reserved
2025-03-25 14:10:54,612 - training - INFO - Epoch: 796/200000, Batch: 30/45, Loss: 1.8613, Throughput: 71.77 samples/sec
2025-03-25 14:11:05,538 - training - INFO - Epoch 796 completed in 35.12s. Average loss: 1.9050
2025-03-25 14:11:05,542 - training - INFO - Starting epoch 797/200000
2025-03-25 14:11:06,303 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9640.0MB reserved
2025-03-25 14:11:06,303 - training - INFO - Epoch: 797/200000, Batch: 0/45, Loss: 1.6740, Throughput: 73.67 samples/sec
2025-03-25 14:11:18,188 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9508.0MB reserved
2025-03-25 14:11:18,188 - training - INFO - Epoch: 797/200000, Batch: 15/45, Loss: 1.8579, Throughput: 70.86 samples/sec
2025-03-25 14:11:29,795 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9510.0MB reserved
2025-03-25 14:11:29,796 - training - INFO - Epoch: 797/200000, Batch: 30/45, Loss: 1.8557, Throughput: 71.58 samples/sec
2025-03-25 14:11:40,799 - training - INFO - Epoch 797 completed in 35.26s. Average loss: 1.8704
2025-03-25 14:11:40,805 - training - INFO - Starting epoch 798/200000
2025-03-25 14:11:41,566 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9648.0MB reserved
2025-03-25 14:11:41,567 - training - INFO - Epoch: 798/200000, Batch: 0/45, Loss: 1.9277, Throughput: 73.75 samples/sec
2025-03-25 14:11:53,458 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9500.0MB reserved
2025-03-25 14:11:53,458 - training - INFO - Epoch: 798/200000, Batch: 15/45, Loss: 1.8851, Throughput: 70.83 samples/sec
2025-03-25 14:12:05,035 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9500.0MB reserved
2025-03-25 14:12:05,035 - training - INFO - Epoch: 798/200000, Batch: 30/45, Loss: 1.8651, Throughput: 71.65 samples/sec
2025-03-25 14:12:15,950 - training - INFO - Epoch 798 completed in 35.14s. Average loss: 1.9545
2025-03-25 14:12:15,953 - training - INFO - Starting epoch 799/200000
2025-03-25 14:12:16,701 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9638.0MB reserved
2025-03-25 14:12:16,701 - training - INFO - Epoch: 799/200000, Batch: 0/45, Loss: 2.2333, Throughput: 74.96 samples/sec
2025-03-25 14:12:28,550 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9500.0MB reserved
2025-03-25 14:12:28,550 - training - INFO - Epoch: 799/200000, Batch: 15/45, Loss: 1.7513, Throughput: 71.15 samples/sec
2025-03-25 14:12:40,266 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9500.0MB reserved
2025-03-25 14:12:40,266 - training - INFO - Epoch: 799/200000, Batch: 30/45, Loss: 1.8565, Throughput: 71.41 samples/sec
2025-03-25 14:12:51,241 - training - INFO - Epoch 799 completed in 35.29s. Average loss: 1.8692
2025-03-25 14:12:51,245 - training - INFO - Starting epoch 800/200000
2025-03-25 14:12:51,989 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9636.0MB reserved
2025-03-25 14:12:51,990 - training - INFO - Epoch: 800/200000, Batch: 0/45, Loss: 1.8644, Throughput: 75.38 samples/sec
2025-03-25 14:13:03,900 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9500.0MB reserved
2025-03-25 14:13:03,901 - training - INFO - Epoch: 800/200000, Batch: 15/45, Loss: 1.8930, Throughput: 70.80 samples/sec
2025-03-25 14:13:15,602 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9500.0MB reserved
2025-03-25 14:13:15,602 - training - INFO - Epoch: 800/200000, Batch: 30/45, Loss: 1.8098, Throughput: 71.28 samples/sec
2025-03-25 14:13:26,568 - training - INFO - Epoch 800 completed in 35.32s. Average loss: 1.8460
2025-03-25 14:13:26,572 - training - INFO - Starting validation...
2025-03-25 14:13:26,889 - training - INFO - Validation Loss: 22.0648
2025-03-25 14:13:26,889 - training - INFO - Validation loss did not improve. Counter: 7/10
2025-03-25 14:18:13,405 - training - INFO - Checkpoint saved at epoch 800
2025-03-25 14:18:13,662 - training - INFO - Starting epoch 801/200000
2025-03-25 14:18:14,403 - training - INFO - Memory: GPU 0: 3567.4MB allocated, 8742.0MB reserved
2025-03-25 14:18:14,403 - training - INFO - Epoch: 801/200000, Batch: 0/45, Loss: 1.8650, Throughput: 75.80 samples/sec
2025-03-25 14:18:26,126 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9478.0MB reserved
2025-03-25 14:18:26,127 - training - INFO - Epoch: 801/200000, Batch: 15/45, Loss: 1.8709, Throughput: 71.89 samples/sec
2025-03-25 14:18:37,769 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9478.0MB reserved
2025-03-25 14:18:37,769 - training - INFO - Epoch: 801/200000, Batch: 30/45, Loss: 1.8286, Throughput: 72.02 samples/sec
2025-03-25 14:18:48,691 - training - INFO - Epoch 801 completed in 35.03s. Average loss: 1.8725
2025-03-25 14:18:48,695 - training - INFO - Starting epoch 802/200000
2025-03-25 14:18:49,453 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9616.0MB reserved
2025-03-25 14:18:49,453 - training - INFO - Epoch: 802/200000, Batch: 0/45, Loss: 1.8046, Throughput: 74.03 samples/sec
2025-03-25 14:19:01,392 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9494.0MB reserved
2025-03-25 14:19:01,392 - training - INFO - Epoch: 802/200000, Batch: 15/45, Loss: 1.8758, Throughput: 70.57 samples/sec
2025-03-25 14:19:13,167 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9494.0MB reserved
2025-03-25 14:19:13,168 - training - INFO - Epoch: 802/200000, Batch: 30/45, Loss: 1.8093, Throughput: 70.94 samples/sec
2025-03-25 14:19:24,139 - training - INFO - Epoch 802 completed in 35.44s. Average loss: 1.8975
2025-03-25 14:19:24,143 - training - INFO - Starting epoch 803/200000
2025-03-25 14:19:24,875 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9630.0MB reserved
2025-03-25 14:19:24,875 - training - INFO - Epoch: 803/200000, Batch: 0/45, Loss: 1.8135, Throughput: 76.65 samples/sec
2025-03-25 14:19:36,771 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9494.0MB reserved
2025-03-25 14:19:36,771 - training - INFO - Epoch: 803/200000, Batch: 15/45, Loss: 1.7632, Throughput: 70.97 samples/sec
2025-03-25 14:19:48,458 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9494.0MB reserved
2025-03-25 14:19:48,459 - training - INFO - Epoch: 803/200000, Batch: 30/45, Loss: 1.7333, Throughput: 71.40 samples/sec
2025-03-25 14:19:59,427 - training - INFO - Epoch 803 completed in 35.28s. Average loss: 1.8849
2025-03-25 14:19:59,431 - training - INFO - Starting epoch 804/200000
2025-03-25 14:20:00,167 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9632.0MB reserved
2025-03-25 14:20:00,168 - training - INFO - Epoch: 804/200000, Batch: 0/45, Loss: 1.7985, Throughput: 76.23 samples/sec
2025-03-25 14:20:12,039 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9496.0MB reserved
2025-03-25 14:20:12,039 - training - INFO - Epoch: 804/200000, Batch: 15/45, Loss: 1.8934, Throughput: 71.07 samples/sec
2025-03-25 14:20:23,683 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9498.0MB reserved
2025-03-25 14:20:23,684 - training - INFO - Epoch: 804/200000, Batch: 30/45, Loss: 1.8872, Throughput: 71.59 samples/sec
2025-03-25 14:20:34,605 - training - INFO - Epoch 804 completed in 35.17s. Average loss: 1.8509
2025-03-25 14:20:34,609 - training - INFO - Starting epoch 805/200000
2025-03-25 14:20:35,354 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9636.0MB reserved
2025-03-25 14:20:35,354 - training - INFO - Epoch: 805/200000, Batch: 0/45, Loss: 2.0333, Throughput: 75.35 samples/sec
2025-03-25 14:20:47,288 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9476.0MB reserved
2025-03-25 14:20:47,289 - training - INFO - Epoch: 805/200000, Batch: 15/45, Loss: 1.8750, Throughput: 70.68 samples/sec
2025-03-25 14:20:58,919 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9478.0MB reserved
2025-03-25 14:20:58,919 - training - INFO - Epoch: 805/200000, Batch: 30/45, Loss: 1.8715, Throughput: 71.42 samples/sec
2025-03-25 14:21:09,874 - training - INFO - Epoch 805 completed in 35.27s. Average loss: 1.8639
2025-03-25 14:21:09,878 - training - INFO - Starting epoch 806/200000
2025-03-25 14:21:10,638 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9616.0MB reserved
2025-03-25 14:21:10,638 - training - INFO - Epoch: 806/200000, Batch: 0/45, Loss: 1.4663, Throughput: 73.90 samples/sec
2025-03-25 14:21:22,569 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9504.0MB reserved
2025-03-25 14:21:22,569 - training - INFO - Epoch: 806/200000, Batch: 15/45, Loss: 1.7770, Throughput: 70.61 samples/sec
2025-03-25 14:21:34,222 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9504.0MB reserved
2025-03-25 14:21:34,223 - training - INFO - Epoch: 806/200000, Batch: 30/45, Loss: 1.8272, Throughput: 71.32 samples/sec
2025-03-25 14:21:45,175 - training - INFO - Epoch 806 completed in 35.30s. Average loss: 1.8723
2025-03-25 14:21:45,179 - training - INFO - Starting epoch 807/200000
2025-03-25 14:21:45,921 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9644.0MB reserved
2025-03-25 14:21:45,921 - training - INFO - Epoch: 807/200000, Batch: 0/45, Loss: 2.0711, Throughput: 75.57 samples/sec
2025-03-25 14:21:57,895 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9490.0MB reserved
2025-03-25 14:21:57,896 - training - INFO - Epoch: 807/200000, Batch: 15/45, Loss: 1.7338, Throughput: 70.48 samples/sec
2025-03-25 14:22:09,460 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9490.0MB reserved
2025-03-25 14:22:09,460 - training - INFO - Epoch: 807/200000, Batch: 30/45, Loss: 1.7942, Throughput: 71.50 samples/sec
2025-03-25 14:22:20,406 - training - INFO - Epoch 807 completed in 35.23s. Average loss: 1.8656
2025-03-25 14:22:20,410 - training - INFO - Starting epoch 808/200000
2025-03-25 14:22:21,145 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9628.0MB reserved
2025-03-25 14:22:21,146 - training - INFO - Epoch: 808/200000, Batch: 0/45, Loss: 1.8456, Throughput: 76.30 samples/sec
2025-03-25 14:22:33,151 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9494.0MB reserved
2025-03-25 14:22:33,151 - training - INFO - Epoch: 808/200000, Batch: 15/45, Loss: 1.8062, Throughput: 70.33 samples/sec
2025-03-25 14:22:44,823 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9494.0MB reserved
2025-03-25 14:22:44,824 - training - INFO - Epoch: 808/200000, Batch: 30/45, Loss: 1.7829, Throughput: 71.11 samples/sec
2025-03-25 14:22:55,822 - training - INFO - Epoch 808 completed in 35.41s. Average loss: 1.8211
2025-03-25 14:22:55,826 - training - INFO - Starting epoch 809/200000
2025-03-25 14:22:56,583 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9632.0MB reserved
2025-03-25 14:22:56,584 - training - INFO - Epoch: 809/200000, Batch: 0/45, Loss: 1.3620, Throughput: 74.16 samples/sec
2025-03-25 14:23:08,472 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9508.0MB reserved
2025-03-25 14:23:08,473 - training - INFO - Epoch: 809/200000, Batch: 15/45, Loss: 1.7259, Throughput: 70.86 samples/sec
2025-03-25 14:23:20,006 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9508.0MB reserved
2025-03-25 14:23:20,007 - training - INFO - Epoch: 809/200000, Batch: 30/45, Loss: 1.7046, Throughput: 71.80 samples/sec
2025-03-25 14:23:30,936 - training - INFO - Epoch 809 completed in 35.11s. Average loss: 1.8247
2025-03-25 14:23:30,940 - training - INFO - Starting epoch 810/200000
2025-03-25 14:23:31,677 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9646.0MB reserved
2025-03-25 14:23:31,677 - training - INFO - Epoch: 810/200000, Batch: 0/45, Loss: 1.6748, Throughput: 76.04 samples/sec
2025-03-25 14:23:43,588 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9482.0MB reserved
2025-03-25 14:23:43,589 - training - INFO - Epoch: 810/200000, Batch: 15/45, Loss: 2.0210, Throughput: 70.85 samples/sec
2025-03-25 14:23:55,217 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9484.0MB reserved
2025-03-25 14:23:55,218 - training - INFO - Epoch: 810/200000, Batch: 30/45, Loss: 1.9923, Throughput: 71.51 samples/sec
2025-03-25 14:24:06,137 - training - INFO - Epoch 810 completed in 35.20s. Average loss: 1.9328
2025-03-25 14:24:06,141 - training - INFO - Starting epoch 811/200000
2025-03-25 14:24:06,886 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9622.0MB reserved
2025-03-25 14:24:06,887 - training - INFO - Epoch: 811/200000, Batch: 0/45, Loss: 1.9161, Throughput: 75.36 samples/sec
2025-03-25 14:24:18,712 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9508.0MB reserved
2025-03-25 14:24:18,712 - training - INFO - Epoch: 811/200000, Batch: 15/45, Loss: 1.7832, Throughput: 71.28 samples/sec
2025-03-25 14:24:30,284 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9508.0MB reserved
2025-03-25 14:24:30,284 - training - INFO - Epoch: 811/200000, Batch: 30/45, Loss: 1.8090, Throughput: 71.91 samples/sec
2025-03-25 14:24:41,193 - training - INFO - Epoch 811 completed in 35.05s. Average loss: 1.8528
2025-03-25 14:24:41,196 - training - INFO - Starting epoch 812/200000
2025-03-25 14:24:41,948 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9646.0MB reserved
2025-03-25 14:24:41,948 - training - INFO - Epoch: 812/200000, Batch: 0/45, Loss: 1.7897, Throughput: 74.66 samples/sec
2025-03-25 14:24:53,895 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9510.0MB reserved
2025-03-25 14:24:53,895 - training - INFO - Epoch: 812/200000, Batch: 15/45, Loss: 1.8942, Throughput: 70.56 samples/sec
2025-03-25 14:25:05,554 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9510.0MB reserved
2025-03-25 14:25:05,555 - training - INFO - Epoch: 812/200000, Batch: 30/45, Loss: 1.8616, Throughput: 71.27 samples/sec
2025-03-25 14:25:16,512 - training - INFO - Epoch 812 completed in 35.32s. Average loss: 1.8325
2025-03-25 14:25:16,516 - training - INFO - Starting epoch 813/200000
2025-03-25 14:25:17,257 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9648.0MB reserved
2025-03-25 14:25:17,257 - training - INFO - Epoch: 813/200000, Batch: 0/45, Loss: 2.0446, Throughput: 75.73 samples/sec
2025-03-25 14:25:29,109 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9496.0MB reserved
2025-03-25 14:25:29,110 - training - INFO - Epoch: 813/200000, Batch: 15/45, Loss: 1.8118, Throughput: 71.16 samples/sec
2025-03-25 14:25:40,690 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9496.0MB reserved
2025-03-25 14:25:40,691 - training - INFO - Epoch: 813/200000, Batch: 30/45, Loss: 1.7600, Throughput: 71.82 samples/sec
2025-03-25 14:25:51,635 - training - INFO - Epoch 813 completed in 35.12s. Average loss: 1.8208
2025-03-25 14:25:51,639 - training - INFO - Starting epoch 814/200000
2025-03-25 14:25:52,401 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9634.0MB reserved
2025-03-25 14:25:52,401 - training - INFO - Epoch: 814/200000, Batch: 0/45, Loss: 1.4977, Throughput: 73.74 samples/sec
2025-03-25 14:26:04,321 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9500.0MB reserved
2025-03-25 14:26:04,322 - training - INFO - Epoch: 814/200000, Batch: 15/45, Loss: 1.6635, Throughput: 70.66 samples/sec
2025-03-25 14:26:15,910 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9502.0MB reserved
2025-03-25 14:26:15,910 - training - INFO - Epoch: 814/200000, Batch: 30/45, Loss: 1.7573, Throughput: 71.53 samples/sec
2025-03-25 14:26:26,849 - training - INFO - Epoch 814 completed in 35.21s. Average loss: 1.8505
2025-03-25 14:26:26,853 - training - INFO - Starting epoch 815/200000
2025-03-25 14:26:27,589 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9640.0MB reserved
2025-03-25 14:26:27,590 - training - INFO - Epoch: 815/200000, Batch: 0/45, Loss: 1.7788, Throughput: 76.20 samples/sec
2025-03-25 14:26:39,530 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9502.0MB reserved
2025-03-25 14:26:39,530 - training - INFO - Epoch: 815/200000, Batch: 15/45, Loss: 1.8157, Throughput: 70.69 samples/sec
2025-03-25 14:26:51,227 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9502.0MB reserved
2025-03-25 14:26:51,228 - training - INFO - Epoch: 815/200000, Batch: 30/45, Loss: 1.8206, Throughput: 71.23 samples/sec
2025-03-25 14:27:02,234 - training - INFO - Epoch 815 completed in 35.38s. Average loss: 1.8505
2025-03-25 14:27:02,238 - training - INFO - Starting epoch 816/200000
2025-03-25 14:27:02,982 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9640.0MB reserved
2025-03-25 14:27:02,983 - training - INFO - Epoch: 816/200000, Batch: 0/45, Loss: 2.2357, Throughput: 75.45 samples/sec
2025-03-25 14:27:14,856 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9508.0MB reserved
2025-03-25 14:27:14,856 - training - INFO - Epoch: 816/200000, Batch: 15/45, Loss: 1.8560, Throughput: 71.01 samples/sec
2025-03-25 14:27:26,433 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9508.0MB reserved
2025-03-25 14:27:26,433 - training - INFO - Epoch: 816/200000, Batch: 30/45, Loss: 1.8164, Throughput: 71.75 samples/sec
2025-03-25 14:27:37,343 - training - INFO - Epoch 816 completed in 35.10s. Average loss: 1.8427
2025-03-25 14:27:37,346 - training - INFO - Starting epoch 817/200000
2025-03-25 14:27:38,079 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9646.0MB reserved
2025-03-25 14:27:38,080 - training - INFO - Epoch: 817/200000, Batch: 0/45, Loss: 1.7683, Throughput: 76.60 samples/sec
2025-03-25 14:27:50,067 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9494.0MB reserved
2025-03-25 14:27:50,067 - training - INFO - Epoch: 817/200000, Batch: 15/45, Loss: 1.8118, Throughput: 70.45 samples/sec
2025-03-25 14:28:01,658 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9494.0MB reserved
2025-03-25 14:28:01,658 - training - INFO - Epoch: 817/200000, Batch: 30/45, Loss: 1.8176, Throughput: 71.41 samples/sec
2025-03-25 14:28:12,606 - training - INFO - Epoch 817 completed in 35.26s. Average loss: 1.8810
2025-03-25 14:28:12,609 - training - INFO - Starting epoch 818/200000
2025-03-25 14:28:13,361 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9632.0MB reserved
2025-03-25 14:28:13,361 - training - INFO - Epoch: 818/200000, Batch: 0/45, Loss: 1.2846, Throughput: 74.76 samples/sec
2025-03-25 14:28:25,354 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9490.0MB reserved
2025-03-25 14:28:25,355 - training - INFO - Epoch: 818/200000, Batch: 15/45, Loss: 1.8173, Throughput: 70.31 samples/sec
2025-03-25 14:28:36,965 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9490.0MB reserved
2025-03-25 14:28:36,966 - training - INFO - Epoch: 818/200000, Batch: 30/45, Loss: 1.8519, Throughput: 71.28 samples/sec
2025-03-25 14:28:47,895 - training - INFO - Epoch 818 completed in 35.29s. Average loss: 1.8106
2025-03-25 14:28:47,899 - training - INFO - Starting epoch 819/200000
2025-03-25 14:28:48,630 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9626.0MB reserved
2025-03-25 14:28:48,630 - training - INFO - Epoch: 819/200000, Batch: 0/45, Loss: 1.8673, Throughput: 76.76 samples/sec
2025-03-25 14:29:00,582 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9394.0MB reserved
2025-03-25 14:29:00,582 - training - INFO - Epoch: 819/200000, Batch: 15/45, Loss: 1.7782, Throughput: 70.66 samples/sec
2025-03-25 14:29:12,135 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9394.0MB reserved
2025-03-25 14:29:12,135 - training - INFO - Epoch: 819/200000, Batch: 30/45, Loss: 1.8151, Throughput: 71.63 samples/sec
2025-03-25 14:29:23,049 - training - INFO - Epoch 819 completed in 35.15s. Average loss: 1.8418
2025-03-25 14:29:23,053 - training - INFO - Starting epoch 820/200000
2025-03-25 14:29:23,780 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9640.0MB reserved
2025-03-25 14:29:23,781 - training - INFO - Epoch: 820/200000, Batch: 0/45, Loss: 1.4389, Throughput: 77.29 samples/sec
2025-03-25 14:29:35,739 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9402.0MB reserved
2025-03-25 14:29:35,740 - training - INFO - Epoch: 820/200000, Batch: 15/45, Loss: 1.8651, Throughput: 70.64 samples/sec
2025-03-25 14:29:47,408 - training - INFO - Memory: GPU 0: 3557.1MB allocated, 9508.0MB reserved
2025-03-25 14:29:47,408 - training - INFO - Epoch: 820/200000, Batch: 30/45, Loss: 1.8125, Throughput: 71.29 samples/sec
2025-03-25 14:29:58,401 - training - INFO - Epoch 820 completed in 35.35s. Average loss: 1.8428
2025-03-25 14:29:58,405 - training - INFO - Starting epoch 821/200000
2025-03-25 14:29:59,158 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9646.0MB reserved
2025-03-25 14:29:59,158 - training - INFO - Epoch: 821/200000, Batch: 0/45, Loss: 2.2777, Throughput: 74.61 samples/sec
2025-03-25 14:30:11,036 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9398.0MB reserved
2025-03-25 14:30:11,036 - training - INFO - Epoch: 821/200000, Batch: 15/45, Loss: 1.8859, Throughput: 70.95 samples/sec
2025-03-25 14:30:22,606 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9398.0MB reserved
2025-03-25 14:30:22,607 - training - INFO - Epoch: 821/200000, Batch: 30/45, Loss: 1.8413, Throughput: 71.74 samples/sec
2025-03-25 14:30:33,496 - training - INFO - Epoch 821 completed in 35.09s. Average loss: 1.8466
2025-03-25 14:30:33,499 - training - INFO - Starting epoch 822/200000
2025-03-25 14:30:34,241 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9536.0MB reserved
2025-03-25 14:30:34,242 - training - INFO - Epoch: 822/200000, Batch: 0/45, Loss: 1.3956, Throughput: 75.60 samples/sec
2025-03-25 14:30:46,211 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9502.0MB reserved
2025-03-25 14:30:46,212 - training - INFO - Epoch: 822/200000, Batch: 15/45, Loss: 1.7469, Throughput: 70.49 samples/sec
2025-03-25 14:30:57,862 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9502.0MB reserved
2025-03-25 14:30:57,862 - training - INFO - Epoch: 822/200000, Batch: 30/45, Loss: 1.7641, Throughput: 71.26 samples/sec
2025-03-25 14:31:08,855 - training - INFO - Epoch 822 completed in 35.36s. Average loss: 1.8249
2025-03-25 14:31:08,859 - training - INFO - Starting epoch 823/200000
2025-03-25 14:31:09,598 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9640.0MB reserved
2025-03-25 14:31:09,598 - training - INFO - Epoch: 823/200000, Batch: 0/45, Loss: 1.8491, Throughput: 75.86 samples/sec
2025-03-25 14:31:21,497 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9492.0MB reserved
2025-03-25 14:31:21,498 - training - INFO - Epoch: 823/200000, Batch: 15/45, Loss: 1.7334, Throughput: 70.90 samples/sec
2025-03-25 14:31:33,202 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9494.0MB reserved
2025-03-25 14:31:33,203 - training - INFO - Epoch: 823/200000, Batch: 30/45, Loss: 1.7543, Throughput: 71.32 samples/sec
2025-03-25 14:31:44,272 - training - INFO - Epoch 823 completed in 35.41s. Average loss: 1.8602
2025-03-25 14:31:44,276 - training - INFO - Starting epoch 824/200000
2025-03-25 14:31:45,015 - training - INFO - Memory: GPU 0: 3556.6MB allocated, 9632.0MB reserved
2025-03-25 14:31:45,016 - training - INFO - Epoch: 824/200000, Batch: 0/45, Loss: 1.7509, Throughput: 75.90 samples/sec
2025-03-25 14:31:57,029 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9482.0MB reserved
2025-03-25 14:31:57,030 - training - INFO - Epoch: 824/200000, Batch: 15/45, Loss: 1.7636, Throughput: 70.27 samples/sec
2025-03-25 14:32:08,699 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9482.0MB reserved
2025-03-25 14:32:08,700 - training - INFO - Epoch: 824/200000, Batch: 30/45, Loss: 1.7825, Throughput: 71.09 samples/sec
2025-03-25 14:32:19,669 - training - INFO - Epoch 824 completed in 35.39s. Average loss: 1.8622
2025-03-25 14:32:19,672 - training - INFO - Starting epoch 825/200000
2025-03-25 14:32:20,406 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9620.0MB reserved
2025-03-25 14:32:20,406 - training - INFO - Epoch: 825/200000, Batch: 0/45, Loss: 1.7606, Throughput: 76.56 samples/sec
2025-03-25 14:32:32,269 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9506.0MB reserved
2025-03-25 14:32:32,270 - training - INFO - Epoch: 825/200000, Batch: 15/45, Loss: 1.8422, Throughput: 71.13 samples/sec
2025-03-25 14:32:43,965 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9506.0MB reserved
2025-03-25 14:32:43,965 - training - INFO - Epoch: 825/200000, Batch: 30/45, Loss: 1.8531, Throughput: 71.47 samples/sec
2025-03-25 14:32:55,066 - training - INFO - Epoch 825 completed in 35.39s. Average loss: 1.8625
2025-03-25 14:32:55,070 - training - INFO - Starting epoch 826/200000
2025-03-25 14:32:55,814 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9644.0MB reserved
2025-03-25 14:32:55,815 - training - INFO - Epoch: 826/200000, Batch: 0/45, Loss: 1.6384, Throughput: 75.32 samples/sec
2025-03-25 14:33:07,643 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9496.0MB reserved
2025-03-25 14:33:07,643 - training - INFO - Epoch: 826/200000, Batch: 15/45, Loss: 1.7679, Throughput: 71.28 samples/sec
2025-03-25 14:33:19,203 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9496.0MB reserved
2025-03-25 14:33:19,204 - training - INFO - Epoch: 826/200000, Batch: 30/45, Loss: 1.8286, Throughput: 71.94 samples/sec
2025-03-25 14:33:30,125 - training - INFO - Epoch 826 completed in 35.06s. Average loss: 1.8460
2025-03-25 14:33:30,129 - training - INFO - Starting epoch 827/200000
2025-03-25 14:33:30,870 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9634.0MB reserved
2025-03-25 14:33:30,870 - training - INFO - Epoch: 827/200000, Batch: 0/45, Loss: 2.1978, Throughput: 75.78 samples/sec
2025-03-25 14:33:42,694 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9494.0MB reserved
2025-03-25 14:33:42,694 - training - INFO - Epoch: 827/200000, Batch: 15/45, Loss: 1.7939, Throughput: 71.32 samples/sec
2025-03-25 14:33:54,278 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9496.0MB reserved
2025-03-25 14:33:54,278 - training - INFO - Epoch: 827/200000, Batch: 30/45, Loss: 1.7744, Throughput: 71.89 samples/sec
2025-03-25 14:34:05,215 - training - INFO - Epoch 827 completed in 35.09s. Average loss: 1.8547
2025-03-25 14:34:05,218 - training - INFO - Starting epoch 828/200000
2025-03-25 14:34:05,961 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9632.0MB reserved
2025-03-25 14:34:05,961 - training - INFO - Epoch: 828/200000, Batch: 0/45, Loss: 2.1233, Throughput: 75.60 samples/sec
2025-03-25 14:34:17,858 - training - INFO - Memory: GPU 0: 3557.2MB allocated, 9510.0MB reserved
2025-03-25 14:34:17,858 - training - INFO - Epoch: 828/200000, Batch: 15/45, Loss: 1.7920, Throughput: 70.90 samples/sec
2025-03-25 14:34:29,577 - training - INFO - Memory: GPU 0: 3557.2MB allocated, 9510.0MB reserved
2025-03-25 14:34:29,577 - training - INFO - Epoch: 828/200000, Batch: 30/45, Loss: 1.8090, Throughput: 71.27 samples/sec
2025-03-25 14:34:40,577 - training - INFO - Epoch 828 completed in 35.36s. Average loss: 1.8323
2025-03-25 14:34:40,580 - training - INFO - Starting epoch 829/200000
2025-03-25 14:34:41,324 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9648.0MB reserved
2025-03-25 14:34:41,324 - training - INFO - Epoch: 829/200000, Batch: 0/45, Loss: 2.0553, Throughput: 75.51 samples/sec
2025-03-25 14:34:53,234 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9506.0MB reserved
2025-03-25 14:34:53,234 - training - INFO - Epoch: 829/200000, Batch: 15/45, Loss: 1.8539, Throughput: 70.82 samples/sec
2025-03-25 14:35:04,858 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9506.0MB reserved
2025-03-25 14:35:04,858 - training - INFO - Epoch: 829/200000, Batch: 30/45, Loss: 1.8570, Throughput: 71.51 samples/sec
2025-03-25 14:35:15,771 - training - INFO - Epoch 829 completed in 35.19s. Average loss: 1.8500
2025-03-25 14:35:15,775 - training - INFO - Starting epoch 830/200000
2025-03-25 14:35:16,515 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9644.0MB reserved
2025-03-25 14:35:16,515 - training - INFO - Epoch: 830/200000, Batch: 0/45, Loss: 1.6894, Throughput: 75.82 samples/sec
2025-03-25 14:35:28,522 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9506.0MB reserved
2025-03-25 14:35:28,523 - training - INFO - Epoch: 830/200000, Batch: 15/45, Loss: 1.8255, Throughput: 70.30 samples/sec
2025-03-25 14:35:40,151 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9506.0MB reserved
2025-03-25 14:35:40,152 - training - INFO - Epoch: 830/200000, Batch: 30/45, Loss: 1.7752, Throughput: 71.22 samples/sec
2025-03-25 14:35:51,119 - training - INFO - Epoch 830 completed in 35.34s. Average loss: 1.8425
2025-03-25 14:35:51,123 - training - INFO - Starting epoch 831/200000
2025-03-25 14:35:51,854 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9644.0MB reserved
2025-03-25 14:35:51,854 - training - INFO - Epoch: 831/200000, Batch: 0/45, Loss: 1.9885, Throughput: 76.74 samples/sec
2025-03-25 14:36:03,851 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9498.0MB reserved
2025-03-25 14:36:03,852 - training - INFO - Epoch: 831/200000, Batch: 15/45, Loss: 1.7599, Throughput: 70.40 samples/sec
2025-03-25 14:36:15,448 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9500.0MB reserved
2025-03-25 14:36:15,448 - training - INFO - Epoch: 831/200000, Batch: 30/45, Loss: 1.8161, Throughput: 71.37 samples/sec
2025-03-25 14:36:26,392 - training - INFO - Epoch 831 completed in 35.27s. Average loss: 1.8869
2025-03-25 14:36:26,396 - training - INFO - Starting epoch 832/200000
2025-03-25 14:36:27,122 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9638.0MB reserved
2025-03-25 14:36:27,122 - training - INFO - Epoch: 832/200000, Batch: 0/45, Loss: 1.5754, Throughput: 77.22 samples/sec
2025-03-25 14:36:39,024 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9494.0MB reserved
2025-03-25 14:36:39,024 - training - INFO - Epoch: 832/200000, Batch: 15/45, Loss: 1.7865, Throughput: 70.97 samples/sec
2025-03-25 14:36:50,634 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9494.0MB reserved
2025-03-25 14:36:50,635 - training - INFO - Epoch: 832/200000, Batch: 30/45, Loss: 1.7696, Throughput: 71.63 samples/sec
2025-03-25 14:37:01,599 - training - INFO - Epoch 832 completed in 35.20s. Average loss: 1.7963
2025-03-25 14:37:01,602 - training - INFO - Starting epoch 833/200000
2025-03-25 14:37:02,359 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9632.0MB reserved
2025-03-25 14:37:02,360 - training - INFO - Epoch: 833/200000, Batch: 0/45, Loss: 1.4953, Throughput: 74.13 samples/sec
2025-03-25 14:37:14,326 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9512.0MB reserved
2025-03-25 14:37:14,326 - training - INFO - Epoch: 833/200000, Batch: 15/45, Loss: 1.7692, Throughput: 70.43 samples/sec
2025-03-25 14:37:25,971 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9512.0MB reserved
2025-03-25 14:37:25,971 - training - INFO - Epoch: 833/200000, Batch: 30/45, Loss: 1.8097, Throughput: 71.25 samples/sec
2025-03-25 14:37:37,005 - training - INFO - Epoch 833 completed in 35.40s. Average loss: 1.8268
2025-03-25 14:37:37,009 - training - INFO - Starting epoch 834/200000
2025-03-25 14:37:37,760 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9650.0MB reserved
2025-03-25 14:37:37,760 - training - INFO - Epoch: 834/200000, Batch: 0/45, Loss: 1.6121, Throughput: 74.71 samples/sec
2025-03-25 14:37:49,618 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9502.0MB reserved
2025-03-25 14:37:49,618 - training - INFO - Epoch: 834/200000, Batch: 15/45, Loss: 1.8264, Throughput: 71.07 samples/sec
2025-03-25 14:38:01,179 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9502.0MB reserved
2025-03-25 14:38:01,179 - training - INFO - Epoch: 834/200000, Batch: 30/45, Loss: 1.8002, Throughput: 71.83 samples/sec
2025-03-25 14:38:12,160 - training - INFO - Epoch 834 completed in 35.15s. Average loss: 1.9129
2025-03-25 14:38:12,164 - training - INFO - Starting epoch 835/200000
2025-03-25 14:38:12,915 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9640.0MB reserved
2025-03-25 14:38:12,915 - training - INFO - Epoch: 835/200000, Batch: 0/45, Loss: 1.5166, Throughput: 74.64 samples/sec
2025-03-25 14:38:24,881 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9500.0MB reserved
2025-03-25 14:38:24,882 - training - INFO - Epoch: 835/200000, Batch: 15/45, Loss: 1.7563, Throughput: 70.46 samples/sec
2025-03-25 14:38:36,527 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9500.0MB reserved
2025-03-25 14:38:36,528 - training - INFO - Epoch: 835/200000, Batch: 30/45, Loss: 1.7912, Throughput: 71.26 samples/sec
2025-03-25 14:38:47,434 - training - INFO - Epoch 835 completed in 35.27s. Average loss: 1.8663
2025-03-25 14:38:47,438 - training - INFO - Starting epoch 836/200000
2025-03-25 14:38:48,187 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9638.0MB reserved
2025-03-25 14:38:48,187 - training - INFO - Epoch: 836/200000, Batch: 0/45, Loss: 1.7805, Throughput: 74.91 samples/sec
2025-03-25 14:39:00,177 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9502.0MB reserved
2025-03-25 14:39:00,178 - training - INFO - Epoch: 836/200000, Batch: 15/45, Loss: 1.7139, Throughput: 70.34 samples/sec
2025-03-25 14:39:11,857 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9502.0MB reserved
2025-03-25 14:39:11,857 - training - INFO - Epoch: 836/200000, Batch: 30/45, Loss: 1.7957, Throughput: 71.10 samples/sec
2025-03-25 14:39:22,875 - training - INFO - Epoch 836 completed in 35.44s. Average loss: 1.8577
2025-03-25 14:39:22,879 - training - INFO - Starting epoch 837/200000
2025-03-25 14:39:23,621 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9640.0MB reserved
2025-03-25 14:39:23,621 - training - INFO - Epoch: 837/200000, Batch: 0/45, Loss: 1.8636, Throughput: 75.64 samples/sec
2025-03-25 14:39:35,557 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9484.0MB reserved
2025-03-25 14:39:35,557 - training - INFO - Epoch: 837/200000, Batch: 15/45, Loss: 1.8099, Throughput: 70.68 samples/sec
2025-03-25 14:39:47,176 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9484.0MB reserved
2025-03-25 14:39:47,177 - training - INFO - Epoch: 837/200000, Batch: 30/45, Loss: 1.7374, Throughput: 71.45 samples/sec
2025-03-25 14:39:58,120 - training - INFO - Epoch 837 completed in 35.24s. Average loss: 1.8803
2025-03-25 14:39:58,123 - training - INFO - Starting epoch 838/200000
2025-03-25 14:39:58,846 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9622.0MB reserved
2025-03-25 14:39:58,847 - training - INFO - Epoch: 838/200000, Batch: 0/45, Loss: 1.5583, Throughput: 77.53 samples/sec
2025-03-25 14:40:10,777 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9496.0MB reserved
2025-03-25 14:40:10,778 - training - INFO - Epoch: 838/200000, Batch: 15/45, Loss: 1.8020, Throughput: 70.81 samples/sec
2025-03-25 14:40:22,453 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9496.0MB reserved
2025-03-25 14:40:22,454 - training - INFO - Epoch: 838/200000, Batch: 30/45, Loss: 1.7903, Throughput: 71.36 samples/sec
2025-03-25 14:40:33,464 - training - INFO - Epoch 838 completed in 35.34s. Average loss: 1.8126
2025-03-25 14:40:33,468 - training - INFO - Starting epoch 839/200000
2025-03-25 14:40:34,199 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9634.0MB reserved
2025-03-25 14:40:34,199 - training - INFO - Epoch: 839/200000, Batch: 0/45, Loss: 2.2651, Throughput: 76.66 samples/sec
2025-03-25 14:40:46,086 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9500.0MB reserved
2025-03-25 14:40:46,086 - training - INFO - Epoch: 839/200000, Batch: 15/45, Loss: 1.8769, Throughput: 71.02 samples/sec
2025-03-25 14:40:57,694 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9500.0MB reserved
2025-03-25 14:40:57,695 - training - INFO - Epoch: 839/200000, Batch: 30/45, Loss: 1.8490, Throughput: 71.66 samples/sec
2025-03-25 14:41:08,596 - training - INFO - Epoch 839 completed in 35.13s. Average loss: 1.8135
2025-03-25 14:41:08,600 - training - INFO - Starting epoch 840/200000
2025-03-25 14:41:09,335 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9636.0MB reserved
2025-03-25 14:41:09,335 - training - INFO - Epoch: 840/200000, Batch: 0/45, Loss: 1.7715, Throughput: 76.26 samples/sec
2025-03-25 14:41:21,215 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9500.0MB reserved
2025-03-25 14:41:21,215 - training - INFO - Epoch: 840/200000, Batch: 15/45, Loss: 1.7795, Throughput: 71.03 samples/sec
2025-03-25 14:41:32,834 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9500.0MB reserved
2025-03-25 14:41:32,834 - training - INFO - Epoch: 840/200000, Batch: 30/45, Loss: 1.7389, Throughput: 71.64 samples/sec
2025-03-25 14:41:43,758 - training - INFO - Epoch 840 completed in 35.16s. Average loss: 1.8322
2025-03-25 14:41:43,761 - training - INFO - Starting epoch 841/200000
2025-03-25 14:41:44,500 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9638.0MB reserved
2025-03-25 14:41:44,500 - training - INFO - Epoch: 841/200000, Batch: 0/45, Loss: 1.5230, Throughput: 75.93 samples/sec
2025-03-25 14:41:56,479 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9504.0MB reserved
2025-03-25 14:41:56,479 - training - INFO - Epoch: 841/200000, Batch: 15/45, Loss: 1.7764, Throughput: 70.46 samples/sec
2025-03-25 14:42:08,131 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9504.0MB reserved
2025-03-25 14:42:08,131 - training - INFO - Epoch: 841/200000, Batch: 30/45, Loss: 1.7333, Throughput: 71.24 samples/sec
2025-03-25 14:42:19,118 - training - INFO - Epoch 841 completed in 35.36s. Average loss: 1.8032
2025-03-25 14:42:19,122 - training - INFO - Starting epoch 842/200000
2025-03-25 14:42:19,867 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9642.0MB reserved
2025-03-25 14:42:19,867 - training - INFO - Epoch: 842/200000, Batch: 0/45, Loss: 1.8498, Throughput: 75.26 samples/sec
2025-03-25 14:42:31,853 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9502.0MB reserved
2025-03-25 14:42:31,854 - training - INFO - Epoch: 842/200000, Batch: 15/45, Loss: 1.7043, Throughput: 70.39 samples/sec
2025-03-25 14:42:43,558 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9502.0MB reserved
2025-03-25 14:42:43,559 - training - INFO - Epoch: 842/200000, Batch: 30/45, Loss: 1.7609, Throughput: 71.05 samples/sec
2025-03-25 14:42:54,516 - training - INFO - Epoch 842 completed in 35.39s. Average loss: 1.8570
2025-03-25 14:42:54,520 - training - INFO - Starting epoch 843/200000
2025-03-25 14:42:55,248 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9640.0MB reserved
2025-03-25 14:42:55,249 - training - INFO - Epoch: 843/200000, Batch: 0/45, Loss: 1.8519, Throughput: 76.95 samples/sec
2025-03-25 14:43:07,142 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9494.0MB reserved
2025-03-25 14:43:07,143 - training - INFO - Epoch: 843/200000, Batch: 15/45, Loss: 1.7470, Throughput: 70.99 samples/sec
2025-03-25 14:43:18,769 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9494.0MB reserved
2025-03-25 14:43:18,770 - training - INFO - Epoch: 843/200000, Batch: 30/45, Loss: 1.7866, Throughput: 71.60 samples/sec
2025-03-25 14:43:29,679 - training - INFO - Epoch 843 completed in 35.16s. Average loss: 1.8294
2025-03-25 14:43:29,683 - training - INFO - Starting epoch 844/200000
2025-03-25 14:43:30,431 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9632.0MB reserved
2025-03-25 14:43:30,431 - training - INFO - Epoch: 844/200000, Batch: 0/45, Loss: 1.6475, Throughput: 75.08 samples/sec
2025-03-25 14:43:42,350 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9494.0MB reserved
2025-03-25 14:43:42,351 - training - INFO - Epoch: 844/200000, Batch: 15/45, Loss: 1.7602, Throughput: 70.74 samples/sec
2025-03-25 14:43:53,963 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9494.0MB reserved
2025-03-25 14:43:53,963 - training - INFO - Epoch: 844/200000, Batch: 30/45, Loss: 1.7927, Throughput: 71.51 samples/sec
2025-03-25 14:44:04,893 - training - INFO - Epoch 844 completed in 35.21s. Average loss: 1.8605
2025-03-25 14:44:04,898 - training - INFO - Starting epoch 845/200000
2025-03-25 14:44:05,644 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9630.0MB reserved
2025-03-25 14:44:05,644 - training - INFO - Epoch: 845/200000, Batch: 0/45, Loss: 2.1406, Throughput: 75.30 samples/sec
2025-03-25 14:44:17,584 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9494.0MB reserved
2025-03-25 14:44:17,584 - training - INFO - Epoch: 845/200000, Batch: 15/45, Loss: 2.0041, Throughput: 70.64 samples/sec
2025-03-25 14:44:29,169 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9494.0MB reserved
2025-03-25 14:44:29,169 - training - INFO - Epoch: 845/200000, Batch: 30/45, Loss: 1.9398, Throughput: 71.53 samples/sec
2025-03-25 14:44:40,136 - training - INFO - Epoch 845 completed in 35.24s. Average loss: 1.8848
2025-03-25 14:44:40,140 - training - INFO - Starting epoch 846/200000
2025-03-25 14:44:40,892 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9632.0MB reserved
2025-03-25 14:44:40,892 - training - INFO - Epoch: 846/200000, Batch: 0/45, Loss: 1.8683, Throughput: 74.62 samples/sec
2025-03-25 14:44:52,771 - training - INFO - Memory: GPU 0: 3557.2MB allocated, 9502.0MB reserved
2025-03-25 14:44:52,772 - training - INFO - Epoch: 846/200000, Batch: 15/45, Loss: 1.7045, Throughput: 70.95 samples/sec
2025-03-25 14:45:04,435 - training - INFO - Memory: GPU 0: 3557.2MB allocated, 9502.0MB reserved
2025-03-25 14:45:04,435 - training - INFO - Epoch: 846/200000, Batch: 30/45, Loss: 1.7357, Throughput: 71.46 samples/sec
2025-03-25 14:45:15,393 - training - INFO - Epoch 846 completed in 35.25s. Average loss: 1.8323
2025-03-25 14:45:15,397 - training - INFO - Starting epoch 847/200000
2025-03-25 14:45:16,135 - training - INFO - Memory: GPU 0: 3557.2MB allocated, 9638.0MB reserved
2025-03-25 14:45:16,136 - training - INFO - Epoch: 847/200000, Batch: 0/45, Loss: 2.0509, Throughput: 75.92 samples/sec
2025-03-25 14:45:28,020 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9502.0MB reserved
2025-03-25 14:45:28,021 - training - INFO - Epoch: 847/200000, Batch: 15/45, Loss: 1.8332, Throughput: 70.99 samples/sec
2025-03-25 14:45:39,628 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9504.0MB reserved
2025-03-25 14:45:39,629 - training - INFO - Epoch: 847/200000, Batch: 30/45, Loss: 1.8339, Throughput: 71.65 samples/sec
2025-03-25 14:45:50,587 - training - INFO - Epoch 847 completed in 35.19s. Average loss: 1.9165
2025-03-25 14:45:50,591 - training - INFO - Starting epoch 848/200000
2025-03-25 14:45:51,335 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9642.0MB reserved
2025-03-25 14:45:51,336 - training - INFO - Epoch: 848/200000, Batch: 0/45, Loss: 1.5603, Throughput: 75.42 samples/sec
2025-03-25 14:46:03,277 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9510.0MB reserved
2025-03-25 14:46:03,277 - training - INFO - Epoch: 848/200000, Batch: 15/45, Loss: 1.7858, Throughput: 70.64 samples/sec
2025-03-25 14:46:14,888 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9510.0MB reserved
2025-03-25 14:46:14,889 - training - INFO - Epoch: 848/200000, Batch: 30/45, Loss: 1.7848, Throughput: 71.45 samples/sec
2025-03-25 14:46:25,903 - training - INFO - Epoch 848 completed in 35.31s. Average loss: 1.8658
2025-03-25 14:46:25,907 - training - INFO - Starting epoch 849/200000
2025-03-25 14:46:26,635 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9648.0MB reserved
2025-03-25 14:46:26,636 - training - INFO - Epoch: 849/200000, Batch: 0/45, Loss: 1.6275, Throughput: 77.08 samples/sec
2025-03-25 14:46:38,575 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9502.0MB reserved
2025-03-25 14:46:38,576 - training - INFO - Epoch: 849/200000, Batch: 15/45, Loss: 1.8870, Throughput: 70.74 samples/sec
2025-03-25 14:46:50,115 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9502.0MB reserved
2025-03-25 14:46:50,116 - training - INFO - Epoch: 849/200000, Batch: 30/45, Loss: 1.8413, Throughput: 71.72 samples/sec
2025-03-25 14:47:01,029 - training - INFO - Epoch 849 completed in 35.12s. Average loss: 1.8380
2025-03-25 14:47:01,032 - training - INFO - Starting epoch 850/200000
2025-03-25 14:47:01,769 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9640.0MB reserved
2025-03-25 14:47:01,769 - training - INFO - Epoch: 850/200000, Batch: 0/45, Loss: 2.1622, Throughput: 76.14 samples/sec
2025-03-25 14:47:13,605 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9488.0MB reserved
2025-03-25 14:47:13,606 - training - INFO - Epoch: 850/200000, Batch: 15/45, Loss: 1.9043, Throughput: 71.28 samples/sec
2025-03-25 14:47:25,126 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9490.0MB reserved
2025-03-25 14:47:25,126 - training - INFO - Epoch: 850/200000, Batch: 30/45, Loss: 1.7871, Throughput: 72.06 samples/sec
2025-03-25 14:47:36,078 - training - INFO - Epoch 850 completed in 35.05s. Average loss: 1.8072
2025-03-25 14:47:36,082 - training - INFO - Starting epoch 851/200000
2025-03-25 14:47:36,828 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9626.0MB reserved
2025-03-25 14:47:36,829 - training - INFO - Epoch: 851/200000, Batch: 0/45, Loss: 1.8761, Throughput: 75.23 samples/sec
2025-03-25 14:47:48,763 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9502.0MB reserved
2025-03-25 14:47:48,763 - training - INFO - Epoch: 851/200000, Batch: 15/45, Loss: 1.8053, Throughput: 70.67 samples/sec
2025-03-25 14:48:00,390 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9502.0MB reserved
2025-03-25 14:48:00,391 - training - INFO - Epoch: 851/200000, Batch: 30/45, Loss: 1.8343, Throughput: 71.42 samples/sec
2025-03-25 14:48:11,303 - training - INFO - Epoch 851 completed in 35.22s. Average loss: 1.8277
2025-03-25 14:48:11,306 - training - INFO - Starting epoch 852/200000
2025-03-25 14:48:12,046 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9640.0MB reserved
2025-03-25 14:48:12,046 - training - INFO - Epoch: 852/200000, Batch: 0/45, Loss: 2.1556, Throughput: 75.82 samples/sec
2025-03-25 14:48:23,938 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9508.0MB reserved
2025-03-25 14:48:23,939 - training - INFO - Epoch: 852/200000, Batch: 15/45, Loss: 1.6735, Throughput: 70.94 samples/sec
2025-03-25 14:48:35,587 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9508.0MB reserved
2025-03-25 14:48:35,587 - training - INFO - Epoch: 852/200000, Batch: 30/45, Loss: 1.7646, Throughput: 71.50 samples/sec
2025-03-25 14:48:46,553 - training - INFO - Epoch 852 completed in 35.25s. Average loss: 1.8143
2025-03-25 14:48:46,557 - training - INFO - Starting epoch 853/200000
2025-03-25 14:48:47,293 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9644.0MB reserved
2025-03-25 14:48:47,293 - training - INFO - Epoch: 853/200000, Batch: 0/45, Loss: 2.0199, Throughput: 76.18 samples/sec
2025-03-25 14:48:59,141 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9504.0MB reserved
2025-03-25 14:48:59,142 - training - INFO - Epoch: 853/200000, Batch: 15/45, Loss: 1.8610, Throughput: 71.21 samples/sec
2025-03-25 14:49:10,714 - training - INFO - Memory: GPU 0: 3556.4MB allocated, 9504.0MB reserved
2025-03-25 14:49:10,715 - training - INFO - Epoch: 853/200000, Batch: 30/45, Loss: 1.8266, Throughput: 71.87 samples/sec
2025-03-25 14:49:21,613 - training - INFO - Epoch 853 completed in 35.06s. Average loss: 1.8167
2025-03-25 14:49:21,616 - training - INFO - Starting epoch 854/200000
2025-03-25 14:49:22,377 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9642.0MB reserved
2025-03-25 14:49:22,377 - training - INFO - Epoch: 854/200000, Batch: 0/45, Loss: 1.5001, Throughput: 73.72 samples/sec
2025-03-25 14:49:34,252 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9504.0MB reserved
2025-03-25 14:49:34,252 - training - INFO - Epoch: 854/200000, Batch: 15/45, Loss: 1.7462, Throughput: 70.92 samples/sec
2025-03-25 14:49:45,822 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9506.0MB reserved
2025-03-25 14:49:45,823 - training - INFO - Epoch: 854/200000, Batch: 30/45, Loss: 1.7584, Throughput: 71.72 samples/sec
2025-03-25 14:49:56,824 - training - INFO - Epoch 854 completed in 35.21s. Average loss: 1.7951
2025-03-25 14:49:56,829 - training - INFO - Starting epoch 855/200000
2025-03-25 14:49:57,579 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9644.0MB reserved
2025-03-25 14:49:57,580 - training - INFO - Epoch: 855/200000, Batch: 0/45, Loss: 1.9400, Throughput: 74.80 samples/sec
2025-03-25 14:50:09,501 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9502.0MB reserved
2025-03-25 14:50:09,501 - training - INFO - Epoch: 855/200000, Batch: 15/45, Loss: 1.8834, Throughput: 70.72 samples/sec
2025-03-25 14:50:21,184 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9502.0MB reserved
2025-03-25 14:50:21,184 - training - INFO - Epoch: 855/200000, Batch: 30/45, Loss: 1.8280, Throughput: 71.28 samples/sec
2025-03-25 14:50:32,079 - training - INFO - Epoch 855 completed in 35.25s. Average loss: 1.8446
2025-03-25 14:50:32,083 - training - INFO - Starting epoch 856/200000
2025-03-25 14:50:32,820 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9640.0MB reserved
2025-03-25 14:50:32,820 - training - INFO - Epoch: 856/200000, Batch: 0/45, Loss: 1.4204, Throughput: 76.12 samples/sec
2025-03-25 14:50:44,832 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9500.0MB reserved
2025-03-25 14:50:44,833 - training - INFO - Epoch: 856/200000, Batch: 15/45, Loss: 1.7761, Throughput: 70.30 samples/sec
2025-03-25 14:50:56,395 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9500.0MB reserved
2025-03-25 14:50:56,396 - training - INFO - Epoch: 856/200000, Batch: 30/45, Loss: 1.7521, Throughput: 71.41 samples/sec
2025-03-25 14:51:07,305 - training - INFO - Epoch 856 completed in 35.22s. Average loss: 1.8443
2025-03-25 14:51:07,309 - training - INFO - Starting epoch 857/200000
2025-03-25 14:51:08,059 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9638.0MB reserved
2025-03-25 14:51:08,060 - training - INFO - Epoch: 857/200000, Batch: 0/45, Loss: 2.2013, Throughput: 74.76 samples/sec
2025-03-25 14:51:19,966 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9402.0MB reserved
2025-03-25 14:51:19,967 - training - INFO - Epoch: 857/200000, Batch: 15/45, Loss: 1.7200, Throughput: 70.80 samples/sec
2025-03-25 14:51:31,588 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9404.0MB reserved
2025-03-25 14:51:31,588 - training - INFO - Epoch: 857/200000, Batch: 30/45, Loss: 1.7180, Throughput: 71.51 samples/sec
2025-03-25 14:51:42,491 - training - INFO - Epoch 857 completed in 35.18s. Average loss: 1.8172
2025-03-25 14:51:42,495 - training - INFO - Starting epoch 858/200000
2025-03-25 14:51:43,232 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9542.0MB reserved
2025-03-25 14:51:43,232 - training - INFO - Epoch: 858/200000, Batch: 0/45, Loss: 2.2141, Throughput: 76.21 samples/sec
2025-03-25 14:51:55,041 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9508.0MB reserved
2025-03-25 14:51:55,041 - training - INFO - Epoch: 858/200000, Batch: 15/45, Loss: 1.7873, Throughput: 71.42 samples/sec
2025-03-25 14:52:06,607 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9508.0MB reserved
2025-03-25 14:52:06,607 - training - INFO - Epoch: 858/200000, Batch: 30/45, Loss: 1.8003, Throughput: 72.01 samples/sec
2025-03-25 14:52:17,544 - training - INFO - Epoch 858 completed in 35.05s. Average loss: 1.8103
2025-03-25 14:52:17,547 - training - INFO - Starting epoch 859/200000
2025-03-25 14:52:18,299 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9646.0MB reserved
2025-03-25 14:52:18,299 - training - INFO - Epoch: 859/200000, Batch: 0/45, Loss: 1.6332, Throughput: 74.58 samples/sec
2025-03-25 14:52:30,220 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9504.0MB reserved
2025-03-25 14:52:30,220 - training - INFO - Epoch: 859/200000, Batch: 15/45, Loss: 1.7714, Throughput: 70.71 samples/sec
2025-03-25 14:52:41,764 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9504.0MB reserved
2025-03-25 14:52:41,764 - training - INFO - Epoch: 859/200000, Batch: 30/45, Loss: 1.7569, Throughput: 71.69 samples/sec
2025-03-25 14:52:52,751 - training - INFO - Epoch 859 completed in 35.20s. Average loss: 1.8089
2025-03-25 14:52:52,755 - training - INFO - Starting epoch 860/200000
2025-03-25 14:52:53,517 - training - INFO - Memory: GPU 0: 3562.6MB allocated, 9640.0MB reserved
2025-03-25 14:52:53,518 - training - INFO - Epoch: 860/200000, Batch: 0/45, Loss: 1.7793, Throughput: 73.72 samples/sec
2025-03-25 14:53:05,523 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9500.0MB reserved
2025-03-25 14:53:05,530 - training - INFO - Epoch: 860/200000, Batch: 15/45, Loss: 1.7871, Throughput: 70.18 samples/sec
2025-03-25 14:53:17,162 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9500.0MB reserved
2025-03-25 14:53:17,163 - training - INFO - Epoch: 860/200000, Batch: 30/45, Loss: 1.8199, Throughput: 71.14 samples/sec
2025-03-25 14:53:28,058 - training - INFO - Epoch 860 completed in 35.30s. Average loss: 1.7944
2025-03-25 14:53:28,062 - training - INFO - Starting epoch 861/200000
2025-03-25 14:53:28,804 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9638.0MB reserved
2025-03-25 14:53:28,805 - training - INFO - Epoch: 861/200000, Batch: 0/45, Loss: 1.6101, Throughput: 75.51 samples/sec
2025-03-25 14:53:40,776 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9504.0MB reserved
2025-03-25 14:53:40,777 - training - INFO - Epoch: 861/200000, Batch: 15/45, Loss: 1.8112, Throughput: 70.47 samples/sec
2025-03-25 14:53:52,341 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9504.0MB reserved
2025-03-25 14:53:52,341 - training - INFO - Epoch: 861/200000, Batch: 30/45, Loss: 1.7981, Throughput: 71.51 samples/sec
2025-03-25 14:54:03,312 - training - INFO - Epoch 861 completed in 35.25s. Average loss: 1.8128
2025-03-25 14:54:03,316 - training - INFO - Starting epoch 862/200000
2025-03-25 14:54:04,077 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9640.0MB reserved
2025-03-25 14:54:04,077 - training - INFO - Epoch: 862/200000, Batch: 0/45, Loss: 1.8773, Throughput: 73.72 samples/sec
2025-03-25 14:54:15,957 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9494.0MB reserved
2025-03-25 14:54:15,957 - training - INFO - Epoch: 862/200000, Batch: 15/45, Loss: 1.9207, Throughput: 70.89 samples/sec
2025-03-25 14:54:27,599 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9494.0MB reserved
2025-03-25 14:54:27,599 - training - INFO - Epoch: 862/200000, Batch: 30/45, Loss: 1.8311, Throughput: 71.50 samples/sec
2025-03-25 14:54:38,601 - training - INFO - Epoch 862 completed in 35.29s. Average loss: 1.8359
2025-03-25 14:54:38,605 - training - INFO - Starting epoch 863/200000
2025-03-25 14:54:39,341 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9630.0MB reserved
2025-03-25 14:54:39,341 - training - INFO - Epoch: 863/200000, Batch: 0/45, Loss: 2.2743, Throughput: 76.23 samples/sec
2025-03-25 14:54:51,248 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9396.0MB reserved
2025-03-25 14:54:51,249 - training - INFO - Epoch: 863/200000, Batch: 15/45, Loss: 1.7462, Throughput: 70.87 samples/sec
2025-03-25 14:55:02,839 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9396.0MB reserved
2025-03-25 14:55:02,839 - training - INFO - Epoch: 863/200000, Batch: 30/45, Loss: 1.7574, Throughput: 71.64 samples/sec
2025-03-25 14:55:13,810 - training - INFO - Epoch 863 completed in 35.20s. Average loss: 1.8075
2025-03-25 14:55:13,814 - training - INFO - Starting epoch 864/200000
2025-03-25 14:55:14,557 - training - INFO - Memory: GPU 0: 3564.0MB allocated, 9642.0MB reserved
2025-03-25 14:55:14,557 - training - INFO - Epoch: 864/200000, Batch: 0/45, Loss: 1.5488, Throughput: 75.49 samples/sec
2025-03-25 14:55:26,456 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9502.0MB reserved
2025-03-25 14:55:26,457 - training - INFO - Epoch: 864/200000, Batch: 15/45, Loss: 1.7751, Throughput: 70.88 samples/sec
2025-03-25 14:55:37,987 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9502.0MB reserved
2025-03-25 14:55:37,987 - training - INFO - Epoch: 864/200000, Batch: 30/45, Loss: 1.8222, Throughput: 71.82 samples/sec
2025-03-25 14:55:48,936 - training - INFO - Epoch 864 completed in 35.12s. Average loss: 1.8540
2025-03-25 14:55:48,940 - training - INFO - Starting epoch 865/200000
2025-03-25 14:55:49,678 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9640.0MB reserved
2025-03-25 14:55:49,678 - training - INFO - Epoch: 865/200000, Batch: 0/45, Loss: 2.1104, Throughput: 75.96 samples/sec
2025-03-25 14:56:01,564 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9510.0MB reserved
2025-03-25 14:56:01,564 - training - INFO - Epoch: 865/200000, Batch: 15/45, Loss: 1.7589, Throughput: 70.98 samples/sec
2025-03-25 14:56:13,172 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9512.0MB reserved
2025-03-25 14:56:13,172 - training - INFO - Epoch: 865/200000, Batch: 30/45, Loss: 1.7808, Throughput: 71.64 samples/sec
2025-03-25 14:56:24,089 - training - INFO - Epoch 865 completed in 35.15s. Average loss: 1.7923
2025-03-25 14:56:24,092 - training - INFO - Starting epoch 866/200000
2025-03-25 14:56:24,841 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9650.0MB reserved
2025-03-25 14:56:24,841 - training - INFO - Epoch: 866/200000, Batch: 0/45, Loss: 1.3772, Throughput: 74.95 samples/sec
2025-03-25 14:56:36,681 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9504.0MB reserved
2025-03-25 14:56:36,681 - training - INFO - Epoch: 866/200000, Batch: 15/45, Loss: 1.6701, Throughput: 71.18 samples/sec
2025-03-25 14:56:48,241 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9504.0MB reserved
2025-03-25 14:56:48,241 - training - INFO - Epoch: 866/200000, Batch: 30/45, Loss: 1.7371, Throughput: 71.89 samples/sec
2025-03-25 14:56:59,150 - training - INFO - Epoch 866 completed in 35.06s. Average loss: 1.8055
2025-03-25 14:56:59,153 - training - INFO - Starting epoch 867/200000
2025-03-25 14:56:59,890 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9642.0MB reserved
2025-03-25 14:56:59,890 - training - INFO - Epoch: 867/200000, Batch: 0/45, Loss: 2.1338, Throughput: 76.08 samples/sec
2025-03-25 14:57:11,758 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9498.0MB reserved
2025-03-25 14:57:11,759 - training - INFO - Epoch: 867/200000, Batch: 15/45, Loss: 1.7976, Throughput: 71.09 samples/sec
2025-03-25 14:57:23,386 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9498.0MB reserved
2025-03-25 14:57:23,386 - training - INFO - Epoch: 867/200000, Batch: 30/45, Loss: 1.8113, Throughput: 71.64 samples/sec
2025-03-25 14:57:34,321 - training - INFO - Epoch 867 completed in 35.17s. Average loss: 1.8911
2025-03-25 14:57:34,324 - training - INFO - Starting epoch 868/200000
2025-03-25 14:57:35,049 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9634.0MB reserved
2025-03-25 14:57:35,049 - training - INFO - Epoch: 868/200000, Batch: 0/45, Loss: 1.8212, Throughput: 77.43 samples/sec
2025-03-25 14:57:46,998 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9514.0MB reserved
2025-03-25 14:57:46,999 - training - INFO - Epoch: 868/200000, Batch: 15/45, Loss: 1.7566, Throughput: 70.70 samples/sec
2025-03-25 14:57:58,735 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9514.0MB reserved
2025-03-25 14:57:58,735 - training - INFO - Epoch: 868/200000, Batch: 30/45, Loss: 1.7241, Throughput: 71.12 samples/sec
2025-03-25 14:58:09,725 - training - INFO - Epoch 868 completed in 35.40s. Average loss: 1.8149
2025-03-25 14:58:09,729 - training - INFO - Starting epoch 869/200000
2025-03-25 14:58:10,453 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9650.0MB reserved
2025-03-25 14:58:10,454 - training - INFO - Epoch: 869/200000, Batch: 0/45, Loss: 1.8767, Throughput: 77.51 samples/sec
2025-03-25 14:58:22,375 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9498.0MB reserved
2025-03-25 14:58:22,376 - training - INFO - Epoch: 869/200000, Batch: 15/45, Loss: 1.8790, Throughput: 70.86 samples/sec
2025-03-25 14:58:33,972 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9498.0MB reserved
2025-03-25 14:58:33,972 - training - INFO - Epoch: 869/200000, Batch: 30/45, Loss: 1.9119, Throughput: 71.61 samples/sec
2025-03-25 14:58:44,915 - training - INFO - Epoch 869 completed in 35.19s. Average loss: 1.8635
2025-03-25 14:58:44,919 - training - INFO - Starting epoch 870/200000
2025-03-25 14:58:45,664 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9636.0MB reserved
2025-03-25 14:58:45,665 - training - INFO - Epoch: 870/200000, Batch: 0/45, Loss: 1.7531, Throughput: 75.25 samples/sec
2025-03-25 14:58:57,531 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9508.0MB reserved
2025-03-25 14:58:57,531 - training - INFO - Epoch: 870/200000, Batch: 15/45, Loss: 1.8812, Throughput: 71.05 samples/sec
2025-03-25 14:59:09,120 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9508.0MB reserved
2025-03-25 14:59:09,120 - training - INFO - Epoch: 870/200000, Batch: 30/45, Loss: 1.7967, Throughput: 71.74 samples/sec
2025-03-25 14:59:20,086 - training - INFO - Epoch 870 completed in 35.17s. Average loss: 1.9175
2025-03-25 14:59:20,090 - training - INFO - Starting epoch 871/200000
2025-03-25 14:59:20,825 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9644.0MB reserved
2025-03-25 14:59:20,825 - training - INFO - Epoch: 871/200000, Batch: 0/45, Loss: 2.0798, Throughput: 76.32 samples/sec
2025-03-25 14:59:32,704 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9512.0MB reserved
2025-03-25 14:59:32,705 - training - INFO - Epoch: 871/200000, Batch: 15/45, Loss: 1.8057, Throughput: 71.04 samples/sec
2025-03-25 14:59:44,364 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9512.0MB reserved
2025-03-25 14:59:44,364 - training - INFO - Epoch: 871/200000, Batch: 30/45, Loss: 1.7710, Throughput: 71.52 samples/sec
2025-03-25 14:59:55,346 - training - INFO - Epoch 871 completed in 35.26s. Average loss: 1.8035
2025-03-25 14:59:55,350 - training - INFO - Starting epoch 872/200000
2025-03-25 14:59:56,089 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9648.0MB reserved
2025-03-25 14:59:56,089 - training - INFO - Epoch: 872/200000, Batch: 0/45, Loss: 1.9670, Throughput: 75.97 samples/sec
2025-03-25 15:00:07,973 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9504.0MB reserved
2025-03-25 15:00:07,974 - training - INFO - Epoch: 872/200000, Batch: 15/45, Loss: 1.7624, Throughput: 70.99 samples/sec
2025-03-25 15:00:19,560 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9504.0MB reserved
2025-03-25 15:00:19,561 - training - INFO - Epoch: 872/200000, Batch: 30/45, Loss: 1.7537, Throughput: 71.71 samples/sec
2025-03-25 15:00:30,468 - training - INFO - Epoch 872 completed in 35.12s. Average loss: 1.7868
2025-03-25 15:00:30,471 - training - INFO - Starting epoch 873/200000
2025-03-25 15:00:31,208 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9644.0MB reserved
2025-03-25 15:00:31,208 - training - INFO - Epoch: 873/200000, Batch: 0/45, Loss: 1.4957, Throughput: 76.15 samples/sec
2025-03-25 15:00:43,211 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9492.0MB reserved
2025-03-25 15:00:43,211 - training - INFO - Epoch: 873/200000, Batch: 15/45, Loss: 1.8181, Throughput: 70.35 samples/sec
2025-03-25 15:00:54,911 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9492.0MB reserved
2025-03-25 15:00:54,911 - training - INFO - Epoch: 873/200000, Batch: 30/45, Loss: 1.7463, Throughput: 71.04 samples/sec
2025-03-25 15:01:05,853 - training - INFO - Epoch 873 completed in 35.38s. Average loss: 1.8220
2025-03-25 15:01:05,857 - training - INFO - Starting epoch 874/200000
2025-03-25 15:01:06,605 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9628.0MB reserved
2025-03-25 15:01:06,606 - training - INFO - Epoch: 874/200000, Batch: 0/45, Loss: 1.6180, Throughput: 75.02 samples/sec
2025-03-25 15:01:18,557 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9496.0MB reserved
2025-03-25 15:01:18,558 - training - INFO - Epoch: 874/200000, Batch: 15/45, Loss: 1.8283, Throughput: 70.57 samples/sec
2025-03-25 15:01:30,119 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9496.0MB reserved
2025-03-25 15:01:30,119 - training - INFO - Epoch: 874/200000, Batch: 30/45, Loss: 1.8376, Throughput: 71.56 samples/sec
2025-03-25 15:01:41,037 - training - INFO - Epoch 874 completed in 35.18s. Average loss: 1.8603
2025-03-25 15:01:41,041 - training - INFO - Starting epoch 875/200000
2025-03-25 15:01:41,797 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9634.0MB reserved
2025-03-25 15:01:41,797 - training - INFO - Epoch: 875/200000, Batch: 0/45, Loss: 2.1902, Throughput: 74.25 samples/sec
2025-03-25 15:01:53,765 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9500.0MB reserved
2025-03-25 15:01:53,765 - training - INFO - Epoch: 875/200000, Batch: 15/45, Loss: 1.8484, Throughput: 70.43 samples/sec
2025-03-25 15:02:05,318 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9500.0MB reserved
2025-03-25 15:02:05,319 - training - INFO - Epoch: 875/200000, Batch: 30/45, Loss: 1.8193, Throughput: 71.52 samples/sec
2025-03-25 15:02:16,260 - training - INFO - Epoch 875 completed in 35.22s. Average loss: 1.7887
2025-03-25 15:02:16,264 - training - INFO - Starting epoch 876/200000
2025-03-25 15:02:17,014 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9638.0MB reserved
2025-03-25 15:02:17,015 - training - INFO - Epoch: 876/200000, Batch: 0/45, Loss: 2.1602, Throughput: 74.65 samples/sec
2025-03-25 15:02:28,922 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9496.0MB reserved
2025-03-25 15:02:28,922 - training - INFO - Epoch: 876/200000, Batch: 15/45, Loss: 1.7430, Throughput: 70.79 samples/sec
2025-03-25 15:02:40,645 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9496.0MB reserved
2025-03-25 15:02:40,645 - training - INFO - Epoch: 876/200000, Batch: 30/45, Loss: 1.7996, Throughput: 71.21 samples/sec
2025-03-25 15:02:51,687 - training - INFO - Epoch 876 completed in 35.42s. Average loss: 1.7954
2025-03-25 15:02:51,690 - training - INFO - Starting epoch 877/200000
2025-03-25 15:02:52,419 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9634.0MB reserved
2025-03-25 15:02:52,420 - training - INFO - Epoch: 877/200000, Batch: 0/45, Loss: 1.7328, Throughput: 76.98 samples/sec
2025-03-25 15:03:04,279 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9494.0MB reserved
2025-03-25 15:03:04,279 - training - INFO - Epoch: 877/200000, Batch: 15/45, Loss: 1.7600, Throughput: 71.19 samples/sec
2025-03-25 15:03:15,952 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9494.0MB reserved
2025-03-25 15:03:15,953 - training - INFO - Epoch: 877/200000, Batch: 30/45, Loss: 1.7586, Throughput: 71.56 samples/sec
2025-03-25 15:03:26,934 - training - INFO - Epoch 877 completed in 35.24s. Average loss: 1.8077
2025-03-25 15:03:26,938 - training - INFO - Starting epoch 878/200000
2025-03-25 15:03:27,686 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9632.0MB reserved
2025-03-25 15:03:27,687 - training - INFO - Epoch: 878/200000, Batch: 0/45, Loss: 2.0942, Throughput: 75.01 samples/sec
2025-03-25 15:03:39,644 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9496.0MB reserved
2025-03-25 15:03:39,644 - training - INFO - Epoch: 878/200000, Batch: 15/45, Loss: 1.7199, Throughput: 70.53 samples/sec
2025-03-25 15:03:51,246 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9496.0MB reserved
2025-03-25 15:03:51,246 - training - INFO - Epoch: 878/200000, Batch: 30/45, Loss: 1.7116, Throughput: 71.42 samples/sec
2025-03-25 15:04:02,168 - training - INFO - Epoch 878 completed in 35.23s. Average loss: 1.8041
2025-03-25 15:04:02,174 - training - INFO - Starting epoch 879/200000
2025-03-25 15:04:02,920 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9634.0MB reserved
2025-03-25 15:04:02,920 - training - INFO - Epoch: 879/200000, Batch: 0/45, Loss: 1.8858, Throughput: 75.33 samples/sec
2025-03-25 15:04:14,858 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9500.0MB reserved
2025-03-25 15:04:14,858 - training - INFO - Epoch: 879/200000, Batch: 15/45, Loss: 1.8072, Throughput: 70.65 samples/sec
2025-03-25 15:04:26,489 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9500.0MB reserved
2025-03-25 15:04:26,489 - training - INFO - Epoch: 879/200000, Batch: 30/45, Loss: 1.7471, Throughput: 71.40 samples/sec
2025-03-25 15:04:37,429 - training - INFO - Epoch 879 completed in 35.26s. Average loss: 1.7808
2025-03-25 15:04:37,433 - training - INFO - Starting epoch 880/200000
2025-03-25 15:04:38,182 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9638.0MB reserved
2025-03-25 15:04:38,182 - training - INFO - Epoch: 880/200000, Batch: 0/45, Loss: 1.5794, Throughput: 74.88 samples/sec
2025-03-25 15:04:50,069 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9508.0MB reserved
2025-03-25 15:04:50,069 - training - INFO - Epoch: 880/200000, Batch: 15/45, Loss: 1.8082, Throughput: 70.91 samples/sec
2025-03-25 15:05:01,725 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9508.0MB reserved
2025-03-25 15:05:01,725 - training - INFO - Epoch: 880/200000, Batch: 30/45, Loss: 1.8088, Throughput: 71.47 samples/sec
2025-03-25 15:05:12,648 - training - INFO - Epoch 880 completed in 35.21s. Average loss: 1.8034
2025-03-25 15:05:12,651 - training - INFO - Starting epoch 881/200000
2025-03-25 15:05:13,402 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9646.0MB reserved
2025-03-25 15:05:13,402 - training - INFO - Epoch: 881/200000, Batch: 0/45, Loss: 1.3454, Throughput: 74.74 samples/sec
2025-03-25 15:05:25,325 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9502.0MB reserved
2025-03-25 15:05:25,325 - training - INFO - Epoch: 881/200000, Batch: 15/45, Loss: 1.8312, Throughput: 70.71 samples/sec
2025-03-25 15:05:36,924 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9502.0MB reserved
2025-03-25 15:05:36,924 - training - INFO - Epoch: 881/200000, Batch: 30/45, Loss: 1.8016, Throughput: 71.53 samples/sec
2025-03-25 15:05:47,880 - training - INFO - Epoch 881 completed in 35.23s. Average loss: 1.7841
2025-03-25 15:05:47,884 - training - INFO - Starting epoch 882/200000
2025-03-25 15:05:48,614 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9638.0MB reserved
2025-03-25 15:05:48,614 - training - INFO - Epoch: 882/200000, Batch: 0/45, Loss: 1.9378, Throughput: 77.04 samples/sec
2025-03-25 15:06:00,572 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9492.0MB reserved
2025-03-25 15:06:00,572 - training - INFO - Epoch: 882/200000, Batch: 15/45, Loss: 1.8248, Throughput: 70.63 samples/sec
2025-03-25 15:06:12,252 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9494.0MB reserved
2025-03-25 15:06:12,252 - training - INFO - Epoch: 882/200000, Batch: 30/45, Loss: 1.7504, Throughput: 71.25 samples/sec
2025-03-25 15:06:23,294 - training - INFO - Epoch 882 completed in 35.41s. Average loss: 1.8599
2025-03-25 15:06:23,297 - training - INFO - Starting epoch 883/200000
2025-03-25 15:06:24,025 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9632.0MB reserved
2025-03-25 15:06:24,026 - training - INFO - Epoch: 883/200000, Batch: 0/45, Loss: 1.6499, Throughput: 77.29 samples/sec
2025-03-25 15:06:35,975 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9496.0MB reserved
2025-03-25 15:06:35,975 - training - INFO - Epoch: 883/200000, Batch: 15/45, Loss: 1.7457, Throughput: 70.69 samples/sec
2025-03-25 15:06:47,589 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9496.0MB reserved
2025-03-25 15:06:47,589 - training - INFO - Epoch: 883/200000, Batch: 30/45, Loss: 1.7758, Throughput: 71.47 samples/sec
2025-03-25 15:06:58,515 - training - INFO - Epoch 883 completed in 35.22s. Average loss: 1.7831
2025-03-25 15:06:58,519 - training - INFO - Starting epoch 884/200000
2025-03-25 15:06:59,250 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9634.0MB reserved
2025-03-25 15:06:59,250 - training - INFO - Epoch: 884/200000, Batch: 0/45, Loss: 2.2248, Throughput: 76.85 samples/sec
2025-03-25 15:07:11,171 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9506.0MB reserved
2025-03-25 15:07:11,172 - training - INFO - Epoch: 884/200000, Batch: 15/45, Loss: 1.7873, Throughput: 70.83 samples/sec
2025-03-25 15:07:22,756 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9506.0MB reserved
2025-03-25 15:07:22,756 - training - INFO - Epoch: 884/200000, Batch: 30/45, Loss: 1.7605, Throughput: 71.63 samples/sec
2025-03-25 15:07:33,699 - training - INFO - Epoch 884 completed in 35.18s. Average loss: 1.8421
2025-03-25 15:07:33,702 - training - INFO - Starting epoch 885/200000
2025-03-25 15:07:34,444 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9642.0MB reserved
2025-03-25 15:07:34,444 - training - INFO - Epoch: 885/200000, Batch: 0/45, Loss: 2.2409, Throughput: 75.70 samples/sec
2025-03-25 15:07:46,298 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9494.0MB reserved
2025-03-25 15:07:46,298 - training - INFO - Epoch: 885/200000, Batch: 15/45, Loss: 1.8223, Throughput: 71.14 samples/sec
2025-03-25 15:07:57,792 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9494.0MB reserved
2025-03-25 15:07:57,793 - training - INFO - Epoch: 885/200000, Batch: 30/45, Loss: 1.8320, Throughput: 72.07 samples/sec
2025-03-25 15:08:08,717 - training - INFO - Epoch 885 completed in 35.01s. Average loss: 1.8739
2025-03-25 15:08:08,721 - training - INFO - Starting epoch 886/200000
2025-03-25 15:08:09,466 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9632.0MB reserved
2025-03-25 15:08:09,467 - training - INFO - Epoch: 886/200000, Batch: 0/45, Loss: 1.5321, Throughput: 75.33 samples/sec
2025-03-25 15:08:21,389 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9500.0MB reserved
2025-03-25 15:08:21,390 - training - INFO - Epoch: 886/200000, Batch: 15/45, Loss: 1.7613, Throughput: 70.73 samples/sec
2025-03-25 15:08:33,011 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9502.0MB reserved
2025-03-25 15:08:33,012 - training - INFO - Epoch: 886/200000, Batch: 30/45, Loss: 1.8456, Throughput: 71.48 samples/sec
2025-03-25 15:08:44,002 - training - INFO - Epoch 886 completed in 35.28s. Average loss: 1.7881
2025-03-25 15:08:44,006 - training - INFO - Starting epoch 887/200000
2025-03-25 15:08:44,752 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9638.0MB reserved
2025-03-25 15:08:44,752 - training - INFO - Epoch: 887/200000, Batch: 0/45, Loss: 1.5897, Throughput: 75.29 samples/sec
2025-03-25 15:08:56,784 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9504.0MB reserved
2025-03-25 15:08:56,785 - training - INFO - Epoch: 887/200000, Batch: 15/45, Loss: 1.8009, Throughput: 70.14 samples/sec
2025-03-25 15:09:08,487 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9504.0MB reserved
2025-03-25 15:09:08,487 - training - INFO - Epoch: 887/200000, Batch: 30/45, Loss: 1.8088, Throughput: 70.92 samples/sec
2025-03-25 15:09:19,408 - training - INFO - Epoch 887 completed in 35.40s. Average loss: 1.8174
2025-03-25 15:09:19,412 - training - INFO - Starting epoch 888/200000
2025-03-25 15:09:20,135 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9642.0MB reserved
2025-03-25 15:09:20,135 - training - INFO - Epoch: 888/200000, Batch: 0/45, Loss: 2.0092, Throughput: 77.65 samples/sec
2025-03-25 15:09:32,024 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9510.0MB reserved
2025-03-25 15:09:32,024 - training - INFO - Epoch: 888/200000, Batch: 15/45, Loss: 1.7892, Throughput: 71.05 samples/sec
2025-03-25 15:09:43,620 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9510.0MB reserved
2025-03-25 15:09:43,620 - training - INFO - Epoch: 888/200000, Batch: 30/45, Loss: 1.7438, Throughput: 71.72 samples/sec
2025-03-25 15:09:54,546 - training - INFO - Epoch 888 completed in 35.13s. Average loss: 1.7974
2025-03-25 15:09:54,550 - training - INFO - Starting epoch 889/200000
2025-03-25 15:09:55,306 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9648.0MB reserved
2025-03-25 15:09:55,307 - training - INFO - Epoch: 889/200000, Batch: 0/45, Loss: 1.2365, Throughput: 74.20 samples/sec
2025-03-25 15:10:07,315 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9504.0MB reserved
2025-03-25 15:10:07,315 - training - INFO - Epoch: 889/200000, Batch: 15/45, Loss: 1.7752, Throughput: 70.20 samples/sec
2025-03-25 15:10:18,968 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9504.0MB reserved
2025-03-25 15:10:18,968 - training - INFO - Epoch: 889/200000, Batch: 30/45, Loss: 1.7735, Throughput: 71.10 samples/sec
2025-03-25 15:10:29,920 - training - INFO - Epoch 889 completed in 35.37s. Average loss: 1.8505
2025-03-25 15:10:29,990 - training - INFO - Starting epoch 890/200000
2025-03-25 15:10:30,684 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9642.0MB reserved
2025-03-25 15:10:30,684 - training - INFO - Epoch: 890/200000, Batch: 0/45, Loss: 1.9032, Throughput: 80.88 samples/sec
2025-03-25 15:10:42,562 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9514.0MB reserved
2025-03-25 15:10:42,562 - training - INFO - Epoch: 890/200000, Batch: 15/45, Loss: 1.8029, Throughput: 71.28 samples/sec
2025-03-25 15:10:54,220 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9514.0MB reserved
2025-03-25 15:10:54,220 - training - INFO - Epoch: 890/200000, Batch: 30/45, Loss: 1.7426, Throughput: 71.65 samples/sec
2025-03-25 15:11:05,262 - training - INFO - Epoch 890 completed in 35.27s. Average loss: 1.8287
2025-03-25 15:11:05,265 - training - INFO - Starting epoch 891/200000
2025-03-25 15:11:05,999 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9652.0MB reserved
2025-03-25 15:11:05,999 - training - INFO - Epoch: 891/200000, Batch: 0/45, Loss: 1.8986, Throughput: 76.50 samples/sec
2025-03-25 15:11:17,830 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9498.0MB reserved
2025-03-25 15:11:17,831 - training - INFO - Epoch: 891/200000, Batch: 15/45, Loss: 1.7862, Throughput: 71.32 samples/sec
2025-03-25 15:11:29,451 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9500.0MB reserved
2025-03-25 15:11:29,451 - training - INFO - Epoch: 891/200000, Batch: 30/45, Loss: 1.7552, Throughput: 71.78 samples/sec
2025-03-25 15:11:40,421 - training - INFO - Epoch 891 completed in 35.16s. Average loss: 1.7786
2025-03-25 15:11:40,425 - training - INFO - Starting epoch 892/200000
2025-03-25 15:11:41,164 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9638.0MB reserved
2025-03-25 15:11:41,164 - training - INFO - Epoch: 892/200000, Batch: 0/45, Loss: 2.0234, Throughput: 75.92 samples/sec
2025-03-25 15:11:53,113 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9498.0MB reserved
2025-03-25 15:11:53,113 - training - INFO - Epoch: 892/200000, Batch: 15/45, Loss: 1.8298, Throughput: 70.63 samples/sec
2025-03-25 15:12:04,665 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9500.0MB reserved
2025-03-25 15:12:04,665 - training - INFO - Epoch: 892/200000, Batch: 30/45, Loss: 1.8123, Throughput: 71.62 samples/sec
2025-03-25 15:12:15,609 - training - INFO - Epoch 892 completed in 35.18s. Average loss: 1.7491
2025-03-25 15:12:15,613 - training - INFO - Starting epoch 893/200000
2025-03-25 15:12:16,363 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9638.0MB reserved
2025-03-25 15:12:16,363 - training - INFO - Epoch: 893/200000, Batch: 0/45, Loss: 1.9595, Throughput: 74.74 samples/sec
2025-03-25 15:12:28,194 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9488.0MB reserved
2025-03-25 15:12:28,195 - training - INFO - Epoch: 893/200000, Batch: 15/45, Loss: 1.7902, Throughput: 71.22 samples/sec
2025-03-25 15:12:39,736 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9488.0MB reserved
2025-03-25 15:12:39,736 - training - INFO - Epoch: 893/200000, Batch: 30/45, Loss: 1.7845, Throughput: 71.97 samples/sec
2025-03-25 15:12:50,624 - training - INFO - Epoch 893 completed in 35.01s. Average loss: 1.8159
2025-03-25 15:12:50,627 - training - INFO - Starting epoch 894/200000
2025-03-25 15:12:51,367 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9626.0MB reserved
2025-03-25 15:12:51,367 - training - INFO - Epoch: 894/200000, Batch: 0/45, Loss: 2.4628, Throughput: 75.93 samples/sec
2025-03-25 15:13:03,273 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9396.0MB reserved
2025-03-25 15:13:03,273 - training - INFO - Epoch: 894/200000, Batch: 15/45, Loss: 1.7987, Throughput: 70.87 samples/sec
2025-03-25 15:13:14,827 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9398.0MB reserved
2025-03-25 15:13:14,828 - training - INFO - Epoch: 894/200000, Batch: 30/45, Loss: 1.8287, Throughput: 71.74 samples/sec
2025-03-25 15:13:25,751 - training - INFO - Epoch 894 completed in 35.12s. Average loss: 1.8138
2025-03-25 15:13:25,755 - training - INFO - Starting epoch 895/200000
2025-03-25 15:13:26,503 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9534.0MB reserved
2025-03-25 15:13:26,503 - training - INFO - Epoch: 895/200000, Batch: 0/45, Loss: 1.7972, Throughput: 75.07 samples/sec
2025-03-25 15:13:38,425 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9494.0MB reserved
2025-03-25 15:13:38,425 - training - INFO - Epoch: 895/200000, Batch: 15/45, Loss: 1.8036, Throughput: 70.73 samples/sec
2025-03-25 15:13:50,043 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9494.0MB reserved
2025-03-25 15:13:50,043 - training - INFO - Epoch: 895/200000, Batch: 30/45, Loss: 1.7395, Throughput: 71.48 samples/sec
2025-03-25 15:14:01,012 - training - INFO - Epoch 895 completed in 35.26s. Average loss: 1.7825
2025-03-25 15:14:01,015 - training - INFO - Starting epoch 896/200000
2025-03-25 15:14:01,772 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9634.0MB reserved
2025-03-25 15:14:01,772 - training - INFO - Epoch: 896/200000, Batch: 0/45, Loss: 1.5757, Throughput: 74.24 samples/sec
2025-03-25 15:14:13,618 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9490.0MB reserved
2025-03-25 15:14:13,618 - training - INFO - Epoch: 896/200000, Batch: 15/45, Loss: 1.8734, Throughput: 71.11 samples/sec
2025-03-25 15:14:25,189 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9490.0MB reserved
2025-03-25 15:14:25,189 - training - INFO - Epoch: 896/200000, Batch: 30/45, Loss: 1.7991, Throughput: 71.82 samples/sec
2025-03-25 15:14:36,120 - training - INFO - Epoch 896 completed in 35.10s. Average loss: 1.7706
2025-03-25 15:14:36,124 - training - INFO - Starting epoch 897/200000
2025-03-25 15:14:36,864 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9628.0MB reserved
2025-03-25 15:14:36,865 - training - INFO - Epoch: 897/200000, Batch: 0/45, Loss: 1.9258, Throughput: 75.80 samples/sec
2025-03-25 15:14:48,669 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9504.0MB reserved
2025-03-25 15:14:48,670 - training - INFO - Epoch: 897/200000, Batch: 15/45, Loss: 1.7516, Throughput: 71.43 samples/sec
2025-03-25 15:15:00,297 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9504.0MB reserved
2025-03-25 15:15:00,297 - training - INFO - Epoch: 897/200000, Batch: 30/45, Loss: 1.7476, Throughput: 71.82 samples/sec
2025-03-25 15:15:11,249 - training - INFO - Epoch 897 completed in 35.12s. Average loss: 1.7572
2025-03-25 15:15:11,252 - training - INFO - Starting epoch 898/200000
2025-03-25 15:15:12,006 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9640.0MB reserved
2025-03-25 15:15:12,006 - training - INFO - Epoch: 898/200000, Batch: 0/45, Loss: 1.9432, Throughput: 74.49 samples/sec
2025-03-25 15:15:23,997 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9502.0MB reserved
2025-03-25 15:15:23,997 - training - INFO - Epoch: 898/200000, Batch: 15/45, Loss: 1.9061, Throughput: 70.32 samples/sec
2025-03-25 15:15:35,666 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9502.0MB reserved
2025-03-25 15:15:35,666 - training - INFO - Epoch: 898/200000, Batch: 30/45, Loss: 1.8651, Throughput: 71.11 samples/sec
2025-03-25 15:15:46,614 - training - INFO - Epoch 898 completed in 35.36s. Average loss: 1.7776
2025-03-25 15:15:46,618 - training - INFO - Starting epoch 899/200000
2025-03-25 15:15:47,373 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9640.0MB reserved
2025-03-25 15:15:47,373 - training - INFO - Epoch: 899/200000, Batch: 0/45, Loss: 1.2353, Throughput: 74.34 samples/sec
2025-03-25 15:15:59,189 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9506.0MB reserved
2025-03-25 15:15:59,189 - training - INFO - Epoch: 899/200000, Batch: 15/45, Loss: 1.6505, Throughput: 71.28 samples/sec
2025-03-25 15:16:10,729 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9506.0MB reserved
2025-03-25 15:16:10,729 - training - INFO - Epoch: 899/200000, Batch: 30/45, Loss: 1.7472, Throughput: 72.00 samples/sec
2025-03-25 15:16:21,656 - training - INFO - Epoch 899 completed in 35.04s. Average loss: 1.8155
2025-03-25 15:16:21,659 - training - INFO - Starting epoch 900/200000
2025-03-25 15:16:22,424 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9644.0MB reserved
2025-03-25 15:16:22,424 - training - INFO - Epoch: 900/200000, Batch: 0/45, Loss: 1.9450, Throughput: 73.29 samples/sec
2025-03-25 15:16:34,259 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9500.0MB reserved
2025-03-25 15:16:34,259 - training - INFO - Epoch: 900/200000, Batch: 15/45, Loss: 1.7559, Throughput: 71.12 samples/sec
2025-03-25 15:16:45,897 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9500.0MB reserved
2025-03-25 15:16:45,897 - training - INFO - Epoch: 900/200000, Batch: 30/45, Loss: 1.6824, Throughput: 71.63 samples/sec
2025-03-25 15:16:56,824 - training - INFO - Epoch 900 completed in 35.16s. Average loss: 1.8349
2025-03-25 15:16:56,827 - training - INFO - Starting validation...
2025-03-25 15:16:57,148 - training - INFO - Validation Loss: 22.7082
2025-03-25 15:16:57,148 - training - INFO - Validation loss did not improve. Counter: 8/10
2025-03-25 15:16:57,477 - training - INFO - Starting epoch 901/200000
2025-03-25 15:16:58,251 - training - INFO - Memory: GPU 0: 3567.4MB allocated, 8742.0MB reserved
2025-03-25 15:16:58,251 - training - INFO - Epoch: 901/200000, Batch: 0/45, Loss: 1.7667, Throughput: 72.46 samples/sec
2025-03-25 15:17:10,132 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9478.0MB reserved
2025-03-25 15:17:10,133 - training - INFO - Epoch: 901/200000, Batch: 15/45, Loss: 1.7944, Throughput: 70.81 samples/sec
2025-03-25 15:17:21,738 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9478.0MB reserved
2025-03-25 15:17:21,739 - training - INFO - Epoch: 901/200000, Batch: 30/45, Loss: 1.8468, Throughput: 71.56 samples/sec
2025-03-25 15:17:32,649 - training - INFO - Epoch 901 completed in 35.17s. Average loss: 1.7797
2025-03-25 15:17:32,653 - training - INFO - Starting epoch 902/200000
2025-03-25 15:17:33,422 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9616.0MB reserved
2025-03-25 15:17:33,422 - training - INFO - Epoch: 902/200000, Batch: 0/45, Loss: 1.9312, Throughput: 73.05 samples/sec
2025-03-25 15:17:45,262 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9486.0MB reserved
2025-03-25 15:17:45,262 - training - INFO - Epoch: 902/200000, Batch: 15/45, Loss: 1.8262, Throughput: 71.07 samples/sec
2025-03-25 15:17:56,834 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9486.0MB reserved
2025-03-25 15:17:56,835 - training - INFO - Epoch: 902/200000, Batch: 30/45, Loss: 1.8320, Throughput: 71.80 samples/sec
2025-03-25 15:18:07,791 - training - INFO - Epoch 902 completed in 35.14s. Average loss: 1.7589
2025-03-25 15:18:07,795 - training - INFO - Starting epoch 903/200000
2025-03-25 15:18:08,542 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9624.0MB reserved
2025-03-25 15:18:08,542 - training - INFO - Epoch: 903/200000, Batch: 0/45, Loss: 1.6039, Throughput: 75.13 samples/sec
2025-03-25 15:18:20,396 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9496.0MB reserved
2025-03-25 15:18:20,396 - training - INFO - Epoch: 903/200000, Batch: 15/45, Loss: 1.6885, Throughput: 71.12 samples/sec
2025-03-25 15:18:31,934 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9496.0MB reserved
2025-03-25 15:18:31,934 - training - INFO - Epoch: 903/200000, Batch: 30/45, Loss: 1.7022, Throughput: 71.92 samples/sec
2025-03-25 15:18:42,901 - training - INFO - Epoch 903 completed in 35.11s. Average loss: 1.7755
2025-03-25 15:18:42,906 - training - INFO - Starting epoch 904/200000
2025-03-25 15:18:43,641 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9634.0MB reserved
2025-03-25 15:18:43,642 - training - INFO - Epoch: 904/200000, Batch: 0/45, Loss: 1.5946, Throughput: 76.19 samples/sec
2025-03-25 15:18:55,497 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9486.0MB reserved
2025-03-25 15:18:55,498 - training - INFO - Epoch: 904/200000, Batch: 15/45, Loss: 1.6812, Throughput: 71.17 samples/sec
2025-03-25 15:19:07,086 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9486.0MB reserved
2025-03-25 15:19:07,086 - training - INFO - Epoch: 904/200000, Batch: 30/45, Loss: 1.7479, Throughput: 71.80 samples/sec
2025-03-25 15:19:18,042 - training - INFO - Epoch 904 completed in 35.14s. Average loss: 1.7679
2025-03-25 15:19:18,045 - training - INFO - Starting epoch 905/200000
2025-03-25 15:19:18,776 - training - INFO - Memory: GPU 0: 3562.6MB allocated, 9624.0MB reserved
2025-03-25 15:19:18,776 - training - INFO - Epoch: 905/200000, Batch: 0/45, Loss: 1.8524, Throughput: 76.84 samples/sec
2025-03-25 15:19:30,645 - training - INFO - Memory: GPU 0: 3564.9MB allocated, 9492.0MB reserved
2025-03-25 15:19:30,645 - training - INFO - Epoch: 905/200000, Batch: 15/45, Loss: 1.8173, Throughput: 71.12 samples/sec
2025-03-25 15:19:42,252 - training - INFO - Memory: GPU 0: 3564.9MB allocated, 9494.0MB reserved
2025-03-25 15:19:42,253 - training - INFO - Epoch: 905/200000, Batch: 30/45, Loss: 1.7877, Throughput: 71.72 samples/sec
2025-03-25 15:19:53,165 - training - INFO - Epoch 905 completed in 35.12s. Average loss: 1.7748
2025-03-25 15:19:53,168 - training - INFO - Starting epoch 906/200000
2025-03-25 15:19:53,912 - training - INFO - Memory: GPU 0: 3564.4MB allocated, 9632.0MB reserved
2025-03-25 15:19:53,912 - training - INFO - Epoch: 906/200000, Batch: 0/45, Loss: 1.7651, Throughput: 75.40 samples/sec
2025-03-25 15:20:05,820 - training - INFO - Memory: GPU 0: 3565.6MB allocated, 9394.0MB reserved
2025-03-25 15:20:05,820 - training - INFO - Epoch: 906/200000, Batch: 15/45, Loss: 1.7102, Throughput: 70.83 samples/sec
2025-03-25 15:20:17,512 - training - INFO - Memory: GPU 0: 3565.6MB allocated, 9394.0MB reserved
2025-03-25 15:20:17,512 - training - INFO - Epoch: 906/200000, Batch: 30/45, Loss: 1.7404, Throughput: 71.32 samples/sec
2025-03-25 15:20:28,496 - training - INFO - Epoch 906 completed in 35.33s. Average loss: 1.7991
2025-03-25 15:20:28,500 - training - INFO - Starting epoch 907/200000
2025-03-25 15:20:29,237 - training - INFO - Memory: GPU 0: 3565.6MB allocated, 9532.0MB reserved
2025-03-25 15:20:29,237 - training - INFO - Epoch: 907/200000, Batch: 0/45, Loss: 1.9434, Throughput: 76.04 samples/sec
2025-03-25 15:20:41,168 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9494.0MB reserved
2025-03-25 15:20:41,169 - training - INFO - Epoch: 907/200000, Batch: 15/45, Loss: 1.8415, Throughput: 70.74 samples/sec
2025-03-25 15:20:52,738 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9496.0MB reserved
2025-03-25 15:20:52,738 - training - INFO - Epoch: 907/200000, Batch: 30/45, Loss: 1.7942, Throughput: 71.63 samples/sec
2025-03-25 15:21:03,691 - training - INFO - Epoch 907 completed in 35.19s. Average loss: 1.8044
2025-03-25 15:21:03,695 - training - INFO - Starting epoch 908/200000
2025-03-25 15:21:04,456 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9634.0MB reserved
2025-03-25 15:21:04,457 - training - INFO - Epoch: 908/200000, Batch: 0/45, Loss: 1.5176, Throughput: 73.71 samples/sec
2025-03-25 15:21:16,280 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9486.0MB reserved
2025-03-25 15:21:16,281 - training - INFO - Epoch: 908/200000, Batch: 15/45, Loss: 1.8027, Throughput: 71.20 samples/sec
2025-03-25 15:21:27,849 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9486.0MB reserved
2025-03-25 15:21:27,849 - training - INFO - Epoch: 908/200000, Batch: 30/45, Loss: 1.8149, Throughput: 71.88 samples/sec
2025-03-25 15:21:38,751 - training - INFO - Epoch 908 completed in 35.06s. Average loss: 1.8830
2025-03-25 15:21:38,755 - training - INFO - Starting epoch 909/200000
2025-03-25 15:21:39,495 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9626.0MB reserved
2025-03-25 15:21:39,495 - training - INFO - Epoch: 909/200000, Batch: 0/45, Loss: 1.8978, Throughput: 75.89 samples/sec
2025-03-25 15:21:51,342 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9498.0MB reserved
2025-03-25 15:21:51,342 - training - INFO - Epoch: 909/200000, Batch: 15/45, Loss: 1.6894, Throughput: 71.20 samples/sec
2025-03-25 15:22:02,844 - training - INFO - Memory: GPU 0: 3556.1MB allocated, 9498.0MB reserved
2025-03-25 15:22:02,844 - training - INFO - Epoch: 909/200000, Batch: 30/45, Loss: 1.7431, Throughput: 72.07 samples/sec
2025-03-25 15:22:13,770 - training - INFO - Epoch 909 completed in 35.01s. Average loss: 1.7737
2025-03-25 15:22:13,773 - training - INFO - Starting epoch 910/200000
2025-03-25 15:22:14,516 - training - INFO - Memory: GPU 0: 3556.1MB allocated, 9636.0MB reserved
2025-03-25 15:22:14,516 - training - INFO - Epoch: 910/200000, Batch: 0/45, Loss: 1.6698, Throughput: 75.62 samples/sec
2025-03-25 15:22:26,374 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9498.0MB reserved
2025-03-25 15:22:26,375 - training - INFO - Epoch: 910/200000, Batch: 15/45, Loss: 1.7545, Throughput: 71.12 samples/sec
2025-03-25 15:22:37,942 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9498.0MB reserved
2025-03-25 15:22:37,942 - training - INFO - Epoch: 910/200000, Batch: 30/45, Loss: 1.7489, Throughput: 71.84 samples/sec
2025-03-25 15:22:48,923 - training - INFO - Epoch 910 completed in 35.15s. Average loss: 1.7753
2025-03-25 15:22:48,928 - training - INFO - Starting epoch 911/200000
2025-03-25 15:22:49,669 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9634.0MB reserved
2025-03-25 15:22:49,669 - training - INFO - Epoch: 911/200000, Batch: 0/45, Loss: 1.9927, Throughput: 75.77 samples/sec
2025-03-25 15:23:01,553 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9490.0MB reserved
2025-03-25 15:23:01,553 - training - INFO - Epoch: 911/200000, Batch: 15/45, Loss: 1.7816, Throughput: 70.98 samples/sec
2025-03-25 15:23:13,125 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9490.0MB reserved
2025-03-25 15:23:13,125 - training - INFO - Epoch: 911/200000, Batch: 30/45, Loss: 1.7483, Throughput: 71.75 samples/sec
2025-03-25 15:23:24,054 - training - INFO - Epoch 911 completed in 35.13s. Average loss: 1.7725
2025-03-25 15:23:24,058 - training - INFO - Starting epoch 912/200000
2025-03-25 15:23:24,816 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9628.0MB reserved
2025-03-25 15:23:24,816 - training - INFO - Epoch: 912/200000, Batch: 0/45, Loss: 1.9630, Throughput: 74.02 samples/sec
2025-03-25 15:23:36,687 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9504.0MB reserved
2025-03-25 15:23:36,687 - training - INFO - Epoch: 912/200000, Batch: 15/45, Loss: 1.8811, Throughput: 70.96 samples/sec
2025-03-25 15:23:48,273 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9504.0MB reserved
2025-03-25 15:23:48,273 - training - INFO - Epoch: 912/200000, Batch: 30/45, Loss: 1.8368, Throughput: 71.70 samples/sec
2025-03-25 15:23:59,237 - training - INFO - Epoch 912 completed in 35.18s. Average loss: 1.7895
2025-03-25 15:23:59,241 - training - INFO - Starting epoch 913/200000
2025-03-25 15:23:59,981 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9640.0MB reserved
2025-03-25 15:23:59,981 - training - INFO - Epoch: 913/200000, Batch: 0/45, Loss: 1.7269, Throughput: 75.92 samples/sec
2025-03-25 15:24:11,875 - training - INFO - Memory: GPU 0: 3562.8MB allocated, 9492.0MB reserved
2025-03-25 15:24:11,875 - training - INFO - Epoch: 913/200000, Batch: 15/45, Loss: 1.8430, Throughput: 70.93 samples/sec
2025-03-25 15:24:23,488 - training - INFO - Memory: GPU 0: 3562.8MB allocated, 9494.0MB reserved
2025-03-25 15:24:23,489 - training - INFO - Epoch: 913/200000, Batch: 30/45, Loss: 1.8263, Throughput: 71.60 samples/sec
2025-03-25 15:24:34,404 - training - INFO - Epoch 913 completed in 35.16s. Average loss: 1.8594
2025-03-25 15:24:34,408 - training - INFO - Starting epoch 914/200000
2025-03-25 15:24:35,156 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9632.0MB reserved
2025-03-25 15:24:35,156 - training - INFO - Epoch: 914/200000, Batch: 0/45, Loss: 1.6752, Throughput: 74.89 samples/sec
2025-03-25 15:24:47,131 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9494.0MB reserved
2025-03-25 15:24:47,132 - training - INFO - Epoch: 914/200000, Batch: 15/45, Loss: 1.7514, Throughput: 70.43 samples/sec
2025-03-25 15:24:58,814 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9494.0MB reserved
2025-03-25 15:24:58,814 - training - INFO - Epoch: 914/200000, Batch: 30/45, Loss: 1.7632, Throughput: 71.14 samples/sec
2025-03-25 15:25:09,777 - training - INFO - Epoch 914 completed in 35.37s. Average loss: 1.7531
2025-03-25 15:25:09,780 - training - INFO - Starting epoch 915/200000
2025-03-25 15:25:10,525 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9632.0MB reserved
2025-03-25 15:25:10,525 - training - INFO - Epoch: 915/200000, Batch: 0/45, Loss: 1.8725, Throughput: 75.31 samples/sec
2025-03-25 15:25:22,332 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9504.0MB reserved
2025-03-25 15:25:22,332 - training - INFO - Epoch: 915/200000, Batch: 15/45, Loss: 1.8289, Throughput: 71.39 samples/sec
2025-03-25 15:25:33,874 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9504.0MB reserved
2025-03-25 15:25:33,875 - training - INFO - Epoch: 915/200000, Batch: 30/45, Loss: 1.7727, Throughput: 72.06 samples/sec
2025-03-25 15:25:44,840 - training - INFO - Epoch 915 completed in 35.06s. Average loss: 1.8206
2025-03-25 15:25:44,843 - training - INFO - Starting epoch 916/200000
2025-03-25 15:25:45,595 - training - INFO - Memory: GPU 0: 3562.8MB allocated, 9644.0MB reserved
2025-03-25 15:25:45,595 - training - INFO - Epoch: 916/200000, Batch: 0/45, Loss: 1.5650, Throughput: 74.63 samples/sec
2025-03-25 15:25:57,535 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9484.0MB reserved
2025-03-25 15:25:57,536 - training - INFO - Epoch: 916/200000, Batch: 15/45, Loss: 1.8342, Throughput: 70.60 samples/sec
2025-03-25 15:26:09,210 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9484.0MB reserved
2025-03-25 15:26:09,210 - training - INFO - Epoch: 916/200000, Batch: 30/45, Loss: 1.7365, Throughput: 71.25 samples/sec
2025-03-25 15:26:20,179 - training - INFO - Epoch 916 completed in 35.34s. Average loss: 1.7670
2025-03-25 15:26:20,183 - training - INFO - Starting epoch 917/200000
2025-03-25 15:26:20,926 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9622.0MB reserved
2025-03-25 15:26:20,926 - training - INFO - Epoch: 917/200000, Batch: 0/45, Loss: 1.7484, Throughput: 75.52 samples/sec
2025-03-25 15:26:32,731 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9494.0MB reserved
2025-03-25 15:26:32,731 - training - INFO - Epoch: 917/200000, Batch: 15/45, Loss: 1.8724, Throughput: 71.42 samples/sec
2025-03-25 15:26:44,232 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9494.0MB reserved
2025-03-25 15:26:44,233 - training - INFO - Epoch: 917/200000, Batch: 30/45, Loss: 1.8097, Throughput: 72.19 samples/sec
2025-03-25 15:26:55,136 - training - INFO - Epoch 917 completed in 34.95s. Average loss: 1.8293
2025-03-25 15:26:55,139 - training - INFO - Starting epoch 918/200000
2025-03-25 15:26:55,886 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9630.0MB reserved
2025-03-25 15:26:55,886 - training - INFO - Epoch: 918/200000, Batch: 0/45, Loss: 1.9615, Throughput: 75.19 samples/sec
2025-03-25 15:27:07,742 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9508.0MB reserved
2025-03-25 15:27:07,743 - training - INFO - Epoch: 918/200000, Batch: 15/45, Loss: 1.6845, Throughput: 71.10 samples/sec
2025-03-25 15:27:19,262 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9508.0MB reserved
2025-03-25 15:27:19,263 - training - INFO - Epoch: 918/200000, Batch: 30/45, Loss: 1.6961, Throughput: 71.97 samples/sec
2025-03-25 15:27:30,216 - training - INFO - Epoch 918 completed in 35.08s. Average loss: 1.8024
2025-03-25 15:27:30,221 - training - INFO - Starting epoch 919/200000
2025-03-25 15:27:30,987 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9646.0MB reserved
2025-03-25 15:27:30,987 - training - INFO - Epoch: 919/200000, Batch: 0/45, Loss: 1.6375, Throughput: 73.18 samples/sec
2025-03-25 15:27:42,968 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9500.0MB reserved
2025-03-25 15:27:42,968 - training - INFO - Epoch: 919/200000, Batch: 15/45, Loss: 1.7911, Throughput: 70.30 samples/sec
2025-03-25 15:27:54,573 - training - INFO - Memory: GPU 0: 3562.5MB allocated, 9500.0MB reserved
2025-03-25 15:27:54,703 - training - INFO - Epoch: 919/200000, Batch: 30/45, Loss: 1.8818, Throughput: 71.29 samples/sec
2025-03-25 15:28:05,606 - training - INFO - Epoch 919 completed in 35.39s. Average loss: 1.8184
2025-03-25 15:28:05,609 - training - INFO - Starting epoch 920/200000
2025-03-25 15:28:06,353 - training - INFO - Memory: GPU 0: 3562.5MB allocated, 9638.0MB reserved
2025-03-25 15:28:06,354 - training - INFO - Epoch: 920/200000, Batch: 0/45, Loss: 1.5711, Throughput: 75.38 samples/sec
2025-03-25 15:28:18,305 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9498.0MB reserved
2025-03-25 15:28:18,306 - training - INFO - Epoch: 920/200000, Batch: 15/45, Loss: 1.5933, Throughput: 70.58 samples/sec
2025-03-25 15:28:30,007 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9498.0MB reserved
2025-03-25 15:28:30,008 - training - INFO - Epoch: 920/200000, Batch: 30/45, Loss: 1.7226, Throughput: 71.16 samples/sec
2025-03-25 15:28:41,010 - training - INFO - Epoch 920 completed in 35.40s. Average loss: 1.7501
2025-03-25 15:28:41,013 - training - INFO - Starting epoch 921/200000
2025-03-25 15:28:41,773 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9634.0MB reserved
2025-03-25 15:28:41,773 - training - INFO - Epoch: 921/200000, Batch: 0/45, Loss: 1.6264, Throughput: 73.88 samples/sec
2025-03-25 15:28:53,685 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9498.0MB reserved
2025-03-25 15:28:53,685 - training - INFO - Epoch: 921/200000, Batch: 15/45, Loss: 1.8695, Throughput: 70.72 samples/sec
2025-03-25 15:29:05,228 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9498.0MB reserved
2025-03-25 15:29:05,229 - training - INFO - Epoch: 921/200000, Batch: 30/45, Loss: 1.7599, Throughput: 71.69 samples/sec
2025-03-25 15:29:16,151 - training - INFO - Epoch 921 completed in 35.14s. Average loss: 1.7848
2025-03-25 15:29:16,155 - training - INFO - Starting epoch 922/200000
2025-03-25 15:29:16,904 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9636.0MB reserved
2025-03-25 15:29:16,904 - training - INFO - Epoch: 922/200000, Batch: 0/45, Loss: 1.4275, Throughput: 74.85 samples/sec
2025-03-25 15:29:28,797 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9512.0MB reserved
2025-03-25 15:29:28,797 - training - INFO - Epoch: 922/200000, Batch: 15/45, Loss: 1.7965, Throughput: 70.88 samples/sec
2025-03-25 15:29:40,372 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9512.0MB reserved
2025-03-25 15:29:40,372 - training - INFO - Epoch: 922/200000, Batch: 30/45, Loss: 1.8076, Throughput: 71.69 samples/sec
2025-03-25 15:29:51,294 - training - INFO - Epoch 922 completed in 35.14s. Average loss: 1.8266
2025-03-25 15:29:51,297 - training - INFO - Starting epoch 923/200000
2025-03-25 15:29:52,032 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9650.0MB reserved
2025-03-25 15:29:52,032 - training - INFO - Epoch: 923/200000, Batch: 0/45, Loss: 1.6175, Throughput: 76.43 samples/sec
2025-03-25 15:30:04,021 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9504.0MB reserved
2025-03-25 15:30:04,021 - training - INFO - Epoch: 923/200000, Batch: 15/45, Loss: 1.7477, Throughput: 70.43 samples/sec
2025-03-25 15:30:15,681 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9504.0MB reserved
2025-03-25 15:30:15,681 - training - INFO - Epoch: 923/200000, Batch: 30/45, Loss: 1.7135, Throughput: 71.20 samples/sec
2025-03-25 15:30:26,615 - training - INFO - Epoch 923 completed in 35.32s. Average loss: 1.7815
2025-03-25 15:30:26,619 - training - INFO - Starting epoch 924/200000
2025-03-25 15:30:27,370 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9640.0MB reserved
2025-03-25 15:30:27,371 - training - INFO - Epoch: 924/200000, Batch: 0/45, Loss: 1.7225, Throughput: 74.73 samples/sec
2025-03-25 15:30:39,253 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9504.0MB reserved
2025-03-25 15:30:39,253 - training - INFO - Epoch: 924/200000, Batch: 15/45, Loss: 1.7042, Throughput: 70.93 samples/sec
2025-03-25 15:30:50,802 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9504.0MB reserved
2025-03-25 15:30:50,802 - training - INFO - Epoch: 924/200000, Batch: 30/45, Loss: 1.8163, Throughput: 71.79 samples/sec
2025-03-25 15:31:01,715 - training - INFO - Epoch 924 completed in 35.10s. Average loss: 1.7873
2025-03-25 15:31:01,719 - training - INFO - Starting epoch 925/200000
2025-03-25 15:31:02,465 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9644.0MB reserved
2025-03-25 15:31:02,466 - training - INFO - Epoch: 925/200000, Batch: 0/45, Loss: 1.4985, Throughput: 75.21 samples/sec
2025-03-25 15:31:14,413 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9506.0MB reserved
2025-03-25 15:31:14,413 - training - INFO - Epoch: 925/200000, Batch: 15/45, Loss: 1.8137, Throughput: 70.59 samples/sec
2025-03-25 15:31:26,149 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9506.0MB reserved
2025-03-25 15:31:26,150 - training - INFO - Epoch: 925/200000, Batch: 30/45, Loss: 1.7859, Throughput: 71.06 samples/sec
2025-03-25 15:31:37,138 - training - INFO - Epoch 925 completed in 35.42s. Average loss: 1.7337
2025-03-25 15:31:37,141 - training - INFO - Starting epoch 926/200000
2025-03-25 15:31:37,912 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9644.0MB reserved
2025-03-25 15:31:37,912 - training - INFO - Epoch: 926/200000, Batch: 0/45, Loss: 1.1987, Throughput: 72.72 samples/sec
2025-03-25 15:31:49,895 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9496.0MB reserved
2025-03-25 15:31:49,896 - training - INFO - Epoch: 926/200000, Batch: 15/45, Loss: 1.7426, Throughput: 70.27 samples/sec
2025-03-25 15:32:01,614 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9496.0MB reserved
2025-03-25 15:32:01,615 - training - INFO - Epoch: 926/200000, Batch: 30/45, Loss: 1.8479, Throughput: 70.94 samples/sec
2025-03-25 15:32:12,583 - training - INFO - Epoch 926 completed in 35.44s. Average loss: 1.8078
2025-03-25 15:32:12,587 - training - INFO - Starting epoch 927/200000
2025-03-25 15:32:13,337 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9634.0MB reserved
2025-03-25 15:32:13,337 - training - INFO - Epoch: 927/200000, Batch: 0/45, Loss: 1.7322, Throughput: 74.72 samples/sec
2025-03-25 15:32:25,405 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9504.0MB reserved
2025-03-25 15:32:25,405 - training - INFO - Epoch: 927/200000, Batch: 15/45, Loss: 1.8131, Throughput: 69.91 samples/sec
2025-03-25 15:32:37,104 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9504.0MB reserved
2025-03-25 15:32:37,105 - training - INFO - Epoch: 927/200000, Batch: 30/45, Loss: 1.8153, Throughput: 70.81 samples/sec
2025-03-25 15:32:48,072 - training - INFO - Epoch 927 completed in 35.49s. Average loss: 1.8056
2025-03-25 15:32:48,076 - training - INFO - Starting epoch 928/200000
2025-03-25 15:32:48,832 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9642.0MB reserved
2025-03-25 15:32:48,832 - training - INFO - Epoch: 928/200000, Batch: 0/45, Loss: 1.7357, Throughput: 74.24 samples/sec
2025-03-25 15:33:00,782 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9508.0MB reserved
2025-03-25 15:33:00,782 - training - INFO - Epoch: 928/200000, Batch: 15/45, Loss: 1.6937, Throughput: 70.53 samples/sec
2025-03-25 15:33:12,453 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9508.0MB reserved
2025-03-25 15:33:12,453 - training - INFO - Epoch: 928/200000, Batch: 30/45, Loss: 1.7556, Throughput: 71.22 samples/sec
2025-03-25 15:33:23,369 - training - INFO - Epoch 928 completed in 35.29s. Average loss: 1.8308
2025-03-25 15:33:23,372 - training - INFO - Starting epoch 929/200000
2025-03-25 15:33:24,097 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9646.0MB reserved
2025-03-25 15:33:24,098 - training - INFO - Epoch: 929/200000, Batch: 0/45, Loss: 1.9945, Throughput: 77.30 samples/sec
2025-03-25 15:33:35,958 - training - INFO - Memory: GPU 0: 3555.5MB allocated, 9496.0MB reserved
2025-03-25 15:33:35,958 - training - INFO - Epoch: 929/200000, Batch: 15/45, Loss: 1.7991, Throughput: 71.20 samples/sec
2025-03-25 15:33:47,509 - training - INFO - Memory: GPU 0: 3555.5MB allocated, 9496.0MB reserved
2025-03-25 15:33:47,510 - training - INFO - Epoch: 929/200000, Batch: 30/45, Loss: 1.7431, Throughput: 71.93 samples/sec
2025-03-25 15:33:58,469 - training - INFO - Epoch 929 completed in 35.10s. Average loss: 1.7445
2025-03-25 15:33:58,473 - training - INFO - Starting epoch 930/200000
2025-03-25 15:33:59,224 - training - INFO - Memory: GPU 0: 3556.2MB allocated, 9636.0MB reserved
2025-03-25 15:33:59,224 - training - INFO - Epoch: 930/200000, Batch: 0/45, Loss: 2.0272, Throughput: 74.70 samples/sec
2025-03-25 15:34:11,045 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9484.0MB reserved
2025-03-25 15:34:11,045 - training - INFO - Epoch: 930/200000, Batch: 15/45, Loss: 1.8755, Throughput: 71.28 samples/sec
2025-03-25 15:34:22,606 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9486.0MB reserved
2025-03-25 15:34:22,607 - training - INFO - Epoch: 930/200000, Batch: 30/45, Loss: 1.8029, Throughput: 71.94 samples/sec
2025-03-25 15:34:33,525 - training - INFO - Epoch 930 completed in 35.05s. Average loss: 1.8225
2025-03-25 15:34:33,529 - training - INFO - Starting epoch 931/200000
2025-03-25 15:34:34,267 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9624.0MB reserved
2025-03-25 15:34:34,267 - training - INFO - Epoch: 931/200000, Batch: 0/45, Loss: 1.9081, Throughput: 76.06 samples/sec
2025-03-25 15:34:46,258 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9502.0MB reserved
2025-03-25 15:34:46,258 - training - INFO - Epoch: 931/200000, Batch: 15/45, Loss: 1.7614, Throughput: 70.40 samples/sec
2025-03-25 15:34:57,913 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9502.0MB reserved
2025-03-25 15:34:57,914 - training - INFO - Epoch: 931/200000, Batch: 30/45, Loss: 1.7436, Throughput: 71.20 samples/sec
2025-03-25 15:35:08,886 - training - INFO - Epoch 931 completed in 35.36s. Average loss: 1.8091
2025-03-25 15:35:08,890 - training - INFO - Starting epoch 932/200000
2025-03-25 15:35:09,636 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9638.0MB reserved
2025-03-25 15:35:09,636 - training - INFO - Epoch: 932/200000, Batch: 0/45, Loss: 1.6871, Throughput: 75.08 samples/sec
2025-03-25 15:35:21,524 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9494.0MB reserved
2025-03-25 15:35:21,525 - training - INFO - Epoch: 932/200000, Batch: 15/45, Loss: 1.8052, Throughput: 70.92 samples/sec
2025-03-25 15:35:33,101 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9494.0MB reserved
2025-03-25 15:35:33,102 - training - INFO - Epoch: 932/200000, Batch: 30/45, Loss: 1.7087, Throughput: 71.71 samples/sec
2025-03-25 15:35:43,988 - training - INFO - Epoch 932 completed in 35.10s. Average loss: 1.7440
2025-03-25 15:35:43,991 - training - INFO - Starting epoch 933/200000
2025-03-25 15:35:44,720 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9630.0MB reserved
2025-03-25 15:35:44,720 - training - INFO - Epoch: 933/200000, Batch: 0/45, Loss: 1.8580, Throughput: 77.10 samples/sec
2025-03-25 15:35:56,718 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9498.0MB reserved
2025-03-25 15:35:56,718 - training - INFO - Epoch: 933/200000, Batch: 15/45, Loss: 1.8222, Throughput: 70.42 samples/sec
2025-03-25 15:36:08,273 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9498.0MB reserved
2025-03-25 15:36:08,274 - training - INFO - Epoch: 933/200000, Batch: 30/45, Loss: 1.8167, Throughput: 71.50 samples/sec
2025-03-25 15:36:19,203 - training - INFO - Epoch 933 completed in 35.21s. Average loss: 1.8024
2025-03-25 15:36:19,206 - training - INFO - Starting epoch 934/200000
2025-03-25 15:36:19,948 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9636.0MB reserved
2025-03-25 15:36:19,948 - training - INFO - Epoch: 934/200000, Batch: 0/45, Loss: 1.9402, Throughput: 75.56 samples/sec
2025-03-25 15:36:31,800 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9484.0MB reserved
2025-03-25 15:36:31,801 - training - INFO - Epoch: 934/200000, Batch: 15/45, Loss: 1.7522, Throughput: 71.15 samples/sec
2025-03-25 15:36:43,337 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9484.0MB reserved
2025-03-25 15:36:43,337 - training - INFO - Epoch: 934/200000, Batch: 30/45, Loss: 1.7608, Throughput: 71.95 samples/sec
2025-03-25 15:36:54,267 - training - INFO - Epoch 934 completed in 35.06s. Average loss: 1.7878
2025-03-25 15:36:54,270 - training - INFO - Starting epoch 935/200000
2025-03-25 15:36:55,014 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9622.0MB reserved
2025-03-25 15:36:55,014 - training - INFO - Epoch: 935/200000, Batch: 0/45, Loss: 2.2287, Throughput: 75.43 samples/sec
2025-03-25 15:37:06,929 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9500.0MB reserved
2025-03-25 15:37:06,930 - training - INFO - Epoch: 935/200000, Batch: 15/45, Loss: 1.7254, Throughput: 70.79 samples/sec
2025-03-25 15:37:18,535 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9500.0MB reserved
2025-03-25 15:37:18,536 - training - INFO - Epoch: 935/200000, Batch: 30/45, Loss: 1.7071, Throughput: 71.55 samples/sec
2025-03-25 15:37:29,441 - training - INFO - Epoch 935 completed in 35.17s. Average loss: 1.7392
2025-03-25 15:37:29,445 - training - INFO - Starting epoch 936/200000
2025-03-25 15:37:30,180 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9640.0MB reserved
2025-03-25 15:37:30,180 - training - INFO - Epoch: 936/200000, Batch: 0/45, Loss: 1.6161, Throughput: 76.34 samples/sec
2025-03-25 15:37:42,023 - training - INFO - Memory: GPU 0: 3556.7MB allocated, 9484.0MB reserved
2025-03-25 15:37:42,024 - training - INFO - Epoch: 936/200000, Batch: 15/45, Loss: 1.7414, Throughput: 71.24 samples/sec
2025-03-25 15:37:53,643 - training - INFO - Memory: GPU 0: 3556.7MB allocated, 9484.0MB reserved
2025-03-25 15:37:53,643 - training - INFO - Epoch: 936/200000, Batch: 30/45, Loss: 1.7142, Throughput: 71.75 samples/sec
2025-03-25 15:38:04,560 - training - INFO - Epoch 936 completed in 35.11s. Average loss: 1.7311
2025-03-25 15:38:04,563 - training - INFO - Starting epoch 937/200000
2025-03-25 15:38:05,310 - training - INFO - Memory: GPU 0: 3556.7MB allocated, 9622.0MB reserved
2025-03-25 15:38:05,311 - training - INFO - Epoch: 937/200000, Batch: 0/45, Loss: 1.6576, Throughput: 75.13 samples/sec
2025-03-25 15:38:17,232 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9480.0MB reserved
2025-03-25 15:38:17,233 - training - INFO - Epoch: 937/200000, Batch: 15/45, Loss: 1.8255, Throughput: 70.73 samples/sec
2025-03-25 15:38:28,863 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9480.0MB reserved
2025-03-25 15:38:28,863 - training - INFO - Epoch: 937/200000, Batch: 30/45, Loss: 1.8329, Throughput: 71.45 samples/sec
2025-03-25 15:38:39,827 - training - INFO - Epoch 937 completed in 35.26s. Average loss: 1.7811
2025-03-25 15:38:39,831 - training - INFO - Starting epoch 938/200000
2025-03-25 15:38:40,564 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9618.0MB reserved
2025-03-25 15:38:40,564 - training - INFO - Epoch: 938/200000, Batch: 0/45, Loss: 1.5385, Throughput: 76.47 samples/sec
2025-03-25 15:38:52,541 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9496.0MB reserved
2025-03-25 15:38:52,542 - training - INFO - Epoch: 938/200000, Batch: 15/45, Loss: 1.7551, Throughput: 70.50 samples/sec
2025-03-25 15:39:04,263 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9496.0MB reserved
2025-03-25 15:39:04,264 - training - INFO - Epoch: 938/200000, Batch: 30/45, Loss: 1.7417, Throughput: 71.06 samples/sec
2025-03-25 15:39:15,186 - training - INFO - Epoch 938 completed in 35.36s. Average loss: 1.7456
2025-03-25 15:39:15,190 - training - INFO - Starting epoch 939/200000
2025-03-25 15:39:15,926 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9634.0MB reserved
2025-03-25 15:39:15,926 - training - INFO - Epoch: 939/200000, Batch: 0/45, Loss: 1.6808, Throughput: 76.30 samples/sec
2025-03-25 15:39:27,888 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9508.0MB reserved
2025-03-25 15:39:27,889 - training - INFO - Epoch: 939/200000, Batch: 15/45, Loss: 1.8141, Throughput: 70.57 samples/sec
2025-03-25 15:39:39,476 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9508.0MB reserved
2025-03-25 15:39:39,476 - training - INFO - Epoch: 939/200000, Batch: 30/45, Loss: 1.7658, Throughput: 71.49 samples/sec
2025-03-25 15:39:50,407 - training - INFO - Epoch 939 completed in 35.22s. Average loss: 1.8154
2025-03-25 15:39:50,411 - training - INFO - Starting epoch 940/200000
2025-03-25 15:39:51,151 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9644.0MB reserved
2025-03-25 15:39:51,151 - training - INFO - Epoch: 940/200000, Batch: 0/45, Loss: 1.5301, Throughput: 75.76 samples/sec
2025-03-25 15:40:03,037 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9494.0MB reserved
2025-03-25 15:40:03,037 - training - INFO - Epoch: 940/200000, Batch: 15/45, Loss: 1.7775, Throughput: 70.98 samples/sec
2025-03-25 15:40:14,656 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9494.0MB reserved
2025-03-25 15:40:14,656 - training - INFO - Epoch: 940/200000, Batch: 30/45, Loss: 1.7579, Throughput: 71.61 samples/sec
2025-03-25 15:40:25,588 - training - INFO - Epoch 940 completed in 35.18s. Average loss: 1.7345
2025-03-25 15:40:25,592 - training - INFO - Starting epoch 941/200000
2025-03-25 15:40:26,336 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9632.0MB reserved
2025-03-25 15:40:26,337 - training - INFO - Epoch: 941/200000, Batch: 0/45, Loss: 1.5503, Throughput: 75.32 samples/sec
2025-03-25 15:40:38,289 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9488.0MB reserved
2025-03-25 15:40:38,290 - training - INFO - Epoch: 941/200000, Batch: 15/45, Loss: 1.7543, Throughput: 70.58 samples/sec
2025-03-25 15:40:49,952 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9488.0MB reserved
2025-03-25 15:40:49,953 - training - INFO - Epoch: 941/200000, Batch: 30/45, Loss: 1.8161, Throughput: 71.27 samples/sec
2025-03-25 15:41:00,898 - training - INFO - Epoch 941 completed in 35.31s. Average loss: 1.7506
2025-03-25 15:41:00,901 - training - INFO - Starting epoch 942/200000
2025-03-25 15:41:01,629 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9624.0MB reserved
2025-03-25 15:41:01,629 - training - INFO - Epoch: 942/200000, Batch: 0/45, Loss: 1.8621, Throughput: 77.06 samples/sec
2025-03-25 15:41:13,508 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9498.0MB reserved
2025-03-25 15:41:13,508 - training - INFO - Epoch: 942/200000, Batch: 15/45, Loss: 1.8376, Throughput: 71.08 samples/sec
2025-03-25 15:41:25,192 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9498.0MB reserved
2025-03-25 15:41:25,192 - training - INFO - Epoch: 942/200000, Batch: 30/45, Loss: 1.8559, Throughput: 71.47 samples/sec
2025-03-25 15:41:36,139 - training - INFO - Epoch 942 completed in 35.24s. Average loss: 1.8380
2025-03-25 15:41:36,142 - training - INFO - Starting epoch 943/200000
2025-03-25 15:41:36,889 - training - INFO - Memory: GPU 0: 3556.7MB allocated, 9636.0MB reserved
2025-03-25 15:41:36,889 - training - INFO - Epoch: 943/200000, Batch: 0/45, Loss: 1.6942, Throughput: 75.16 samples/sec
2025-03-25 15:41:48,799 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9510.0MB reserved
2025-03-25 15:41:48,799 - training - INFO - Epoch: 943/200000, Batch: 15/45, Loss: 1.7380, Throughput: 70.81 samples/sec
2025-03-25 15:42:00,398 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9510.0MB reserved
2025-03-25 15:42:00,399 - training - INFO - Epoch: 943/200000, Batch: 30/45, Loss: 1.7391, Throughput: 71.58 samples/sec
2025-03-25 15:42:11,329 - training - INFO - Epoch 943 completed in 35.19s. Average loss: 1.7792
2025-03-25 15:42:11,333 - training - INFO - Starting epoch 944/200000
2025-03-25 15:42:12,069 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9646.0MB reserved
2025-03-25 15:42:12,069 - training - INFO - Epoch: 944/200000, Batch: 0/45, Loss: 1.7478, Throughput: 76.32 samples/sec
2025-03-25 15:42:23,916 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9494.0MB reserved
2025-03-25 15:42:23,916 - training - INFO - Epoch: 944/200000, Batch: 15/45, Loss: 1.8251, Throughput: 71.21 samples/sec
2025-03-25 15:42:35,396 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9496.0MB reserved
2025-03-25 15:42:35,396 - training - INFO - Epoch: 944/200000, Batch: 30/45, Loss: 1.7440, Throughput: 72.15 samples/sec
2025-03-25 15:42:46,291 - training - INFO - Epoch 944 completed in 34.96s. Average loss: 1.8183
2025-03-25 15:42:46,295 - training - INFO - Starting epoch 945/200000
2025-03-25 15:42:47,054 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9634.0MB reserved
2025-03-25 15:42:47,054 - training - INFO - Epoch: 945/200000, Batch: 0/45, Loss: 2.1431, Throughput: 73.97 samples/sec
2025-03-25 15:42:58,898 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9502.0MB reserved
2025-03-25 15:42:58,898 - training - INFO - Epoch: 945/200000, Batch: 15/45, Loss: 1.7676, Throughput: 71.10 samples/sec
2025-03-25 15:43:10,514 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9502.0MB reserved
2025-03-25 15:43:10,514 - training - INFO - Epoch: 945/200000, Batch: 30/45, Loss: 1.8304, Throughput: 71.69 samples/sec
2025-03-25 15:43:21,518 - training - INFO - Epoch 945 completed in 35.22s. Average loss: 1.7345
2025-03-25 15:43:21,522 - training - INFO - Starting epoch 946/200000
2025-03-25 15:43:22,287 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9640.0MB reserved
2025-03-25 15:43:22,287 - training - INFO - Epoch: 946/200000, Batch: 0/45, Loss: 1.6632, Throughput: 73.37 samples/sec
2025-03-25 15:43:34,137 - training - INFO - Memory: GPU 0: 3555.9MB allocated, 9494.0MB reserved
2025-03-25 15:43:34,138 - training - INFO - Epoch: 946/200000, Batch: 15/45, Loss: 1.7981, Throughput: 71.04 samples/sec
2025-03-25 15:43:45,752 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9496.0MB reserved
2025-03-25 15:43:45,752 - training - INFO - Epoch: 946/200000, Batch: 30/45, Loss: 1.8086, Throughput: 71.65 samples/sec
2025-03-25 15:43:56,747 - training - INFO - Epoch 946 completed in 35.22s. Average loss: 1.7716
2025-03-25 15:43:56,751 - training - INFO - Starting epoch 947/200000
2025-03-25 15:43:57,475 - training - INFO - Memory: GPU 0: 3555.9MB allocated, 9634.0MB reserved
2025-03-25 15:43:57,476 - training - INFO - Epoch: 947/200000, Batch: 0/45, Loss: 1.4802, Throughput: 77.48 samples/sec
2025-03-25 15:44:09,400 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9506.0MB reserved
2025-03-25 15:44:09,401 - training - INFO - Epoch: 947/200000, Batch: 15/45, Loss: 1.7163, Throughput: 70.84 samples/sec
2025-03-25 15:44:20,966 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9506.0MB reserved
2025-03-25 15:44:20,966 - training - INFO - Epoch: 947/200000, Batch: 30/45, Loss: 1.7630, Throughput: 71.69 samples/sec
2025-03-25 15:44:31,861 - training - INFO - Epoch 947 completed in 35.11s. Average loss: 1.7505
2025-03-25 15:44:31,865 - training - INFO - Starting epoch 948/200000
2025-03-25 15:44:32,603 - training - INFO - Memory: GPU 0: 3556.0MB allocated, 9644.0MB reserved
2025-03-25 15:44:32,603 - training - INFO - Epoch: 948/200000, Batch: 0/45, Loss: 1.7454, Throughput: 76.09 samples/sec
2025-03-25 15:44:44,465 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9504.0MB reserved
2025-03-25 15:44:44,465 - training - INFO - Epoch: 948/200000, Batch: 15/45, Loss: 1.6296, Throughput: 71.12 samples/sec
2025-03-25 15:44:56,046 - training - INFO - Memory: GPU 0: 3562.2MB allocated, 9504.0MB reserved
2025-03-25 15:44:56,047 - training - INFO - Epoch: 948/200000, Batch: 30/45, Loss: 1.6640, Throughput: 71.80 samples/sec
2025-03-25 15:45:06,943 - training - INFO - Epoch 948 completed in 35.08s. Average loss: 1.7711
2025-03-25 15:45:06,946 - training - INFO - Starting epoch 949/200000
2025-03-25 15:45:07,681 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9642.0MB reserved
2025-03-25 15:45:07,681 - training - INFO - Epoch: 949/200000, Batch: 0/45, Loss: 1.6030, Throughput: 76.47 samples/sec
2025-03-25 15:45:19,611 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9482.0MB reserved
2025-03-25 15:45:19,611 - training - INFO - Epoch: 949/200000, Batch: 15/45, Loss: 1.7374, Throughput: 70.76 samples/sec
2025-03-25 15:45:31,191 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9482.0MB reserved
2025-03-25 15:45:31,191 - training - INFO - Epoch: 949/200000, Batch: 30/45, Loss: 1.7354, Throughput: 71.61 samples/sec
2025-03-25 15:45:42,084 - training - INFO - Epoch 949 completed in 35.14s. Average loss: 1.7849
2025-03-25 15:45:42,088 - training - INFO - Starting epoch 950/200000
2025-03-25 15:45:42,826 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9620.0MB reserved
2025-03-25 15:45:42,826 - training - INFO - Epoch: 950/200000, Batch: 0/45, Loss: 1.2873, Throughput: 76.08 samples/sec
2025-03-25 15:45:54,662 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9488.0MB reserved
2025-03-25 15:45:54,662 - training - INFO - Epoch: 950/200000, Batch: 15/45, Loss: 1.7553, Throughput: 71.27 samples/sec
2025-03-25 15:46:06,214 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9488.0MB reserved
2025-03-25 15:46:06,215 - training - INFO - Epoch: 950/200000, Batch: 30/45, Loss: 1.7425, Throughput: 71.96 samples/sec
2025-03-25 15:46:17,165 - training - INFO - Epoch 950 completed in 35.08s. Average loss: 1.7420
2025-03-25 15:46:17,169 - training - INFO - Starting epoch 951/200000
2025-03-25 15:46:17,912 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9626.0MB reserved
2025-03-25 15:46:17,913 - training - INFO - Epoch: 951/200000, Batch: 0/45, Loss: 2.0204, Throughput: 75.46 samples/sec
2025-03-25 15:46:29,831 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9500.0MB reserved
2025-03-25 15:46:29,831 - training - INFO - Epoch: 951/200000, Batch: 15/45, Loss: 1.8177, Throughput: 70.77 samples/sec
2025-03-25 15:46:41,401 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9500.0MB reserved
2025-03-25 15:46:41,402 - training - INFO - Epoch: 951/200000, Batch: 30/45, Loss: 1.7987, Throughput: 71.65 samples/sec
2025-03-25 15:46:52,322 - training - INFO - Epoch 951 completed in 35.15s. Average loss: 1.7836
2025-03-25 15:46:52,326 - training - INFO - Starting epoch 952/200000
2025-03-25 15:46:53,074 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9636.0MB reserved
2025-03-25 15:46:53,075 - training - INFO - Epoch: 952/200000, Batch: 0/45, Loss: 1.2633, Throughput: 74.99 samples/sec
2025-03-25 15:47:04,920 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9500.0MB reserved
2025-03-25 15:47:04,920 - training - INFO - Epoch: 952/200000, Batch: 15/45, Loss: 1.7091, Throughput: 71.16 samples/sec
2025-03-25 15:47:16,447 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9500.0MB reserved
2025-03-25 15:47:16,447 - training - INFO - Epoch: 952/200000, Batch: 30/45, Loss: 1.7699, Throughput: 71.98 samples/sec
2025-03-25 15:47:27,357 - training - INFO - Epoch 952 completed in 35.03s. Average loss: 1.7896
2025-03-25 15:47:27,360 - training - INFO - Starting epoch 953/200000
2025-03-25 15:47:28,095 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9636.0MB reserved
2025-03-25 15:47:28,095 - training - INFO - Epoch: 953/200000, Batch: 0/45, Loss: 1.3175, Throughput: 76.45 samples/sec
2025-03-25 15:47:39,989 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9510.0MB reserved
2025-03-25 15:47:39,989 - training - INFO - Epoch: 953/200000, Batch: 15/45, Loss: 1.7556, Throughput: 70.96 samples/sec
2025-03-25 15:47:51,592 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9510.0MB reserved
2025-03-25 15:47:51,593 - training - INFO - Epoch: 953/200000, Batch: 30/45, Loss: 1.7690, Throughput: 71.65 samples/sec
2025-03-25 15:48:02,645 - training - INFO - Epoch 953 completed in 35.28s. Average loss: 1.7212
2025-03-25 15:48:02,650 - training - INFO - Starting epoch 954/200000
2025-03-25 15:48:03,388 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9648.0MB reserved
2025-03-25 15:48:03,388 - training - INFO - Epoch: 954/200000, Batch: 0/45, Loss: 2.0139, Throughput: 76.13 samples/sec
2025-03-25 15:48:15,279 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9506.0MB reserved
2025-03-25 15:48:15,279 - training - INFO - Epoch: 954/200000, Batch: 15/45, Loss: 1.8184, Throughput: 70.96 samples/sec
2025-03-25 15:48:26,826 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9506.0MB reserved
2025-03-25 15:48:26,826 - training - INFO - Epoch: 954/200000, Batch: 30/45, Loss: 1.7758, Throughput: 71.81 samples/sec
2025-03-25 15:48:37,728 - training - INFO - Epoch 954 completed in 35.08s. Average loss: 1.7663
2025-03-25 15:48:37,732 - training - INFO - Starting epoch 955/200000
2025-03-25 15:48:38,490 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9644.0MB reserved
2025-03-25 15:48:38,490 - training - INFO - Epoch: 955/200000, Batch: 0/45, Loss: 1.5880, Throughput: 74.00 samples/sec
2025-03-25 15:48:50,410 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9498.0MB reserved
2025-03-25 15:48:50,411 - training - INFO - Epoch: 955/200000, Batch: 15/45, Loss: 1.6880, Throughput: 70.69 samples/sec
2025-03-25 15:49:01,990 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9498.0MB reserved
2025-03-25 15:49:01,991 - training - INFO - Epoch: 955/200000, Batch: 30/45, Loss: 1.7452, Throughput: 71.57 samples/sec
2025-03-25 15:49:12,942 - training - INFO - Epoch 955 completed in 35.21s. Average loss: 1.7697
2025-03-25 15:49:12,945 - training - INFO - Starting epoch 956/200000
2025-03-25 15:49:13,700 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9636.0MB reserved
2025-03-25 15:49:13,700 - training - INFO - Epoch: 956/200000, Batch: 0/45, Loss: 1.7412, Throughput: 74.40 samples/sec
2025-03-25 15:49:25,710 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9482.0MB reserved
2025-03-25 15:49:25,710 - training - INFO - Epoch: 956/200000, Batch: 15/45, Loss: 1.6694, Throughput: 70.20 samples/sec
2025-03-25 15:49:37,480 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9482.0MB reserved
2025-03-25 15:49:37,481 - training - INFO - Epoch: 956/200000, Batch: 30/45, Loss: 1.7509, Throughput: 70.76 samples/sec
2025-03-25 15:49:48,442 - training - INFO - Epoch 956 completed in 35.50s. Average loss: 1.7660
2025-03-25 15:49:48,445 - training - INFO - Starting epoch 957/200000
2025-03-25 15:49:49,178 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9620.0MB reserved
2025-03-25 15:49:49,178 - training - INFO - Epoch: 957/200000, Batch: 0/45, Loss: 1.3565, Throughput: 76.64 samples/sec
2025-03-25 15:50:01,027 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9396.0MB reserved
2025-03-25 15:50:01,028 - training - INFO - Epoch: 957/200000, Batch: 15/45, Loss: 1.7277, Throughput: 71.23 samples/sec
2025-03-25 15:50:12,557 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9396.0MB reserved
2025-03-25 15:50:12,557 - training - INFO - Epoch: 957/200000, Batch: 30/45, Loss: 1.7237, Throughput: 72.00 samples/sec
2025-03-25 15:50:23,449 - training - INFO - Epoch 957 completed in 35.00s. Average loss: 1.7733
2025-03-25 15:50:23,453 - training - INFO - Starting epoch 958/200000
2025-03-25 15:50:24,186 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9534.0MB reserved
2025-03-25 15:50:24,186 - training - INFO - Epoch: 958/200000, Batch: 0/45, Loss: 1.8857, Throughput: 76.55 samples/sec
2025-03-25 15:50:36,096 - training - INFO - Memory: GPU 0: 3562.8MB allocated, 9482.0MB reserved
2025-03-25 15:50:36,096 - training - INFO - Epoch: 958/200000, Batch: 15/45, Loss: 1.7563, Throughput: 70.87 samples/sec
2025-03-25 15:50:47,685 - training - INFO - Memory: GPU 0: 3562.8MB allocated, 9482.0MB reserved
2025-03-25 15:50:47,685 - training - INFO - Epoch: 958/200000, Batch: 30/45, Loss: 1.7147, Throughput: 71.64 samples/sec
2025-03-25 15:50:58,642 - training - INFO - Epoch 958 completed in 35.19s. Average loss: 1.7695
2025-03-25 15:50:58,646 - training - INFO - Starting epoch 959/200000
2025-03-25 15:50:59,389 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9620.0MB reserved
2025-03-25 15:50:59,389 - training - INFO - Epoch: 959/200000, Batch: 0/45, Loss: 1.3610, Throughput: 75.60 samples/sec
2025-03-25 15:51:11,375 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9486.0MB reserved
2025-03-25 15:51:11,375 - training - INFO - Epoch: 959/200000, Batch: 15/45, Loss: 1.6845, Throughput: 70.40 samples/sec
2025-03-25 15:51:23,173 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9486.0MB reserved
2025-03-25 15:51:23,173 - training - INFO - Epoch: 959/200000, Batch: 30/45, Loss: 1.7534, Throughput: 70.78 samples/sec
2025-03-25 15:51:34,176 - training - INFO - Epoch 959 completed in 35.53s. Average loss: 1.7200
2025-03-25 15:51:34,180 - training - INFO - Starting epoch 960/200000
2025-03-25 15:51:34,915 - training - INFO - Memory: GPU 0: 3560.2MB allocated, 9624.0MB reserved
2025-03-25 15:51:34,915 - training - INFO - Epoch: 960/200000, Batch: 0/45, Loss: 1.9959, Throughput: 76.41 samples/sec
2025-03-25 15:51:46,828 - training - INFO - Memory: GPU 0: 3564.3MB allocated, 9508.0MB reserved
2025-03-25 15:51:46,828 - training - INFO - Epoch: 960/200000, Batch: 15/45, Loss: 1.7690, Throughput: 70.84 samples/sec
2025-03-25 15:51:58,459 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9508.0MB reserved
2025-03-25 15:51:58,459 - training - INFO - Epoch: 960/200000, Batch: 30/45, Loss: 1.7394, Throughput: 71.51 samples/sec
2025-03-25 15:52:09,420 - training - INFO - Epoch 960 completed in 35.24s. Average loss: 1.8043
2025-03-25 15:52:09,424 - training - INFO - Starting epoch 961/200000
2025-03-25 15:52:10,174 - training - INFO - Memory: GPU 0: 3563.5MB allocated, 9646.0MB reserved
2025-03-25 15:52:10,174 - training - INFO - Epoch: 961/200000, Batch: 0/45, Loss: 1.9309, Throughput: 74.86 samples/sec
2025-03-25 15:52:22,107 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9388.0MB reserved
2025-03-25 15:52:22,107 - training - INFO - Epoch: 961/200000, Batch: 15/45, Loss: 1.8087, Throughput: 70.65 samples/sec
2025-03-25 15:52:33,746 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9388.0MB reserved
2025-03-25 15:52:33,746 - training - INFO - Epoch: 961/200000, Batch: 30/45, Loss: 1.7662, Throughput: 71.38 samples/sec
2025-03-25 15:52:44,700 - training - INFO - Epoch 961 completed in 35.28s. Average loss: 1.8081
2025-03-25 15:52:44,703 - training - INFO - Starting epoch 962/200000
2025-03-25 15:52:45,438 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9526.0MB reserved
2025-03-25 15:52:45,438 - training - INFO - Epoch: 962/200000, Batch: 0/45, Loss: 1.7178, Throughput: 76.37 samples/sec
2025-03-25 15:52:57,310 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9494.0MB reserved
2025-03-25 15:52:57,310 - training - INFO - Epoch: 962/200000, Batch: 15/45, Loss: 1.6195, Throughput: 71.08 samples/sec
2025-03-25 15:53:08,844 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9494.0MB reserved
2025-03-25 15:53:08,844 - training - INFO - Epoch: 962/200000, Batch: 30/45, Loss: 1.7045, Throughput: 71.92 samples/sec
2025-03-25 15:53:19,783 - training - INFO - Epoch 962 completed in 35.08s. Average loss: 1.7301
2025-03-25 15:53:19,787 - training - INFO - Starting epoch 963/200000
2025-03-25 15:53:20,526 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9634.0MB reserved
2025-03-25 15:53:20,526 - training - INFO - Epoch: 963/200000, Batch: 0/45, Loss: 1.9188, Throughput: 75.89 samples/sec
2025-03-25 15:53:32,555 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9498.0MB reserved
2025-03-25 15:53:32,555 - training - INFO - Epoch: 963/200000, Batch: 15/45, Loss: 1.7490, Throughput: 70.19 samples/sec
2025-03-25 15:53:44,199 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9498.0MB reserved
2025-03-25 15:53:44,200 - training - INFO - Epoch: 963/200000, Batch: 30/45, Loss: 1.7527, Throughput: 71.12 samples/sec
2025-03-25 15:53:55,171 - training - INFO - Epoch 963 completed in 35.38s. Average loss: 1.8336
2025-03-25 15:53:55,175 - training - INFO - Starting epoch 964/200000
2025-03-25 15:53:55,936 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9636.0MB reserved
2025-03-25 15:53:55,937 - training - INFO - Epoch: 964/200000, Batch: 0/45, Loss: 1.4926, Throughput: 73.76 samples/sec
2025-03-25 15:54:07,794 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9492.0MB reserved
2025-03-25 15:54:07,795 - training - INFO - Epoch: 964/200000, Batch: 15/45, Loss: 1.7892, Throughput: 71.02 samples/sec
2025-03-25 15:54:19,477 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9496.0MB reserved
2025-03-25 15:54:19,477 - training - INFO - Epoch: 964/200000, Batch: 30/45, Loss: 1.7592, Throughput: 71.44 samples/sec
2025-03-25 15:54:30,387 - training - INFO - Epoch 964 completed in 35.21s. Average loss: 1.7323
2025-03-25 15:54:30,391 - training - INFO - Starting epoch 965/200000
2025-03-25 15:54:31,147 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9632.0MB reserved
2025-03-25 15:54:31,147 - training - INFO - Epoch: 965/200000, Batch: 0/45, Loss: 1.8911, Throughput: 74.22 samples/sec
2025-03-25 15:54:42,965 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9504.0MB reserved
2025-03-25 15:54:42,965 - training - INFO - Epoch: 965/200000, Batch: 15/45, Loss: 1.7751, Throughput: 71.27 samples/sec
2025-03-25 15:54:54,510 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9504.0MB reserved
2025-03-25 15:54:54,510 - training - INFO - Epoch: 965/200000, Batch: 30/45, Loss: 1.7670, Throughput: 71.98 samples/sec
2025-03-25 15:55:05,414 - training - INFO - Epoch 965 completed in 35.02s. Average loss: 1.7586
2025-03-25 15:55:05,418 - training - INFO - Starting epoch 966/200000
2025-03-25 15:55:06,157 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9642.0MB reserved
2025-03-25 15:55:06,157 - training - INFO - Epoch: 966/200000, Batch: 0/45, Loss: 1.5944, Throughput: 75.99 samples/sec
2025-03-25 15:55:18,119 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9500.0MB reserved
2025-03-25 15:55:18,119 - training - INFO - Epoch: 966/200000, Batch: 15/45, Loss: 1.7093, Throughput: 70.56 samples/sec
2025-03-25 15:55:29,950 - training - INFO - Memory: GPU 0: 3563.0MB allocated, 9502.0MB reserved
2025-03-25 15:55:29,951 - training - INFO - Epoch: 966/200000, Batch: 30/45, Loss: 1.7167, Throughput: 70.77 samples/sec
2025-03-25 15:55:40,923 - training - INFO - Epoch 966 completed in 35.50s. Average loss: 1.8328
2025-03-25 15:55:40,926 - training - INFO - Starting epoch 967/200000
2025-03-25 15:55:41,672 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9640.0MB reserved
2025-03-25 15:55:41,672 - training - INFO - Epoch: 967/200000, Batch: 0/45, Loss: 1.9296, Throughput: 75.34 samples/sec
2025-03-25 15:55:53,558 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9400.0MB reserved
2025-03-25 15:55:53,559 - training - INFO - Epoch: 967/200000, Batch: 15/45, Loss: 1.7857, Throughput: 70.95 samples/sec
2025-03-25 15:56:05,132 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9508.0MB reserved
2025-03-25 15:56:05,133 - training - INFO - Epoch: 967/200000, Batch: 30/45, Loss: 1.7394, Throughput: 71.72 samples/sec
2025-03-25 15:56:16,061 - training - INFO - Epoch 967 completed in 35.13s. Average loss: 1.8279
2025-03-25 15:56:16,065 - training - INFO - Starting epoch 968/200000
2025-03-25 15:56:16,804 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9646.0MB reserved
2025-03-25 15:56:16,804 - training - INFO - Epoch: 968/200000, Batch: 0/45, Loss: 1.6639, Throughput: 75.96 samples/sec
2025-03-25 15:56:28,686 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9498.0MB reserved
2025-03-25 15:56:28,687 - training - INFO - Epoch: 968/200000, Batch: 15/45, Loss: 1.6956, Throughput: 71.00 samples/sec
2025-03-25 15:56:40,345 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9498.0MB reserved
2025-03-25 15:56:40,345 - training - INFO - Epoch: 968/200000, Batch: 30/45, Loss: 1.7244, Throughput: 71.50 samples/sec
2025-03-25 15:56:51,395 - training - INFO - Epoch 968 completed in 35.33s. Average loss: 1.7822
2025-03-25 15:56:51,398 - training - INFO - Starting epoch 969/200000
2025-03-25 15:56:52,166 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9636.0MB reserved
2025-03-25 15:56:52,166 - training - INFO - Epoch: 969/200000, Batch: 0/45, Loss: 1.5889, Throughput: 73.14 samples/sec
2025-03-25 15:57:04,140 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9496.0MB reserved
2025-03-25 15:57:04,140 - training - INFO - Epoch: 969/200000, Batch: 15/45, Loss: 1.7013, Throughput: 70.34 samples/sec
2025-03-25 15:57:15,798 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9496.0MB reserved
2025-03-25 15:57:15,798 - training - INFO - Epoch: 969/200000, Batch: 30/45, Loss: 1.7214, Throughput: 71.16 samples/sec
2025-03-25 15:57:26,777 - training - INFO - Epoch 969 completed in 35.38s. Average loss: 1.7346
2025-03-25 15:57:26,780 - training - INFO - Starting epoch 970/200000
2025-03-25 15:57:27,537 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9632.0MB reserved
2025-03-25 15:57:27,537 - training - INFO - Epoch: 970/200000, Batch: 0/45, Loss: 1.9227, Throughput: 74.15 samples/sec
2025-03-25 15:57:39,523 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9500.0MB reserved
2025-03-25 15:57:39,523 - training - INFO - Epoch: 970/200000, Batch: 15/45, Loss: 1.7260, Throughput: 70.33 samples/sec
2025-03-25 15:57:51,092 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9500.0MB reserved
2025-03-25 15:57:51,092 - training - INFO - Epoch: 970/200000, Batch: 30/45, Loss: 1.6930, Throughput: 71.41 samples/sec
2025-03-25 15:58:01,997 - training - INFO - Epoch 970 completed in 35.22s. Average loss: 1.7326
2025-03-25 15:58:02,001 - training - INFO - Starting epoch 971/200000
2025-03-25 15:58:02,743 - training - INFO - Memory: GPU 0: 3557.6MB allocated, 9638.0MB reserved
2025-03-25 15:58:02,743 - training - INFO - Epoch: 971/200000, Batch: 0/45, Loss: 2.0259, Throughput: 75.58 samples/sec
2025-03-25 15:58:14,734 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9510.0MB reserved
2025-03-25 15:58:14,734 - training - INFO - Epoch: 971/200000, Batch: 15/45, Loss: 1.8326, Throughput: 70.38 samples/sec
2025-03-25 15:58:26,407 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9510.0MB reserved
2025-03-25 15:58:26,407 - training - INFO - Epoch: 971/200000, Batch: 30/45, Loss: 1.7872, Throughput: 71.13 samples/sec
2025-03-25 15:58:37,305 - training - INFO - Epoch 971 completed in 35.30s. Average loss: 1.7202
2025-03-25 15:58:37,308 - training - INFO - Starting epoch 972/200000
2025-03-25 15:58:38,042 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9648.0MB reserved
2025-03-25 15:58:38,042 - training - INFO - Epoch: 972/200000, Batch: 0/45, Loss: 1.7243, Throughput: 76.56 samples/sec
2025-03-25 15:58:50,042 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9502.0MB reserved
2025-03-25 15:58:50,042 - training - INFO - Epoch: 972/200000, Batch: 15/45, Loss: 1.7151, Throughput: 70.37 samples/sec
2025-03-25 15:59:01,726 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9502.0MB reserved
2025-03-25 15:59:01,726 - training - INFO - Epoch: 972/200000, Batch: 30/45, Loss: 1.7137, Throughput: 71.10 samples/sec
2025-03-25 15:59:12,666 - training - INFO - Epoch 972 completed in 35.36s. Average loss: 1.7233
2025-03-25 15:59:12,669 - training - INFO - Starting epoch 973/200000
2025-03-25 15:59:13,428 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9642.0MB reserved
2025-03-25 15:59:13,428 - training - INFO - Epoch: 973/200000, Batch: 0/45, Loss: 1.8000, Throughput: 73.99 samples/sec
2025-03-25 15:59:25,353 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9498.0MB reserved
2025-03-25 15:59:25,354 - training - INFO - Epoch: 973/200000, Batch: 15/45, Loss: 1.7515, Throughput: 70.65 samples/sec
2025-03-25 15:59:37,065 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9498.0MB reserved
2025-03-25 15:59:37,066 - training - INFO - Epoch: 973/200000, Batch: 30/45, Loss: 1.7500, Throughput: 71.16 samples/sec
2025-03-25 15:59:47,989 - training - INFO - Epoch 973 completed in 35.32s. Average loss: 1.7579
2025-03-25 15:59:47,993 - training - INFO - Starting epoch 974/200000
2025-03-25 15:59:48,736 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9636.0MB reserved
2025-03-25 15:59:48,736 - training - INFO - Epoch: 974/200000, Batch: 0/45, Loss: 1.8825, Throughput: 75.56 samples/sec
2025-03-25 16:00:00,629 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9502.0MB reserved
2025-03-25 16:00:00,629 - training - INFO - Epoch: 974/200000, Batch: 15/45, Loss: 1.6725, Throughput: 70.92 samples/sec
2025-03-25 16:00:12,308 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9502.0MB reserved
2025-03-25 16:00:12,308 - training - INFO - Epoch: 974/200000, Batch: 30/45, Loss: 1.6946, Throughput: 71.40 samples/sec
2025-03-25 16:00:23,269 - training - INFO - Epoch 974 completed in 35.28s. Average loss: 1.7114
2025-03-25 16:00:23,272 - training - INFO - Starting epoch 975/200000
2025-03-25 16:00:24,009 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9640.0MB reserved
2025-03-25 16:00:24,009 - training - INFO - Epoch: 975/200000, Batch: 0/45, Loss: 1.4441, Throughput: 76.25 samples/sec
2025-03-25 16:00:35,996 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9496.0MB reserved
2025-03-25 16:00:35,997 - training - INFO - Epoch: 975/200000, Batch: 15/45, Loss: 1.7967, Throughput: 70.43 samples/sec
2025-03-25 16:00:47,707 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9496.0MB reserved
2025-03-25 16:00:47,708 - training - INFO - Epoch: 975/200000, Batch: 30/45, Loss: 1.7815, Throughput: 71.05 samples/sec
2025-03-25 16:00:58,698 - training - INFO - Epoch 975 completed in 35.42s. Average loss: 1.7464
2025-03-25 16:00:58,701 - training - INFO - Starting epoch 976/200000
2025-03-25 16:00:59,433 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9634.0MB reserved
2025-03-25 16:00:59,433 - training - INFO - Epoch: 976/200000, Batch: 0/45, Loss: 1.9402, Throughput: 76.76 samples/sec
2025-03-25 16:01:11,353 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9494.0MB reserved
2025-03-25 16:01:11,353 - training - INFO - Epoch: 976/200000, Batch: 15/45, Loss: 1.7392, Throughput: 70.84 samples/sec
2025-03-25 16:01:23,028 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9494.0MB reserved
2025-03-25 16:01:23,029 - training - INFO - Epoch: 976/200000, Batch: 30/45, Loss: 1.7040, Throughput: 71.37 samples/sec
2025-03-25 16:01:33,963 - training - INFO - Epoch 976 completed in 35.26s. Average loss: 1.7024
2025-03-25 16:01:33,967 - training - INFO - Starting epoch 977/200000
2025-03-25 16:01:34,701 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9632.0MB reserved
2025-03-25 16:01:34,701 - training - INFO - Epoch: 977/200000, Batch: 0/45, Loss: 1.8060, Throughput: 76.38 samples/sec
2025-03-25 16:01:46,636 - training - INFO - Memory: GPU 0: 3562.7MB allocated, 9512.0MB reserved
2025-03-25 16:01:46,636 - training - INFO - Epoch: 977/200000, Batch: 15/45, Loss: 1.7687, Throughput: 70.73 samples/sec
2025-03-25 16:01:58,357 - training - INFO - Memory: GPU 0: 3562.7MB allocated, 9512.0MB reserved
2025-03-25 16:01:58,358 - training - INFO - Epoch: 977/200000, Batch: 30/45, Loss: 1.7812, Throughput: 71.18 samples/sec
2025-03-25 16:02:09,334 - training - INFO - Epoch 977 completed in 35.37s. Average loss: 1.7597
2025-03-25 16:02:09,338 - training - INFO - Starting epoch 978/200000
2025-03-25 16:02:10,090 - training - INFO - Memory: GPU 0: 3562.5MB allocated, 9652.0MB reserved
2025-03-25 16:02:10,090 - training - INFO - Epoch: 978/200000, Batch: 0/45, Loss: 1.7266, Throughput: 74.63 samples/sec
2025-03-25 16:02:21,975 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9498.0MB reserved
2025-03-25 16:02:21,975 - training - INFO - Epoch: 978/200000, Batch: 15/45, Loss: 1.7871, Throughput: 70.91 samples/sec
2025-03-25 16:02:33,562 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9500.0MB reserved
2025-03-25 16:02:33,563 - training - INFO - Epoch: 978/200000, Batch: 30/45, Loss: 1.7823, Throughput: 71.67 samples/sec
2025-03-25 16:02:44,445 - training - INFO - Epoch 978 completed in 35.11s. Average loss: 1.7243
2025-03-25 16:02:44,448 - training - INFO - Starting epoch 979/200000
2025-03-25 16:02:45,202 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9638.0MB reserved
2025-03-25 16:02:45,202 - training - INFO - Epoch: 979/200000, Batch: 0/45, Loss: 1.9168, Throughput: 74.45 samples/sec
2025-03-25 16:02:57,212 - training - INFO - Memory: GPU 0: 3563.4MB allocated, 9490.0MB reserved
2025-03-25 16:02:57,212 - training - INFO - Epoch: 979/200000, Batch: 15/45, Loss: 1.6984, Throughput: 70.21 samples/sec
2025-03-25 16:03:08,881 - training - INFO - Memory: GPU 0: 3563.4MB allocated, 9490.0MB reserved
2025-03-25 16:03:08,881 - training - INFO - Epoch: 979/200000, Batch: 30/45, Loss: 1.7030, Throughput: 71.05 samples/sec
2025-03-25 16:03:19,857 - training - INFO - Epoch 979 completed in 35.41s. Average loss: 1.7864
2025-03-25 16:03:19,860 - training - INFO - Starting epoch 980/200000
2025-03-25 16:03:20,599 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9628.0MB reserved
2025-03-25 16:03:20,599 - training - INFO - Epoch: 980/200000, Batch: 0/45, Loss: 1.5638, Throughput: 75.96 samples/sec
2025-03-25 16:03:32,564 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9498.0MB reserved
2025-03-25 16:03:32,564 - training - INFO - Epoch: 980/200000, Batch: 15/45, Loss: 1.8291, Throughput: 70.54 samples/sec
2025-03-25 16:03:44,149 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9498.0MB reserved
2025-03-25 16:03:44,150 - training - INFO - Epoch: 980/200000, Batch: 30/45, Loss: 1.7948, Throughput: 71.48 samples/sec
2025-03-25 16:03:55,108 - training - INFO - Epoch 980 completed in 35.25s. Average loss: 1.7947
2025-03-25 16:03:55,112 - training - INFO - Starting epoch 981/200000
2025-03-25 16:03:55,849 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9636.0MB reserved
2025-03-25 16:03:55,850 - training - INFO - Epoch: 981/200000, Batch: 0/45, Loss: 1.6136, Throughput: 76.06 samples/sec
2025-03-25 16:04:07,814 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9494.0MB reserved
2025-03-25 16:04:07,815 - training - INFO - Epoch: 981/200000, Batch: 15/45, Loss: 1.6926, Throughput: 70.54 samples/sec
2025-03-25 16:04:19,541 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9496.0MB reserved
2025-03-25 16:04:19,542 - training - INFO - Epoch: 981/200000, Batch: 30/45, Loss: 1.7551, Throughput: 71.06 samples/sec
2025-03-25 16:04:30,484 - training - INFO - Epoch 981 completed in 35.37s. Average loss: 1.8303
2025-03-25 16:04:30,487 - training - INFO - Starting epoch 982/200000
2025-03-25 16:04:31,222 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9634.0MB reserved
2025-03-25 16:04:31,222 - training - INFO - Epoch: 982/200000, Batch: 0/45, Loss: 1.9959, Throughput: 76.39 samples/sec
2025-03-25 16:04:43,223 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9494.0MB reserved
2025-03-25 16:04:43,223 - training - INFO - Epoch: 982/200000, Batch: 15/45, Loss: 1.8139, Throughput: 70.36 samples/sec
2025-03-25 16:04:54,946 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9494.0MB reserved
2025-03-25 16:04:54,947 - training - INFO - Epoch: 982/200000, Batch: 30/45, Loss: 1.7933, Throughput: 70.98 samples/sec
2025-03-25 16:05:05,858 - training - INFO - Epoch 982 completed in 35.37s. Average loss: 1.7601
2025-03-25 16:05:05,862 - training - INFO - Starting epoch 983/200000
2025-03-25 16:05:06,600 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9632.0MB reserved
2025-03-25 16:05:06,601 - training - INFO - Epoch: 983/200000, Batch: 0/45, Loss: 1.5972, Throughput: 76.02 samples/sec
2025-03-25 16:05:18,561 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9506.0MB reserved
2025-03-25 16:05:18,561 - training - INFO - Epoch: 983/200000, Batch: 15/45, Loss: 1.7870, Throughput: 70.56 samples/sec
2025-03-25 16:05:30,161 - training - INFO - Memory: GPU 0: 3556.0MB allocated, 9506.0MB reserved
2025-03-25 16:05:30,162 - training - INFO - Epoch: 983/200000, Batch: 30/45, Loss: 1.7192, Throughput: 71.45 samples/sec
2025-03-25 16:05:41,174 - training - INFO - Epoch 983 completed in 35.31s. Average loss: 1.7779
2025-03-25 16:05:41,177 - training - INFO - Starting epoch 984/200000
2025-03-25 16:05:41,922 - training - INFO - Memory: GPU 0: 3556.0MB allocated, 9644.0MB reserved
2025-03-25 16:05:41,923 - training - INFO - Epoch: 984/200000, Batch: 0/45, Loss: 1.3801, Throughput: 75.37 samples/sec
2025-03-25 16:05:53,813 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9504.0MB reserved
2025-03-25 16:05:53,813 - training - INFO - Epoch: 984/200000, Batch: 15/45, Loss: 1.6912, Throughput: 70.92 samples/sec
2025-03-25 16:06:05,518 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9504.0MB reserved
2025-03-25 16:06:05,518 - training - INFO - Epoch: 984/200000, Batch: 30/45, Loss: 1.6848, Throughput: 71.32 samples/sec
2025-03-25 16:06:16,547 - training - INFO - Epoch 984 completed in 35.37s. Average loss: 1.7381
2025-03-25 16:06:16,551 - training - INFO - Starting epoch 985/200000
2025-03-25 16:06:17,281 - training - INFO - Memory: GPU 0: 3559.7MB allocated, 9642.0MB reserved
2025-03-25 16:06:17,281 - training - INFO - Epoch: 985/200000, Batch: 0/45, Loss: 1.4827, Throughput: 76.90 samples/sec
2025-03-25 16:06:29,240 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9488.0MB reserved
2025-03-25 16:06:29,241 - training - INFO - Epoch: 985/200000, Batch: 15/45, Loss: 1.6955, Throughput: 70.62 samples/sec
2025-03-25 16:06:40,865 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9488.0MB reserved
2025-03-25 16:06:40,865 - training - INFO - Epoch: 985/200000, Batch: 30/45, Loss: 1.7517, Throughput: 71.40 samples/sec
2025-03-25 16:06:51,811 - training - INFO - Epoch 985 completed in 35.26s. Average loss: 1.6891
2025-03-25 16:06:51,815 - training - INFO - Starting epoch 986/200000
2025-03-25 16:06:52,563 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9626.0MB reserved
2025-03-25 16:06:52,563 - training - INFO - Epoch: 986/200000, Batch: 0/45, Loss: 1.3654, Throughput: 75.10 samples/sec
2025-03-25 16:07:04,393 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9498.0MB reserved
2025-03-25 16:07:04,394 - training - INFO - Epoch: 986/200000, Batch: 15/45, Loss: 1.7947, Throughput: 71.24 samples/sec
2025-03-25 16:07:15,929 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9498.0MB reserved
2025-03-25 16:07:15,929 - training - INFO - Epoch: 986/200000, Batch: 30/45, Loss: 1.7848, Throughput: 72.00 samples/sec
2025-03-25 16:07:26,834 - training - INFO - Epoch 986 completed in 35.02s. Average loss: 1.7203
2025-03-25 16:07:26,837 - training - INFO - Starting epoch 987/200000
2025-03-25 16:07:27,577 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9638.0MB reserved
2025-03-25 16:07:27,577 - training - INFO - Epoch: 987/200000, Batch: 0/45, Loss: 1.8850, Throughput: 75.83 samples/sec
2025-03-25 16:07:39,440 - training - INFO - Memory: GPU 0: 3556.2MB allocated, 9510.0MB reserved
2025-03-25 16:07:39,441 - training - INFO - Epoch: 987/200000, Batch: 15/45, Loss: 1.7303, Throughput: 71.10 samples/sec
2025-03-25 16:07:50,949 - training - INFO - Memory: GPU 0: 3556.2MB allocated, 9512.0MB reserved
2025-03-25 16:07:50,949 - training - INFO - Epoch: 987/200000, Batch: 30/45, Loss: 1.7818, Throughput: 72.00 samples/sec
2025-03-25 16:08:01,953 - training - INFO - Epoch 987 completed in 35.12s. Average loss: 1.7387
2025-03-25 16:08:01,956 - training - INFO - Starting epoch 988/200000
2025-03-25 16:08:02,703 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9650.0MB reserved
2025-03-25 16:08:02,703 - training - INFO - Epoch: 988/200000, Batch: 0/45, Loss: 1.2853, Throughput: 75.11 samples/sec
2025-03-25 16:08:14,598 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9496.0MB reserved
2025-03-25 16:08:14,598 - training - INFO - Epoch: 988/200000, Batch: 15/45, Loss: 1.7536, Throughput: 70.88 samples/sec
2025-03-25 16:08:26,132 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9496.0MB reserved
2025-03-25 16:08:26,132 - training - INFO - Epoch: 988/200000, Batch: 30/45, Loss: 1.7486, Throughput: 71.81 samples/sec
2025-03-25 16:08:37,065 - training - INFO - Epoch 988 completed in 35.11s. Average loss: 1.7986
2025-03-25 16:08:37,068 - training - INFO - Starting epoch 989/200000
2025-03-25 16:08:37,814 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9634.0MB reserved
2025-03-25 16:08:37,815 - training - INFO - Epoch: 989/200000, Batch: 0/45, Loss: 2.0309, Throughput: 75.25 samples/sec
2025-03-25 16:08:49,663 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9508.0MB reserved
2025-03-25 16:08:49,663 - training - INFO - Epoch: 989/200000, Batch: 15/45, Loss: 1.7133, Throughput: 71.16 samples/sec
2025-03-25 16:09:01,328 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9510.0MB reserved
2025-03-25 16:09:01,328 - training - INFO - Epoch: 989/200000, Batch: 30/45, Loss: 1.7481, Throughput: 71.57 samples/sec
2025-03-25 16:09:12,296 - training - INFO - Epoch 989 completed in 35.23s. Average loss: 1.7399
2025-03-25 16:09:12,300 - training - INFO - Starting epoch 990/200000
2025-03-25 16:09:13,055 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9646.0MB reserved
2025-03-25 16:09:13,055 - training - INFO - Epoch: 990/200000, Batch: 0/45, Loss: 1.7277, Throughput: 74.32 samples/sec
2025-03-25 16:09:24,899 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9500.0MB reserved
2025-03-25 16:09:24,900 - training - INFO - Epoch: 990/200000, Batch: 15/45, Loss: 1.7368, Throughput: 71.12 samples/sec
2025-03-25 16:09:36,454 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9500.0MB reserved
2025-03-25 16:09:36,455 - training - INFO - Epoch: 990/200000, Batch: 30/45, Loss: 1.7437, Throughput: 71.88 samples/sec
2025-03-25 16:09:47,387 - training - INFO - Epoch 990 completed in 35.09s. Average loss: 1.7428
2025-03-25 16:09:47,391 - training - INFO - Starting epoch 991/200000
2025-03-25 16:09:48,132 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9638.0MB reserved
2025-03-25 16:09:48,133 - training - INFO - Epoch: 991/200000, Batch: 0/45, Loss: 1.6707, Throughput: 75.74 samples/sec
2025-03-25 16:10:00,014 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9506.0MB reserved
2025-03-25 16:10:00,015 - training - INFO - Epoch: 991/200000, Batch: 15/45, Loss: 1.7842, Throughput: 70.99 samples/sec
2025-03-25 16:10:11,631 - training - INFO - Memory: GPU 0: 3563.3MB allocated, 9506.0MB reserved
2025-03-25 16:10:11,631 - training - INFO - Epoch: 991/200000, Batch: 30/45, Loss: 1.7236, Throughput: 71.62 samples/sec
2025-03-25 16:10:22,557 - training - INFO - Epoch 991 completed in 35.17s. Average loss: 1.7456
2025-03-25 16:10:22,561 - training - INFO - Starting epoch 992/200000
2025-03-25 16:10:23,289 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9644.0MB reserved
2025-03-25 16:10:23,289 - training - INFO - Epoch: 992/200000, Batch: 0/45, Loss: 1.8882, Throughput: 77.03 samples/sec
2025-03-25 16:10:35,234 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9500.0MB reserved
2025-03-25 16:10:35,234 - training - INFO - Epoch: 992/200000, Batch: 15/45, Loss: 1.6968, Throughput: 70.71 samples/sec
2025-03-25 16:10:46,895 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9500.0MB reserved
2025-03-25 16:10:46,895 - training - INFO - Epoch: 992/200000, Batch: 30/45, Loss: 1.7264, Throughput: 71.34 samples/sec
2025-03-25 16:10:57,878 - training - INFO - Epoch 992 completed in 35.32s. Average loss: 1.7367
2025-03-25 16:10:57,881 - training - INFO - Starting epoch 993/200000
2025-03-25 16:10:58,624 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9636.0MB reserved
2025-03-25 16:10:58,624 - training - INFO - Epoch: 993/200000, Batch: 0/45, Loss: 2.4323, Throughput: 75.59 samples/sec
2025-03-25 16:11:10,461 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9504.0MB reserved
2025-03-25 16:11:10,462 - training - INFO - Epoch: 993/200000, Batch: 15/45, Loss: 1.7043, Throughput: 71.23 samples/sec
2025-03-25 16:11:22,028 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9506.0MB reserved
2025-03-25 16:11:22,028 - training - INFO - Epoch: 993/200000, Batch: 30/45, Loss: 1.7459, Throughput: 71.90 samples/sec
2025-03-25 16:11:32,956 - training - INFO - Epoch 993 completed in 35.07s. Average loss: 1.7776
2025-03-25 16:11:32,960 - training - INFO - Starting epoch 994/200000
2025-03-25 16:11:33,708 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9644.0MB reserved
2025-03-25 16:11:33,708 - training - INFO - Epoch: 994/200000, Batch: 0/45, Loss: 1.8148, Throughput: 74.90 samples/sec
2025-03-25 16:11:45,662 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9504.0MB reserved
2025-03-25 16:11:45,662 - training - INFO - Epoch: 994/200000, Batch: 15/45, Loss: 1.7943, Throughput: 70.55 samples/sec
2025-03-25 16:11:57,290 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9504.0MB reserved
2025-03-25 16:11:57,290 - training - INFO - Epoch: 994/200000, Batch: 30/45, Loss: 1.7687, Throughput: 71.35 samples/sec
2025-03-25 16:12:08,223 - training - INFO - Epoch 994 completed in 35.26s. Average loss: 1.7425
2025-03-25 16:12:08,226 - training - INFO - Starting epoch 995/200000
2025-03-25 16:12:08,975 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9642.0MB reserved
2025-03-25 16:12:08,975 - training - INFO - Epoch: 995/200000, Batch: 0/45, Loss: 1.7891, Throughput: 74.94 samples/sec
2025-03-25 16:12:20,917 - training - INFO - Memory: GPU 0: 3563.3MB allocated, 9506.0MB reserved
2025-03-25 16:12:20,917 - training - INFO - Epoch: 995/200000, Batch: 15/45, Loss: 1.6663, Throughput: 70.62 samples/sec
2025-03-25 16:12:32,581 - training - INFO - Memory: GPU 0: 3563.3MB allocated, 9506.0MB reserved
2025-03-25 16:12:32,582 - training - INFO - Epoch: 995/200000, Batch: 30/45, Loss: 1.6996, Throughput: 71.29 samples/sec
2025-03-25 16:12:43,545 - training - INFO - Epoch 995 completed in 35.32s. Average loss: 1.6890
2025-03-25 16:12:43,549 - training - INFO - Starting epoch 996/200000
2025-03-25 16:12:44,272 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9644.0MB reserved
2025-03-25 16:12:44,272 - training - INFO - Epoch: 996/200000, Batch: 0/45, Loss: 1.7750, Throughput: 77.63 samples/sec
2025-03-25 16:12:56,155 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9508.0MB reserved
2025-03-25 16:12:56,156 - training - INFO - Epoch: 996/200000, Batch: 15/45, Loss: 1.7354, Throughput: 71.08 samples/sec
2025-03-25 16:13:07,742 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9510.0MB reserved
2025-03-25 16:13:07,742 - training - INFO - Epoch: 996/200000, Batch: 30/45, Loss: 1.7560, Throughput: 71.76 samples/sec
2025-03-25 16:13:18,648 - training - INFO - Epoch 996 completed in 35.10s. Average loss: 1.7135
2025-03-25 16:13:18,652 - training - INFO - Starting epoch 997/200000
2025-03-25 16:13:19,389 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9648.0MB reserved
2025-03-25 16:13:19,389 - training - INFO - Epoch: 997/200000, Batch: 0/45, Loss: 1.7924, Throughput: 76.17 samples/sec
2025-03-25 16:13:31,296 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9512.0MB reserved
2025-03-25 16:13:31,296 - training - INFO - Epoch: 997/200000, Batch: 15/45, Loss: 1.6843, Throughput: 70.87 samples/sec
2025-03-25 16:13:43,083 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9512.0MB reserved
2025-03-25 16:13:43,083 - training - INFO - Epoch: 997/200000, Batch: 30/45, Loss: 1.7026, Throughput: 71.06 samples/sec
2025-03-25 16:13:54,072 - training - INFO - Epoch 997 completed in 35.42s. Average loss: 1.7431
2025-03-25 16:13:54,076 - training - INFO - Starting epoch 998/200000
2025-03-25 16:13:54,818 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9650.0MB reserved
2025-03-25 16:13:54,819 - training - INFO - Epoch: 998/200000, Batch: 0/45, Loss: 1.4805, Throughput: 75.58 samples/sec
2025-03-25 16:14:06,707 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9494.0MB reserved
2025-03-25 16:14:06,707 - training - INFO - Epoch: 998/200000, Batch: 15/45, Loss: 1.6740, Throughput: 70.95 samples/sec
2025-03-25 16:14:18,227 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9494.0MB reserved
2025-03-25 16:14:18,228 - training - INFO - Epoch: 998/200000, Batch: 30/45, Loss: 1.7260, Throughput: 71.89 samples/sec
2025-03-25 16:14:29,149 - training - INFO - Epoch 998 completed in 35.07s. Average loss: 1.7468
2025-03-25 16:14:29,153 - training - INFO - Starting epoch 999/200000
2025-03-25 16:14:29,905 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9632.0MB reserved
2025-03-25 16:14:30,015 - training - INFO - Epoch: 999/200000, Batch: 0/45, Loss: 1.4811, Throughput: 74.57 samples/sec
2025-03-25 16:14:41,846 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9486.0MB reserved
2025-03-25 16:14:41,847 - training - INFO - Epoch: 999/200000, Batch: 15/45, Loss: 1.7117, Throughput: 70.60 samples/sec
2025-03-25 16:14:53,464 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9486.0MB reserved
2025-03-25 16:14:53,465 - training - INFO - Epoch: 999/200000, Batch: 30/45, Loss: 1.7540, Throughput: 71.41 samples/sec
2025-03-25 16:15:04,424 - training - INFO - Epoch 999 completed in 35.27s. Average loss: 1.7228
2025-03-25 16:15:04,428 - training - INFO - Starting epoch 1000/200000
2025-03-25 16:15:05,176 - training - INFO - Memory: GPU 0: 3557.5MB allocated, 9624.0MB reserved
2025-03-25 16:15:05,177 - training - INFO - Epoch: 1000/200000, Batch: 0/45, Loss: 1.7538, Throughput: 74.92 samples/sec
2025-03-25 16:15:17,085 - training - INFO - Memory: GPU 0: 3556.0MB allocated, 9512.0MB reserved
2025-03-25 16:15:17,086 - training - INFO - Epoch: 1000/200000, Batch: 15/45, Loss: 1.6622, Throughput: 70.80 samples/sec
2025-03-25 16:15:28,668 - training - INFO - Memory: GPU 0: 3556.0MB allocated, 9512.0MB reserved
2025-03-25 16:15:28,668 - training - INFO - Epoch: 1000/200000, Batch: 30/45, Loss: 1.6651, Throughput: 71.62 samples/sec
2025-03-25 16:15:39,592 - training - INFO - Epoch 1000 completed in 35.16s. Average loss: 1.7060
2025-03-25 16:15:39,595 - training - INFO - Starting validation...
2025-03-25 16:15:39,908 - training - INFO - Validation Loss: 23.5875
2025-03-25 16:15:39,908 - training - INFO - Validation loss did not improve. Counter: 9/10
2025-03-25 16:15:40,238 - training - INFO - Starting epoch 1001/200000
2025-03-25 16:15:41,002 - training - INFO - Memory: GPU 0: 3565.6MB allocated, 8742.0MB reserved
2025-03-25 16:15:41,002 - training - INFO - Epoch: 1001/200000, Batch: 0/45, Loss: 1.5675, Throughput: 73.46 samples/sec
2025-03-25 16:15:52,759 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9476.0MB reserved
2025-03-25 16:15:52,759 - training - INFO - Epoch: 1001/200000, Batch: 15/45, Loss: 1.7538, Throughput: 71.57 samples/sec
2025-03-25 16:16:04,377 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9478.0MB reserved
2025-03-25 16:16:04,377 - training - INFO - Epoch: 1001/200000, Batch: 30/45, Loss: 1.7221, Throughput: 71.92 samples/sec
2025-03-25 16:16:15,293 - training - INFO - Epoch 1001 completed in 35.05s. Average loss: 1.7555
2025-03-25 16:16:15,296 - training - INFO - Starting epoch 1002/200000
2025-03-25 16:16:16,022 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9616.0MB reserved
2025-03-25 16:16:16,023 - training - INFO - Epoch: 1002/200000, Batch: 0/45, Loss: 1.5764, Throughput: 77.32 samples/sec
2025-03-25 16:16:27,847 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9478.0MB reserved
2025-03-25 16:16:27,847 - training - INFO - Epoch: 1002/200000, Batch: 15/45, Loss: 1.7909, Throughput: 71.40 samples/sec
2025-03-25 16:16:39,354 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9478.0MB reserved
2025-03-25 16:16:39,354 - training - INFO - Epoch: 1002/200000, Batch: 30/45, Loss: 1.7969, Throughput: 72.17 samples/sec
2025-03-25 16:16:50,389 - training - INFO - Epoch 1002 completed in 35.09s. Average loss: 1.7325
2025-03-25 16:16:50,393 - training - INFO - Starting epoch 1003/200000
2025-03-25 16:16:51,155 - training - INFO - Memory: GPU 0: 3563.9MB allocated, 9618.0MB reserved
2025-03-25 16:16:51,156 - training - INFO - Epoch: 1003/200000, Batch: 0/45, Loss: 1.7964, Throughput: 73.80 samples/sec
2025-03-25 16:17:03,057 - training - INFO - Memory: GPU 0: 3562.5MB allocated, 9500.0MB reserved
2025-03-25 16:17:03,058 - training - INFO - Epoch: 1003/200000, Batch: 15/45, Loss: 1.6703, Throughput: 70.76 samples/sec
2025-03-25 16:17:14,579 - training - INFO - Memory: GPU 0: 3562.5MB allocated, 9500.0MB reserved
2025-03-25 16:17:14,580 - training - INFO - Epoch: 1003/200000, Batch: 30/45, Loss: 1.7350, Throughput: 71.78 samples/sec
2025-03-25 16:17:25,505 - training - INFO - Epoch 1003 completed in 35.11s. Average loss: 1.7115
2025-03-25 16:17:25,508 - training - INFO - Starting epoch 1004/200000
2025-03-25 16:17:26,238 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9638.0MB reserved
2025-03-25 16:17:26,239 - training - INFO - Epoch: 1004/200000, Batch: 0/45, Loss: 1.7193, Throughput: 76.88 samples/sec
2025-03-25 16:17:38,207 - training - INFO - Memory: GPU 0: 3562.8MB allocated, 9494.0MB reserved
2025-03-25 16:17:38,207 - training - INFO - Epoch: 1004/200000, Batch: 15/45, Loss: 1.7614, Throughput: 70.57 samples/sec
2025-03-25 16:17:49,897 - training - INFO - Memory: GPU 0: 3562.8MB allocated, 9494.0MB reserved
2025-03-25 16:17:49,897 - training - INFO - Epoch: 1004/200000, Batch: 30/45, Loss: 1.7444, Throughput: 71.19 samples/sec
2025-03-25 16:18:00,841 - training - INFO - Epoch 1004 completed in 35.33s. Average loss: 1.7199
2025-03-25 16:18:00,845 - training - INFO - Starting epoch 1005/200000
2025-03-25 16:18:01,578 - training - INFO - Memory: GPU 0: 3564.3MB allocated, 9630.0MB reserved
2025-03-25 16:18:01,579 - training - INFO - Epoch: 1005/200000, Batch: 0/45, Loss: 1.4856, Throughput: 76.45 samples/sec
2025-03-25 16:18:13,488 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9492.0MB reserved
2025-03-25 16:18:13,488 - training - INFO - Epoch: 1005/200000, Batch: 15/45, Loss: 1.7936, Throughput: 70.88 samples/sec
2025-03-25 16:18:25,113 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9492.0MB reserved
2025-03-25 16:18:25,113 - training - INFO - Epoch: 1005/200000, Batch: 30/45, Loss: 1.7956, Throughput: 71.54 samples/sec
2025-03-25 16:18:36,052 - training - INFO - Epoch 1005 completed in 35.21s. Average loss: 1.7574
2025-03-25 16:18:36,055 - training - INFO - Starting epoch 1006/200000
2025-03-25 16:18:36,793 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9630.0MB reserved
2025-03-25 16:18:36,793 - training - INFO - Epoch: 1006/200000, Batch: 0/45, Loss: 2.3969, Throughput: 76.05 samples/sec
2025-03-25 16:18:48,630 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9496.0MB reserved
2025-03-25 16:18:48,631 - training - INFO - Epoch: 1006/200000, Batch: 15/45, Loss: 1.8806, Throughput: 71.26 samples/sec
2025-03-25 16:19:00,219 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9498.0MB reserved
2025-03-25 16:19:00,220 - training - INFO - Epoch: 1006/200000, Batch: 30/45, Loss: 1.8462, Throughput: 71.85 samples/sec
2025-03-25 16:19:11,204 - training - INFO - Epoch 1006 completed in 35.15s. Average loss: 1.7875
2025-03-25 16:19:11,208 - training - INFO - Starting epoch 1007/200000
2025-03-25 16:19:11,935 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9636.0MB reserved
2025-03-25 16:19:11,936 - training - INFO - Epoch: 1007/200000, Batch: 0/45, Loss: 1.7002, Throughput: 77.23 samples/sec
2025-03-25 16:19:23,855 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9486.0MB reserved
2025-03-25 16:19:23,856 - training - INFO - Epoch: 1007/200000, Batch: 15/45, Loss: 1.7478, Throughput: 70.86 samples/sec
2025-03-25 16:19:35,508 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9486.0MB reserved
2025-03-25 16:19:35,509 - training - INFO - Epoch: 1007/200000, Batch: 30/45, Loss: 1.6996, Throughput: 71.45 samples/sec
2025-03-25 16:19:46,451 - training - INFO - Epoch 1007 completed in 35.24s. Average loss: 1.7865
2025-03-25 16:19:46,455 - training - INFO - Starting epoch 1008/200000
2025-03-25 16:19:47,199 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9622.0MB reserved
2025-03-25 16:19:47,199 - training - INFO - Epoch: 1008/200000, Batch: 0/45, Loss: 1.5548, Throughput: 75.43 samples/sec
2025-03-25 16:19:59,035 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9506.0MB reserved
2025-03-25 16:19:59,035 - training - INFO - Epoch: 1008/200000, Batch: 15/45, Loss: 1.6157, Throughput: 71.24 samples/sec
2025-03-25 16:20:10,688 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9506.0MB reserved
2025-03-25 16:20:10,688 - training - INFO - Epoch: 1008/200000, Batch: 30/45, Loss: 1.6960, Throughput: 71.64 samples/sec
2025-03-25 16:20:21,682 - training - INFO - Epoch 1008 completed in 35.23s. Average loss: 1.7427
2025-03-25 16:20:21,686 - training - INFO - Starting epoch 1009/200000
2025-03-25 16:20:22,427 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9644.0MB reserved
2025-03-25 16:20:22,427 - training - INFO - Epoch: 1009/200000, Batch: 0/45, Loss: 1.5058, Throughput: 75.75 samples/sec
2025-03-25 16:20:34,442 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9504.0MB reserved
2025-03-25 16:20:34,442 - training - INFO - Epoch: 1009/200000, Batch: 15/45, Loss: 1.6811, Throughput: 70.25 samples/sec
2025-03-25 16:20:46,105 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9504.0MB reserved
2025-03-25 16:20:46,106 - training - INFO - Epoch: 1009/200000, Batch: 30/45, Loss: 1.6775, Throughput: 71.10 samples/sec
2025-03-25 16:20:57,075 - training - INFO - Epoch 1009 completed in 35.39s. Average loss: 1.7172
2025-03-25 16:20:57,079 - training - INFO - Starting epoch 1010/200000
2025-03-25 16:20:57,861 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9642.0MB reserved
2025-03-25 16:20:57,861 - training - INFO - Epoch: 1010/200000, Batch: 0/45, Loss: 1.7285, Throughput: 71.76 samples/sec
2025-03-25 16:21:09,719 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9492.0MB reserved
2025-03-25 16:21:09,720 - training - INFO - Epoch: 1010/200000, Batch: 15/45, Loss: 1.6858, Throughput: 70.89 samples/sec
2025-03-25 16:21:21,271 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9492.0MB reserved
2025-03-25 16:21:21,271 - training - INFO - Epoch: 1010/200000, Batch: 30/45, Loss: 1.7099, Throughput: 71.76 samples/sec
2025-03-25 16:21:32,157 - training - INFO - Epoch 1010 completed in 35.08s. Average loss: 1.7869
2025-03-25 16:21:32,161 - training - INFO - Starting epoch 1011/200000
2025-03-25 16:21:32,908 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9632.0MB reserved
2025-03-25 16:21:32,908 - training - INFO - Epoch: 1011/200000, Batch: 0/45, Loss: 1.6129, Throughput: 75.05 samples/sec
2025-03-25 16:21:44,909 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9502.0MB reserved
2025-03-25 16:21:44,909 - training - INFO - Epoch: 1011/200000, Batch: 15/45, Loss: 1.7095, Throughput: 70.29 samples/sec
2025-03-25 16:21:56,632 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9502.0MB reserved
2025-03-25 16:21:56,632 - training - INFO - Epoch: 1011/200000, Batch: 30/45, Loss: 1.7025, Throughput: 70.94 samples/sec
2025-03-25 16:22:07,644 - training - INFO - Epoch 1011 completed in 35.48s. Average loss: 1.7130
2025-03-25 16:22:07,647 - training - INFO - Starting epoch 1012/200000
2025-03-25 16:22:08,413 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9640.0MB reserved
2025-03-25 16:22:08,413 - training - INFO - Epoch: 1012/200000, Batch: 0/45, Loss: 1.7118, Throughput: 73.37 samples/sec
2025-03-25 16:22:20,333 - training - INFO - Memory: GPU 0: 3556.9MB allocated, 9504.0MB reserved
2025-03-25 16:22:20,333 - training - INFO - Epoch: 1012/200000, Batch: 15/45, Loss: 1.6920, Throughput: 70.64 samples/sec
2025-03-25 16:22:31,920 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9504.0MB reserved
2025-03-25 16:22:31,921 - training - INFO - Epoch: 1012/200000, Batch: 30/45, Loss: 1.7047, Throughput: 71.53 samples/sec
2025-03-25 16:22:42,840 - training - INFO - Epoch 1012 completed in 35.19s. Average loss: 1.7247
2025-03-25 16:22:42,844 - training - INFO - Starting epoch 1013/200000
2025-03-25 16:22:43,579 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9642.0MB reserved
2025-03-25 16:22:43,579 - training - INFO - Epoch: 1013/200000, Batch: 0/45, Loss: 1.8502, Throughput: 76.36 samples/sec
2025-03-25 16:22:55,374 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9504.0MB reserved
2025-03-25 16:22:55,374 - training - INFO - Epoch: 1013/200000, Batch: 15/45, Loss: 1.7063, Throughput: 71.52 samples/sec
2025-03-25 16:23:06,975 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9504.0MB reserved
2025-03-25 16:23:06,975 - training - INFO - Epoch: 1013/200000, Batch: 30/45, Loss: 1.7345, Throughput: 71.95 samples/sec
2025-03-25 16:23:17,967 - training - INFO - Epoch 1013 completed in 35.12s. Average loss: 1.7318
2025-03-25 16:23:17,971 - training - INFO - Starting epoch 1014/200000
2025-03-25 16:23:18,712 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9642.0MB reserved
2025-03-25 16:23:18,712 - training - INFO - Epoch: 1014/200000, Batch: 0/45, Loss: 2.1435, Throughput: 75.65 samples/sec
2025-03-25 16:23:30,704 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9492.0MB reserved
2025-03-25 16:23:30,705 - training - INFO - Epoch: 1014/200000, Batch: 15/45, Loss: 1.7629, Throughput: 70.38 samples/sec
2025-03-25 16:23:42,292 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9492.0MB reserved
2025-03-25 16:23:42,292 - training - INFO - Epoch: 1014/200000, Batch: 30/45, Loss: 1.7649, Throughput: 71.38 samples/sec
2025-03-25 16:23:53,207 - training - INFO - Epoch 1014 completed in 35.24s. Average loss: 1.7503
2025-03-25 16:23:53,210 - training - INFO - Starting epoch 1015/200000
2025-03-25 16:23:53,946 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9628.0MB reserved
2025-03-25 16:23:53,946 - training - INFO - Epoch: 1015/200000, Batch: 0/45, Loss: 2.0835, Throughput: 76.27 samples/sec
2025-03-25 16:24:05,875 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9488.0MB reserved
2025-03-25 16:24:05,875 - training - INFO - Epoch: 1015/200000, Batch: 15/45, Loss: 1.7876, Throughput: 70.75 samples/sec
2025-03-25 16:24:17,468 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9488.0MB reserved
2025-03-25 16:24:17,468 - training - INFO - Epoch: 1015/200000, Batch: 30/45, Loss: 1.7447, Throughput: 71.57 samples/sec
2025-03-25 16:24:28,393 - training - INFO - Epoch 1015 completed in 35.18s. Average loss: 1.6933
2025-03-25 16:24:28,397 - training - INFO - Starting epoch 1016/200000
2025-03-25 16:24:29,147 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9626.0MB reserved
2025-03-25 16:24:29,147 - training - INFO - Epoch: 1016/200000, Batch: 0/45, Loss: 1.8600, Throughput: 74.84 samples/sec
2025-03-25 16:24:40,979 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9494.0MB reserved
2025-03-25 16:24:40,979 - training - INFO - Epoch: 1016/200000, Batch: 15/45, Loss: 1.7257, Throughput: 71.22 samples/sec
2025-03-25 16:24:52,569 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9494.0MB reserved
2025-03-25 16:24:52,569 - training - INFO - Epoch: 1016/200000, Batch: 30/45, Loss: 1.8077, Throughput: 71.82 samples/sec
2025-03-25 16:25:03,463 - training - INFO - Epoch 1016 completed in 35.07s. Average loss: 1.7516
2025-03-25 16:25:03,467 - training - INFO - Starting epoch 1017/200000
2025-03-25 16:25:04,206 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9632.0MB reserved
2025-03-25 16:25:04,206 - training - INFO - Epoch: 1017/200000, Batch: 0/45, Loss: 2.0307, Throughput: 75.88 samples/sec
2025-03-25 16:25:16,064 - training - INFO - Memory: GPU 0: 3563.7MB allocated, 9482.0MB reserved
2025-03-25 16:25:16,064 - training - INFO - Epoch: 1017/200000, Batch: 15/45, Loss: 1.7952, Throughput: 71.14 samples/sec
2025-03-25 16:25:27,642 - training - INFO - Memory: GPU 0: 3563.7MB allocated, 9484.0MB reserved
2025-03-25 16:25:27,643 - training - INFO - Epoch: 1017/200000, Batch: 30/45, Loss: 1.7066, Throughput: 71.81 samples/sec
2025-03-25 16:25:38,575 - training - INFO - Epoch 1017 completed in 35.11s. Average loss: 1.7829
2025-03-25 16:25:38,579 - training - INFO - Starting epoch 1018/200000
2025-03-25 16:25:39,325 - training - INFO - Memory: GPU 0: 3563.7MB allocated, 9620.0MB reserved
2025-03-25 16:25:39,325 - training - INFO - Epoch: 1018/200000, Batch: 0/45, Loss: 1.8481, Throughput: 75.18 samples/sec
2025-03-25 16:25:51,189 - training - INFO - Memory: GPU 0: 3562.9MB allocated, 9498.0MB reserved
2025-03-25 16:25:51,189 - training - INFO - Epoch: 1018/200000, Batch: 15/45, Loss: 1.6473, Throughput: 71.06 samples/sec
2025-03-25 16:26:02,722 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9500.0MB reserved
2025-03-25 16:26:02,722 - training - INFO - Epoch: 1018/200000, Batch: 30/45, Loss: 1.7120, Throughput: 71.91 samples/sec
2025-03-25 16:26:13,642 - training - INFO - Epoch 1018 completed in 35.06s. Average loss: 1.7031
2025-03-25 16:26:13,645 - training - INFO - Starting epoch 1019/200000
2025-03-25 16:26:14,409 - training - INFO - Memory: GPU 0: 3562.9MB allocated, 9636.0MB reserved
2025-03-25 16:26:14,409 - training - INFO - Epoch: 1019/200000, Batch: 0/45, Loss: 1.6691, Throughput: 73.39 samples/sec
2025-03-25 16:26:26,365 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9490.0MB reserved
2025-03-25 16:26:26,366 - training - INFO - Epoch: 1019/200000, Batch: 15/45, Loss: 1.7806, Throughput: 70.45 samples/sec
2025-03-25 16:26:37,977 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9490.0MB reserved
2025-03-25 16:26:37,977 - training - INFO - Epoch: 1019/200000, Batch: 30/45, Loss: 1.7167, Throughput: 71.35 samples/sec
2025-03-25 16:26:48,935 - training - INFO - Epoch 1019 completed in 35.29s. Average loss: 1.6888
2025-03-25 16:26:48,939 - training - INFO - Starting epoch 1020/200000
2025-03-25 16:26:49,679 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9626.0MB reserved
2025-03-25 16:26:49,679 - training - INFO - Epoch: 1020/200000, Batch: 0/45, Loss: 2.1897, Throughput: 75.77 samples/sec
2025-03-25 16:27:01,523 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9504.0MB reserved
2025-03-25 16:27:01,524 - training - INFO - Epoch: 1020/200000, Batch: 15/45, Loss: 1.7707, Throughput: 71.21 samples/sec
2025-03-25 16:27:13,069 - training - INFO - Memory: GPU 0: 3560.5MB allocated, 9504.0MB reserved
2025-03-25 16:27:13,069 - training - INFO - Epoch: 1020/200000, Batch: 30/45, Loss: 1.7058, Throughput: 71.95 samples/sec
2025-03-25 16:27:24,037 - training - INFO - Epoch 1020 completed in 35.10s. Average loss: 1.7098
2025-03-25 16:27:24,040 - training - INFO - Starting epoch 1021/200000
2025-03-25 16:27:24,803 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9642.0MB reserved
2025-03-25 16:27:24,803 - training - INFO - Epoch: 1021/200000, Batch: 0/45, Loss: 1.7798, Throughput: 73.55 samples/sec
2025-03-25 16:27:36,763 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9498.0MB reserved
2025-03-25 16:27:36,763 - training - INFO - Epoch: 1021/200000, Batch: 15/45, Loss: 1.6309, Throughput: 70.44 samples/sec
2025-03-25 16:27:48,344 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9498.0MB reserved
2025-03-25 16:27:48,345 - training - INFO - Epoch: 1021/200000, Batch: 30/45, Loss: 1.6610, Throughput: 71.44 samples/sec
2025-03-25 16:27:59,254 - training - INFO - Epoch 1021 completed in 35.21s. Average loss: 1.7157
2025-03-25 16:27:59,258 - training - INFO - Starting epoch 1022/200000
2025-03-25 16:28:00,007 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9636.0MB reserved
2025-03-25 16:28:00,007 - training - INFO - Epoch: 1022/200000, Batch: 0/45, Loss: 1.7458, Throughput: 74.96 samples/sec
2025-03-25 16:28:11,888 - training - INFO - Memory: GPU 0: 3559.3MB allocated, 9504.0MB reserved
2025-03-25 16:28:11,889 - training - INFO - Epoch: 1022/200000, Batch: 15/45, Loss: 1.7304, Throughput: 70.94 samples/sec
2025-03-25 16:28:23,586 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9504.0MB reserved
2025-03-25 16:28:23,586 - training - INFO - Epoch: 1022/200000, Batch: 30/45, Loss: 1.7145, Throughput: 71.37 samples/sec
2025-03-25 16:28:34,578 - training - INFO - Epoch 1022 completed in 35.32s. Average loss: 1.6934
2025-03-25 16:28:34,582 - training - INFO - Starting epoch 1023/200000
2025-03-25 16:28:35,321 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9642.0MB reserved
2025-03-25 16:28:35,322 - training - INFO - Epoch: 1023/200000, Batch: 0/45, Loss: 1.7444, Throughput: 75.96 samples/sec
2025-03-25 16:28:47,295 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9494.0MB reserved
2025-03-25 16:28:47,296 - training - INFO - Epoch: 1023/200000, Batch: 15/45, Loss: 1.7254, Throughput: 70.49 samples/sec
2025-03-25 16:28:58,904 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9494.0MB reserved
2025-03-25 16:28:58,904 - training - INFO - Epoch: 1023/200000, Batch: 30/45, Loss: 1.7294, Throughput: 71.38 samples/sec
2025-03-25 16:29:09,819 - training - INFO - Epoch 1023 completed in 35.24s. Average loss: 1.7080
2025-03-25 16:29:09,823 - training - INFO - Starting epoch 1024/200000
2025-03-25 16:29:10,558 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9632.0MB reserved
2025-03-25 16:29:10,558 - training - INFO - Epoch: 1024/200000, Batch: 0/45, Loss: 1.4007, Throughput: 76.39 samples/sec
2025-03-25 16:29:22,539 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9514.0MB reserved
2025-03-25 16:29:22,540 - training - INFO - Epoch: 1024/200000, Batch: 15/45, Loss: 1.6649, Throughput: 70.47 samples/sec
2025-03-25 16:29:34,222 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9516.0MB reserved
2025-03-25 16:29:34,223 - training - INFO - Epoch: 1024/200000, Batch: 30/45, Loss: 1.7037, Throughput: 71.15 samples/sec
2025-03-25 16:29:45,200 - training - INFO - Epoch 1024 completed in 35.38s. Average loss: 1.7064
2025-03-25 16:29:45,204 - training - INFO - Starting epoch 1025/200000
2025-03-25 16:29:45,947 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9654.0MB reserved
2025-03-25 16:29:45,948 - training - INFO - Epoch: 1025/200000, Batch: 0/45, Loss: 1.3322, Throughput: 75.49 samples/sec
2025-03-25 16:29:57,976 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9510.0MB reserved
2025-03-25 16:29:57,976 - training - INFO - Epoch: 1025/200000, Batch: 15/45, Loss: 1.7162, Throughput: 70.16 samples/sec
2025-03-25 16:30:09,756 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9510.0MB reserved
2025-03-25 16:30:09,757 - training - INFO - Epoch: 1025/200000, Batch: 30/45, Loss: 1.7151, Throughput: 70.71 samples/sec
2025-03-25 16:30:20,783 - training - INFO - Epoch 1025 completed in 35.58s. Average loss: 1.7517
2025-03-25 16:30:20,786 - training - INFO - Starting epoch 1026/200000
2025-03-25 16:30:21,519 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9646.0MB reserved
2025-03-25 16:30:21,520 - training - INFO - Epoch: 1026/200000, Batch: 0/45, Loss: 2.1602, Throughput: 76.57 samples/sec
2025-03-25 16:30:33,373 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9518.0MB reserved
2025-03-25 16:30:33,374 - training - INFO - Epoch: 1026/200000, Batch: 15/45, Loss: 1.7427, Throughput: 71.19 samples/sec
2025-03-25 16:30:44,898 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9518.0MB reserved
2025-03-25 16:30:44,898 - training - INFO - Epoch: 1026/200000, Batch: 30/45, Loss: 1.7414, Throughput: 72.00 samples/sec
2025-03-25 16:30:55,890 - training - INFO - Epoch 1026 completed in 35.10s. Average loss: 1.7040
2025-03-25 16:30:55,894 - training - INFO - Starting epoch 1027/200000
2025-03-25 16:30:56,629 - training - INFO - Memory: GPU 0: 3560.7MB allocated, 9654.0MB reserved
2025-03-25 16:30:56,630 - training - INFO - Epoch: 1027/200000, Batch: 0/45, Loss: 1.6495, Throughput: 76.20 samples/sec
2025-03-25 16:31:08,629 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9502.0MB reserved
2025-03-25 16:31:08,629 - training - INFO - Epoch: 1027/200000, Batch: 15/45, Loss: 1.6809, Throughput: 70.37 samples/sec
2025-03-25 16:31:20,324 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9502.0MB reserved
2025-03-25 16:31:20,325 - training - INFO - Epoch: 1027/200000, Batch: 30/45, Loss: 1.6858, Throughput: 71.06 samples/sec
2025-03-25 16:31:31,353 - training - INFO - Epoch 1027 completed in 35.46s. Average loss: 1.7513
2025-03-25 16:31:31,357 - training - INFO - Starting epoch 1028/200000
2025-03-25 16:31:32,114 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9640.0MB reserved
2025-03-25 16:31:32,114 - training - INFO - Epoch: 1028/200000, Batch: 0/45, Loss: 1.4157, Throughput: 74.15 samples/sec
2025-03-25 16:31:44,050 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9490.0MB reserved
2025-03-25 16:31:44,050 - training - INFO - Epoch: 1028/200000, Batch: 15/45, Loss: 1.7464, Throughput: 70.60 samples/sec
2025-03-25 16:31:55,694 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9490.0MB reserved
2025-03-25 16:31:55,695 - training - INFO - Epoch: 1028/200000, Batch: 30/45, Loss: 1.7166, Throughput: 71.34 samples/sec
2025-03-25 16:32:06,637 - training - INFO - Epoch 1028 completed in 35.28s. Average loss: 1.6573
2025-03-25 16:32:06,641 - training - INFO - Starting epoch 1029/200000
2025-03-25 16:32:07,375 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9626.0MB reserved
2025-03-25 16:32:07,375 - training - INFO - Epoch: 1029/200000, Batch: 0/45, Loss: 1.7178, Throughput: 76.39 samples/sec
2025-03-25 16:32:19,205 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9510.0MB reserved
2025-03-25 16:32:19,205 - training - INFO - Epoch: 1029/200000, Batch: 15/45, Loss: 1.6692, Throughput: 71.32 samples/sec
2025-03-25 16:32:30,737 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9510.0MB reserved
2025-03-25 16:32:30,737 - training - INFO - Epoch: 1029/200000, Batch: 30/45, Loss: 1.6755, Throughput: 72.05 samples/sec
2025-03-25 16:32:41,665 - training - INFO - Epoch 1029 completed in 35.02s. Average loss: 1.6801
2025-03-25 16:32:41,669 - training - INFO - Starting epoch 1030/200000
2025-03-25 16:32:42,409 - training - INFO - Memory: GPU 0: 3562.4MB allocated, 9648.0MB reserved
2025-03-25 16:32:42,410 - training - INFO - Epoch: 1030/200000, Batch: 0/45, Loss: 1.7611, Throughput: 75.67 samples/sec
2025-03-25 16:32:54,368 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9492.0MB reserved
2025-03-25 16:32:54,368 - training - INFO - Epoch: 1030/200000, Batch: 15/45, Loss: 1.7006, Throughput: 70.57 samples/sec
2025-03-25 16:33:06,007 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9492.0MB reserved
2025-03-25 16:33:06,008 - training - INFO - Epoch: 1030/200000, Batch: 30/45, Loss: 1.7493, Throughput: 71.33 samples/sec
2025-03-25 16:33:17,029 - training - INFO - Epoch 1030 completed in 35.36s. Average loss: 1.7346
2025-03-25 16:33:17,033 - training - INFO - Starting epoch 1031/200000
2025-03-25 16:33:17,768 - training - INFO - Memory: GPU 0: 3558.2MB allocated, 9632.0MB reserved
2025-03-25 16:33:17,768 - training - INFO - Epoch: 1031/200000, Batch: 0/45, Loss: 1.7555, Throughput: 76.33 samples/sec
2025-03-25 16:33:29,676 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9500.0MB reserved
2025-03-25 16:33:29,677 - training - INFO - Epoch: 1031/200000, Batch: 15/45, Loss: 1.7842, Throughput: 70.88 samples/sec
2025-03-25 16:33:41,279 - training - INFO - Memory: GPU 0: 3558.5MB allocated, 9500.0MB reserved
2025-03-25 16:33:41,279 - training - INFO - Epoch: 1031/200000, Batch: 30/45, Loss: 1.7507, Throughput: 71.61 samples/sec
2025-03-25 16:33:52,220 - training - INFO - Epoch 1031 completed in 35.19s. Average loss: 1.7748
2025-03-25 16:33:52,223 - training - INFO - Starting epoch 1032/200000
2025-03-25 16:33:52,953 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9636.0MB reserved
2025-03-25 16:33:52,953 - training - INFO - Epoch: 1032/200000, Batch: 0/45, Loss: 1.4026, Throughput: 76.89 samples/sec
2025-03-25 16:34:04,896 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9402.0MB reserved
2025-03-25 16:34:04,897 - training - INFO - Epoch: 1032/200000, Batch: 15/45, Loss: 1.8080, Throughput: 70.71 samples/sec
2025-03-25 16:34:16,504 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9508.0MB reserved
2025-03-25 16:34:16,504 - training - INFO - Epoch: 1032/200000, Batch: 30/45, Loss: 1.7551, Throughput: 71.50 samples/sec
2025-03-25 16:34:27,441 - training - INFO - Epoch 1032 completed in 35.22s. Average loss: 1.6906
2025-03-25 16:34:27,444 - training - INFO - Starting epoch 1033/200000
2025-03-25 16:34:28,178 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9646.0MB reserved
2025-03-25 16:34:28,178 - training - INFO - Epoch: 1033/200000, Batch: 0/45, Loss: 1.6398, Throughput: 76.49 samples/sec
2025-03-25 16:34:40,044 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9502.0MB reserved
2025-03-25 16:34:40,044 - training - INFO - Epoch: 1033/200000, Batch: 15/45, Loss: 1.6727, Throughput: 71.12 samples/sec
2025-03-25 16:34:51,667 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9504.0MB reserved
2025-03-25 16:34:51,667 - training - INFO - Epoch: 1033/200000, Batch: 30/45, Loss: 1.6830, Throughput: 71.67 samples/sec
2025-03-25 16:35:02,656 - training - INFO - Epoch 1033 completed in 35.21s. Average loss: 1.7331
2025-03-25 16:35:02,660 - training - INFO - Starting epoch 1034/200000
2025-03-25 16:35:03,396 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9642.0MB reserved
2025-03-25 16:35:03,396 - training - INFO - Epoch: 1034/200000, Batch: 0/45, Loss: 1.4522, Throughput: 76.18 samples/sec
2025-03-25 16:35:15,419 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9496.0MB reserved
2025-03-25 16:35:15,419 - training - INFO - Epoch: 1034/200000, Batch: 15/45, Loss: 1.8045, Throughput: 70.24 samples/sec
2025-03-25 16:35:27,093 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9498.0MB reserved
2025-03-25 16:35:27,094 - training - INFO - Epoch: 1034/200000, Batch: 30/45, Loss: 1.7548, Throughput: 71.06 samples/sec
2025-03-25 16:35:38,045 - training - INFO - Epoch 1034 completed in 35.38s. Average loss: 1.7000
2025-03-25 16:35:38,048 - training - INFO - Starting epoch 1035/200000
2025-03-25 16:35:38,795 - training - INFO - Memory: GPU 0: 3561.7MB allocated, 9636.0MB reserved
2025-03-25 16:35:38,795 - training - INFO - Epoch: 1035/200000, Batch: 0/45, Loss: 1.6412, Throughput: 75.18 samples/sec
2025-03-25 16:35:50,709 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9494.0MB reserved
2025-03-25 16:35:50,709 - training - INFO - Epoch: 1035/200000, Batch: 15/45, Loss: 1.7107, Throughput: 70.78 samples/sec
2025-03-25 16:36:02,320 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9494.0MB reserved
2025-03-25 16:36:02,321 - training - INFO - Epoch: 1035/200000, Batch: 30/45, Loss: 1.7198, Throughput: 71.53 samples/sec
2025-03-25 16:36:13,274 - training - INFO - Epoch 1035 completed in 35.23s. Average loss: 1.7410
2025-03-25 16:36:13,277 - training - INFO - Starting epoch 1036/200000
2025-03-25 16:36:14,012 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9632.0MB reserved
2025-03-25 16:36:14,012 - training - INFO - Epoch: 1036/200000, Batch: 0/45, Loss: 1.5227, Throughput: 76.36 samples/sec
2025-03-25 16:36:25,815 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9494.0MB reserved
2025-03-25 16:36:25,816 - training - INFO - Epoch: 1036/200000, Batch: 15/45, Loss: 1.7093, Throughput: 71.47 samples/sec
2025-03-25 16:36:37,404 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9494.0MB reserved
2025-03-25 16:36:37,404 - training - INFO - Epoch: 1036/200000, Batch: 30/45, Loss: 1.7273, Throughput: 71.96 samples/sec
2025-03-25 16:36:48,368 - training - INFO - Epoch 1036 completed in 35.09s. Average loss: 1.7051
2025-03-25 16:36:48,372 - training - INFO - Starting epoch 1037/200000
2025-03-25 16:36:49,111 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9632.0MB reserved
2025-03-25 16:36:49,111 - training - INFO - Epoch: 1037/200000, Batch: 0/45, Loss: 1.7264, Throughput: 75.91 samples/sec
2025-03-25 16:37:01,079 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9506.0MB reserved
2025-03-25 16:37:01,080 - training - INFO - Epoch: 1037/200000, Batch: 15/45, Loss: 1.6670, Throughput: 70.52 samples/sec
2025-03-25 16:37:12,797 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9506.0MB reserved
2025-03-25 16:37:12,798 - training - INFO - Epoch: 1037/200000, Batch: 30/45, Loss: 1.7071, Throughput: 71.08 samples/sec
2025-03-25 16:37:23,727 - training - INFO - Epoch 1037 completed in 35.36s. Average loss: 1.7122
2025-03-25 16:37:23,731 - training - INFO - Starting epoch 1038/200000
2025-03-25 16:37:24,481 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9646.0MB reserved
2025-03-25 16:37:24,481 - training - INFO - Epoch: 1038/200000, Batch: 0/45, Loss: 1.8881, Throughput: 74.77 samples/sec
2025-03-25 16:37:36,439 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9494.0MB reserved
2025-03-25 16:37:36,439 - training - INFO - Epoch: 1038/200000, Batch: 15/45, Loss: 1.7050, Throughput: 70.51 samples/sec
2025-03-25 16:37:48,127 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9494.0MB reserved
2025-03-25 16:37:48,127 - training - INFO - Epoch: 1038/200000, Batch: 30/45, Loss: 1.7104, Throughput: 71.16 samples/sec
2025-03-25 16:37:59,091 - training - INFO - Epoch 1038 completed in 35.36s. Average loss: 1.7422
2025-03-25 16:37:59,095 - training - INFO - Starting epoch 1039/200000
2025-03-25 16:37:59,831 - training - INFO - Memory: GPU 0: 3561.8MB allocated, 9632.0MB reserved
2025-03-25 16:37:59,831 - training - INFO - Epoch: 1039/200000, Batch: 0/45, Loss: 1.8862, Throughput: 76.19 samples/sec
2025-03-25 16:38:11,704 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9492.0MB reserved
2025-03-25 16:38:11,704 - training - INFO - Epoch: 1039/200000, Batch: 15/45, Loss: 1.6930, Throughput: 71.07 samples/sec
2025-03-25 16:38:23,346 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9492.0MB reserved
2025-03-25 16:38:23,347 - training - INFO - Epoch: 1039/200000, Batch: 30/45, Loss: 1.7007, Throughput: 71.59 samples/sec
2025-03-25 16:38:34,268 - training - INFO - Epoch 1039 completed in 35.17s. Average loss: 1.6766
2025-03-25 16:38:34,272 - training - INFO - Starting epoch 1040/200000
2025-03-25 16:38:35,008 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9630.0MB reserved
2025-03-25 16:38:35,008 - training - INFO - Epoch: 1040/200000, Batch: 0/45, Loss: 1.5939, Throughput: 76.31 samples/sec
2025-03-25 16:38:46,858 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9502.0MB reserved
2025-03-25 16:38:46,859 - training - INFO - Epoch: 1040/200000, Batch: 15/45, Loss: 1.6420, Throughput: 71.20 samples/sec
2025-03-25 16:38:58,503 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9502.0MB reserved
2025-03-25 16:38:58,504 - training - INFO - Epoch: 1040/200000, Batch: 30/45, Loss: 1.6880, Throughput: 71.65 samples/sec
2025-03-25 16:39:09,451 - training - INFO - Epoch 1040 completed in 35.18s. Average loss: 1.7263
2025-03-25 16:39:09,455 - training - INFO - Starting epoch 1041/200000
2025-03-25 16:39:10,208 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9640.0MB reserved
2025-03-25 16:39:10,209 - training - INFO - Epoch: 1041/200000, Batch: 0/45, Loss: 1.7503, Throughput: 74.51 samples/sec
2025-03-25 16:39:22,141 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9502.0MB reserved
2025-03-25 16:39:22,141 - training - INFO - Epoch: 1041/200000, Batch: 15/45, Loss: 1.6408, Throughput: 70.64 samples/sec
2025-03-25 16:39:33,779 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9504.0MB reserved
2025-03-25 16:39:33,779 - training - INFO - Epoch: 1041/200000, Batch: 30/45, Loss: 1.6402, Throughput: 71.38 samples/sec
2025-03-25 16:39:44,685 - training - INFO - Epoch 1041 completed in 35.23s. Average loss: 1.6508
2025-03-25 16:39:44,689 - training - INFO - Starting epoch 1042/200000
2025-03-25 16:39:45,439 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9642.0MB reserved
2025-03-25 16:39:45,439 - training - INFO - Epoch: 1042/200000, Batch: 0/45, Loss: 1.5974, Throughput: 74.87 samples/sec
2025-03-25 16:39:57,443 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9506.0MB reserved
2025-03-25 16:39:57,444 - training - INFO - Epoch: 1042/200000, Batch: 15/45, Loss: 1.7429, Throughput: 70.26 samples/sec
2025-03-25 16:40:09,161 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9506.0MB reserved
2025-03-25 16:40:09,161 - training - INFO - Epoch: 1042/200000, Batch: 30/45, Loss: 1.7293, Throughput: 70.94 samples/sec
2025-03-25 16:40:20,139 - training - INFO - Epoch 1042 completed in 35.45s. Average loss: 1.6897
2025-03-25 16:40:20,142 - training - INFO - Starting epoch 1043/200000
2025-03-25 16:40:20,870 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9644.0MB reserved
2025-03-25 16:40:20,870 - training - INFO - Epoch: 1043/200000, Batch: 0/45, Loss: 1.5261, Throughput: 77.03 samples/sec
2025-03-25 16:40:32,744 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9498.0MB reserved
2025-03-25 16:40:32,745 - training - INFO - Epoch: 1043/200000, Batch: 15/45, Loss: 1.6395, Throughput: 71.12 samples/sec
2025-03-25 16:40:44,437 - training - INFO - Memory: GPU 0: 3561.0MB allocated, 9500.0MB reserved
2025-03-25 16:40:44,437 - training - INFO - Epoch: 1043/200000, Batch: 30/45, Loss: 1.6770, Throughput: 71.46 samples/sec
2025-03-25 16:40:55,425 - training - INFO - Epoch 1043 completed in 35.28s. Average loss: 1.7282
2025-03-25 16:40:55,428 - training - INFO - Starting epoch 1044/200000
2025-03-25 16:40:56,158 - training - INFO - Memory: GPU 0: 3559.5MB allocated, 9638.0MB reserved
2025-03-25 16:40:56,158 - training - INFO - Epoch: 1044/200000, Batch: 0/45, Loss: 1.6713, Throughput: 76.89 samples/sec
2025-03-25 16:41:08,010 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9486.0MB reserved
2025-03-25 16:41:08,011 - training - INFO - Epoch: 1044/200000, Batch: 15/45, Loss: 1.6413, Throughput: 71.22 samples/sec
2025-03-25 16:41:19,695 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9486.0MB reserved
2025-03-25 16:41:19,695 - training - INFO - Epoch: 1044/200000, Batch: 30/45, Loss: 1.6376, Throughput: 71.54 samples/sec
2025-03-25 16:41:30,696 - training - INFO - Epoch 1044 completed in 35.27s. Average loss: 1.6812
2025-03-25 16:41:30,700 - training - INFO - Starting epoch 1045/200000
2025-03-25 16:41:31,438 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9624.0MB reserved
2025-03-25 16:41:31,438 - training - INFO - Epoch: 1045/200000, Batch: 0/45, Loss: 1.7888, Throughput: 75.96 samples/sec
2025-03-25 16:41:43,308 - training - INFO - Memory: GPU 0: 3556.6MB allocated, 9500.0MB reserved
2025-03-25 16:41:43,309 - training - INFO - Epoch: 1045/200000, Batch: 15/45, Loss: 1.6847, Throughput: 71.07 samples/sec
2025-03-25 16:41:54,899 - training - INFO - Memory: GPU 0: 3556.6MB allocated, 9500.0MB reserved
2025-03-25 16:41:54,899 - training - INFO - Epoch: 1045/200000, Batch: 30/45, Loss: 1.7021, Throughput: 71.74 samples/sec
2025-03-25 16:42:05,866 - training - INFO - Epoch 1045 completed in 35.17s. Average loss: 1.7516
2025-03-25 16:42:05,870 - training - INFO - Starting epoch 1046/200000
2025-03-25 16:42:06,609 - training - INFO - Memory: GPU 0: 3556.9MB allocated, 9638.0MB reserved
2025-03-25 16:42:06,609 - training - INFO - Epoch: 1046/200000, Batch: 0/45, Loss: 1.9776, Throughput: 75.99 samples/sec
2025-03-25 16:42:18,606 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9492.0MB reserved
2025-03-25 16:42:18,607 - training - INFO - Epoch: 1046/200000, Batch: 15/45, Loss: 1.6450, Throughput: 70.36 samples/sec
2025-03-25 16:42:30,210 - training - INFO - Memory: GPU 0: 3557.0MB allocated, 9494.0MB reserved
2025-03-25 16:42:30,211 - training - INFO - Epoch: 1046/200000, Batch: 30/45, Loss: 1.6675, Throughput: 71.33 samples/sec
2025-03-25 16:42:41,106 - training - INFO - Epoch 1046 completed in 35.24s. Average loss: 1.7849
2025-03-25 16:42:41,109 - training - INFO - Starting epoch 1047/200000
2025-03-25 16:42:41,856 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9632.0MB reserved
2025-03-25 16:42:41,857 - training - INFO - Epoch: 1047/200000, Batch: 0/45, Loss: 2.1208, Throughput: 75.24 samples/sec
2025-03-25 16:42:53,724 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9504.0MB reserved
2025-03-25 16:42:53,724 - training - INFO - Epoch: 1047/200000, Batch: 15/45, Loss: 1.7335, Throughput: 71.04 samples/sec
2025-03-25 16:43:05,362 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9504.0MB reserved
2025-03-25 16:43:05,362 - training - INFO - Epoch: 1047/200000, Batch: 30/45, Loss: 1.6950, Throughput: 71.59 samples/sec
2025-03-25 16:43:16,325 - training - INFO - Epoch 1047 completed in 35.22s. Average loss: 1.7039
2025-03-25 16:43:16,329 - training - INFO - Starting epoch 1048/200000
2025-03-25 16:43:17,094 - training - INFO - Memory: GPU 0: 3562.3MB allocated, 9642.0MB reserved
2025-03-25 16:43:17,095 - training - INFO - Epoch: 1048/200000, Batch: 0/45, Loss: 1.6909, Throughput: 73.25 samples/sec
2025-03-25 16:43:28,930 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9400.0MB reserved
2025-03-25 16:43:28,930 - training - INFO - Epoch: 1048/200000, Batch: 15/45, Loss: 1.6989, Throughput: 71.11 samples/sec
2025-03-25 16:43:40,507 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9400.0MB reserved
2025-03-25 16:43:40,508 - training - INFO - Epoch: 1048/200000, Batch: 30/45, Loss: 1.7147, Throughput: 71.80 samples/sec
2025-03-25 16:43:51,432 - training - INFO - Epoch 1048 completed in 35.10s. Average loss: 1.7210
2025-03-25 16:43:51,436 - training - INFO - Starting epoch 1049/200000
2025-03-25 16:43:52,176 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9536.0MB reserved
2025-03-25 16:43:52,177 - training - INFO - Epoch: 1049/200000, Batch: 0/45, Loss: 1.7041, Throughput: 75.81 samples/sec
2025-03-25 16:44:04,117 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9406.0MB reserved
2025-03-25 16:44:04,117 - training - INFO - Epoch: 1049/200000, Batch: 15/45, Loss: 1.6967, Throughput: 70.66 samples/sec
2025-03-25 16:44:15,772 - training - INFO - Memory: GPU 0: 3560.3MB allocated, 9408.0MB reserved
2025-03-25 16:44:15,772 - training - INFO - Epoch: 1049/200000, Batch: 30/45, Loss: 1.7345, Throughput: 71.34 samples/sec
2025-03-25 16:44:26,739 - training - INFO - Epoch 1049 completed in 35.30s. Average loss: 1.7682
2025-03-25 16:44:26,743 - training - INFO - Starting epoch 1050/200000
2025-03-25 16:44:27,497 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9546.0MB reserved
2025-03-25 16:44:27,497 - training - INFO - Epoch: 1050/200000, Batch: 0/45, Loss: 1.8337, Throughput: 74.32 samples/sec
2025-03-25 16:44:39,491 - training - INFO - Memory: GPU 0: 3556.7MB allocated, 9498.0MB reserved
2025-03-25 16:44:39,491 - training - INFO - Epoch: 1050/200000, Batch: 15/45, Loss: 1.6483, Throughput: 70.30 samples/sec
2025-03-25 16:44:51,088 - training - INFO - Memory: GPU 0: 3556.7MB allocated, 9498.0MB reserved
2025-03-25 16:44:51,088 - training - INFO - Epoch: 1050/200000, Batch: 30/45, Loss: 1.6867, Throughput: 71.31 samples/sec
2025-03-25 16:45:01,978 - training - INFO - Epoch 1050 completed in 35.24s. Average loss: 1.7876
2025-03-25 16:45:01,983 - training - INFO - Starting epoch 1051/200000
2025-03-25 16:45:02,731 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9636.0MB reserved
2025-03-25 16:45:02,731 - training - INFO - Epoch: 1051/200000, Batch: 0/45, Loss: 1.7909, Throughput: 75.04 samples/sec
2025-03-25 16:45:14,612 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9496.0MB reserved
2025-03-25 16:45:14,613 - training - INFO - Epoch: 1051/200000, Batch: 15/45, Loss: 1.7239, Throughput: 70.96 samples/sec
2025-03-25 16:45:26,158 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9496.0MB reserved
2025-03-25 16:45:26,158 - training - INFO - Epoch: 1051/200000, Batch: 30/45, Loss: 1.6886, Throughput: 71.82 samples/sec
2025-03-25 16:45:37,079 - training - INFO - Epoch 1051 completed in 35.10s. Average loss: 1.6906
2025-03-25 16:45:37,082 - training - INFO - Starting epoch 1052/200000
2025-03-25 16:45:37,823 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9636.0MB reserved
2025-03-25 16:45:37,824 - training - INFO - Epoch: 1052/200000, Batch: 0/45, Loss: 2.2542, Throughput: 75.77 samples/sec
2025-03-25 16:45:49,679 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9516.0MB reserved
2025-03-25 16:45:49,680 - training - INFO - Epoch: 1052/200000, Batch: 15/45, Loss: 1.8130, Throughput: 71.14 samples/sec
2025-03-25 16:46:01,213 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9516.0MB reserved
2025-03-25 16:46:01,213 - training - INFO - Epoch: 1052/200000, Batch: 30/45, Loss: 1.7496, Throughput: 71.95 samples/sec
2025-03-25 16:46:12,182 - training - INFO - Epoch 1052 completed in 35.10s. Average loss: 1.7290
2025-03-25 16:46:12,186 - training - INFO - Starting epoch 1053/200000
2025-03-25 16:46:12,929 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9654.0MB reserved
2025-03-25 16:46:12,929 - training - INFO - Epoch: 1053/200000, Batch: 0/45, Loss: 1.6148, Throughput: 75.48 samples/sec
2025-03-25 16:46:24,916 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9486.0MB reserved
2025-03-25 16:46:24,917 - training - INFO - Epoch: 1053/200000, Batch: 15/45, Loss: 1.7520, Throughput: 70.39 samples/sec
2025-03-25 16:46:36,627 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9486.0MB reserved
2025-03-25 16:46:36,627 - training - INFO - Epoch: 1053/200000, Batch: 30/45, Loss: 1.7333, Throughput: 71.03 samples/sec
2025-03-25 16:46:47,575 - training - INFO - Epoch 1053 completed in 35.39s. Average loss: 1.7181
2025-03-25 16:46:47,579 - training - INFO - Starting epoch 1054/200000
2025-03-25 16:46:48,313 - training - INFO - Memory: GPU 0: 3563.7MB allocated, 9624.0MB reserved
2025-03-25 16:46:48,313 - training - INFO - Epoch: 1054/200000, Batch: 0/45, Loss: 1.8641, Throughput: 76.51 samples/sec
2025-03-25 16:47:00,202 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9492.0MB reserved
2025-03-25 16:47:00,203 - training - INFO - Epoch: 1054/200000, Batch: 15/45, Loss: 1.7792, Throughput: 71.00 samples/sec
2025-03-25 16:47:11,886 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9492.0MB reserved
2025-03-25 16:47:11,886 - training - INFO - Epoch: 1054/200000, Batch: 30/45, Loss: 1.7899, Throughput: 71.43 samples/sec
2025-03-25 16:47:22,856 - training - INFO - Epoch 1054 completed in 35.28s. Average loss: 1.7075
2025-03-25 16:47:22,860 - training - INFO - Starting epoch 1055/200000
2025-03-25 16:47:23,618 - training - INFO - Memory: GPU 0: 3561.3MB allocated, 9632.0MB reserved
2025-03-25 16:47:23,618 - training - INFO - Epoch: 1055/200000, Batch: 0/45, Loss: 1.1852, Throughput: 74.06 samples/sec
2025-03-25 16:47:35,514 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9492.0MB reserved
2025-03-25 16:47:35,514 - training - INFO - Epoch: 1055/200000, Batch: 15/45, Loss: 1.6404, Throughput: 70.82 samples/sec
2025-03-25 16:47:47,133 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9494.0MB reserved
2025-03-25 16:47:47,133 - training - INFO - Epoch: 1055/200000, Batch: 30/45, Loss: 1.6967, Throughput: 71.52 samples/sec
2025-03-25 16:47:58,047 - training - INFO - Epoch 1055 completed in 35.19s. Average loss: 1.7632
2025-03-25 16:47:58,051 - training - INFO - Starting epoch 1056/200000
2025-03-25 16:47:58,785 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9632.0MB reserved
2025-03-25 16:47:58,785 - training - INFO - Epoch: 1056/200000, Batch: 0/45, Loss: 1.9174, Throughput: 76.40 samples/sec
2025-03-25 16:48:10,636 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9502.0MB reserved
2025-03-25 16:48:10,637 - training - INFO - Epoch: 1056/200000, Batch: 15/45, Loss: 1.7001, Throughput: 71.20 samples/sec
2025-03-25 16:48:22,173 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9502.0MB reserved
2025-03-25 16:48:22,173 - training - INFO - Epoch: 1056/200000, Batch: 30/45, Loss: 1.7209, Throughput: 71.97 samples/sec
2025-03-25 16:48:33,105 - training - INFO - Epoch 1056 completed in 35.05s. Average loss: 1.7274
2025-03-25 16:48:33,110 - training - INFO - Starting epoch 1057/200000
2025-03-25 16:48:33,874 - training - INFO - Memory: GPU 0: 3560.6MB allocated, 9640.0MB reserved
2025-03-25 16:48:33,874 - training - INFO - Epoch: 1057/200000, Batch: 0/45, Loss: 1.7507, Throughput: 73.42 samples/sec
2025-03-25 16:48:45,745 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9490.0MB reserved
2025-03-25 16:48:45,746 - training - INFO - Epoch: 1057/200000, Batch: 15/45, Loss: 1.6596, Throughput: 70.93 samples/sec
2025-03-25 16:48:57,318 - training - INFO - Memory: GPU 0: 3559.2MB allocated, 9490.0MB reserved
2025-03-25 16:48:57,318 - training - INFO - Epoch: 1057/200000, Batch: 30/45, Loss: 1.7040, Throughput: 71.71 samples/sec
2025-03-25 16:49:08,268 - training - INFO - Epoch 1057 completed in 35.16s. Average loss: 1.6763
2025-03-25 16:49:08,271 - training - INFO - Starting epoch 1058/200000
2025-03-25 16:49:09,010 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9628.0MB reserved
2025-03-25 16:49:09,010 - training - INFO - Epoch: 1058/200000, Batch: 0/45, Loss: 1.6266, Throughput: 75.89 samples/sec
2025-03-25 16:49:21,004 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9486.0MB reserved
2025-03-25 16:49:21,005 - training - INFO - Epoch: 1058/200000, Batch: 15/45, Loss: 1.8500, Throughput: 70.38 samples/sec
2025-03-25 16:49:32,741 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9486.0MB reserved
2025-03-25 16:49:32,741 - training - INFO - Epoch: 1058/200000, Batch: 30/45, Loss: 1.7496, Throughput: 70.95 samples/sec
2025-03-25 16:49:43,746 - training - INFO - Epoch 1058 completed in 35.47s. Average loss: 1.6979
2025-03-25 16:49:43,750 - training - INFO - Starting epoch 1059/200000
2025-03-25 16:49:44,482 - training - INFO - Memory: GPU 0: 3558.7MB allocated, 9624.0MB reserved
2025-03-25 16:49:44,483 - training - INFO - Epoch: 1059/200000, Batch: 0/45, Loss: 1.7341, Throughput: 76.63 samples/sec
2025-03-25 16:49:56,356 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9496.0MB reserved
2025-03-25 16:49:56,357 - training - INFO - Epoch: 1059/200000, Batch: 15/45, Loss: 1.7427, Throughput: 71.09 samples/sec
2025-03-25 16:50:07,873 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9498.0MB reserved
2025-03-25 16:50:07,874 - training - INFO - Epoch: 1059/200000, Batch: 30/45, Loss: 1.6883, Throughput: 71.97 samples/sec
2025-03-25 16:50:18,803 - training - INFO - Epoch 1059 completed in 35.05s. Average loss: 1.6661
2025-03-25 16:50:18,807 - training - INFO - Starting epoch 1060/200000
2025-03-25 16:50:19,548 - training - INFO - Memory: GPU 0: 3558.6MB allocated, 9636.0MB reserved
2025-03-25 16:50:19,548 - training - INFO - Epoch: 1060/200000, Batch: 0/45, Loss: 2.5914, Throughput: 75.64 samples/sec
2025-03-25 16:50:31,424 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9488.0MB reserved
2025-03-25 16:50:31,424 - training - INFO - Epoch: 1060/200000, Batch: 15/45, Loss: 1.7581, Throughput: 71.03 samples/sec
2025-03-25 16:50:43,052 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9488.0MB reserved
2025-03-25 16:50:43,052 - training - INFO - Epoch: 1060/200000, Batch: 30/45, Loss: 1.6969, Throughput: 71.61 samples/sec
2025-03-25 16:50:53,973 - training - INFO - Epoch 1060 completed in 35.17s. Average loss: 1.6760
2025-03-25 16:50:53,977 - training - INFO - Starting epoch 1061/200000
2025-03-25 16:50:54,711 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9626.0MB reserved
2025-03-25 16:50:54,712 - training - INFO - Epoch: 1061/200000, Batch: 0/45, Loss: 1.6617, Throughput: 76.44 samples/sec
2025-03-25 16:51:06,539 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9502.0MB reserved
2025-03-25 16:51:06,539 - training - INFO - Epoch: 1061/200000, Batch: 15/45, Loss: 1.6847, Throughput: 71.33 samples/sec
2025-03-25 16:51:18,097 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9502.0MB reserved
2025-03-25 16:51:18,097 - training - INFO - Epoch: 1061/200000, Batch: 30/45, Loss: 1.6482, Throughput: 71.98 samples/sec
2025-03-25 16:51:29,000 - training - INFO - Epoch 1061 completed in 35.02s. Average loss: 1.7258
2025-03-25 16:51:29,005 - training - INFO - Starting epoch 1062/200000
2025-03-25 16:51:29,739 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9640.0MB reserved
2025-03-25 16:51:29,739 - training - INFO - Epoch: 1062/200000, Batch: 0/45, Loss: 1.6775, Throughput: 76.39 samples/sec
2025-03-25 16:51:41,568 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9506.0MB reserved
2025-03-25 16:51:41,568 - training - INFO - Epoch: 1062/200000, Batch: 15/45, Loss: 1.7463, Throughput: 71.33 samples/sec
2025-03-25 16:51:53,097 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9506.0MB reserved
2025-03-25 16:51:53,097 - training - INFO - Epoch: 1062/200000, Batch: 30/45, Loss: 1.7122, Throughput: 72.06 samples/sec
2025-03-25 16:52:03,988 - training - INFO - Epoch 1062 completed in 34.98s. Average loss: 1.7078
2025-03-25 16:52:03,992 - training - INFO - Starting epoch 1063/200000
2025-03-25 16:52:04,730 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9644.0MB reserved
2025-03-25 16:52:04,731 - training - INFO - Epoch: 1063/200000, Batch: 0/45, Loss: 1.5902, Throughput: 75.89 samples/sec
2025-03-25 16:52:16,576 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9494.0MB reserved
2025-03-25 16:52:16,576 - training - INFO - Epoch: 1063/200000, Batch: 15/45, Loss: 1.6965, Throughput: 71.22 samples/sec
2025-03-25 16:52:28,116 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9494.0MB reserved
2025-03-25 16:52:28,116 - training - INFO - Epoch: 1063/200000, Batch: 30/45, Loss: 1.6844, Throughput: 71.96 samples/sec
2025-03-25 16:52:39,047 - training - INFO - Epoch 1063 completed in 35.06s. Average loss: 1.6676
2025-03-25 16:52:39,051 - training - INFO - Starting epoch 1064/200000
2025-03-25 16:52:39,799 - training - INFO - Memory: GPU 0: 3564.1MB allocated, 9632.0MB reserved
2025-03-25 16:52:39,799 - training - INFO - Epoch: 1064/200000, Batch: 0/45, Loss: 1.5723, Throughput: 74.92 samples/sec
2025-03-25 16:52:51,642 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9494.0MB reserved
2025-03-25 16:52:51,642 - training - INFO - Epoch: 1064/200000, Batch: 15/45, Loss: 1.7281, Throughput: 71.17 samples/sec
2025-03-25 16:53:03,321 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9494.0MB reserved
2025-03-25 16:53:03,321 - training - INFO - Epoch: 1064/200000, Batch: 30/45, Loss: 1.6991, Throughput: 71.53 samples/sec
2025-03-25 16:53:14,274 - training - INFO - Epoch 1064 completed in 35.22s. Average loss: 1.6566
2025-03-25 16:53:14,277 - training - INFO - Starting epoch 1065/200000
2025-03-25 16:53:15,003 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9632.0MB reserved
2025-03-25 16:53:15,003 - training - INFO - Epoch: 1065/200000, Batch: 0/45, Loss: 1.6566, Throughput: 77.23 samples/sec
2025-03-25 16:53:26,874 - training - INFO - Memory: GPU 0: 3562.5MB allocated, 9494.0MB reserved
2025-03-25 16:53:26,874 - training - INFO - Epoch: 1065/200000, Batch: 15/45, Loss: 1.6716, Throughput: 71.14 samples/sec
2025-03-25 16:53:38,396 - training - INFO - Memory: GPU 0: 3562.5MB allocated, 9494.0MB reserved
2025-03-25 16:53:38,397 - training - INFO - Epoch: 1065/200000, Batch: 30/45, Loss: 1.7646, Throughput: 71.98 samples/sec
2025-03-25 16:53:49,312 - training - INFO - Epoch 1065 completed in 35.03s. Average loss: 1.6984
2025-03-25 16:53:49,316 - training - INFO - Starting epoch 1066/200000
2025-03-25 16:53:50,049 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9630.0MB reserved
2025-03-25 16:53:50,050 - training - INFO - Epoch: 1066/200000, Batch: 0/45, Loss: 1.6227, Throughput: 76.55 samples/sec
2025-03-25 16:54:01,913 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9500.0MB reserved
2025-03-25 16:54:01,913 - training - INFO - Epoch: 1066/200000, Batch: 15/45, Loss: 1.6223, Throughput: 71.14 samples/sec
2025-03-25 16:54:13,540 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9500.0MB reserved
2025-03-25 16:54:13,540 - training - INFO - Epoch: 1066/200000, Batch: 30/45, Loss: 1.6710, Throughput: 71.67 samples/sec
2025-03-25 16:54:24,436 - training - INFO - Epoch 1066 completed in 35.12s. Average loss: 1.6977
2025-03-25 16:54:24,439 - training - INFO - Starting epoch 1067/200000
2025-03-25 16:54:25,172 - training - INFO - Memory: GPU 0: 3558.3MB allocated, 9638.0MB reserved
2025-03-25 16:54:25,173 - training - INFO - Epoch: 1067/200000, Batch: 0/45, Loss: 1.5534, Throughput: 76.56 samples/sec
2025-03-25 16:54:36,966 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9494.0MB reserved
2025-03-25 16:54:36,966 - training - INFO - Epoch: 1067/200000, Batch: 15/45, Loss: 1.6492, Throughput: 71.54 samples/sec
2025-03-25 16:54:48,559 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9496.0MB reserved
2025-03-25 16:54:48,559 - training - INFO - Epoch: 1067/200000, Batch: 30/45, Loss: 1.6815, Throughput: 71.98 samples/sec
2025-03-25 16:54:59,478 - training - INFO - Epoch 1067 completed in 35.04s. Average loss: 1.6662
2025-03-25 16:54:59,481 - training - INFO - Starting epoch 1068/200000
2025-03-25 16:55:00,234 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9634.0MB reserved
2025-03-25 16:55:00,234 - training - INFO - Epoch: 1068/200000, Batch: 0/45, Loss: 1.3338, Throughput: 74.53 samples/sec
2025-03-25 16:55:12,182 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9506.0MB reserved
2025-03-25 16:55:12,183 - training - INFO - Epoch: 1068/200000, Batch: 15/45, Loss: 1.6681, Throughput: 70.55 samples/sec
2025-03-25 16:55:23,808 - training - INFO - Memory: GPU 0: 3561.6MB allocated, 9506.0MB reserved
2025-03-25 16:55:23,809 - training - INFO - Epoch: 1068/200000, Batch: 30/45, Loss: 1.6708, Throughput: 71.37 samples/sec
2025-03-25 16:55:34,764 - training - INFO - Epoch 1068 completed in 35.28s. Average loss: 1.7006
2025-03-25 16:55:34,768 - training - INFO - Starting epoch 1069/200000
2025-03-25 16:55:35,522 - training - INFO - Memory: GPU 0: 3561.5MB allocated, 9644.0MB reserved
2025-03-25 16:55:35,523 - training - INFO - Epoch: 1069/200000, Batch: 0/45, Loss: 1.3839, Throughput: 74.28 samples/sec
2025-03-25 16:55:47,436 - training - INFO - Memory: GPU 0: 3564.9MB allocated, 9488.0MB reserved
2025-03-25 16:55:47,437 - training - INFO - Epoch: 1069/200000, Batch: 15/45, Loss: 1.7266, Throughput: 70.74 samples/sec
2025-03-25 16:55:58,998 - training - INFO - Memory: GPU 0: 3564.9MB allocated, 9490.0MB reserved
2025-03-25 16:55:58,999 - training - INFO - Epoch: 1069/200000, Batch: 30/45, Loss: 1.7040, Throughput: 71.65 samples/sec
2025-03-25 16:56:09,939 - training - INFO - Epoch 1069 completed in 35.17s. Average loss: 1.7241
2025-03-25 16:56:09,942 - training - INFO - Starting epoch 1070/200000
2025-03-25 16:56:10,686 - training - INFO - Memory: GPU 0: 3563.3MB allocated, 9628.0MB reserved
2025-03-25 16:56:10,686 - training - INFO - Epoch: 1070/200000, Batch: 0/45, Loss: 1.8958, Throughput: 75.43 samples/sec
2025-03-25 16:56:22,534 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9502.0MB reserved
2025-03-25 16:56:22,534 - training - INFO - Epoch: 1070/200000, Batch: 15/45, Loss: 1.8024, Throughput: 71.17 samples/sec
2025-03-25 16:56:34,162 - training - INFO - Memory: GPU 0: 3560.8MB allocated, 9504.0MB reserved
2025-03-25 16:56:34,162 - training - INFO - Epoch: 1070/200000, Batch: 30/45, Loss: 1.7505, Throughput: 71.68 samples/sec
2025-03-25 16:56:45,082 - training - INFO - Epoch 1070 completed in 35.14s. Average loss: 1.7743
2025-03-25 16:56:45,085 - training - INFO - Starting epoch 1071/200000
2025-03-25 16:56:45,839 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9642.0MB reserved
2025-03-25 16:56:45,839 - training - INFO - Epoch: 1071/200000, Batch: 0/45, Loss: 1.5699, Throughput: 74.42 samples/sec
2025-03-25 16:56:57,749 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9502.0MB reserved
2025-03-25 16:56:57,749 - training - INFO - Epoch: 1071/200000, Batch: 15/45, Loss: 1.7114, Throughput: 70.77 samples/sec
2025-03-25 16:57:09,340 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9502.0MB reserved
2025-03-25 16:57:09,340 - training - INFO - Epoch: 1071/200000, Batch: 30/45, Loss: 1.7206, Throughput: 71.58 samples/sec
2025-03-25 16:57:20,286 - training - INFO - Epoch 1071 completed in 35.20s. Average loss: 1.7382
2025-03-25 16:57:20,290 - training - INFO - Starting epoch 1072/200000
2025-03-25 16:57:21,033 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9640.0MB reserved
2025-03-25 16:57:21,034 - training - INFO - Epoch: 1072/200000, Batch: 0/45, Loss: 2.2550, Throughput: 75.44 samples/sec
2025-03-25 16:57:32,944 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9502.0MB reserved
2025-03-25 16:57:32,944 - training - INFO - Epoch: 1072/200000, Batch: 15/45, Loss: 1.6979, Throughput: 70.82 samples/sec
2025-03-25 16:57:44,500 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9502.0MB reserved
2025-03-25 16:57:44,501 - training - INFO - Epoch: 1072/200000, Batch: 30/45, Loss: 1.7318, Throughput: 71.71 samples/sec
2025-03-25 16:57:55,416 - training - INFO - Epoch 1072 completed in 35.13s. Average loss: 1.6888
2025-03-25 16:57:55,419 - training - INFO - Starting epoch 1073/200000
2025-03-25 16:57:56,162 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9640.0MB reserved
2025-03-25 16:57:56,163 - training - INFO - Epoch: 1073/200000, Batch: 0/45, Loss: 1.9932, Throughput: 75.54 samples/sec
2025-03-25 16:58:08,126 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9512.0MB reserved
2025-03-25 16:58:08,127 - training - INFO - Epoch: 1073/200000, Batch: 15/45, Loss: 1.7535, Throughput: 70.53 samples/sec
2025-03-25 16:58:19,858 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9512.0MB reserved
2025-03-25 16:58:19,858 - training - INFO - Epoch: 1073/200000, Batch: 30/45, Loss: 1.7753, Throughput: 71.04 samples/sec
2025-03-25 16:58:30,818 - training - INFO - Epoch 1073 completed in 35.40s. Average loss: 1.7717
2025-03-25 16:58:30,822 - training - INFO - Starting epoch 1074/200000
2025-03-25 16:58:31,583 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9650.0MB reserved
2025-03-25 16:58:31,583 - training - INFO - Epoch: 1074/200000, Batch: 0/45, Loss: 1.1836, Throughput: 73.69 samples/sec
2025-03-25 16:58:43,575 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9506.0MB reserved
2025-03-25 16:58:43,576 - training - INFO - Epoch: 1074/200000, Batch: 15/45, Loss: 1.7007, Throughput: 70.27 samples/sec
2025-03-25 16:58:55,188 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9506.0MB reserved
2025-03-25 16:58:55,188 - training - INFO - Epoch: 1074/200000, Batch: 30/45, Loss: 1.7090, Throughput: 71.25 samples/sec
2025-03-25 16:59:06,137 - training - INFO - Epoch 1074 completed in 35.31s. Average loss: 1.6826
2025-03-25 16:59:06,141 - training - INFO - Starting epoch 1075/200000
2025-03-25 16:59:06,879 - training - INFO - Memory: GPU 0: 3558.0MB allocated, 9644.0MB reserved
2025-03-25 16:59:06,879 - training - INFO - Epoch: 1075/200000, Batch: 0/45, Loss: 1.6837, Throughput: 76.04 samples/sec
2025-03-25 16:59:18,759 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9490.0MB reserved
2025-03-25 16:59:18,759 - training - INFO - Epoch: 1075/200000, Batch: 15/45, Loss: 1.7172, Throughput: 71.02 samples/sec
2025-03-25 16:59:30,358 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9490.0MB reserved
2025-03-25 16:59:30,359 - training - INFO - Epoch: 1075/200000, Batch: 30/45, Loss: 1.6659, Throughput: 71.69 samples/sec
2025-03-25 16:59:41,291 - training - INFO - Epoch 1075 completed in 35.15s. Average loss: 1.6757
2025-03-25 16:59:41,295 - training - INFO - Starting epoch 1076/200000
2025-03-25 16:59:42,048 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9628.0MB reserved
2025-03-25 16:59:42,049 - training - INFO - Epoch: 1076/200000, Batch: 0/45, Loss: 1.6121, Throughput: 74.52 samples/sec
2025-03-25 16:59:53,960 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9504.0MB reserved
2025-03-25 16:59:53,961 - training - INFO - Epoch: 1076/200000, Batch: 15/45, Loss: 1.6888, Throughput: 70.76 samples/sec
2025-03-25 17:00:05,599 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9504.0MB reserved
2025-03-25 17:00:05,599 - training - INFO - Epoch: 1076/200000, Batch: 30/45, Loss: 1.7352, Throughput: 71.43 samples/sec
2025-03-25 17:00:16,500 - training - INFO - Epoch 1076 completed in 35.21s. Average loss: 1.6934
2025-03-25 17:00:16,504 - training - INFO - Starting epoch 1077/200000
2025-03-25 17:00:17,249 - training - INFO - Memory: GPU 0: 3558.9MB allocated, 9642.0MB reserved
2025-03-25 17:00:17,249 - training - INFO - Epoch: 1077/200000, Batch: 0/45, Loss: 1.5238, Throughput: 75.36 samples/sec
2025-03-25 17:00:29,063 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9512.0MB reserved
2025-03-25 17:00:29,064 - training - INFO - Epoch: 1077/200000, Batch: 15/45, Loss: 1.7233, Throughput: 71.35 samples/sec
2025-03-25 17:00:40,609 - training - INFO - Memory: GPU 0: 3555.7MB allocated, 9514.0MB reserved
2025-03-25 17:00:40,609 - training - INFO - Epoch: 1077/200000, Batch: 30/45, Loss: 1.7325, Throughput: 72.02 samples/sec
2025-03-25 17:00:51,513 - training - INFO - Epoch 1077 completed in 35.01s. Average loss: 1.6800
2025-03-25 17:00:51,516 - training - INFO - Starting epoch 1078/200000
2025-03-25 17:00:52,265 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9652.0MB reserved
2025-03-25 17:00:52,266 - training - INFO - Epoch: 1078/200000, Batch: 0/45, Loss: 1.9601, Throughput: 74.86 samples/sec
2025-03-25 17:01:04,188 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9502.0MB reserved
2025-03-25 17:01:04,188 - training - INFO - Epoch: 1078/200000, Batch: 15/45, Loss: 1.8483, Throughput: 70.72 samples/sec
2025-03-25 17:01:15,854 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9502.0MB reserved
2025-03-25 17:01:15,855 - training - INFO - Epoch: 1078/200000, Batch: 30/45, Loss: 1.7892, Throughput: 71.34 samples/sec
2025-03-25 17:01:26,774 - training - INFO - Epoch 1078 completed in 35.26s. Average loss: 1.7291
2025-03-25 17:01:26,778 - training - INFO - Starting epoch 1079/200000
2025-03-25 17:01:27,537 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9640.0MB reserved
2025-03-25 17:01:27,537 - training - INFO - Epoch: 1079/200000, Batch: 0/45, Loss: 1.4016, Throughput: 74.04 samples/sec
2025-03-25 17:01:39,423 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9520.0MB reserved
2025-03-25 17:01:39,424 - training - INFO - Epoch: 1079/200000, Batch: 15/45, Loss: 1.7540, Throughput: 70.86 samples/sec
2025-03-25 17:01:51,041 - training - INFO - Memory: GPU 0: 3561.9MB allocated, 9520.0MB reserved
2025-03-25 17:01:51,041 - training - INFO - Epoch: 1079/200000, Batch: 30/45, Loss: 1.7325, Throughput: 71.55 samples/sec
2025-03-25 17:02:01,980 - training - INFO - Epoch 1079 completed in 35.20s. Average loss: 1.7049
2025-03-25 17:02:01,983 - training - INFO - Starting epoch 1080/200000
2025-03-25 17:02:02,729 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9658.0MB reserved
2025-03-25 17:02:02,730 - training - INFO - Epoch: 1080/200000, Batch: 0/45, Loss: 1.9327, Throughput: 75.23 samples/sec
2025-03-25 17:02:14,768 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9402.0MB reserved
2025-03-25 17:02:14,769 - training - INFO - Epoch: 1080/200000, Batch: 15/45, Loss: 1.6839, Throughput: 70.10 samples/sec
2025-03-25 17:02:26,404 - training - INFO - Memory: GPU 0: 3561.4MB allocated, 9402.0MB reserved
2025-03-25 17:02:26,404 - training - INFO - Epoch: 1080/200000, Batch: 30/45, Loss: 1.6675, Throughput: 71.09 samples/sec
2025-03-25 17:02:37,370 - training - INFO - Epoch 1080 completed in 35.39s. Average loss: 1.7162
2025-03-25 17:02:37,373 - training - INFO - Starting epoch 1081/200000
2025-03-25 17:02:38,118 - training - INFO - Memory: GPU 0: 3557.3MB allocated, 9540.0MB reserved
2025-03-25 17:02:38,119 - training - INFO - Epoch: 1081/200000, Batch: 0/45, Loss: 1.6823, Throughput: 75.24 samples/sec
2025-03-25 17:02:49,985 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9502.0MB reserved
2025-03-25 17:02:49,985 - training - INFO - Epoch: 1081/200000, Batch: 15/45, Loss: 1.7278, Throughput: 71.06 samples/sec
2025-03-25 17:03:01,606 - training - INFO - Memory: GPU 0: 3557.8MB allocated, 9504.0MB reserved
2025-03-25 17:03:01,606 - training - INFO - Epoch: 1081/200000, Batch: 30/45, Loss: 1.6997, Throughput: 71.65 samples/sec
2025-03-25 17:03:12,557 - training - INFO - Epoch 1081 completed in 35.18s. Average loss: 1.7494
2025-03-25 17:03:12,561 - training - INFO - Starting epoch 1082/200000
2025-03-25 17:03:13,317 - training - INFO - Memory: GPU 0: 3557.3MB allocated, 9642.0MB reserved
2025-03-25 17:03:13,318 - training - INFO - Epoch: 1082/200000, Batch: 0/45, Loss: 1.7010, Throughput: 74.17 samples/sec
2025-03-25 17:03:25,213 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9510.0MB reserved
2025-03-25 17:03:25,213 - training - INFO - Epoch: 1082/200000, Batch: 15/45, Loss: 1.7253, Throughput: 70.82 samples/sec
2025-03-25 17:03:36,809 - training - INFO - Memory: GPU 0: 3557.9MB allocated, 9512.0MB reserved
2025-03-25 17:03:36,809 - training - INFO - Epoch: 1082/200000, Batch: 30/45, Loss: 1.6690, Throughput: 71.60 samples/sec
2025-03-25 17:03:47,763 - training - INFO - Epoch 1082 completed in 35.20s. Average loss: 1.7262
2025-03-25 17:03:47,767 - training - INFO - Starting epoch 1083/200000
2025-03-25 17:03:48,512 - training - INFO - Memory: GPU 0: 3559.8MB allocated, 9650.0MB reserved
2025-03-25 17:03:48,513 - training - INFO - Epoch: 1083/200000, Batch: 0/45, Loss: 1.5104, Throughput: 75.31 samples/sec
2025-03-25 17:04:00,369 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9486.0MB reserved
2025-03-25 17:04:00,369 - training - INFO - Epoch: 1083/200000, Batch: 15/45, Loss: 1.6232, Throughput: 71.12 samples/sec
2025-03-25 17:04:11,926 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9486.0MB reserved
2025-03-25 17:04:11,926 - training - INFO - Epoch: 1083/200000, Batch: 30/45, Loss: 1.6357, Throughput: 71.86 samples/sec
2025-03-25 17:04:22,841 - training - INFO - Epoch 1083 completed in 35.07s. Average loss: 1.6352
2025-03-25 17:04:22,845 - training - INFO - Starting epoch 1084/200000
2025-03-25 17:04:23,612 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9624.0MB reserved
2025-03-25 17:04:23,612 - training - INFO - Epoch: 1084/200000, Batch: 0/45, Loss: 1.8454, Throughput: 73.22 samples/sec
2025-03-25 17:04:35,561 - training - INFO - Memory: GPU 0: 3556.2MB allocated, 9400.0MB reserved
2025-03-25 17:04:35,562 - training - INFO - Epoch: 1084/200000, Batch: 15/45, Loss: 1.7220, Throughput: 70.47 samples/sec
2025-03-25 17:04:47,220 - training - INFO - Memory: GPU 0: 3556.2MB allocated, 9400.0MB reserved
2025-03-25 17:04:47,220 - training - INFO - Epoch: 1084/200000, Batch: 30/45, Loss: 1.6752, Throughput: 71.23 samples/sec
2025-03-25 17:04:58,185 - training - INFO - Epoch 1084 completed in 35.34s. Average loss: 1.6473
2025-03-25 17:04:58,189 - training - INFO - Starting epoch 1085/200000
2025-03-25 17:04:58,942 - training - INFO - Memory: GPU 0: 3560.9MB allocated, 9644.0MB reserved
2025-03-25 17:04:58,942 - training - INFO - Epoch: 1085/200000, Batch: 0/45, Loss: 1.5989, Throughput: 74.52 samples/sec
2025-03-25 17:05:10,889 - training - INFO - Memory: GPU 0: 3556.5MB allocated, 9496.0MB reserved
2025-03-25 17:05:10,890 - training - INFO - Epoch: 1085/200000, Batch: 15/45, Loss: 1.6847, Throughput: 70.56 samples/sec
2025-03-25 17:05:22,551 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9496.0MB reserved
2025-03-25 17:05:22,552 - training - INFO - Epoch: 1085/200000, Batch: 30/45, Loss: 1.7926, Throughput: 71.26 samples/sec
2025-03-25 17:05:33,499 - training - INFO - Epoch 1085 completed in 35.31s. Average loss: 1.6856
2025-03-25 17:05:33,503 - training - INFO - Starting epoch 1086/200000
2025-03-25 17:05:34,235 - training - INFO - Memory: GPU 0: 3562.1MB allocated, 9634.0MB reserved
2025-03-25 17:05:34,235 - training - INFO - Epoch: 1086/200000, Batch: 0/45, Loss: 1.7441, Throughput: 76.66 samples/sec
2025-03-25 17:05:46,078 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9496.0MB reserved
2025-03-25 17:05:46,079 - training - INFO - Epoch: 1086/200000, Batch: 15/45, Loss: 1.6215, Throughput: 71.26 samples/sec
2025-03-25 17:05:57,652 - training - INFO - Memory: GPU 0: 3562.0MB allocated, 9496.0MB reserved
2025-03-25 17:05:57,653 - training - INFO - Epoch: 1086/200000, Batch: 30/45, Loss: 1.6990, Throughput: 71.89 samples/sec
2025-03-25 17:06:08,560 - training - INFO - Epoch 1086 completed in 35.06s. Average loss: 1.6683
2025-03-25 17:06:08,564 - training - INFO - Starting epoch 1087/200000
2025-03-25 17:06:09,319 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9634.0MB reserved
2025-03-25 17:06:09,319 - training - INFO - Epoch: 1087/200000, Batch: 0/45, Loss: 1.5279, Throughput: 74.24 samples/sec
2025-03-25 17:06:21,205 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9500.0MB reserved
2025-03-25 17:06:21,206 - training - INFO - Epoch: 1087/200000, Batch: 15/45, Loss: 1.6539, Throughput: 70.88 samples/sec
2025-03-25 17:06:32,782 - training - INFO - Memory: GPU 0: 3559.9MB allocated, 9500.0MB reserved
2025-03-25 17:06:32,783 - training - INFO - Epoch: 1087/200000, Batch: 30/45, Loss: 1.6525, Throughput: 71.69 samples/sec
2025-03-25 17:06:43,681 - training - INFO - Epoch 1087 completed in 35.12s. Average loss: 1.7002
2025-03-25 17:06:43,684 - training - INFO - Starting epoch 1088/200000
2025-03-25 17:06:44,430 - training - INFO - Memory: GPU 0: 3558.1MB allocated, 9638.0MB reserved
2025-03-25 17:06:44,430 - training - INFO - Epoch: 1088/200000, Batch: 0/45, Loss: 1.7808, Throughput: 75.28 samples/sec
2025-03-25 17:06:56,343 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9514.0MB reserved
2025-03-25 17:06:56,344 - training - INFO - Epoch: 1088/200000, Batch: 15/45, Loss: 1.7911, Throughput: 70.79 samples/sec
2025-03-25 17:07:08,026 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9514.0MB reserved
2025-03-25 17:07:08,027 - training - INFO - Epoch: 1088/200000, Batch: 30/45, Loss: 1.7772, Throughput: 71.32 samples/sec
2025-03-25 17:07:19,003 - training - INFO - Epoch 1088 completed in 35.32s. Average loss: 1.7017
2025-03-25 17:07:19,007 - training - INFO - Starting epoch 1089/200000
2025-03-25 17:07:19,758 - training - INFO - Memory: GPU 0: 3560.4MB allocated, 9652.0MB reserved
2025-03-25 17:07:19,759 - training - INFO - Epoch: 1089/200000, Batch: 0/45, Loss: 1.8126, Throughput: 74.72 samples/sec
2025-03-25 17:07:31,743 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9488.0MB reserved
2025-03-25 17:07:31,744 - training - INFO - Epoch: 1089/200000, Batch: 15/45, Loss: 1.6977, Throughput: 70.36 samples/sec
2025-03-25 17:07:43,365 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9488.0MB reserved
2025-03-25 17:07:43,366 - training - INFO - Epoch: 1089/200000, Batch: 30/45, Loss: 1.7609, Throughput: 71.28 samples/sec
2025-03-25 17:07:54,315 - training - INFO - Epoch 1089 completed in 35.31s. Average loss: 1.6639
2025-03-25 17:07:54,319 - training - INFO - Starting epoch 1090/200000
2025-03-25 17:07:55,040 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9624.0MB reserved
2025-03-25 17:07:55,040 - training - INFO - Epoch: 1090/200000, Batch: 0/45, Loss: 1.9833, Throughput: 77.86 samples/sec
2025-03-25 17:08:06,943 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9488.0MB reserved
2025-03-25 17:08:06,943 - training - INFO - Epoch: 1090/200000, Batch: 15/45, Loss: 1.6977, Throughput: 70.98 samples/sec
2025-03-25 17:08:18,483 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9490.0MB reserved
2025-03-25 17:08:18,484 - training - INFO - Epoch: 1090/200000, Batch: 30/45, Loss: 1.7135, Throughput: 71.84 samples/sec
2025-03-25 17:08:29,461 - training - INFO - Epoch 1090 completed in 35.14s. Average loss: 1.6573
2025-03-25 17:08:29,464 - training - INFO - Starting epoch 1091/200000
2025-03-25 17:08:30,212 - training - INFO - Memory: GPU 0: 3557.7MB allocated, 9628.0MB reserved
2025-03-25 17:08:30,212 - training - INFO - Epoch: 1091/200000, Batch: 0/45, Loss: 1.4888, Throughput: 75.08 samples/sec
2025-03-25 17:08:42,079 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9500.0MB reserved
2025-03-25 17:08:42,079 - training - INFO - Epoch: 1091/200000, Batch: 15/45, Loss: 1.6690, Throughput: 71.04 samples/sec
2025-03-25 17:08:53,789 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9500.0MB reserved
2025-03-25 17:08:53,789 - training - INFO - Epoch: 1091/200000, Batch: 30/45, Loss: 1.6824, Throughput: 71.37 samples/sec
2025-03-25 17:09:04,784 - training - INFO - Epoch 1091 completed in 35.32s. Average loss: 1.6296
2025-03-25 17:09:04,788 - training - INFO - Starting epoch 1092/200000
2025-03-25 17:09:05,527 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9638.0MB reserved
2025-03-25 17:09:05,527 - training - INFO - Epoch: 1092/200000, Batch: 0/45, Loss: 1.6412, Throughput: 75.89 samples/sec
2025-03-25 17:09:17,328 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9498.0MB reserved
2025-03-25 17:09:17,328 - training - INFO - Epoch: 1092/200000, Batch: 15/45, Loss: 1.6775, Throughput: 71.46 samples/sec
2025-03-25 17:09:28,867 - training - INFO - Memory: GPU 0: 3558.8MB allocated, 9498.0MB reserved
2025-03-25 17:09:28,868 - training - INFO - Epoch: 1092/200000, Batch: 30/45, Loss: 1.7232, Throughput: 72.10 samples/sec
2025-03-25 17:09:39,806 - training - INFO - Epoch 1092 completed in 35.02s. Average loss: 1.6994
2025-03-25 17:09:39,810 - training - INFO - Starting epoch 1093/200000
2025-03-25 17:09:40,544 - training - INFO - Memory: GPU 0: 3560.1MB allocated, 9638.0MB reserved
2025-03-25 17:09:40,545 - training - INFO - Epoch: 1093/200000, Batch: 0/45, Loss: 1.5040, Throughput: 76.30 samples/sec
2025-03-25 17:09:52,491 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9498.0MB reserved
2025-03-25 17:09:52,492 - training - INFO - Epoch: 1093/200000, Batch: 15/45, Loss: 1.8023, Throughput: 70.66 samples/sec
2025-03-25 17:10:04,099 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9498.0MB reserved
2025-03-25 17:10:04,099 - training - INFO - Epoch: 1093/200000, Batch: 30/45, Loss: 1.7649, Throughput: 71.48 samples/sec
2025-03-25 17:10:15,039 - training - INFO - Epoch 1093 completed in 35.23s. Average loss: 1.6727
2025-03-25 17:10:15,042 - training - INFO - Starting epoch 1094/200000
2025-03-25 17:10:15,800 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9636.0MB reserved
2025-03-25 17:10:15,800 - training - INFO - Epoch: 1094/200000, Batch: 0/45, Loss: 1.7215, Throughput: 74.17 samples/sec
2025-03-25 17:10:27,735 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9504.0MB reserved
2025-03-25 17:10:27,735 - training - INFO - Epoch: 1094/200000, Batch: 15/45, Loss: 1.6558, Throughput: 70.62 samples/sec
2025-03-25 17:10:39,444 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9504.0MB reserved
2025-03-25 17:10:39,445 - training - INFO - Epoch: 1094/200000, Batch: 30/45, Loss: 1.6209, Throughput: 71.15 samples/sec
2025-03-25 17:10:50,466 - training - INFO - Epoch 1094 completed in 35.42s. Average loss: 1.6537
2025-03-25 17:10:50,470 - training - INFO - Starting epoch 1095/200000
2025-03-25 17:10:51,217 - training - INFO - Memory: GPU 0: 3560.0MB allocated, 9642.0MB reserved
2025-03-25 17:10:51,217 - training - INFO - Epoch: 1095/200000, Batch: 0/45, Loss: 1.3103, Throughput: 75.14 samples/sec
2025-03-25 17:11:03,206 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9512.0MB reserved
2025-03-25 17:11:03,207 - training - INFO - Epoch: 1095/200000, Batch: 15/45, Loss: 1.7155, Throughput: 70.36 samples/sec
2025-03-25 17:11:14,905 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9512.0MB reserved
2025-03-25 17:11:14,906 - training - INFO - Epoch: 1095/200000, Batch: 30/45, Loss: 1.6955, Throughput: 71.05 samples/sec
2025-03-25 17:11:25,917 - training - INFO - Epoch 1095 completed in 35.45s. Average loss: 1.7842
2025-03-25 17:11:25,922 - training - INFO - Starting epoch 1096/200000
2025-03-25 17:11:26,655 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9650.0MB reserved
2025-03-25 17:11:26,655 - training - INFO - Epoch: 1096/200000, Batch: 0/45, Loss: 1.9356, Throughput: 76.53 samples/sec
2025-03-25 17:11:38,564 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9498.0MB reserved
2025-03-25 17:11:38,564 - training - INFO - Epoch: 1096/200000, Batch: 15/45, Loss: 1.7573, Throughput: 70.89 samples/sec
2025-03-25 17:11:50,147 - training - INFO - Memory: GPU 0: 3561.1MB allocated, 9498.0MB reserved
2025-03-25 17:11:50,148 - training - INFO - Epoch: 1096/200000, Batch: 30/45, Loss: 1.7339, Throughput: 71.67 samples/sec
2025-03-25 17:12:01,083 - training - INFO - Epoch 1096 completed in 35.16s. Average loss: 1.7406
2025-03-25 17:12:01,086 - training - INFO - Starting epoch 1097/200000
2025-03-25 17:12:01,831 - training - INFO - Memory: GPU 0: 3558.4MB allocated, 9636.0MB reserved
2025-03-25 17:12:01,831 - training - INFO - Epoch: 1097/200000, Batch: 0/45, Loss: 1.7451, Throughput: 75.33 samples/sec
2025-03-25 17:12:13,691 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9512.0MB reserved
2025-03-25 17:12:13,691 - training - INFO - Epoch: 1097/200000, Batch: 15/45, Loss: 1.7835, Throughput: 71.09 samples/sec
2025-03-25 17:12:25,237 - training - INFO - Memory: GPU 0: 3559.0MB allocated, 9514.0MB reserved
2025-03-25 17:12:25,238 - training - INFO - Epoch: 1097/200000, Batch: 30/45, Loss: 1.7311, Throughput: 71.89 samples/sec
2025-03-25 17:12:36,154 - training - INFO - Epoch 1097 completed in 35.07s. Average loss: 1.6732
2025-03-25 17:12:36,158 - training - INFO - Starting epoch 1098/200000
2025-03-25 17:12:36,902 - training - INFO - Memory: GPU 0: 3559.6MB allocated, 9652.0MB reserved
2025-03-25 17:12:36,903 - training - INFO - Epoch: 1098/200000, Batch: 0/45, Loss: 1.4575, Throughput: 75.27 samples/sec
2025-03-25 17:12:48,741 - training - INFO - Memory: GPU 0: 3559.1MB allocated, 9510.0MB reserved
2025-03-25 17:12:48,741 - training - INFO - Epoch: 1098/200000, Batch: 15/45, Loss: 1.7079, Throughput: 71.21 samples/sec
2025-03-25 17:13:00,282 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9510.0MB reserved
2025-03-25 17:13:00,282 - training - INFO - Epoch: 1098/200000, Batch: 30/45, Loss: 1.6864, Throughput: 71.97 samples/sec
2025-03-25 17:13:11,207 - training - INFO - Epoch 1098 completed in 35.05s. Average loss: 1.6646
2025-03-25 17:13:11,211 - training - INFO - Starting epoch 1099/200000
2025-03-25 17:13:11,985 - training - INFO - Memory: GPU 0: 3561.2MB allocated, 9650.0MB reserved
2025-03-25 17:13:11,985 - training - INFO - Epoch: 1099/200000, Batch: 0/45, Loss: 1.5125, Throughput: 72.53 samples/sec
2025-03-25 17:13:23,978 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9496.0MB reserved
2025-03-25 17:13:23,978 - training - INFO - Epoch: 1099/200000, Batch: 15/45, Loss: 1.7182, Throughput: 70.19 samples/sec
2025-03-25 17:13:35,533 - training - INFO - Memory: GPU 0: 3559.4MB allocated, 9496.0MB reserved
2025-03-25 17:13:35,534 - training - INFO - Epoch: 1099/200000, Batch: 30/45, Loss: 1.7120, Throughput: 71.38 samples/sec
2025-03-25 17:13:46,486 - training - INFO - Epoch 1099 completed in 35.28s. Average loss: 1.6150
2025-03-25 17:13:46,490 - training - INFO - Starting epoch 1100/200000
2025-03-25 17:13:47,234 - training - INFO - Memory: GPU 0: 3556.9MB allocated, 9634.0MB reserved
2025-03-25 17:13:47,234 - training - INFO - Epoch: 1100/200000, Batch: 0/45, Loss: 1.2985, Throughput: 75.45 samples/sec
2025-03-25 17:13:59,105 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9498.0MB reserved
2025-03-25 17:13:59,105 - training - INFO - Epoch: 1100/200000, Batch: 15/45, Loss: 1.7178, Throughput: 71.03 samples/sec
2025-03-25 17:14:10,656 - training - INFO - Memory: GPU 0: 3557.4MB allocated, 9498.0MB reserved
2025-03-25 17:14:10,657 - training - INFO - Epoch: 1100/200000, Batch: 30/45, Loss: 1.7084, Throughput: 71.84 samples/sec
2025-03-25 17:14:21,570 - training - INFO - Epoch 1100 completed in 35.08s. Average loss: 1.6941
2025-03-25 17:14:21,573 - training - INFO - Starting validation...
2025-03-25 17:14:21,886 - training - INFO - Validation Loss: 24.4105
2025-03-25 17:14:21,886 - training - INFO - Validation loss did not improve. Counter: 10/10
2025-03-25 17:14:21,886 - training - INFO - Early stopping triggered! No improvement for 10 epochs.
2025-03-25 17:14:22,226 - training - INFO - Early stopping triggered after 1100 epochs
2025-03-25 17:14:22,400 - training - INFO - Training completed successfully.
2025-03-25 17:46:23,831 - training - ERROR - Critical error in training batch 0: NCCL communicator was aborted on rank 7.  Original reason for failure was: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802908 milliseconds before timing out.
2025-03-25 17:46:23,836 - training - ERROR - Critical error in training batch 0: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1803078 milliseconds before timing out.
2025-03-25 17:46:23,836 - training - ERROR - Critical error in training batch 0: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802916 milliseconds before timing out.
2025-03-25 17:46:24,002 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 7.  Original reason for failure was: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802908 milliseconds before timing out.

2025-03-25 17:46:24,002 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802916 milliseconds before timing out.

2025-03-25 17:46:24,002 - training - ERROR - Fatal error in training loop: NCCL communicator was aborted on rank 7.  Original reason for failure was: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802908 milliseconds before timing out.
2025-03-25 17:46:24,003 - training - ERROR - Fatal error in training loop: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802916 milliseconds before timing out.
2025-03-25 17:46:24,003 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 230, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 7.  Original reason for failure was: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802908 milliseconds before timing out.

2025-03-25 17:46:24,004 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 230, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802916 milliseconds before timing out.

2025-03-25 17:46:24,005 - training - ERROR - Fatal error in main function: NCCL communicator was aborted on rank 7.  Original reason for failure was: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802908 milliseconds before timing out.
2025-03-25 17:46:24,005 - training - ERROR - Fatal error in main function: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802916 milliseconds before timing out.
2025-03-25 17:46:24,005 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 705, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 347, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 230, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 7.  Original reason for failure was: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802908 milliseconds before timing out.

2025-03-25 17:46:24,005 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 705, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 347, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 230, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802916 milliseconds before timing out.

2025-03-25 17:46:24,006 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1803078 milliseconds before timing out.

2025-03-25 17:46:24,006 - training - ERROR - Fatal error in training loop: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1803078 milliseconds before timing out.
2025-03-25 17:46:24,007 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 230, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1803078 milliseconds before timing out.

2025-03-25 17:46:24,008 - training - ERROR - Fatal error in main function: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1803078 milliseconds before timing out.
2025-03-25 17:46:24,009 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 705, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 347, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 230, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1803078 milliseconds before timing out.

2025-03-25 17:46:24,901 - training - ERROR - Critical error in training batch 0: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802931 milliseconds before timing out.
2025-03-25 17:46:24,906 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802931 milliseconds before timing out.

2025-03-25 17:46:24,906 - training - ERROR - Fatal error in training loop: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802931 milliseconds before timing out.
2025-03-25 17:46:24,907 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 230, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802931 milliseconds before timing out.

2025-03-25 17:46:24,907 - training - ERROR - Fatal error in main function: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802931 milliseconds before timing out.
2025-03-25 17:46:24,907 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 705, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 347, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 230, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802931 milliseconds before timing out.

2025-03-25 17:46:25,811 - training - ERROR - Critical error in training batch 0: NCCL communicator was aborted on rank 5.  Original reason for failure was: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802926 milliseconds before timing out.
2025-03-25 17:46:25,812 - training - ERROR - Critical error in training batch 0: NCCL communicator was aborted on rank 6.  Original reason for failure was: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1803021 milliseconds before timing out.
2025-03-25 17:46:25,814 - training - ERROR - Critical error in training batch 0: NCCL communicator was aborted on rank 4.  Original reason for failure was: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802938 milliseconds before timing out.
2025-03-25 17:46:25,817 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 6.  Original reason for failure was: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1803021 milliseconds before timing out.

2025-03-25 17:46:25,818 - training - ERROR - Fatal error in training loop: NCCL communicator was aborted on rank 6.  Original reason for failure was: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1803021 milliseconds before timing out.
2025-03-25 17:46:25,818 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 230, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 6.  Original reason for failure was: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1803021 milliseconds before timing out.

2025-03-25 17:46:25,818 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 5.  Original reason for failure was: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802926 milliseconds before timing out.

2025-03-25 17:46:25,819 - training - ERROR - Fatal error in training loop: NCCL communicator was aborted on rank 5.  Original reason for failure was: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802926 milliseconds before timing out.
2025-03-25 17:46:25,819 - training - ERROR - Fatal error in main function: NCCL communicator was aborted on rank 6.  Original reason for failure was: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1803021 milliseconds before timing out.
2025-03-25 17:46:25,819 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 705, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 347, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 230, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 6.  Original reason for failure was: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1803021 milliseconds before timing out.

2025-03-25 17:46:25,819 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 230, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 5.  Original reason for failure was: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802926 milliseconds before timing out.

2025-03-25 17:46:25,820 - training - ERROR - Fatal error in main function: NCCL communicator was aborted on rank 5.  Original reason for failure was: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802926 milliseconds before timing out.
2025-03-25 17:46:25,820 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 705, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 347, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 230, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 5.  Original reason for failure was: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802926 milliseconds before timing out.

2025-03-25 17:46:25,822 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 4.  Original reason for failure was: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802938 milliseconds before timing out.

2025-03-25 17:46:25,822 - training - ERROR - Fatal error in training loop: NCCL communicator was aborted on rank 4.  Original reason for failure was: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802938 milliseconds before timing out.
2025-03-25 17:46:25,823 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 230, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 4.  Original reason for failure was: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802938 milliseconds before timing out.

2025-03-25 17:46:25,823 - training - ERROR - Fatal error in main function: NCCL communicator was aborted on rank 4.  Original reason for failure was: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802938 milliseconds before timing out.
2025-03-25 17:46:25,824 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 705, in main
    train_model(
  File "AnsweringAgent/src/train.py", line 347, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 230, in train_model
    raise e
  File "AnsweringAgent/src/train.py", line 168, in train_model
    outputs = model(text_input, current_view, previous_views)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/answering_agent.py", line 222, in forward
    visual_features = self.feature_extractor(current_view, previous_views)  # [batch_size, hidden_size]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 136, in forward
    prev_features = self._extract_features(prev_views_reshaped)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/feature_extractor.py", line 89, in _extract_features
    features = self.darknet(x)  # [batch_size, channels, height, width]
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/app/UAV-Language-Guided-Navigation/AnsweringAgent/src/models/darknet.py", line 238, in forward
    x = module(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 748, in forward
    return sync_batch_norm.apply(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/_functions.py", line 42, in forward
    dist._all_gather_base(combined_flat, combined, process_group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2128, in _all_gather_base
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: NCCL communicator was aborted on rank 4.  Original reason for failure was: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7376621, OpType=_ALLGATHER_BASE, Timeout(ms)=1800000) ran for 1802938 milliseconds before timing out.

2025-03-26 00:51:54,513 - training - INFO - Overriding batch size with command-line value: 7
2025-03-26 00:51:54,514 - training - INFO - Overriding gradient accumulation steps with command-line value: 3
2025-03-26 00:51:54,514 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_b7ugvwcl.log
2025-03-26 00:51:54,514 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 8
2025-03-26 00:51:54,514 - training - INFO - Device: cuda:0
2025-03-26 00:51:55,118 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-26 00:51:55,118 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-26 00:51:55,118 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-26 00:51:55,120 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-26 00:51:55,120 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-26 00:51:59,809 - training - INFO - Starting model initialization...
2025-03-26 00:52:03,124 - training - ERROR - Fatal error in main function: local variable 'train_loader' referenced before assignment
2025-03-26 00:52:03,124 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 657, in main
    total_steps = len(train_loader) * config.training.num_epochs // config.training.gradient_accumulation_steps
UnboundLocalError: local variable 'train_loader' referenced before assignment

2025-03-26 00:52:03,324 - training - ERROR - Fatal error in main function: local variable 'train_loader' referenced before assignment
2025-03-26 00:52:03,325 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 657, in main
    total_steps = len(train_loader) * config.training.num_epochs // config.training.gradient_accumulation_steps
UnboundLocalError: local variable 'train_loader' referenced before assignment

2025-03-26 00:52:03,540 - training - ERROR - Fatal error in main function: local variable 'train_loader' referenced before assignment
2025-03-26 00:52:03,540 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 657, in main
    total_steps = len(train_loader) * config.training.num_epochs // config.training.gradient_accumulation_steps
UnboundLocalError: local variable 'train_loader' referenced before assignment

2025-03-26 00:52:03,723 - training - ERROR - Fatal error in main function: local variable 'train_loader' referenced before assignment
2025-03-26 00:52:03,723 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 657, in main
    total_steps = len(train_loader) * config.training.num_epochs // config.training.gradient_accumulation_steps
UnboundLocalError: local variable 'train_loader' referenced before assignment

2025-03-26 00:52:03,724 - training - ERROR - Fatal error in main function: local variable 'train_loader' referenced before assignment
2025-03-26 00:52:03,724 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 657, in main
    total_steps = len(train_loader) * config.training.num_epochs // config.training.gradient_accumulation_steps
UnboundLocalError: local variable 'train_loader' referenced before assignment

2025-03-26 00:52:03,724 - training - ERROR - Fatal error in main function: local variable 'train_loader' referenced before assignment
2025-03-26 00:52:03,724 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 657, in main
    total_steps = len(train_loader) * config.training.num_epochs // config.training.gradient_accumulation_steps
UnboundLocalError: local variable 'train_loader' referenced before assignment

2025-03-26 00:52:03,728 - training - ERROR - Fatal error in main function: local variable 'train_loader' referenced before assignment
2025-03-26 00:52:03,728 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 657, in main
    total_steps = len(train_loader) * config.training.num_epochs // config.training.gradient_accumulation_steps
UnboundLocalError: local variable 'train_loader' referenced before assignment

2025-03-26 00:52:04,395 - training - ERROR - Fatal error in main function: local variable 'train_loader' referenced before assignment
2025-03-26 00:52:04,395 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 657, in main
    total_steps = len(train_loader) * config.training.num_epochs // config.training.gradient_accumulation_steps
UnboundLocalError: local variable 'train_loader' referenced before assignment

2025-03-26 00:52:04,644 - training - INFO - Distributed process group destroyed successfully
2025-03-26 00:52:04,717 - training - ERROR - Training failed with errors
2025-03-26 00:54:49,395 - training - INFO - Overriding batch size with command-line value: 7
2025-03-26 00:54:49,395 - training - INFO - Overriding gradient accumulation steps with command-line value: 3
2025-03-26 00:54:49,396 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_cgdmgewi.log
2025-03-26 00:54:49,396 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 8
2025-03-26 00:54:49,396 - training - INFO - Device: cuda:0
2025-03-26 00:54:49,936 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-26 00:54:49,936 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-26 00:54:49,936 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-26 00:54:49,938 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-26 00:54:49,938 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-26 00:54:54,620 - training - INFO - Starting model initialization...
2025-03-26 00:54:57,985 - training - ERROR - Fatal error in main function: local variable 'train_loader' referenced before assignment
2025-03-26 00:54:57,985 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 657, in main
    total_steps = len(train_loader) * config.training.num_epochs // config.training.gradient_accumulation_steps
UnboundLocalError: local variable 'train_loader' referenced before assignment

2025-03-26 00:54:57,992 - training - ERROR - Fatal error in main function: local variable 'train_loader' referenced before assignment
2025-03-26 00:54:57,992 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 657, in main
    total_steps = len(train_loader) * config.training.num_epochs // config.training.gradient_accumulation_steps
UnboundLocalError: local variable 'train_loader' referenced before assignment

2025-03-26 00:54:58,329 - training - ERROR - Fatal error in main function: local variable 'train_loader' referenced before assignment
2025-03-26 00:54:58,330 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 657, in main
    total_steps = len(train_loader) * config.training.num_epochs // config.training.gradient_accumulation_steps
UnboundLocalError: local variable 'train_loader' referenced before assignment

2025-03-26 00:54:58,330 - training - ERROR - Fatal error in main function: local variable 'train_loader' referenced before assignment
2025-03-26 00:54:58,330 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 657, in main
    total_steps = len(train_loader) * config.training.num_epochs // config.training.gradient_accumulation_steps
UnboundLocalError: local variable 'train_loader' referenced before assignment

2025-03-26 00:54:58,676 - training - ERROR - Fatal error in main function: local variable 'train_loader' referenced before assignment
2025-03-26 00:54:58,676 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 657, in main
    total_steps = len(train_loader) * config.training.num_epochs // config.training.gradient_accumulation_steps
UnboundLocalError: local variable 'train_loader' referenced before assignment

2025-03-26 00:54:58,676 - training - ERROR - Fatal error in main function: local variable 'train_loader' referenced before assignment
2025-03-26 00:54:58,677 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 657, in main
    total_steps = len(train_loader) * config.training.num_epochs // config.training.gradient_accumulation_steps
UnboundLocalError: local variable 'train_loader' referenced before assignment

2025-03-26 00:54:58,677 - training - ERROR - Fatal error in main function: local variable 'train_loader' referenced before assignment
2025-03-26 00:54:58,677 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 657, in main
    total_steps = len(train_loader) * config.training.num_epochs // config.training.gradient_accumulation_steps
UnboundLocalError: local variable 'train_loader' referenced before assignment

2025-03-26 00:54:58,683 - training - ERROR - Fatal error in main function: local variable 'train_loader' referenced before assignment
2025-03-26 00:54:58,684 - training - ERROR - Traceback (most recent call last):
  File "AnsweringAgent/src/train.py", line 657, in main
    total_steps = len(train_loader) * config.training.num_epochs // config.training.gradient_accumulation_steps
UnboundLocalError: local variable 'train_loader' referenced before assignment

2025-03-26 00:54:58,930 - training - INFO - Distributed process group destroyed successfully
2025-03-26 00:54:59,004 - training - ERROR - Training failed with errors
2025-03-26 00:55:24,150 - training - INFO - Overriding batch size with command-line value: 7
2025-03-26 00:55:24,150 - training - INFO - Overriding gradient accumulation steps with command-line value: 3
2025-03-26 00:55:24,150 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_0w4_rtqo.log
2025-03-26 00:55:24,150 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 8
2025-03-26 00:55:24,150 - training - INFO - Device: cuda:0
2025-03-26 00:55:24,685 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-26 00:55:24,685 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-26 00:55:24,685 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-26 00:55:24,685 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-26 00:55:24,685 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-26 00:55:29,347 - training - INFO - Starting model initialization...
2025-03-26 00:55:39,033 - training - INFO - Dataset split: 2466 training, 137 validation, 137 test samples
2025-03-26 00:55:39,395 - training - INFO - Test dataset indices saved to /app/UAV-Language-Guided-Navigation/AnsweringAgent/outputs/logs/test_indices.pt
2025-03-26 00:55:39,395 - training - INFO - Per-GPU batch size: 7 (effective batch size: 168, with gradient_accumulation_steps=3)
2025-03-26 00:55:39,417 - training - INFO - Created 24 gradient buckets for efficient all-reduce
2025-03-26 00:55:39,455 - training - INFO - Starting epoch 1/200000
2025-03-26 00:55:45,801 - training - INFO - Memory: GPU 0: 2032.9MB allocated, 5794.0MB reserved
2025-03-26 00:55:45,802 - training - INFO - Epoch: 1/200000, Batch: 0/45, Loss: 11.0928, Throughput: 8.83 samples/sec
2025-03-26 00:55:56,574 - training - INFO - Memory: GPU 0: 3192.2MB allocated, 8530.0MB reserved
2025-03-26 00:55:56,574 - training - INFO - Epoch: 1/200000, Batch: 15/45, Loss: 11.0629, Throughput: 52.34 samples/sec
2025-03-26 00:56:07,120 - training - INFO - Memory: GPU 0: 3191.6MB allocated, 8638.0MB reserved
2025-03-26 00:56:07,121 - training - INFO - Epoch: 1/200000, Batch: 30/45, Loss: 11.0531, Throughput: 62.75 samples/sec
2025-03-26 00:56:18,242 - training - INFO - Epoch 1 completed in 38.79s. Average loss: 11.0406
2025-03-26 00:56:18,246 - training - INFO - Starting epoch 2/200000
2025-03-26 00:56:18,967 - training - INFO - Memory: GPU 0: 3195.7MB allocated, 8378.0MB reserved
2025-03-26 00:56:18,968 - training - INFO - Epoch: 2/200000, Batch: 0/45, Loss: 11.1975, Throughput: 77.85 samples/sec
2025-03-26 00:56:29,685 - training - INFO - Memory: GPU 0: 3194.5MB allocated, 8626.0MB reserved
2025-03-26 00:56:29,686 - training - INFO - Epoch: 2/200000, Batch: 15/45, Loss: 10.8640, Throughput: 78.33 samples/sec
2025-03-26 00:56:40,160 - training - INFO - Memory: GPU 0: 3194.5MB allocated, 8626.0MB reserved
2025-03-26 00:56:40,161 - training - INFO - Epoch: 2/200000, Batch: 30/45, Loss: 10.7736, Throughput: 79.22 samples/sec
2025-03-26 00:56:50,086 - training - INFO - Epoch 2 completed in 31.84s. Average loss: 10.7216
2025-03-26 00:56:50,090 - training - INFO - Starting epoch 3/200000
2025-03-26 00:56:50,825 - training - INFO - Memory: GPU 0: 3194.5MB allocated, 8748.0MB reserved
2025-03-26 00:56:50,825 - training - INFO - Epoch: 3/200000, Batch: 0/45, Loss: 10.0921, Throughput: 76.31 samples/sec
2025-03-26 00:57:01,607 - training - INFO - Memory: GPU 0: 3194.3MB allocated, 8674.0MB reserved
2025-03-26 00:57:01,607 - training - INFO - Epoch: 3/200000, Batch: 15/45, Loss: 10.2692, Throughput: 77.81 samples/sec
2025-03-26 00:57:12,039 - training - INFO - Memory: GPU 0: 3194.3MB allocated, 8674.0MB reserved
2025-03-26 00:57:12,040 - training - INFO - Epoch: 3/200000, Batch: 30/45, Loss: 10.1682, Throughput: 79.10 samples/sec
2025-03-26 00:57:21,898 - training - INFO - Epoch 3 completed in 31.81s. Average loss: 10.0677
2025-03-26 00:57:21,902 - training - INFO - Starting epoch 4/200000
2025-03-26 00:57:22,599 - training - INFO - Memory: GPU 0: 3194.3MB allocated, 8796.0MB reserved
2025-03-26 00:57:22,599 - training - INFO - Epoch: 4/200000, Batch: 0/45, Loss: 9.7331, Throughput: 80.57 samples/sec
2025-03-26 00:57:33,413 - training - INFO - Memory: GPU 0: 3192.3MB allocated, 8668.0MB reserved
2025-03-26 00:57:33,413 - training - INFO - Epoch: 4/200000, Batch: 15/45, Loss: 9.5067, Throughput: 77.85 samples/sec
2025-03-26 00:57:41,777 - training - INFO - Training completed successfully.
2025-03-26 01:02:11,399 - training - INFO - Overriding batch size with command-line value: 7
2025-03-26 01:02:11,399 - training - INFO - Overriding gradient accumulation steps with command-line value: 3
2025-03-26 01:02:11,399 - training - INFO - Starting training with debugging enabled. Error file: /tmp/torch_elastic_error_mz466362.log
2025-03-26 01:02:11,399 - training - INFO - Process information: Rank 0, Local Rank 0, World Size 8
2025-03-26 01:02:11,400 - training - INFO - Device: cuda:0
2025-03-26 01:02:12,017 - training - INFO - Training on 9 GPUs, distributed mode: True
2025-03-26 01:02:12,017 - training - INFO - Starting dataset preprocessing on rank 0...
2025-03-26 01:02:12,017 - training - INFO - Starting dataset preprocessing (this will run only once)...
2025-03-26 01:02:12,019 - training - INFO - Preprocessed data already exists at /app/datasets/processed_dataset.pkl. Skipping preprocessing.
2025-03-26 01:02:12,019 - training - INFO - Dataset preprocessing complete. Took 0.00 seconds.
2025-03-26 01:02:16,660 - training - INFO - Starting model initialization...
2025-03-26 01:02:26,353 - training - INFO - Dataset split: 2466 training, 137 validation, 137 test samples
2025-03-26 01:02:26,476 - training - INFO - Test dataset indices saved to /app/UAV-Language-Guided-Navigation/AnsweringAgent/outputs/logs/test_indices.pt
2025-03-26 01:02:26,476 - training - INFO - Per-GPU batch size: 7 (effective batch size: 168, with gradient_accumulation_steps=3)
2025-03-26 01:02:26,501 - training - INFO - Created 24 gradient buckets for efficient all-reduce
2025-03-26 01:02:26,506 - training - INFO - Starting epoch 1/200000
2025-03-26 01:02:32,875 - training - INFO - Memory: GPU 0: 2035.7MB allocated, 5848.0MB reserved
2025-03-26 01:02:32,875 - training - INFO - Epoch: 1/200000, Batch: 0/45, Loss: 11.0932, Throughput: 8.79 samples/sec
2025-03-26 01:02:43,647 - training - INFO - Memory: GPU 0: 3191.5MB allocated, 8530.0MB reserved
2025-03-26 01:02:43,648 - training - INFO - Epoch: 1/200000, Batch: 15/45, Loss: 11.0640, Throughput: 52.28 samples/sec
2025-03-26 01:02:54,268 - training - INFO - Memory: GPU 0: 3191.5MB allocated, 8530.0MB reserved
2025-03-26 01:02:54,268 - training - INFO - Epoch: 1/200000, Batch: 30/45, Loss: 11.0536, Throughput: 62.53 samples/sec
2025-03-26 01:03:05,454 - training - INFO - Epoch 1 completed in 38.95s. Average loss: 11.0405
2025-03-26 01:03:05,460 - training - INFO - Starting epoch 2/200000
2025-03-26 01:03:06,200 - training - INFO - Memory: GPU 0: 3191.6MB allocated, 8284.0MB reserved
2025-03-26 01:03:06,200 - training - INFO - Epoch: 2/200000, Batch: 0/45, Loss: 11.1971, Throughput: 75.78 samples/sec
2025-03-26 01:03:16,859 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8650.0MB reserved
2025-03-26 01:03:16,859 - training - INFO - Epoch: 2/200000, Batch: 15/45, Loss: 10.8636, Throughput: 78.61 samples/sec
2025-03-26 01:03:27,554 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8650.0MB reserved
2025-03-26 01:03:27,554 - training - INFO - Epoch: 2/200000, Batch: 30/45, Loss: 10.7731, Throughput: 78.58 samples/sec
2025-03-26 01:03:37,469 - training - INFO - Epoch 2 completed in 32.01s. Average loss: 10.7216
2025-03-26 01:03:37,474 - training - INFO - Starting epoch 3/200000
2025-03-26 01:03:38,212 - training - INFO - Memory: GPU 0: 3191.0MB allocated, 8772.0MB reserved
2025-03-26 01:03:38,212 - training - INFO - Epoch: 3/200000, Batch: 0/45, Loss: 10.0946, Throughput: 76.13 samples/sec
2025-03-26 01:03:49,016 - training - INFO - Memory: GPU 0: 3190.0MB allocated, 8776.0MB reserved
2025-03-26 01:03:49,017 - training - INFO - Epoch: 3/200000, Batch: 15/45, Loss: 10.2696, Throughput: 77.65 samples/sec
2025-03-26 01:03:59,655 - training - INFO - Memory: GPU 0: 3190.0MB allocated, 8776.0MB reserved
2025-03-26 01:03:59,655 - training - INFO - Epoch: 3/200000, Batch: 30/45, Loss: 10.1681, Throughput: 78.28 samples/sec
2025-03-26 01:04:09,596 - training - INFO - Epoch 3 completed in 32.12s. Average loss: 10.0677
2025-03-26 01:04:09,600 - training - INFO - Starting epoch 4/200000
2025-03-26 01:04:10,300 - training - INFO - Memory: GPU 0: 3190.0MB allocated, 8898.0MB reserved
2025-03-26 01:04:10,300 - training - INFO - Epoch: 4/200000, Batch: 0/45, Loss: 9.7353, Throughput: 80.16 samples/sec
2025-03-26 01:04:21,175 - training - INFO - Memory: GPU 0: 3188.3MB allocated, 8660.0MB reserved
2025-03-26 01:04:21,175 - training - INFO - Epoch: 4/200000, Batch: 15/45, Loss: 9.5068, Throughput: 77.42 samples/sec
2025-03-26 01:04:31,714 - training - INFO - Memory: GPU 0: 3188.3MB allocated, 8660.0MB reserved
2025-03-26 01:04:31,715 - training - INFO - Epoch: 4/200000, Batch: 30/45, Loss: 9.3567, Throughput: 78.51 samples/sec
2025-03-26 01:04:41,611 - training - INFO - Epoch 4 completed in 32.01s. Average loss: 9.2050
2025-03-26 01:04:41,615 - training - INFO - Starting epoch 5/200000
2025-03-26 01:04:42,321 - training - INFO - Memory: GPU 0: 3188.3MB allocated, 8782.0MB reserved
2025-03-26 01:04:42,321 - training - INFO - Epoch: 5/200000, Batch: 0/45, Loss: 8.9318, Throughput: 79.46 samples/sec
2025-03-26 01:04:53,075 - training - INFO - Memory: GPU 0: 3187.5MB allocated, 8660.0MB reserved
2025-03-26 01:04:53,076 - training - INFO - Epoch: 5/200000, Batch: 15/45, Loss: 8.5916, Throughput: 78.20 samples/sec
2025-03-26 01:05:03,656 - training - INFO - Memory: GPU 0: 3186.8MB allocated, 8660.0MB reserved
2025-03-26 01:05:03,657 - training - INFO - Epoch: 5/200000, Batch: 30/45, Loss: 8.4202, Throughput: 78.76 samples/sec
2025-03-26 01:05:13,523 - training - INFO - Epoch 5 completed in 31.91s. Average loss: 8.2899
2025-03-26 01:05:13,527 - training - INFO - Starting epoch 6/200000
2025-03-26 01:05:14,228 - training - INFO - Memory: GPU 0: 3187.5MB allocated, 8784.0MB reserved
2025-03-26 01:05:14,228 - training - INFO - Epoch: 6/200000, Batch: 0/45, Loss: 7.6650, Throughput: 80.07 samples/sec
2025-03-26 01:05:24,969 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8664.0MB reserved
2025-03-26 01:05:24,969 - training - INFO - Epoch: 6/200000, Batch: 15/45, Loss: 7.7617, Throughput: 78.32 samples/sec
2025-03-26 01:05:35,595 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8664.0MB reserved
2025-03-26 01:05:35,596 - training - INFO - Epoch: 6/200000, Batch: 30/45, Loss: 7.6445, Throughput: 78.67 samples/sec
2025-03-26 01:05:45,534 - training - INFO - Epoch 6 completed in 32.01s. Average loss: 7.5829
2025-03-26 01:05:45,537 - training - INFO - Starting epoch 7/200000
2025-03-26 01:05:46,233 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8784.0MB reserved
2025-03-26 01:05:46,234 - training - INFO - Epoch: 7/200000, Batch: 0/45, Loss: 7.1256, Throughput: 80.71 samples/sec
2025-03-26 01:05:57,104 - training - INFO - Memory: GPU 0: 3189.6MB allocated, 8666.0MB reserved
2025-03-26 01:05:57,105 - training - INFO - Epoch: 7/200000, Batch: 15/45, Loss: 7.2731, Throughput: 77.47 samples/sec
2025-03-26 01:06:07,613 - training - INFO - Memory: GPU 0: 3189.6MB allocated, 8666.0MB reserved
2025-03-26 01:06:07,614 - training - INFO - Epoch: 7/200000, Batch: 30/45, Loss: 7.1818, Throughput: 78.65 samples/sec
2025-03-26 01:06:17,559 - training - INFO - Epoch 7 completed in 32.02s. Average loss: 7.1114
2025-03-26 01:06:17,563 - training - INFO - Starting epoch 8/200000
2025-03-26 01:06:18,285 - training - INFO - Memory: GPU 0: 3189.6MB allocated, 8788.0MB reserved
2025-03-26 01:06:18,286 - training - INFO - Epoch: 8/200000, Batch: 0/45, Loss: 6.8648, Throughput: 77.75 samples/sec
2025-03-26 01:06:29,114 - training - INFO - Memory: GPU 0: 3186.8MB allocated, 8672.0MB reserved
2025-03-26 01:06:29,114 - training - INFO - Epoch: 8/200000, Batch: 15/45, Loss: 6.8925, Throughput: 77.58 samples/sec
2025-03-26 01:06:39,718 - training - INFO - Memory: GPU 0: 3186.8MB allocated, 8672.0MB reserved
2025-03-26 01:06:39,719 - training - INFO - Epoch: 8/200000, Batch: 30/45, Loss: 6.8441, Throughput: 78.37 samples/sec
2025-03-26 01:06:49,681 - training - INFO - Epoch 8 completed in 32.12s. Average loss: 6.7733
2025-03-26 01:06:49,685 - training - INFO - Starting epoch 9/200000
2025-03-26 01:06:50,396 - training - INFO - Memory: GPU 0: 3186.3MB allocated, 8792.0MB reserved
2025-03-26 01:06:50,397 - training - INFO - Epoch: 9/200000, Batch: 0/45, Loss: 6.4557, Throughput: 78.84 samples/sec
2025-03-26 01:07:01,199 - training - INFO - Memory: GPU 0: 3186.5MB allocated, 8664.0MB reserved
2025-03-26 01:07:01,200 - training - INFO - Epoch: 9/200000, Batch: 15/45, Loss: 6.5260, Throughput: 77.83 samples/sec
2025-03-26 01:07:11,752 - training - INFO - Memory: GPU 0: 3186.5MB allocated, 8664.0MB reserved
2025-03-26 01:07:11,752 - training - INFO - Epoch: 9/200000, Batch: 30/45, Loss: 6.5213, Throughput: 78.67 samples/sec
2025-03-26 01:07:21,636 - training - INFO - Epoch 9 completed in 31.95s. Average loss: 6.5075
2025-03-26 01:07:21,640 - training - INFO - Starting epoch 10/200000
2025-03-26 01:07:22,346 - training - INFO - Memory: GPU 0: 3185.9MB allocated, 8788.0MB reserved
2025-03-26 01:07:22,347 - training - INFO - Epoch: 10/200000, Batch: 0/45, Loss: 6.2711, Throughput: 79.38 samples/sec
2025-03-26 01:07:33,117 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8660.0MB reserved
2025-03-26 01:07:33,118 - training - INFO - Epoch: 10/200000, Batch: 15/45, Loss: 6.3424, Throughput: 78.08 samples/sec
2025-03-26 01:07:43,640 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8660.0MB reserved
2025-03-26 01:07:43,641 - training - INFO - Epoch: 10/200000, Batch: 30/45, Loss: 6.3367, Throughput: 78.92 samples/sec
2025-03-26 01:07:53,536 - training - INFO - Epoch 10 completed in 31.90s. Average loss: 6.3089
2025-03-26 01:07:53,540 - training - INFO - Starting validation...
2025-03-26 01:07:56,254 - training - INFO - Validation Loss: 6.3324
2025-03-26 01:07:56,255 - training - INFO - Validation loss improved from inf to 6.3324
2025-03-26 01:12:12,779 - training - INFO - New best model saved at epoch 10 (val_loss: 6.3324)
2025-03-26 01:12:12,800 - training - INFO - Starting epoch 11/200000
2025-03-26 01:12:13,496 - training - INFO - Memory: GPU 0: 3196.5MB allocated, 7940.0MB reserved
2025-03-26 01:12:13,496 - training - INFO - Epoch: 11/200000, Batch: 0/45, Loss: 6.5547, Throughput: 80.59 samples/sec
2025-03-26 01:12:24,142 - training - INFO - Memory: GPU 0: 3186.3MB allocated, 8648.0MB reserved
2025-03-26 01:12:24,143 - training - INFO - Epoch: 11/200000, Batch: 15/45, Loss: 6.2507, Throughput: 79.01 samples/sec
2025-03-26 01:12:34,659 - training - INFO - Memory: GPU 0: 3186.3MB allocated, 8650.0MB reserved
2025-03-26 01:12:34,659 - training - INFO - Epoch: 11/200000, Batch: 30/45, Loss: 6.2242, Throughput: 79.45 samples/sec
2025-03-26 01:12:44,626 - training - INFO - Epoch 11 completed in 31.83s. Average loss: 6.1688
2025-03-26 01:12:44,630 - training - INFO - Starting epoch 12/200000
2025-03-26 01:12:45,313 - training - INFO - Memory: GPU 0: 3186.3MB allocated, 8772.0MB reserved
2025-03-26 01:12:45,314 - training - INFO - Epoch: 12/200000, Batch: 0/45, Loss: 6.0953, Throughput: 82.30 samples/sec
2025-03-26 01:12:55,988 - training - INFO - Memory: GPU 0: 3190.8MB allocated, 8678.0MB reserved
2025-03-26 01:12:55,988 - training - INFO - Epoch: 12/200000, Batch: 15/45, Loss: 6.1514, Throughput: 78.91 samples/sec
2025-03-26 01:13:06,495 - training - INFO - Memory: GPU 0: 3190.8MB allocated, 8678.0MB reserved
2025-03-26 01:13:06,495 - training - INFO - Epoch: 12/200000, Batch: 30/45, Loss: 6.1076, Throughput: 79.40 samples/sec
2025-03-26 01:13:16,451 - training - INFO - Epoch 12 completed in 31.82s. Average loss: 6.0665
2025-03-26 01:13:16,455 - training - INFO - Starting epoch 13/200000
2025-03-26 01:13:17,171 - training - INFO - Memory: GPU 0: 3190.8MB allocated, 8800.0MB reserved
2025-03-26 01:13:17,172 - training - INFO - Epoch: 13/200000, Batch: 0/45, Loss: 6.2915, Throughput: 78.32 samples/sec
2025-03-26 01:13:27,936 - training - INFO - Memory: GPU 0: 3189.9MB allocated, 8654.0MB reserved
2025-03-26 01:13:27,937 - training - INFO - Epoch: 13/200000, Batch: 15/45, Loss: 6.0649, Throughput: 78.05 samples/sec
2025-03-26 01:13:38,461 - training - INFO - Memory: GPU 0: 3189.9MB allocated, 8656.0MB reserved
2025-03-26 01:13:38,461 - training - INFO - Epoch: 13/200000, Batch: 30/45, Loss: 6.0179, Throughput: 78.89 samples/sec
2025-03-26 01:13:48,396 - training - INFO - Epoch 13 completed in 31.94s. Average loss: 5.9801
2025-03-26 01:13:48,414 - training - INFO - Starting epoch 14/200000
2025-03-26 01:13:49,090 - training - INFO - Memory: GPU 0: 3189.9MB allocated, 8778.0MB reserved
2025-03-26 01:13:49,090 - training - INFO - Epoch: 14/200000, Batch: 0/45, Loss: 5.8007, Throughput: 82.99 samples/sec
2025-03-26 01:13:59,868 - training - INFO - Memory: GPU 0: 3184.4MB allocated, 8658.0MB reserved
2025-03-26 01:13:59,869 - training - INFO - Epoch: 14/200000, Batch: 15/45, Loss: 5.8960, Throughput: 78.24 samples/sec
2025-03-26 01:14:10,350 - training - INFO - Memory: GPU 0: 3184.4MB allocated, 8660.0MB reserved
2025-03-26 01:14:10,351 - training - INFO - Epoch: 14/200000, Batch: 30/45, Loss: 5.9007, Throughput: 79.14 samples/sec
2025-03-26 01:14:20,299 - training - INFO - Epoch 14 completed in 31.88s. Average loss: 5.9071
2025-03-26 01:14:20,303 - training - INFO - Starting epoch 15/200000
2025-03-26 01:14:21,000 - training - INFO - Memory: GPU 0: 3184.4MB allocated, 8782.0MB reserved
2025-03-26 01:14:21,000 - training - INFO - Epoch: 15/200000, Batch: 0/45, Loss: 6.3704, Throughput: 80.38 samples/sec
2025-03-26 01:14:31,807 - training - INFO - Memory: GPU 0: 3188.3MB allocated, 8664.0MB reserved
2025-03-26 01:14:31,807 - training - INFO - Epoch: 15/200000, Batch: 15/45, Loss: 5.9004, Throughput: 77.89 samples/sec
2025-03-26 01:14:42,326 - training - INFO - Memory: GPU 0: 3188.3MB allocated, 8664.0MB reserved
2025-03-26 01:14:42,326 - training - INFO - Epoch: 15/200000, Batch: 30/45, Loss: 5.8674, Throughput: 78.83 samples/sec
2025-03-26 01:14:52,250 - training - INFO - Epoch 15 completed in 31.95s. Average loss: 5.8538
2025-03-26 01:14:52,254 - training - INFO - Starting epoch 16/200000
2025-03-26 01:14:52,974 - training - INFO - Memory: GPU 0: 3188.3MB allocated, 8788.0MB reserved
2025-03-26 01:14:52,974 - training - INFO - Epoch: 16/200000, Batch: 0/45, Loss: 5.9540, Throughput: 77.90 samples/sec
2025-03-26 01:15:03,776 - training - INFO - Memory: GPU 0: 3187.0MB allocated, 8662.0MB reserved
2025-03-26 01:15:03,777 - training - INFO - Epoch: 16/200000, Batch: 15/45, Loss: 5.8400, Throughput: 77.77 samples/sec
2025-03-26 01:15:14,374 - training - INFO - Memory: GPU 0: 3187.0MB allocated, 8662.0MB reserved
2025-03-26 01:15:14,375 - training - INFO - Epoch: 16/200000, Batch: 30/45, Loss: 5.7950, Throughput: 78.49 samples/sec
2025-03-26 01:15:24,333 - training - INFO - Epoch 16 completed in 32.08s. Average loss: 5.8004
2025-03-26 01:15:24,337 - training - INFO - Starting epoch 17/200000
2025-03-26 01:15:25,028 - training - INFO - Memory: GPU 0: 3187.0MB allocated, 8784.0MB reserved
2025-03-26 01:15:25,028 - training - INFO - Epoch: 17/200000, Batch: 0/45, Loss: 5.8701, Throughput: 81.29 samples/sec
2025-03-26 01:15:35,912 - training - INFO - Memory: GPU 0: 3186.6MB allocated, 8676.0MB reserved
2025-03-26 01:15:35,912 - training - INFO - Epoch: 17/200000, Batch: 15/45, Loss: 5.7642, Throughput: 77.42 samples/sec
2025-03-26 01:15:46,494 - training - INFO - Memory: GPU 0: 3186.6MB allocated, 8676.0MB reserved
2025-03-26 01:15:46,494 - training - INFO - Epoch: 17/200000, Batch: 30/45, Loss: 5.7850, Throughput: 78.36 samples/sec
2025-03-26 01:15:56,397 - training - INFO - Epoch 17 completed in 32.06s. Average loss: 5.7627
2025-03-26 01:15:56,401 - training - INFO - Starting epoch 18/200000
2025-03-26 01:15:57,104 - training - INFO - Memory: GPU 0: 3186.6MB allocated, 8798.0MB reserved
2025-03-26 01:15:57,105 - training - INFO - Epoch: 18/200000, Batch: 0/45, Loss: 5.9635, Throughput: 79.79 samples/sec
2025-03-26 01:16:07,989 - training - INFO - Memory: GPU 0: 3187.3MB allocated, 8664.0MB reserved
2025-03-26 01:16:07,989 - training - INFO - Epoch: 18/200000, Batch: 15/45, Loss: 5.7430, Throughput: 77.33 samples/sec
2025-03-26 01:16:18,601 - training - INFO - Memory: GPU 0: 3187.3MB allocated, 8666.0MB reserved
2025-03-26 01:16:18,602 - training - INFO - Epoch: 18/200000, Batch: 30/45, Loss: 5.7476, Throughput: 78.20 samples/sec
2025-03-26 01:16:28,515 - training - INFO - Epoch 18 completed in 32.11s. Average loss: 5.7338
2025-03-26 01:16:28,519 - training - INFO - Starting epoch 19/200000
2025-03-26 01:16:29,217 - training - INFO - Memory: GPU 0: 3187.3MB allocated, 8786.0MB reserved
2025-03-26 01:16:29,218 - training - INFO - Epoch: 19/200000, Batch: 0/45, Loss: 5.6785, Throughput: 80.30 samples/sec
2025-03-26 01:16:39,984 - training - INFO - Memory: GPU 0: 3186.8MB allocated, 8664.0MB reserved
2025-03-26 01:16:39,984 - training - INFO - Epoch: 19/200000, Batch: 15/45, Loss: 5.7516, Throughput: 78.16 samples/sec
2025-03-26 01:16:50,543 - training - INFO - Memory: GPU 0: 3186.8MB allocated, 8664.0MB reserved
2025-03-26 01:16:50,544 - training - INFO - Epoch: 19/200000, Batch: 30/45, Loss: 5.6978, Throughput: 78.82 samples/sec
2025-03-26 01:17:00,481 - training - INFO - Epoch 19 completed in 31.96s. Average loss: 5.6981
2025-03-26 01:17:00,485 - training - INFO - Starting epoch 20/200000
2025-03-26 01:17:01,186 - training - INFO - Memory: GPU 0: 3186.8MB allocated, 8784.0MB reserved
2025-03-26 01:17:01,187 - training - INFO - Epoch: 20/200000, Batch: 0/45, Loss: 5.9919, Throughput: 79.87 samples/sec
2025-03-26 01:17:11,921 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8666.0MB reserved
2025-03-26 01:17:11,921 - training - INFO - Epoch: 20/200000, Batch: 15/45, Loss: 5.6588, Throughput: 78.35 samples/sec
2025-03-26 01:17:22,437 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8666.0MB reserved
2025-03-26 01:17:22,438 - training - INFO - Epoch: 20/200000, Batch: 30/45, Loss: 5.6763, Throughput: 79.08 samples/sec
2025-03-26 01:17:32,355 - training - INFO - Epoch 20 completed in 31.87s. Average loss: 5.6763
2025-03-26 01:17:32,358 - training - INFO - Starting validation...
2025-03-26 01:17:32,688 - training - INFO - Validation Loss: 6.0247
2025-03-26 01:17:32,688 - training - INFO - Validation loss improved from 6.3324 to 6.0247
2025-03-26 01:17:33,548 - training - INFO - Removed previous best model from epoch 10
2025-03-26 01:21:41,807 - training - INFO - New best model saved at epoch 20 (val_loss: 6.0247)
2025-03-26 01:21:42,040 - training - INFO - Starting epoch 21/200000
2025-03-26 01:21:42,687 - training - INFO - Memory: GPU 0: 3196.5MB allocated, 7940.0MB reserved
2025-03-26 01:21:42,687 - training - INFO - Epoch: 21/200000, Batch: 0/45, Loss: 5.5651, Throughput: 86.81 samples/sec
2025-03-26 01:21:53,337 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8646.0MB reserved
2025-03-26 01:21:53,337 - training - INFO - Epoch: 21/200000, Batch: 15/45, Loss: 5.7325, Throughput: 79.33 samples/sec
2025-03-26 01:22:03,931 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8648.0MB reserved
2025-03-26 01:22:03,932 - training - INFO - Epoch: 21/200000, Batch: 30/45, Loss: 5.6914, Throughput: 79.31 samples/sec
2025-03-26 01:22:13,861 - training - INFO - Epoch 21 completed in 31.82s. Average loss: 5.6366
2025-03-26 01:22:13,864 - training - INFO - Starting epoch 22/200000
2025-03-26 01:22:14,557 - training - INFO - Memory: GPU 0: 3190.3MB allocated, 8770.0MB reserved
2025-03-26 01:22:14,557 - training - INFO - Epoch: 22/200000, Batch: 0/45, Loss: 5.4794, Throughput: 80.98 samples/sec
2025-03-26 01:22:25,282 - training - INFO - Memory: GPU 0: 3190.0MB allocated, 8660.0MB reserved
2025-03-26 01:22:25,283 - training - INFO - Epoch: 22/200000, Batch: 15/45, Loss: 5.6140, Throughput: 78.49 samples/sec
2025-03-26 01:22:35,870 - training - INFO - Memory: GPU 0: 3190.0MB allocated, 8662.0MB reserved
2025-03-26 01:22:35,870 - training - INFO - Epoch: 22/200000, Batch: 30/45, Loss: 5.6229, Throughput: 78.90 samples/sec
2025-03-26 01:22:45,774 - training - INFO - Epoch 22 completed in 31.91s. Average loss: 5.6122
2025-03-26 01:22:45,778 - training - INFO - Starting epoch 23/200000
2025-03-26 01:22:46,471 - training - INFO - Memory: GPU 0: 3192.8MB allocated, 8784.0MB reserved
2025-03-26 01:22:46,471 - training - INFO - Epoch: 23/200000, Batch: 0/45, Loss: 5.7267, Throughput: 80.85 samples/sec
2025-03-26 01:22:57,256 - training - INFO - Memory: GPU 0: 3187.0MB allocated, 8670.0MB reserved
2025-03-26 01:22:57,256 - training - INFO - Epoch: 23/200000, Batch: 15/45, Loss: 5.6210, Throughput: 78.07 samples/sec
2025-03-26 01:23:07,768 - training - INFO - Memory: GPU 0: 3187.0MB allocated, 8670.0MB reserved
2025-03-26 01:23:07,769 - training - INFO - Epoch: 23/200000, Batch: 30/45, Loss: 5.6085, Throughput: 78.95 samples/sec
2025-03-26 01:23:17,644 - training - INFO - Epoch 23 completed in 31.87s. Average loss: 5.5935
2025-03-26 01:23:17,648 - training - INFO - Starting epoch 24/200000
2025-03-26 01:23:18,344 - training - INFO - Memory: GPU 0: 3187.0MB allocated, 8790.0MB reserved
2025-03-26 01:23:18,344 - training - INFO - Epoch: 24/200000, Batch: 0/45, Loss: 5.7328, Throughput: 80.67 samples/sec
2025-03-26 01:23:29,110 - training - INFO - Memory: GPU 0: 3189.1MB allocated, 8656.0MB reserved
2025-03-26 01:23:29,110 - training - INFO - Epoch: 24/200000, Batch: 15/45, Loss: 5.5450, Throughput: 78.18 samples/sec
2025-03-26 01:23:39,620 - training - INFO - Memory: GPU 0: 3189.1MB allocated, 8656.0MB reserved
2025-03-26 01:23:39,620 - training - INFO - Epoch: 24/200000, Batch: 30/45, Loss: 5.5417, Throughput: 79.01 samples/sec
2025-03-26 01:23:49,551 - training - INFO - Epoch 24 completed in 31.90s. Average loss: 5.5765
2025-03-26 01:23:49,554 - training - INFO - Starting epoch 25/200000
2025-03-26 01:23:50,245 - training - INFO - Memory: GPU 0: 3189.1MB allocated, 8778.0MB reserved
2025-03-26 01:23:50,245 - training - INFO - Epoch: 25/200000, Batch: 0/45, Loss: 5.5165, Throughput: 81.25 samples/sec
2025-03-26 01:24:00,940 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8668.0MB reserved
2025-03-26 01:24:00,941 - training - INFO - Epoch: 25/200000, Batch: 15/45, Loss: 5.5868, Throughput: 78.70 samples/sec
2025-03-26 01:24:11,500 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8668.0MB reserved
2025-03-26 01:24:11,500 - training - INFO - Epoch: 25/200000, Batch: 30/45, Loss: 5.5235, Throughput: 79.11 samples/sec
2025-03-26 01:24:21,385 - training - INFO - Epoch 25 completed in 31.83s. Average loss: 5.5547
2025-03-26 01:24:21,388 - training - INFO - Starting epoch 26/200000
2025-03-26 01:24:22,090 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8790.0MB reserved
2025-03-26 01:24:22,091 - training - INFO - Epoch: 26/200000, Batch: 0/45, Loss: 6.0070, Throughput: 79.83 samples/sec
2025-03-26 01:24:32,840 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8660.0MB reserved
2025-03-26 01:24:32,841 - training - INFO - Epoch: 26/200000, Batch: 15/45, Loss: 5.5093, Throughput: 78.25 samples/sec
2025-03-26 01:24:43,336 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8660.0MB reserved
2025-03-26 01:24:43,337 - training - INFO - Epoch: 26/200000, Batch: 30/45, Loss: 5.5297, Throughput: 79.10 samples/sec
2025-03-26 01:24:53,209 - training - INFO - Epoch 26 completed in 31.82s. Average loss: 5.5278
2025-03-26 01:24:53,212 - training - INFO - Starting epoch 27/200000
2025-03-26 01:24:53,936 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8784.0MB reserved
2025-03-26 01:24:53,936 - training - INFO - Epoch: 27/200000, Batch: 0/45, Loss: 5.7176, Throughput: 77.58 samples/sec
2025-03-26 01:25:04,689 - training - INFO - Memory: GPU 0: 3190.1MB allocated, 8656.0MB reserved
2025-03-26 01:25:04,689 - training - INFO - Epoch: 27/200000, Batch: 15/45, Loss: 5.5303, Throughput: 78.09 samples/sec
2025-03-26 01:25:15,259 - training - INFO - Memory: GPU 0: 3190.1MB allocated, 8656.0MB reserved
2025-03-26 01:25:15,260 - training - INFO - Epoch: 27/200000, Batch: 30/45, Loss: 5.5154, Throughput: 78.75 samples/sec
2025-03-26 01:25:25,285 - training - INFO - Epoch 27 completed in 32.07s. Average loss: 5.5157
2025-03-26 01:25:25,289 - training - INFO - Starting epoch 28/200000
2025-03-26 01:25:25,986 - training - INFO - Memory: GPU 0: 3190.1MB allocated, 8778.0MB reserved
2025-03-26 01:25:25,986 - training - INFO - Epoch: 28/200000, Batch: 0/45, Loss: 5.5662, Throughput: 80.55 samples/sec
2025-03-26 01:25:36,832 - training - INFO - Memory: GPU 0: 3184.8MB allocated, 8666.0MB reserved
2025-03-26 01:25:36,832 - training - INFO - Epoch: 28/200000, Batch: 15/45, Loss: 5.5396, Throughput: 77.64 samples/sec
2025-03-26 01:25:47,388 - training - INFO - Memory: GPU 0: 3184.8MB allocated, 8668.0MB reserved
2025-03-26 01:25:47,388 - training - INFO - Epoch: 28/200000, Batch: 30/45, Loss: 5.5185, Throughput: 78.56 samples/sec
2025-03-26 01:25:57,281 - training - INFO - Epoch 28 completed in 31.99s. Average loss: 5.5000
2025-03-26 01:25:57,284 - training - INFO - Starting epoch 29/200000
2025-03-26 01:25:57,978 - training - INFO - Memory: GPU 0: 3184.8MB allocated, 8790.0MB reserved
2025-03-26 01:25:57,979 - training - INFO - Epoch: 29/200000, Batch: 0/45, Loss: 5.5227, Throughput: 80.85 samples/sec
2025-03-26 01:26:08,797 - training - INFO - Memory: GPU 0: 3188.1MB allocated, 8664.0MB reserved
2025-03-26 01:26:08,797 - training - INFO - Epoch: 29/200000, Batch: 15/45, Loss: 5.4652, Throughput: 77.84 samples/sec
2025-03-26 01:26:19,380 - training - INFO - Memory: GPU 0: 3188.1MB allocated, 8666.0MB reserved
2025-03-26 01:26:19,381 - training - INFO - Epoch: 29/200000, Batch: 30/45, Loss: 5.4690, Throughput: 78.57 samples/sec
2025-03-26 01:26:29,349 - training - INFO - Epoch 29 completed in 32.06s. Average loss: 5.4798
2025-03-26 01:26:29,354 - training - INFO - Starting epoch 30/200000
2025-03-26 01:26:30,037 - training - INFO - Memory: GPU 0: 3188.1MB allocated, 8788.0MB reserved
2025-03-26 01:26:30,038 - training - INFO - Epoch: 30/200000, Batch: 0/45, Loss: 5.4028, Throughput: 82.15 samples/sec
2025-03-26 01:26:40,816 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8662.0MB reserved
2025-03-26 01:26:40,816 - training - INFO - Epoch: 30/200000, Batch: 15/45, Loss: 5.4621, Throughput: 78.19 samples/sec
2025-03-26 01:26:51,334 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8662.0MB reserved
2025-03-26 01:26:51,335 - training - INFO - Epoch: 30/200000, Batch: 30/45, Loss: 5.4833, Throughput: 78.99 samples/sec
2025-03-26 01:27:01,212 - training - INFO - Epoch 30 completed in 31.86s. Average loss: 5.4753
2025-03-26 01:27:01,216 - training - INFO - Starting validation...
2025-03-26 01:27:01,526 - training - INFO - Validation Loss: 6.1797
2025-03-26 01:27:01,526 - training - INFO - Validation loss did not improve. Counter: 1/10
2025-03-26 01:27:01,827 - training - INFO - Starting epoch 31/200000
2025-03-26 01:27:02,531 - training - INFO - Memory: GPU 0: 3196.5MB allocated, 7940.0MB reserved
2025-03-26 01:27:02,532 - training - INFO - Epoch: 31/200000, Batch: 0/45, Loss: 5.4169, Throughput: 79.62 samples/sec
2025-03-26 01:27:13,253 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8648.0MB reserved
2025-03-26 01:27:13,254 - training - INFO - Epoch: 31/200000, Batch: 15/45, Loss: 5.5271, Throughput: 78.43 samples/sec
2025-03-26 01:27:23,805 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8648.0MB reserved
2025-03-26 01:27:23,805 - training - INFO - Epoch: 31/200000, Batch: 30/45, Loss: 5.4740, Throughput: 79.00 samples/sec
2025-03-26 01:27:33,696 - training - INFO - Epoch 31 completed in 31.87s. Average loss: 5.4649
2025-03-26 01:27:33,700 - training - INFO - Starting epoch 32/200000
2025-03-26 01:27:34,381 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8768.0MB reserved
2025-03-26 01:27:34,381 - training - INFO - Epoch: 32/200000, Batch: 0/45, Loss: 5.3289, Throughput: 82.54 samples/sec
2025-03-26 01:27:45,114 - training - INFO - Memory: GPU 0: 3187.8MB allocated, 8674.0MB reserved
2025-03-26 01:27:45,115 - training - INFO - Epoch: 32/200000, Batch: 15/45, Loss: 5.5064, Throughput: 78.51 samples/sec
2025-03-26 01:27:55,670 - training - INFO - Memory: GPU 0: 3187.8MB allocated, 8674.0MB reserved
2025-03-26 01:27:55,670 - training - INFO - Epoch: 32/200000, Batch: 30/45, Loss: 5.4621, Throughput: 79.03 samples/sec
2025-03-26 01:28:05,565 - training - INFO - Epoch 32 completed in 31.87s. Average loss: 5.4536
2025-03-26 01:28:05,569 - training - INFO - Starting epoch 33/200000
2025-03-26 01:28:06,252 - training - INFO - Memory: GPU 0: 3187.3MB allocated, 8796.0MB reserved
2025-03-26 01:28:06,252 - training - INFO - Epoch: 33/200000, Batch: 0/45, Loss: 5.6297, Throughput: 82.12 samples/sec
2025-03-26 01:28:16,951 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8654.0MB reserved
2025-03-26 01:28:16,951 - training - INFO - Epoch: 33/200000, Batch: 15/45, Loss: 5.4373, Throughput: 78.73 samples/sec
2025-03-26 01:28:27,443 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8656.0MB reserved
2025-03-26 01:28:27,444 - training - INFO - Epoch: 33/200000, Batch: 30/45, Loss: 5.3980, Throughput: 79.37 samples/sec
2025-03-26 01:28:37,323 - training - INFO - Epoch 33 completed in 31.75s. Average loss: 5.4370
2025-03-26 01:28:37,327 - training - INFO - Starting epoch 34/200000
2025-03-26 01:28:38,026 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8776.0MB reserved
2025-03-26 01:28:38,026 - training - INFO - Epoch: 34/200000, Batch: 0/45, Loss: 5.2439, Throughput: 80.31 samples/sec
2025-03-26 01:28:48,842 - training - INFO - Memory: GPU 0: 3188.3MB allocated, 8668.0MB reserved
2025-03-26 01:28:48,843 - training - INFO - Epoch: 34/200000, Batch: 15/45, Loss: 5.4397, Throughput: 77.82 samples/sec
2025-03-26 01:28:59,330 - training - INFO - Memory: GPU 0: 3188.3MB allocated, 8668.0MB reserved
2025-03-26 01:28:59,331 - training - INFO - Epoch: 34/200000, Batch: 30/45, Loss: 5.4663, Throughput: 78.90 samples/sec
2025-03-26 01:29:09,223 - training - INFO - Epoch 34 completed in 31.90s. Average loss: 5.4432
2025-03-26 01:29:09,227 - training - INFO - Starting epoch 35/200000
2025-03-26 01:29:09,927 - training - INFO - Memory: GPU 0: 3188.3MB allocated, 8788.0MB reserved
2025-03-26 01:29:09,927 - training - INFO - Epoch: 35/200000, Batch: 0/45, Loss: 5.5888, Throughput: 80.09 samples/sec
2025-03-26 01:29:20,709 - training - INFO - Memory: GPU 0: 3187.3MB allocated, 8666.0MB reserved
2025-03-26 01:29:20,710 - training - INFO - Epoch: 35/200000, Batch: 15/45, Loss: 5.3053, Throughput: 78.05 samples/sec
2025-03-26 01:29:31,211 - training - INFO - Memory: GPU 0: 3187.3MB allocated, 8666.0MB reserved
2025-03-26 01:29:31,211 - training - INFO - Epoch: 35/200000, Batch: 30/45, Loss: 5.3627, Throughput: 78.97 samples/sec
2025-03-26 01:29:41,140 - training - INFO - Epoch 35 completed in 31.91s. Average loss: 5.4291
2025-03-26 01:29:41,144 - training - INFO - Starting epoch 36/200000
2025-03-26 01:29:41,849 - training - INFO - Memory: GPU 0: 3188.1MB allocated, 8786.0MB reserved
2025-03-26 01:29:41,849 - training - INFO - Epoch: 36/200000, Batch: 0/45, Loss: 5.4721, Throughput: 79.46 samples/sec
2025-03-26 01:29:52,649 - training - INFO - Memory: GPU 0: 3184.4MB allocated, 8654.0MB reserved
2025-03-26 01:29:52,650 - training - INFO - Epoch: 36/200000, Batch: 15/45, Loss: 5.4181, Throughput: 77.89 samples/sec
2025-03-26 01:30:03,248 - training - INFO - Memory: GPU 0: 3184.4MB allocated, 8656.0MB reserved
2025-03-26 01:30:03,249 - training - INFO - Epoch: 36/200000, Batch: 30/45, Loss: 5.4302, Throughput: 78.54 samples/sec
2025-03-26 01:30:13,189 - training - INFO - Epoch 36 completed in 32.05s. Average loss: 5.4214
2025-03-26 01:30:13,193 - training - INFO - Starting epoch 37/200000
2025-03-26 01:30:13,885 - training - INFO - Memory: GPU 0: 3184.4MB allocated, 8776.0MB reserved
2025-03-26 01:30:13,886 - training - INFO - Epoch: 37/200000, Batch: 0/45, Loss: 5.0830, Throughput: 80.92 samples/sec
2025-03-26 01:30:24,662 - training - INFO - Memory: GPU 0: 3187.0MB allocated, 8672.0MB reserved
2025-03-26 01:30:24,662 - training - INFO - Epoch: 37/200000, Batch: 15/45, Loss: 5.4281, Throughput: 78.14 samples/sec
2025-03-26 01:30:35,268 - training - INFO - Memory: GPU 0: 3187.0MB allocated, 8674.0MB reserved
2025-03-26 01:30:35,268 - training - INFO - Epoch: 37/200000, Batch: 30/45, Loss: 5.4072, Throughput: 78.65 samples/sec
2025-03-26 01:30:45,199 - training - INFO - Epoch 37 completed in 32.01s. Average loss: 5.4136
2025-03-26 01:30:45,203 - training - INFO - Starting epoch 38/200000
2025-03-26 01:30:45,904 - training - INFO - Memory: GPU 0: 3187.0MB allocated, 8794.0MB reserved
2025-03-26 01:30:45,904 - training - INFO - Epoch: 38/200000, Batch: 0/45, Loss: 5.2399, Throughput: 80.11 samples/sec
2025-03-26 01:30:56,618 - training - INFO - Memory: GPU 0: 3184.5MB allocated, 8664.0MB reserved
2025-03-26 01:30:56,619 - training - INFO - Epoch: 38/200000, Batch: 15/45, Loss: 5.4380, Throughput: 78.50 samples/sec
2025-03-26 01:31:07,148 - training - INFO - Memory: GPU 0: 3184.5MB allocated, 8664.0MB reserved
2025-03-26 01:31:07,148 - training - INFO - Epoch: 38/200000, Batch: 30/45, Loss: 5.4270, Throughput: 79.11 samples/sec
2025-03-26 01:31:17,089 - training - INFO - Epoch 38 completed in 31.89s. Average loss: 5.4139
2025-03-26 01:31:17,093 - training - INFO - Starting epoch 39/200000
2025-03-26 01:31:17,789 - training - INFO - Memory: GPU 0: 3184.3MB allocated, 8786.0MB reserved
2025-03-26 01:31:17,790 - training - INFO - Epoch: 39/200000, Batch: 0/45, Loss: 5.5107, Throughput: 80.58 samples/sec
2025-03-26 01:31:28,599 - training - INFO - Memory: GPU 0: 3188.3MB allocated, 8666.0MB reserved
2025-03-26 01:31:28,600 - training - INFO - Epoch: 39/200000, Batch: 15/45, Loss: 5.4322, Throughput: 77.88 samples/sec
2025-03-26 01:31:39,166 - training - INFO - Memory: GPU 0: 3188.3MB allocated, 8666.0MB reserved
2025-03-26 01:31:39,167 - training - INFO - Epoch: 39/200000, Batch: 30/45, Loss: 5.4041, Throughput: 78.66 samples/sec
2025-03-26 01:31:49,047 - training - INFO - Epoch 39 completed in 31.95s. Average loss: 5.4006
2025-03-26 01:31:49,051 - training - INFO - Starting epoch 40/200000
2025-03-26 01:31:49,776 - training - INFO - Memory: GPU 0: 3188.3MB allocated, 8790.0MB reserved
2025-03-26 01:31:49,776 - training - INFO - Epoch: 40/200000, Batch: 0/45, Loss: 5.4572, Throughput: 77.38 samples/sec
2025-03-26 01:32:00,576 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8674.0MB reserved
2025-03-26 01:32:00,576 - training - INFO - Epoch: 40/200000, Batch: 15/45, Loss: 5.4161, Throughput: 77.76 samples/sec
2025-03-26 01:32:11,158 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8674.0MB reserved
2025-03-26 01:32:11,158 - training - INFO - Epoch: 40/200000, Batch: 30/45, Loss: 5.4102, Throughput: 78.54 samples/sec
2025-03-26 01:32:21,006 - training - INFO - Epoch 40 completed in 31.95s. Average loss: 5.3953
2025-03-26 01:32:21,009 - training - INFO - Starting validation...
2025-03-26 01:32:21,309 - training - INFO - Validation Loss: 6.1784
2025-03-26 01:32:21,309 - training - INFO - Validation loss did not improve. Counter: 2/10
2025-03-26 01:32:21,594 - training - INFO - Starting epoch 41/200000
2025-03-26 01:32:22,291 - training - INFO - Memory: GPU 0: 3196.5MB allocated, 7940.0MB reserved
2025-03-26 01:32:22,291 - training - INFO - Epoch: 41/200000, Batch: 0/45, Loss: 5.4250, Throughput: 80.52 samples/sec
2025-03-26 01:32:32,892 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8646.0MB reserved
2025-03-26 01:32:32,893 - training - INFO - Epoch: 41/200000, Batch: 15/45, Loss: 5.3633, Throughput: 79.32 samples/sec
2025-03-26 01:32:43,412 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8648.0MB reserved
2025-03-26 01:32:43,412 - training - INFO - Epoch: 41/200000, Batch: 30/45, Loss: 5.3711, Throughput: 79.57 samples/sec
2025-03-26 01:32:53,309 - training - INFO - Epoch 41 completed in 31.71s. Average loss: 5.3880
2025-03-26 01:32:53,312 - training - INFO - Starting epoch 42/200000
2025-03-26 01:32:54,008 - training - INFO - Memory: GPU 0: 3188.5MB allocated, 8772.0MB reserved
2025-03-26 01:32:54,008 - training - INFO - Epoch: 42/200000, Batch: 0/45, Loss: 5.2514, Throughput: 80.68 samples/sec
2025-03-26 01:33:04,803 - training - INFO - Memory: GPU 0: 3189.8MB allocated, 8664.0MB reserved
2025-03-26 01:33:04,803 - training - INFO - Epoch: 42/200000, Batch: 15/45, Loss: 5.3770, Throughput: 77.99 samples/sec
2025-03-26 01:33:15,413 - training - INFO - Memory: GPU 0: 3187.6MB allocated, 8664.0MB reserved
2025-03-26 01:33:15,414 - training - INFO - Epoch: 42/200000, Batch: 30/45, Loss: 5.3987, Throughput: 78.55 samples/sec
2025-03-26 01:33:25,332 - training - INFO - Epoch 42 completed in 32.02s. Average loss: 5.3803
2025-03-26 01:33:25,336 - training - INFO - Starting epoch 43/200000
2025-03-26 01:33:26,026 - training - INFO - Memory: GPU 0: 3189.8MB allocated, 8788.0MB reserved
2025-03-26 01:33:26,026 - training - INFO - Epoch: 43/200000, Batch: 0/45, Loss: 5.3257, Throughput: 81.28 samples/sec
2025-03-26 01:33:36,668 - training - INFO - Memory: GPU 0: 3192.8MB allocated, 8672.0MB reserved
2025-03-26 01:33:36,669 - training - INFO - Epoch: 43/200000, Batch: 15/45, Loss: 5.4449, Throughput: 79.08 samples/sec
2025-03-26 01:33:47,099 - training - INFO - Memory: GPU 0: 3192.8MB allocated, 8672.0MB reserved
2025-03-26 01:33:47,099 - training - INFO - Epoch: 43/200000, Batch: 30/45, Loss: 5.4148, Throughput: 79.78 samples/sec
2025-03-26 01:33:57,012 - training - INFO - Epoch 43 completed in 31.68s. Average loss: 5.3792
2025-03-26 01:33:57,016 - training - INFO - Starting epoch 44/200000
2025-03-26 01:33:57,726 - training - INFO - Memory: GPU 0: 3192.8MB allocated, 8794.0MB reserved
2025-03-26 01:33:57,726 - training - INFO - Epoch: 44/200000, Batch: 0/45, Loss: 5.4303, Throughput: 78.96 samples/sec
2025-03-26 01:34:08,514 - training - INFO - Memory: GPU 0: 3188.5MB allocated, 8670.0MB reserved
2025-03-26 01:34:08,514 - training - INFO - Epoch: 44/200000, Batch: 15/45, Loss: 5.3441, Throughput: 77.93 samples/sec
2025-03-26 01:34:19,135 - training - INFO - Memory: GPU 0: 3188.5MB allocated, 8672.0MB reserved
2025-03-26 01:34:19,135 - training - INFO - Epoch: 44/200000, Batch: 30/45, Loss: 5.3748, Throughput: 78.49 samples/sec
2025-03-26 01:34:29,018 - training - INFO - Epoch 44 completed in 32.00s. Average loss: 5.3798
2025-03-26 01:34:29,021 - training - INFO - Starting epoch 45/200000
2025-03-26 01:34:29,720 - training - INFO - Memory: GPU 0: 3188.5MB allocated, 8794.0MB reserved
2025-03-26 01:34:29,721 - training - INFO - Epoch: 45/200000, Batch: 0/45, Loss: 5.5134, Throughput: 80.19 samples/sec
2025-03-26 01:34:40,565 - training - INFO - Memory: GPU 0: 3188.3MB allocated, 8660.0MB reserved
2025-03-26 01:34:40,565 - training - INFO - Epoch: 45/200000, Batch: 15/45, Loss: 5.3200, Throughput: 77.63 samples/sec
2025-03-26 01:34:51,117 - training - INFO - Memory: GPU 0: 3188.3MB allocated, 8660.0MB reserved
2025-03-26 01:34:51,118 - training - INFO - Epoch: 45/200000, Batch: 30/45, Loss: 5.3288, Throughput: 78.57 samples/sec
2025-03-26 01:35:01,030 - training - INFO - Epoch 45 completed in 32.01s. Average loss: 5.3681
2025-03-26 01:35:01,034 - training - INFO - Starting epoch 46/200000
2025-03-26 01:35:01,729 - training - INFO - Memory: GPU 0: 3186.0MB allocated, 8780.0MB reserved
2025-03-26 01:35:01,729 - training - INFO - Epoch: 46/200000, Batch: 0/45, Loss: 5.6363, Throughput: 80.82 samples/sec
2025-03-26 01:35:12,591 - training - INFO - Memory: GPU 0: 3187.0MB allocated, 8664.0MB reserved
2025-03-26 01:35:12,591 - training - INFO - Epoch: 46/200000, Batch: 15/45, Loss: 5.3539, Throughput: 77.55 samples/sec
2025-03-26 01:35:23,172 - training - INFO - Memory: GPU 0: 3187.0MB allocated, 8664.0MB reserved
2025-03-26 01:35:23,173 - training - INFO - Epoch: 46/200000, Batch: 30/45, Loss: 5.3822, Throughput: 78.42 samples/sec
2025-03-26 01:35:33,158 - training - INFO - Epoch 46 completed in 32.12s. Average loss: 5.3771
2025-03-26 01:35:33,162 - training - INFO - Starting epoch 47/200000
2025-03-26 01:35:33,871 - training - INFO - Memory: GPU 0: 3187.8MB allocated, 8788.0MB reserved
2025-03-26 01:35:34,078 - training - INFO - Epoch: 47/200000, Batch: 0/45, Loss: 5.3763, Throughput: 79.21 samples/sec
2025-03-26 01:35:44,731 - training - INFO - Memory: GPU 0: 3185.5MB allocated, 8664.0MB reserved
2025-03-26 01:35:44,731 - training - INFO - Epoch: 47/200000, Batch: 15/45, Loss: 5.3482, Throughput: 77.45 samples/sec
2025-03-26 01:35:55,350 - training - INFO - Memory: GPU 0: 3185.5MB allocated, 8664.0MB reserved
2025-03-26 01:35:55,351 - training - INFO - Epoch: 47/200000, Batch: 30/45, Loss: 5.3560, Throughput: 78.24 samples/sec
2025-03-26 01:36:05,214 - training - INFO - Epoch 47 completed in 32.05s. Average loss: 5.3660
2025-03-26 01:36:05,217 - training - INFO - Starting epoch 48/200000
2025-03-26 01:36:05,898 - training - INFO - Memory: GPU 0: 3185.5MB allocated, 8786.0MB reserved
2025-03-26 01:36:05,898 - training - INFO - Epoch: 48/200000, Batch: 0/45, Loss: 5.3669, Throughput: 82.37 samples/sec
2025-03-26 01:36:16,694 - training - INFO - Memory: GPU 0: 3187.7MB allocated, 8678.0MB reserved
2025-03-26 01:36:16,695 - training - INFO - Epoch: 48/200000, Batch: 15/45, Loss: 5.3881, Throughput: 78.08 samples/sec
2025-03-26 01:36:27,202 - training - INFO - Memory: GPU 0: 3187.7MB allocated, 8680.0MB reserved
2025-03-26 01:36:27,203 - training - INFO - Epoch: 48/200000, Batch: 30/45, Loss: 5.3534, Throughput: 78.97 samples/sec
2025-03-26 01:36:37,151 - training - INFO - Epoch 48 completed in 31.93s. Average loss: 5.3598
2025-03-26 01:36:37,155 - training - INFO - Starting epoch 49/200000
2025-03-26 01:36:37,868 - training - INFO - Memory: GPU 0: 3187.7MB allocated, 8800.0MB reserved
2025-03-26 01:36:37,868 - training - INFO - Epoch: 49/200000, Batch: 0/45, Loss: 5.5105, Throughput: 78.80 samples/sec
2025-03-26 01:36:48,702 - training - INFO - Memory: GPU 0: 3185.0MB allocated, 8672.0MB reserved
2025-03-26 01:36:48,702 - training - INFO - Epoch: 49/200000, Batch: 15/45, Loss: 5.4111, Throughput: 77.61 samples/sec
2025-03-26 01:36:59,273 - training - INFO - Memory: GPU 0: 3185.0MB allocated, 8672.0MB reserved
2025-03-26 01:36:59,274 - training - INFO - Epoch: 49/200000, Batch: 30/45, Loss: 5.3948, Throughput: 78.49 samples/sec
2025-03-26 01:37:09,176 - training - INFO - Epoch 49 completed in 32.02s. Average loss: 5.3617
2025-03-26 01:37:09,180 - training - INFO - Starting epoch 50/200000
2025-03-26 01:37:09,902 - training - INFO - Memory: GPU 0: 3185.0MB allocated, 8792.0MB reserved
2025-03-26 01:37:09,902 - training - INFO - Epoch: 50/200000, Batch: 0/45, Loss: 5.3992, Throughput: 77.69 samples/sec
2025-03-26 01:37:20,667 - training - INFO - Memory: GPU 0: 3189.0MB allocated, 8662.0MB reserved
2025-03-26 01:37:20,668 - training - INFO - Epoch: 50/200000, Batch: 15/45, Loss: 5.3149, Throughput: 78.02 samples/sec
2025-03-26 01:37:31,204 - training - INFO - Memory: GPU 0: 3189.0MB allocated, 8662.0MB reserved
2025-03-26 01:37:31,205 - training - INFO - Epoch: 50/200000, Batch: 30/45, Loss: 5.3323, Throughput: 78.83 samples/sec
2025-03-26 01:37:41,164 - training - INFO - Epoch 50 completed in 31.98s. Average loss: 5.3636
2025-03-26 01:37:41,167 - training - INFO - Starting validation...
2025-03-26 01:37:41,467 - training - INFO - Validation Loss: 6.1235
2025-03-26 01:37:41,468 - training - INFO - Validation loss did not improve. Counter: 3/10
2025-03-26 01:37:41,763 - training - INFO - Starting epoch 51/200000
2025-03-26 01:37:42,468 - training - INFO - Memory: GPU 0: 3196.5MB allocated, 7940.0MB reserved
2025-03-26 01:37:42,468 - training - INFO - Epoch: 51/200000, Batch: 0/45, Loss: 5.2169, Throughput: 79.55 samples/sec
2025-03-26 01:37:53,196 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8640.0MB reserved
2025-03-26 01:37:53,196 - training - INFO - Epoch: 51/200000, Batch: 15/45, Loss: 5.3644, Throughput: 78.39 samples/sec
2025-03-26 01:38:03,827 - training - INFO - Memory: GPU 0: 3188.3MB allocated, 8640.0MB reserved
2025-03-26 01:38:03,827 - training - INFO - Epoch: 51/200000, Batch: 30/45, Loss: 5.3704, Throughput: 78.69 samples/sec
2025-03-26 01:38:13,715 - training - INFO - Epoch 51 completed in 31.95s. Average loss: 5.3618
2025-03-26 01:38:13,719 - training - INFO - Starting epoch 52/200000
2025-03-26 01:38:14,408 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8762.0MB reserved
2025-03-26 01:38:14,408 - training - INFO - Epoch: 52/200000, Batch: 0/45, Loss: 5.4191, Throughput: 81.45 samples/sec
2025-03-26 01:38:25,184 - training - INFO - Memory: GPU 0: 3193.0MB allocated, 8680.0MB reserved
2025-03-26 01:38:25,184 - training - INFO - Epoch: 52/200000, Batch: 15/45, Loss: 5.3320, Throughput: 78.17 samples/sec
2025-03-26 01:38:35,637 - training - INFO - Memory: GPU 0: 3193.0MB allocated, 8680.0MB reserved
2025-03-26 01:38:35,637 - training - INFO - Epoch: 52/200000, Batch: 30/45, Loss: 5.3472, Throughput: 79.21 samples/sec
2025-03-26 01:38:45,539 - training - INFO - Epoch 52 completed in 31.82s. Average loss: 5.3440
2025-03-26 01:38:45,543 - training - INFO - Starting epoch 53/200000
2025-03-26 01:38:46,241 - training - INFO - Memory: GPU 0: 3193.0MB allocated, 8802.0MB reserved
2025-03-26 01:38:46,241 - training - INFO - Epoch: 53/200000, Batch: 0/45, Loss: 5.3838, Throughput: 80.27 samples/sec
2025-03-26 01:38:57,017 - training - INFO - Memory: GPU 0: 3189.3MB allocated, 8664.0MB reserved
2025-03-26 01:38:57,018 - training - INFO - Epoch: 53/200000, Batch: 15/45, Loss: 5.2994, Throughput: 78.10 samples/sec
2025-03-26 01:39:07,600 - training - INFO - Memory: GPU 0: 3189.3MB allocated, 8664.0MB reserved
2025-03-26 01:39:07,600 - training - INFO - Epoch: 53/200000, Batch: 30/45, Loss: 5.3024, Throughput: 78.71 samples/sec
2025-03-26 01:39:17,524 - training - INFO - Epoch 53 completed in 31.98s. Average loss: 5.3406
2025-03-26 01:39:17,528 - training - INFO - Starting epoch 54/200000
2025-03-26 01:39:18,222 - training - INFO - Memory: GPU 0: 3189.3MB allocated, 8784.0MB reserved
2025-03-26 01:39:18,222 - training - INFO - Epoch: 54/200000, Batch: 0/45, Loss: 5.1164, Throughput: 80.90 samples/sec
2025-03-26 01:39:29,010 - training - INFO - Memory: GPU 0: 3188.6MB allocated, 8680.0MB reserved
2025-03-26 01:39:29,010 - training - INFO - Epoch: 54/200000, Batch: 15/45, Loss: 5.2976, Throughput: 78.05 samples/sec
2025-03-26 01:39:39,447 - training - INFO - Memory: GPU 0: 3188.6MB allocated, 8680.0MB reserved
2025-03-26 01:39:39,447 - training - INFO - Epoch: 54/200000, Batch: 30/45, Loss: 5.3261, Throughput: 79.20 samples/sec
2025-03-26 01:39:49,289 - training - INFO - Epoch 54 completed in 31.76s. Average loss: 5.3390
2025-03-26 01:39:49,293 - training - INFO - Starting epoch 55/200000
2025-03-26 01:39:49,985 - training - INFO - Memory: GPU 0: 3188.6MB allocated, 8802.0MB reserved
2025-03-26 01:39:49,985 - training - INFO - Epoch: 55/200000, Batch: 0/45, Loss: 5.6699, Throughput: 81.11 samples/sec
2025-03-26 01:40:00,748 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8668.0MB reserved
2025-03-26 01:40:00,748 - training - INFO - Epoch: 55/200000, Batch: 15/45, Loss: 5.3252, Throughput: 78.23 samples/sec
2025-03-26 01:40:11,238 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8668.0MB reserved
2025-03-26 01:40:11,238 - training - INFO - Epoch: 55/200000, Batch: 30/45, Loss: 5.3421, Throughput: 79.11 samples/sec
2025-03-26 01:40:21,105 - training - INFO - Epoch 55 completed in 31.81s. Average loss: 5.3311
2025-03-26 01:40:21,109 - training - INFO - Starting epoch 56/200000
2025-03-26 01:40:21,798 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8788.0MB reserved
2025-03-26 01:40:21,798 - training - INFO - Epoch: 56/200000, Batch: 0/45, Loss: 5.2421, Throughput: 81.30 samples/sec
2025-03-26 01:40:32,602 - training - INFO - Memory: GPU 0: 3189.3MB allocated, 8666.0MB reserved
2025-03-26 01:40:32,602 - training - INFO - Epoch: 56/200000, Batch: 15/45, Loss: 5.3603, Throughput: 77.98 samples/sec
2025-03-26 01:40:43,150 - training - INFO - Memory: GPU 0: 3189.3MB allocated, 8666.0MB reserved
2025-03-26 01:40:43,150 - training - INFO - Epoch: 56/200000, Batch: 30/45, Loss: 5.2917, Throughput: 78.77 samples/sec
2025-03-26 01:40:53,048 - training - INFO - Epoch 56 completed in 31.94s. Average loss: 5.3306
2025-03-26 01:40:53,052 - training - INFO - Starting epoch 57/200000
2025-03-26 01:40:53,742 - training - INFO - Memory: GPU 0: 3189.3MB allocated, 8786.0MB reserved
2025-03-26 01:40:53,742 - training - INFO - Epoch: 57/200000, Batch: 0/45, Loss: 5.4984, Throughput: 81.35 samples/sec
2025-03-26 01:41:04,484 - training - INFO - Memory: GPU 0: 3185.8MB allocated, 8654.0MB reserved
2025-03-26 01:41:04,484 - training - INFO - Epoch: 57/200000, Batch: 15/45, Loss: 5.3578, Throughput: 78.39 samples/sec
2025-03-26 01:41:15,008 - training - INFO - Memory: GPU 0: 3185.8MB allocated, 8656.0MB reserved
2025-03-26 01:41:15,008 - training - INFO - Epoch: 57/200000, Batch: 30/45, Loss: 5.3025, Throughput: 79.07 samples/sec
2025-03-26 01:41:24,879 - training - INFO - Epoch 57 completed in 31.83s. Average loss: 5.3261
2025-03-26 01:41:24,883 - training - INFO - Starting epoch 58/200000
2025-03-26 01:41:25,582 - training - INFO - Memory: GPU 0: 3185.8MB allocated, 8778.0MB reserved
2025-03-26 01:41:25,582 - training - INFO - Epoch: 58/200000, Batch: 0/45, Loss: 5.3615, Throughput: 80.33 samples/sec
2025-03-26 01:41:36,301 - training - INFO - Memory: GPU 0: 3186.8MB allocated, 8676.0MB reserved
2025-03-26 01:41:36,302 - training - INFO - Epoch: 58/200000, Batch: 15/45, Loss: 5.2142, Throughput: 78.48 samples/sec
2025-03-26 01:41:46,760 - training - INFO - Memory: GPU 0: 3186.8MB allocated, 8676.0MB reserved
2025-03-26 01:41:46,760 - training - INFO - Epoch: 58/200000, Batch: 30/45, Loss: 5.2727, Throughput: 79.36 samples/sec
2025-03-26 01:41:56,618 - training - INFO - Epoch 58 completed in 31.74s. Average loss: 5.3231
2025-03-26 01:41:56,622 - training - INFO - Starting epoch 59/200000
2025-03-26 01:41:57,302 - training - INFO - Memory: GPU 0: 3186.0MB allocated, 8800.0MB reserved
2025-03-26 01:41:57,302 - training - INFO - Epoch: 59/200000, Batch: 0/45, Loss: 5.1978, Throughput: 82.42 samples/sec
2025-03-26 01:42:08,062 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8670.0MB reserved
2025-03-26 01:42:08,063 - training - INFO - Epoch: 59/200000, Batch: 15/45, Loss: 5.3120, Throughput: 78.33 samples/sec
2025-03-26 01:42:18,671 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8672.0MB reserved
2025-03-26 01:42:18,671 - training - INFO - Epoch: 59/200000, Batch: 30/45, Loss: 5.3068, Throughput: 78.74 samples/sec
2025-03-26 01:42:28,623 - training - INFO - Epoch 59 completed in 32.00s. Average loss: 5.3217
2025-03-26 01:42:28,627 - training - INFO - Starting epoch 60/200000
2025-03-26 01:42:29,311 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8794.0MB reserved
2025-03-26 01:42:29,312 - training - INFO - Epoch: 60/200000, Batch: 0/45, Loss: 5.3661, Throughput: 82.03 samples/sec
2025-03-26 01:42:40,066 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8678.0MB reserved
2025-03-26 01:42:40,066 - training - INFO - Epoch: 60/200000, Batch: 15/45, Loss: 5.3365, Throughput: 78.33 samples/sec
2025-03-26 01:42:50,651 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8680.0MB reserved
2025-03-26 01:42:50,652 - training - INFO - Epoch: 60/200000, Batch: 30/45, Loss: 5.3377, Throughput: 78.82 samples/sec
2025-03-26 01:43:00,553 - training - INFO - Epoch 60 completed in 31.93s. Average loss: 5.3184
2025-03-26 01:43:00,556 - training - INFO - Starting validation...
2025-03-26 01:43:00,859 - training - INFO - Validation Loss: 6.2340
2025-03-26 01:43:00,859 - training - INFO - Validation loss did not improve. Counter: 4/10
2025-03-26 01:43:01,158 - training - INFO - Starting epoch 61/200000
2025-03-26 01:43:01,863 - training - INFO - Memory: GPU 0: 3196.5MB allocated, 7940.0MB reserved
2025-03-26 01:43:01,863 - training - INFO - Epoch: 61/200000, Batch: 0/45, Loss: 5.2216, Throughput: 79.42 samples/sec
2025-03-26 01:43:12,457 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8648.0MB reserved
2025-03-26 01:43:12,457 - training - INFO - Epoch: 61/200000, Batch: 15/45, Loss: 5.3336, Throughput: 79.31 samples/sec
2025-03-26 01:43:22,894 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8648.0MB reserved
2025-03-26 01:43:22,894 - training - INFO - Epoch: 61/200000, Batch: 30/45, Loss: 5.3365, Throughput: 79.87 samples/sec
2025-03-26 01:43:32,913 - training - INFO - Epoch 61 completed in 31.75s. Average loss: 5.3220
2025-03-26 01:43:32,916 - training - INFO - Starting epoch 62/200000
2025-03-26 01:43:33,608 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8770.0MB reserved
2025-03-26 01:43:33,609 - training - INFO - Epoch: 62/200000, Batch: 0/45, Loss: 5.2019, Throughput: 81.08 samples/sec
2025-03-26 01:43:44,327 - training - INFO - Memory: GPU 0: 3189.6MB allocated, 8654.0MB reserved
2025-03-26 01:43:44,328 - training - INFO - Epoch: 62/200000, Batch: 15/45, Loss: 5.3194, Throughput: 78.53 samples/sec
2025-03-26 01:43:54,810 - training - INFO - Memory: GPU 0: 3189.6MB allocated, 8654.0MB reserved
2025-03-26 01:43:54,810 - training - INFO - Epoch: 62/200000, Batch: 30/45, Loss: 5.3506, Throughput: 79.30 samples/sec
2025-03-26 01:44:04,772 - training - INFO - Epoch 62 completed in 31.86s. Average loss: 5.3085
2025-03-26 01:44:04,775 - training - INFO - Starting epoch 63/200000
2025-03-26 01:44:05,487 - training - INFO - Memory: GPU 0: 3189.6MB allocated, 8778.0MB reserved
2025-03-26 01:44:05,488 - training - INFO - Epoch: 63/200000, Batch: 0/45, Loss: 5.1431, Throughput: 78.85 samples/sec
2025-03-26 01:44:16,304 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8672.0MB reserved
2025-03-26 01:44:16,357 - training - INFO - Epoch: 63/200000, Batch: 15/45, Loss: 5.3327, Throughput: 77.74 samples/sec
2025-03-26 01:44:26,969 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8672.0MB reserved
2025-03-26 01:44:26,969 - training - INFO - Epoch: 63/200000, Batch: 30/45, Loss: 5.2937, Throughput: 78.23 samples/sec
2025-03-26 01:44:36,903 - training - INFO - Epoch 63 completed in 32.13s. Average loss: 5.3010
2025-03-26 01:44:36,906 - training - INFO - Starting epoch 64/200000
2025-03-26 01:44:37,621 - training - INFO - Memory: GPU 0: 3189.3MB allocated, 8794.0MB reserved
2025-03-26 01:44:37,621 - training - INFO - Epoch: 64/200000, Batch: 0/45, Loss: 5.3288, Throughput: 78.58 samples/sec
2025-03-26 01:44:48,396 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8674.0MB reserved
2025-03-26 01:44:48,396 - training - INFO - Epoch: 64/200000, Batch: 15/45, Loss: 5.2693, Throughput: 78.00 samples/sec
2025-03-26 01:44:58,896 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8674.0MB reserved
2025-03-26 01:44:58,896 - training - INFO - Epoch: 64/200000, Batch: 30/45, Loss: 5.2964, Throughput: 78.95 samples/sec
2025-03-26 01:45:08,923 - training - INFO - Epoch 64 completed in 32.02s. Average loss: 5.3239
2025-03-26 01:45:08,926 - training - INFO - Starting epoch 65/200000
2025-03-26 01:45:09,627 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8796.0MB reserved
2025-03-26 01:45:09,628 - training - INFO - Epoch: 65/200000, Batch: 0/45, Loss: 4.9617, Throughput: 79.94 samples/sec
2025-03-26 01:45:20,496 - training - INFO - Memory: GPU 0: 3186.6MB allocated, 8668.0MB reserved
2025-03-26 01:45:20,496 - training - INFO - Epoch: 65/200000, Batch: 15/45, Loss: 5.3302, Throughput: 77.45 samples/sec
2025-03-26 01:45:30,984 - training - INFO - Memory: GPU 0: 3186.6MB allocated, 8668.0MB reserved
2025-03-26 01:45:30,984 - training - INFO - Epoch: 65/200000, Batch: 30/45, Loss: 5.3005, Throughput: 78.71 samples/sec
2025-03-26 01:45:40,913 - training - INFO - Epoch 65 completed in 31.99s. Average loss: 5.3135
2025-03-26 01:45:40,917 - training - INFO - Starting epoch 66/200000
2025-03-26 01:45:41,628 - training - INFO - Memory: GPU 0: 3186.6MB allocated, 8790.0MB reserved
2025-03-26 01:45:41,628 - training - INFO - Epoch: 66/200000, Batch: 0/45, Loss: 5.4476, Throughput: 79.00 samples/sec
2025-03-26 01:45:52,325 - training - INFO - Memory: GPU 0: 3188.5MB allocated, 8672.0MB reserved
2025-03-26 01:45:52,326 - training - INFO - Epoch: 66/200000, Batch: 15/45, Loss: 5.2905, Throughput: 78.55 samples/sec
2025-03-26 01:46:02,797 - training - INFO - Memory: GPU 0: 3188.5MB allocated, 8672.0MB reserved
2025-03-26 01:46:02,798 - training - INFO - Epoch: 66/200000, Batch: 30/45, Loss: 5.2879, Throughput: 79.35 samples/sec
2025-03-26 01:46:12,670 - training - INFO - Epoch 66 completed in 31.75s. Average loss: 5.3056
2025-03-26 01:46:12,673 - training - INFO - Starting epoch 67/200000
2025-03-26 01:46:13,375 - training - INFO - Memory: GPU 0: 3188.5MB allocated, 8794.0MB reserved
2025-03-26 01:46:13,375 - training - INFO - Epoch: 67/200000, Batch: 0/45, Loss: 5.3110, Throughput: 79.96 samples/sec
2025-03-26 01:46:24,091 - training - INFO - Memory: GPU 0: 3185.5MB allocated, 8668.0MB reserved
2025-03-26 01:46:24,091 - training - INFO - Epoch: 67/200000, Batch: 15/45, Loss: 5.3469, Throughput: 78.48 samples/sec
2025-03-26 01:46:34,586 - training - INFO - Memory: GPU 0: 3185.5MB allocated, 8668.0MB reserved
2025-03-26 01:46:34,587 - training - INFO - Epoch: 67/200000, Batch: 30/45, Loss: 5.2933, Throughput: 79.23 samples/sec
2025-03-26 01:46:44,495 - training - INFO - Epoch 67 completed in 31.82s. Average loss: 5.3066
2025-03-26 01:46:44,499 - training - INFO - Starting epoch 68/200000
2025-03-26 01:46:45,180 - training - INFO - Memory: GPU 0: 3185.5MB allocated, 8790.0MB reserved
2025-03-26 01:46:45,180 - training - INFO - Epoch: 68/200000, Batch: 0/45, Loss: 5.4944, Throughput: 82.47 samples/sec
2025-03-26 01:46:56,061 - training - INFO - Memory: GPU 0: 3188.3MB allocated, 8680.0MB reserved
2025-03-26 01:46:56,062 - training - INFO - Epoch: 68/200000, Batch: 15/45, Loss: 5.3421, Throughput: 77.51 samples/sec
2025-03-26 01:47:06,588 - training - INFO - Memory: GPU 0: 3188.3MB allocated, 8680.0MB reserved
2025-03-26 01:47:06,588 - training - INFO - Epoch: 68/200000, Batch: 30/45, Loss: 5.3070, Throughput: 78.59 samples/sec
2025-03-26 01:47:16,516 - training - INFO - Epoch 68 completed in 32.02s. Average loss: 5.2928
2025-03-26 01:47:16,520 - training - INFO - Starting epoch 69/200000
2025-03-26 01:47:17,222 - training - INFO - Memory: GPU 0: 3188.3MB allocated, 8802.0MB reserved
2025-03-26 01:47:17,223 - training - INFO - Epoch: 69/200000, Batch: 0/45, Loss: 5.3290, Throughput: 79.98 samples/sec
2025-03-26 01:47:28,013 - training - INFO - Memory: GPU 0: 3185.5MB allocated, 8670.0MB reserved
2025-03-26 01:47:28,013 - training - INFO - Epoch: 69/200000, Batch: 15/45, Loss: 5.2593, Throughput: 77.97 samples/sec
2025-03-26 01:47:38,525 - training - INFO - Memory: GPU 0: 3185.5MB allocated, 8672.0MB reserved
2025-03-26 01:47:38,525 - training - INFO - Epoch: 69/200000, Batch: 30/45, Loss: 5.2957, Throughput: 78.89 samples/sec
2025-03-26 01:47:48,488 - training - INFO - Epoch 69 completed in 31.97s. Average loss: 5.3009
2025-03-26 01:47:48,492 - training - INFO - Starting epoch 70/200000
2025-03-26 01:47:49,181 - training - INFO - Memory: GPU 0: 3185.5MB allocated, 8794.0MB reserved
2025-03-26 01:47:49,182 - training - INFO - Epoch: 70/200000, Batch: 0/45, Loss: 5.1086, Throughput: 81.46 samples/sec
2025-03-26 01:47:59,958 - training - INFO - Memory: GPU 0: 3185.5MB allocated, 8676.0MB reserved
2025-03-26 01:47:59,959 - training - INFO - Epoch: 70/200000, Batch: 15/45, Loss: 5.3206, Throughput: 78.16 samples/sec
2025-03-26 01:48:10,461 - training - INFO - Memory: GPU 0: 3185.5MB allocated, 8676.0MB reserved
2025-03-26 01:48:10,461 - training - INFO - Epoch: 70/200000, Batch: 30/45, Loss: 5.3037, Throughput: 79.03 samples/sec
2025-03-26 01:48:20,305 - training - INFO - Epoch 70 completed in 31.81s. Average loss: 5.2830
2025-03-26 01:48:20,308 - training - INFO - Starting validation...
2025-03-26 01:48:20,610 - training - INFO - Validation Loss: 6.3297
2025-03-26 01:48:20,610 - training - INFO - Validation loss did not improve. Counter: 5/10
2025-03-26 01:48:20,898 - training - INFO - Starting epoch 71/200000
2025-03-26 01:48:21,606 - training - INFO - Memory: GPU 0: 3196.5MB allocated, 7940.0MB reserved
2025-03-26 01:48:21,606 - training - INFO - Epoch: 71/200000, Batch: 0/45, Loss: 5.3388, Throughput: 79.31 samples/sec
2025-03-26 01:48:32,368 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8646.0MB reserved
2025-03-26 01:48:32,368 - training - INFO - Epoch: 71/200000, Batch: 15/45, Loss: 5.2206, Throughput: 78.12 samples/sec
2025-03-26 01:48:42,914 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8648.0MB reserved
2025-03-26 01:48:42,914 - training - INFO - Epoch: 71/200000, Batch: 30/45, Loss: 5.2629, Throughput: 78.86 samples/sec
2025-03-26 01:48:52,862 - training - INFO - Epoch 71 completed in 31.96s. Average loss: 5.2797
2025-03-26 01:48:52,865 - training - INFO - Starting epoch 72/200000
2025-03-26 01:48:53,550 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8770.0MB reserved
2025-03-26 01:48:53,551 - training - INFO - Epoch: 72/200000, Batch: 0/45, Loss: 5.3072, Throughput: 82.06 samples/sec
2025-03-26 01:49:04,382 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8662.0MB reserved
2025-03-26 01:49:04,382 - training - INFO - Epoch: 72/200000, Batch: 15/45, Loss: 5.2672, Throughput: 77.82 samples/sec
2025-03-26 01:49:14,997 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8662.0MB reserved
2025-03-26 01:49:14,997 - training - INFO - Epoch: 72/200000, Batch: 30/45, Loss: 5.3022, Throughput: 78.45 samples/sec
2025-03-26 01:49:24,994 - training - INFO - Epoch 72 completed in 32.13s. Average loss: 5.2815
2025-03-26 01:49:24,998 - training - INFO - Starting epoch 73/200000
2025-03-26 01:49:25,693 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8784.0MB reserved
2025-03-26 01:49:25,693 - training - INFO - Epoch: 73/200000, Batch: 0/45, Loss: 5.5222, Throughput: 80.71 samples/sec
2025-03-26 01:49:36,542 - training - INFO - Memory: GPU 0: 3188.1MB allocated, 8662.0MB reserved
2025-03-26 01:49:36,543 - training - INFO - Epoch: 73/200000, Batch: 15/45, Loss: 5.2883, Throughput: 77.63 samples/sec
2025-03-26 01:49:47,110 - training - INFO - Memory: GPU 0: 3188.1MB allocated, 8662.0MB reserved
2025-03-26 01:49:47,111 - training - INFO - Epoch: 73/200000, Batch: 30/45, Loss: 5.2797, Throughput: 78.51 samples/sec
2025-03-26 01:49:57,097 - training - INFO - Epoch 73 completed in 32.10s. Average loss: 5.2817
2025-03-26 01:49:57,101 - training - INFO - Starting epoch 74/200000
2025-03-26 01:49:57,782 - training - INFO - Memory: GPU 0: 3188.1MB allocated, 8782.0MB reserved
2025-03-26 01:49:57,782 - training - INFO - Epoch: 74/200000, Batch: 0/45, Loss: 5.2266, Throughput: 82.37 samples/sec
2025-03-26 01:50:08,528 - training - INFO - Memory: GPU 0: 3189.8MB allocated, 8656.0MB reserved
2025-03-26 01:50:08,529 - training - INFO - Epoch: 74/200000, Batch: 15/45, Loss: 5.3034, Throughput: 78.41 samples/sec
2025-03-26 01:50:19,022 - training - INFO - Memory: GPU 0: 3189.8MB allocated, 8656.0MB reserved
2025-03-26 01:50:19,023 - training - INFO - Epoch: 74/200000, Batch: 30/45, Loss: 5.3120, Throughput: 79.20 samples/sec
2025-03-26 01:50:28,881 - training - INFO - Epoch 74 completed in 31.78s. Average loss: 5.2711
2025-03-26 01:50:28,885 - training - INFO - Starting epoch 75/200000
2025-03-26 01:50:29,590 - training - INFO - Memory: GPU 0: 3189.8MB allocated, 8780.0MB reserved
2025-03-26 01:50:29,591 - training - INFO - Epoch: 75/200000, Batch: 0/45, Loss: 5.3055, Throughput: 79.49 samples/sec
2025-03-26 01:50:40,326 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8660.0MB reserved
2025-03-26 01:50:40,327 - training - INFO - Epoch: 75/200000, Batch: 15/45, Loss: 5.3289, Throughput: 78.32 samples/sec
2025-03-26 01:50:50,878 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8660.0MB reserved
2025-03-26 01:50:50,878 - training - INFO - Epoch: 75/200000, Batch: 30/45, Loss: 5.2977, Throughput: 78.94 samples/sec
2025-03-26 01:51:00,819 - training - INFO - Epoch 75 completed in 31.93s. Average loss: 5.2685
2025-03-26 01:51:00,823 - training - INFO - Starting epoch 76/200000
2025-03-26 01:51:01,536 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8780.0MB reserved
2025-03-26 01:51:01,537 - training - INFO - Epoch: 76/200000, Batch: 0/45, Loss: 5.4130, Throughput: 78.56 samples/sec
2025-03-26 01:51:12,281 - training - INFO - Memory: GPU 0: 3190.3MB allocated, 8656.0MB reserved
2025-03-26 01:51:12,282 - training - INFO - Epoch: 76/200000, Batch: 15/45, Loss: 5.2027, Throughput: 78.20 samples/sec
2025-03-26 01:51:22,781 - training - INFO - Memory: GPU 0: 3190.3MB allocated, 8656.0MB reserved
2025-03-26 01:51:22,781 - training - INFO - Epoch: 76/200000, Batch: 30/45, Loss: 5.2595, Throughput: 79.06 samples/sec
2025-03-26 01:51:32,727 - training - INFO - Epoch 76 completed in 31.90s. Average loss: 5.2635
2025-03-26 01:51:32,731 - training - INFO - Starting epoch 77/200000
2025-03-26 01:51:33,446 - training - INFO - Memory: GPU 0: 3190.3MB allocated, 8776.0MB reserved
2025-03-26 01:51:33,446 - training - INFO - Epoch: 77/200000, Batch: 0/45, Loss: 5.3761, Throughput: 78.34 samples/sec
2025-03-26 01:51:44,215 - training - INFO - Memory: GPU 0: 3185.8MB allocated, 8660.0MB reserved
2025-03-26 01:51:44,215 - training - INFO - Epoch: 77/200000, Batch: 15/45, Loss: 5.2410, Throughput: 78.02 samples/sec
2025-03-26 01:51:54,706 - training - INFO - Memory: GPU 0: 3185.8MB allocated, 8662.0MB reserved
2025-03-26 01:51:54,707 - training - INFO - Epoch: 77/200000, Batch: 30/45, Loss: 5.2488, Throughput: 79.00 samples/sec
2025-03-26 01:52:04,585 - training - INFO - Epoch 77 completed in 31.85s. Average loss: 5.2510
2025-03-26 01:52:04,589 - training - INFO - Starting epoch 78/200000
2025-03-26 01:52:05,266 - training - INFO - Memory: GPU 0: 3185.8MB allocated, 8784.0MB reserved
2025-03-26 01:52:05,266 - training - INFO - Epoch: 78/200000, Batch: 0/45, Loss: 5.4859, Throughput: 82.76 samples/sec
2025-03-26 01:52:16,102 - training - INFO - Memory: GPU 0: 3187.4MB allocated, 8666.0MB reserved
2025-03-26 01:52:16,102 - training - INFO - Epoch: 78/200000, Batch: 15/45, Loss: 5.3184, Throughput: 77.83 samples/sec
2025-03-26 01:52:26,691 - training - INFO - Memory: GPU 0: 3187.4MB allocated, 8666.0MB reserved
2025-03-26 01:52:26,691 - training - INFO - Epoch: 78/200000, Batch: 30/45, Loss: 5.2836, Throughput: 78.55 samples/sec
2025-03-26 01:52:36,640 - training - INFO - Epoch 78 completed in 32.05s. Average loss: 5.2421
2025-03-26 01:52:36,644 - training - INFO - Starting epoch 79/200000
2025-03-26 01:52:37,319 - training - INFO - Memory: GPU 0: 3187.4MB allocated, 8788.0MB reserved
2025-03-26 01:52:37,319 - training - INFO - Epoch: 79/200000, Batch: 0/45, Loss: 5.1021, Throughput: 83.10 samples/sec
2025-03-26 01:52:48,050 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8668.0MB reserved
2025-03-26 01:52:48,050 - training - INFO - Epoch: 79/200000, Batch: 15/45, Loss: 5.3199, Throughput: 78.57 samples/sec
2025-03-26 01:52:58,570 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8668.0MB reserved
2025-03-26 01:52:58,670 - training - INFO - Epoch: 79/200000, Batch: 30/45, Loss: 5.2585, Throughput: 79.18 samples/sec
2025-03-26 01:53:08,590 - training - INFO - Epoch 79 completed in 31.95s. Average loss: 5.2606
2025-03-26 01:53:08,594 - training - INFO - Starting epoch 80/200000
2025-03-26 01:53:09,279 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8790.0MB reserved
2025-03-26 01:53:09,279 - training - INFO - Epoch: 80/200000, Batch: 0/45, Loss: 5.2386, Throughput: 81.88 samples/sec
2025-03-26 01:53:20,042 - training - INFO - Memory: GPU 0: 3185.3MB allocated, 8668.0MB reserved
2025-03-26 01:53:20,042 - training - INFO - Epoch: 80/200000, Batch: 15/45, Loss: 5.2234, Throughput: 78.28 samples/sec
2025-03-26 01:53:30,586 - training - INFO - Memory: GPU 0: 3185.3MB allocated, 8668.0MB reserved
2025-03-26 01:53:30,586 - training - INFO - Epoch: 80/200000, Batch: 30/45, Loss: 5.1597, Throughput: 78.94 samples/sec
2025-03-26 01:53:40,492 - training - INFO - Epoch 80 completed in 31.90s. Average loss: 5.2390
2025-03-26 01:53:40,495 - training - INFO - Starting validation...
2025-03-26 01:53:40,792 - training - INFO - Validation Loss: 6.3818
2025-03-26 01:53:40,793 - training - INFO - Validation loss did not improve. Counter: 6/10
2025-03-26 01:53:41,082 - training - INFO - Starting epoch 81/200000
2025-03-26 01:53:41,795 - training - INFO - Memory: GPU 0: 3196.5MB allocated, 7940.0MB reserved
2025-03-26 01:53:41,795 - training - INFO - Epoch: 81/200000, Batch: 0/45, Loss: 5.2106, Throughput: 78.73 samples/sec
2025-03-26 01:53:52,447 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8646.0MB reserved
2025-03-26 01:53:52,447 - training - INFO - Epoch: 81/200000, Batch: 15/45, Loss: 5.2069, Throughput: 78.85 samples/sec
2025-03-26 01:54:02,979 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8646.0MB reserved
2025-03-26 01:54:02,979 - training - INFO - Epoch: 81/200000, Batch: 30/45, Loss: 5.2059, Throughput: 79.28 samples/sec
2025-03-26 01:54:12,850 - training - INFO - Epoch 81 completed in 31.77s. Average loss: 5.2361
2025-03-26 01:54:12,853 - training - INFO - Starting epoch 82/200000
2025-03-26 01:54:13,537 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8770.0MB reserved
2025-03-26 01:54:13,537 - training - INFO - Epoch: 82/200000, Batch: 0/45, Loss: 4.9272, Throughput: 82.03 samples/sec
2025-03-26 01:54:24,345 - training - INFO - Memory: GPU 0: 3189.6MB allocated, 8654.0MB reserved
2025-03-26 01:54:24,345 - training - INFO - Epoch: 82/200000, Batch: 15/45, Loss: 5.2459, Throughput: 77.98 samples/sec
2025-03-26 01:54:34,901 - training - INFO - Memory: GPU 0: 3189.6MB allocated, 8656.0MB reserved
2025-03-26 01:54:34,901 - training - INFO - Epoch: 82/200000, Batch: 30/45, Loss: 5.2292, Throughput: 78.75 samples/sec
2025-03-26 01:54:44,880 - training - INFO - Epoch 82 completed in 32.03s. Average loss: 5.2340
2025-03-26 01:54:44,884 - training - INFO - Starting epoch 83/200000
2025-03-26 01:54:45,588 - training - INFO - Memory: GPU 0: 3189.6MB allocated, 8778.0MB reserved
2025-03-26 01:54:45,588 - training - INFO - Epoch: 83/200000, Batch: 0/45, Loss: 5.1173, Throughput: 79.71 samples/sec
2025-03-26 01:54:56,466 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8672.0MB reserved
2025-03-26 01:54:56,466 - training - INFO - Epoch: 83/200000, Batch: 15/45, Loss: 5.2538, Throughput: 77.36 samples/sec
2025-03-26 01:55:07,108 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8672.0MB reserved
2025-03-26 01:55:07,108 - training - INFO - Epoch: 83/200000, Batch: 30/45, Loss: 5.2036, Throughput: 78.12 samples/sec
2025-03-26 01:55:17,010 - training - INFO - Epoch 83 completed in 32.13s. Average loss: 5.2306
2025-03-26 01:55:17,014 - training - INFO - Starting epoch 84/200000
2025-03-26 01:55:17,724 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8794.0MB reserved
2025-03-26 01:55:17,724 - training - INFO - Epoch: 84/200000, Batch: 0/45, Loss: 5.1842, Throughput: 79.02 samples/sec
2025-03-26 01:55:28,566 - training - INFO - Memory: GPU 0: 3186.9MB allocated, 8664.0MB reserved
2025-03-26 01:55:28,567 - training - INFO - Epoch: 84/200000, Batch: 15/45, Loss: 5.2568, Throughput: 77.57 samples/sec
2025-03-26 01:55:39,059 - training - INFO - Memory: GPU 0: 3186.9MB allocated, 8664.0MB reserved
2025-03-26 01:55:39,060 - training - INFO - Epoch: 84/200000, Batch: 30/45, Loss: 5.2331, Throughput: 78.76 samples/sec
2025-03-26 01:55:48,944 - training - INFO - Epoch 84 completed in 31.93s. Average loss: 5.2025
2025-03-26 01:55:48,948 - training - INFO - Starting epoch 85/200000
2025-03-26 01:55:49,653 - training - INFO - Memory: GPU 0: 3187.0MB allocated, 8786.0MB reserved
2025-03-26 01:55:49,653 - training - INFO - Epoch: 85/200000, Batch: 0/45, Loss: 5.4579, Throughput: 79.62 samples/sec
2025-03-26 01:56:00,356 - training - INFO - Memory: GPU 0: 3188.4MB allocated, 8680.0MB reserved
2025-03-26 01:56:00,356 - training - INFO - Epoch: 85/200000, Batch: 15/45, Loss: 5.2342, Throughput: 78.56 samples/sec
2025-03-26 01:56:10,852 - training - INFO - Memory: GPU 0: 3188.4MB allocated, 8682.0MB reserved
2025-03-26 01:56:10,853 - training - INFO - Epoch: 85/200000, Batch: 30/45, Loss: 5.2270, Throughput: 79.26 samples/sec
2025-03-26 01:56:20,715 - training - INFO - Epoch 85 completed in 31.77s. Average loss: 5.2118
2025-03-26 01:56:20,719 - training - INFO - Starting epoch 86/200000
2025-03-26 01:56:21,416 - training - INFO - Memory: GPU 0: 3188.4MB allocated, 8802.0MB reserved
2025-03-26 01:56:21,416 - training - INFO - Epoch: 86/200000, Batch: 0/45, Loss: 5.1887, Throughput: 80.42 samples/sec
2025-03-26 01:56:32,189 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8660.0MB reserved
2025-03-26 01:56:32,189 - training - INFO - Epoch: 86/200000, Batch: 15/45, Loss: 5.1349, Throughput: 78.12 samples/sec
2025-03-26 01:56:42,714 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8662.0MB reserved
2025-03-26 01:56:42,714 - training - INFO - Epoch: 86/200000, Batch: 30/45, Loss: 5.2020, Throughput: 78.93 samples/sec
2025-03-26 01:56:52,560 - training - INFO - Epoch 86 completed in 31.84s. Average loss: 5.1951
2025-03-26 01:56:52,564 - training - INFO - Starting epoch 87/200000
2025-03-26 01:56:53,252 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8784.0MB reserved
2025-03-26 01:56:53,253 - training - INFO - Epoch: 87/200000, Batch: 0/45, Loss: 5.3741, Throughput: 81.55 samples/sec
2025-03-26 01:57:04,031 - training - INFO - Memory: GPU 0: 3189.3MB allocated, 8680.0MB reserved
2025-03-26 01:57:04,032 - training - INFO - Epoch: 87/200000, Batch: 15/45, Loss: 5.2557, Throughput: 78.15 samples/sec
2025-03-26 01:57:14,500 - training - INFO - Memory: GPU 0: 3189.3MB allocated, 8680.0MB reserved
2025-03-26 01:57:14,500 - training - INFO - Epoch: 87/200000, Batch: 30/45, Loss: 5.1883, Throughput: 79.14 samples/sec
2025-03-26 01:57:24,357 - training - INFO - Epoch 87 completed in 31.79s. Average loss: 5.2002
2025-03-26 01:57:24,361 - training - INFO - Starting epoch 88/200000
2025-03-26 01:57:25,045 - training - INFO - Memory: GPU 0: 3189.3MB allocated, 8802.0MB reserved
2025-03-26 01:57:25,045 - training - INFO - Epoch: 88/200000, Batch: 0/45, Loss: 5.1429, Throughput: 82.08 samples/sec
2025-03-26 01:57:35,837 - training - INFO - Memory: GPU 0: 3186.3MB allocated, 8668.0MB reserved
2025-03-26 01:57:35,838 - training - INFO - Epoch: 88/200000, Batch: 15/45, Loss: 5.1768, Throughput: 78.09 samples/sec
2025-03-26 01:57:46,354 - training - INFO - Memory: GPU 0: 3186.3MB allocated, 8670.0MB reserved
2025-03-26 01:57:46,354 - training - INFO - Epoch: 88/200000, Batch: 30/45, Loss: 5.1767, Throughput: 78.94 samples/sec
2025-03-26 01:57:56,243 - training - INFO - Epoch 88 completed in 31.88s. Average loss: 5.1827
2025-03-26 01:57:56,247 - training - INFO - Starting epoch 89/200000
2025-03-26 01:57:56,938 - training - INFO - Memory: GPU 0: 3186.3MB allocated, 8790.0MB reserved
2025-03-26 01:57:56,938 - training - INFO - Epoch: 89/200000, Batch: 0/45, Loss: 5.2076, Throughput: 81.27 samples/sec
2025-03-26 01:58:07,689 - training - INFO - Memory: GPU 0: 3184.4MB allocated, 8672.0MB reserved
2025-03-26 01:58:07,689 - training - INFO - Epoch: 89/200000, Batch: 15/45, Loss: 5.1052, Throughput: 78.32 samples/sec
2025-03-26 01:58:18,176 - training - INFO - Memory: GPU 0: 3184.4MB allocated, 8672.0MB reserved
2025-03-26 01:58:18,176 - training - INFO - Epoch: 89/200000, Batch: 30/45, Loss: 5.1128, Throughput: 79.17 samples/sec
2025-03-26 01:58:28,046 - training - INFO - Epoch 89 completed in 31.80s. Average loss: 5.1748
2025-03-26 01:58:28,050 - training - INFO - Starting epoch 90/200000
2025-03-26 01:58:28,758 - training - INFO - Memory: GPU 0: 3184.4MB allocated, 8794.0MB reserved
2025-03-26 01:58:28,759 - training - INFO - Epoch: 90/200000, Batch: 0/45, Loss: 5.2053, Throughput: 79.09 samples/sec
2025-03-26 01:58:39,541 - training - INFO - Memory: GPU 0: 3186.3MB allocated, 8674.0MB reserved
2025-03-26 01:58:39,542 - training - INFO - Epoch: 90/200000, Batch: 15/45, Loss: 5.2113, Throughput: 77.99 samples/sec
2025-03-26 01:58:50,097 - training - INFO - Memory: GPU 0: 3186.3MB allocated, 8674.0MB reserved
2025-03-26 01:58:50,098 - training - INFO - Epoch: 90/200000, Batch: 30/45, Loss: 5.2098, Throughput: 78.74 samples/sec
2025-03-26 01:58:59,977 - training - INFO - Epoch 90 completed in 31.93s. Average loss: 5.1790
2025-03-26 01:58:59,980 - training - INFO - Starting validation...
2025-03-26 01:59:00,277 - training - INFO - Validation Loss: 6.4478
2025-03-26 01:59:00,277 - training - INFO - Validation loss did not improve. Counter: 7/10
2025-03-26 01:59:00,576 - training - INFO - Starting epoch 91/200000
2025-03-26 01:59:01,301 - training - INFO - Memory: GPU 0: 3196.5MB allocated, 7940.0MB reserved
2025-03-26 01:59:01,301 - training - INFO - Epoch: 91/200000, Batch: 0/45, Loss: 4.8306, Throughput: 77.42 samples/sec
2025-03-26 01:59:11,918 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8648.0MB reserved
2025-03-26 01:59:11,919 - training - INFO - Epoch: 91/200000, Batch: 15/45, Loss: 5.1386, Throughput: 79.01 samples/sec
2025-03-26 01:59:22,381 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8648.0MB reserved
2025-03-26 01:59:22,381 - training - INFO - Epoch: 91/200000, Batch: 30/45, Loss: 5.1832, Throughput: 79.62 samples/sec
2025-03-26 01:59:32,232 - training - INFO - Epoch 91 completed in 31.66s. Average loss: 5.1572
2025-03-26 01:59:32,236 - training - INFO - Starting epoch 92/200000
2025-03-26 01:59:32,915 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8768.0MB reserved
2025-03-26 01:59:32,915 - training - INFO - Epoch: 92/200000, Batch: 0/45, Loss: 5.1502, Throughput: 82.61 samples/sec
2025-03-26 01:59:43,585 - training - INFO - Memory: GPU 0: 3191.5MB allocated, 8656.0MB reserved
2025-03-26 01:59:43,585 - training - INFO - Epoch: 92/200000, Batch: 15/45, Loss: 5.0857, Throughput: 78.96 samples/sec
2025-03-26 01:59:54,120 - training - INFO - Memory: GPU 0: 3191.5MB allocated, 8656.0MB reserved
2025-03-26 01:59:54,120 - training - INFO - Epoch: 92/200000, Batch: 30/45, Loss: 5.1205, Throughput: 79.34 samples/sec
2025-03-26 02:00:04,056 - training - INFO - Epoch 92 completed in 31.82s. Average loss: 5.1652
2025-03-26 02:00:04,059 - training - INFO - Starting epoch 93/200000
2025-03-26 02:00:04,754 - training - INFO - Memory: GPU 0: 3191.5MB allocated, 8778.0MB reserved
2025-03-26 02:00:04,754 - training - INFO - Epoch: 93/200000, Batch: 0/45, Loss: 5.1891, Throughput: 80.77 samples/sec
2025-03-26 02:00:15,529 - training - INFO - Memory: GPU 0: 3187.3MB allocated, 8680.0MB reserved
2025-03-26 02:00:15,529 - training - INFO - Epoch: 93/200000, Batch: 15/45, Loss: 5.1402, Throughput: 78.14 samples/sec
2025-03-26 02:00:26,058 - training - INFO - Memory: GPU 0: 3187.3MB allocated, 8680.0MB reserved
2025-03-26 02:00:26,058 - training - INFO - Epoch: 93/200000, Batch: 30/45, Loss: 5.1737, Throughput: 78.92 samples/sec
2025-03-26 02:00:36,030 - training - INFO - Epoch 93 completed in 31.97s. Average loss: 5.1515
2025-03-26 02:00:36,033 - training - INFO - Starting epoch 94/200000
2025-03-26 02:00:36,710 - training - INFO - Memory: GPU 0: 3187.3MB allocated, 8802.0MB reserved
2025-03-26 02:00:36,710 - training - INFO - Epoch: 94/200000, Batch: 0/45, Loss: 5.1576, Throughput: 82.97 samples/sec
2025-03-26 02:00:47,523 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8670.0MB reserved
2025-03-26 02:00:47,524 - training - INFO - Epoch: 94/200000, Batch: 15/45, Loss: 5.1279, Throughput: 77.99 samples/sec
2025-03-26 02:00:58,079 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8670.0MB reserved
2025-03-26 02:00:58,079 - training - INFO - Epoch: 94/200000, Batch: 30/45, Loss: 5.1534, Throughput: 78.76 samples/sec
2025-03-26 02:01:08,031 - training - INFO - Epoch 94 completed in 32.00s. Average loss: 5.1541
2025-03-26 02:01:08,035 - training - INFO - Starting epoch 95/200000
2025-03-26 02:01:08,732 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8790.0MB reserved
2025-03-26 02:01:08,732 - training - INFO - Epoch: 95/200000, Batch: 0/45, Loss: 4.8191, Throughput: 80.53 samples/sec
2025-03-26 02:01:19,522 - training - INFO - Memory: GPU 0: 3190.8MB allocated, 8670.0MB reserved
2025-03-26 02:01:19,523 - training - INFO - Epoch: 95/200000, Batch: 15/45, Loss: 5.1188, Throughput: 78.01 samples/sec
2025-03-26 02:01:29,990 - training - INFO - Memory: GPU 0: 3190.8MB allocated, 8670.0MB reserved
2025-03-26 02:01:29,991 - training - INFO - Epoch: 95/200000, Batch: 30/45, Loss: 5.1303, Throughput: 79.08 samples/sec
2025-03-26 02:01:39,857 - training - INFO - Epoch 95 completed in 31.82s. Average loss: 5.1342
2025-03-26 02:01:39,860 - training - INFO - Starting epoch 96/200000
2025-03-26 02:01:40,570 - training - INFO - Memory: GPU 0: 3190.8MB allocated, 8794.0MB reserved
2025-03-26 02:01:40,789 - training - INFO - Epoch: 96/200000, Batch: 0/45, Loss: 5.4684, Throughput: 79.14 samples/sec
2025-03-26 02:01:51,516 - training - INFO - Memory: GPU 0: 3190.3MB allocated, 8658.0MB reserved
2025-03-26 02:01:51,516 - training - INFO - Epoch: 96/200000, Batch: 15/45, Loss: 5.1035, Throughput: 76.88 samples/sec
2025-03-26 02:02:02,112 - training - INFO - Memory: GPU 0: 3190.3MB allocated, 8658.0MB reserved
2025-03-26 02:02:02,113 - training - INFO - Epoch: 96/200000, Batch: 30/45, Loss: 5.1519, Throughput: 78.02 samples/sec
2025-03-26 02:02:12,082 - training - INFO - Epoch 96 completed in 32.22s. Average loss: 5.1355
2025-03-26 02:02:12,086 - training - INFO - Starting epoch 97/200000
2025-03-26 02:02:12,768 - training - INFO - Memory: GPU 0: 3190.3MB allocated, 8780.0MB reserved
2025-03-26 02:02:12,768 - training - INFO - Epoch: 97/200000, Batch: 0/45, Loss: 5.3185, Throughput: 82.20 samples/sec
2025-03-26 02:02:23,600 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8664.0MB reserved
2025-03-26 02:02:23,601 - training - INFO - Epoch: 97/200000, Batch: 15/45, Loss: 5.1437, Throughput: 77.83 samples/sec
2025-03-26 02:02:34,149 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8664.0MB reserved
2025-03-26 02:02:34,149 - training - INFO - Epoch: 97/200000, Batch: 30/45, Loss: 5.0891, Throughput: 78.69 samples/sec
2025-03-26 02:02:44,117 - training - INFO - Epoch 97 completed in 32.03s. Average loss: 5.1239
2025-03-26 02:02:44,121 - training - INFO - Starting epoch 98/200000
2025-03-26 02:02:44,818 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8786.0MB reserved
2025-03-26 02:02:44,818 - training - INFO - Epoch: 98/200000, Batch: 0/45, Loss: 5.4138, Throughput: 80.51 samples/sec
2025-03-26 02:02:55,536 - training - INFO - Memory: GPU 0: 3186.5MB allocated, 8666.0MB reserved
2025-03-26 02:02:55,537 - training - INFO - Epoch: 98/200000, Batch: 15/45, Loss: 5.0811, Throughput: 78.50 samples/sec
2025-03-26 02:03:05,974 - training - INFO - Memory: GPU 0: 3186.5MB allocated, 8666.0MB reserved
2025-03-26 02:03:05,974 - training - INFO - Epoch: 98/200000, Batch: 30/45, Loss: 5.0974, Throughput: 79.44 samples/sec
2025-03-26 02:03:15,821 - training - INFO - Epoch 98 completed in 31.70s. Average loss: 5.1169
2025-03-26 02:03:15,825 - training - INFO - Starting epoch 99/200000
2025-03-26 02:03:16,536 - training - INFO - Memory: GPU 0: 3186.5MB allocated, 8788.0MB reserved
2025-03-26 02:03:16,536 - training - INFO - Epoch: 99/200000, Batch: 0/45, Loss: 5.1016, Throughput: 78.90 samples/sec
2025-03-26 02:03:27,265 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8666.0MB reserved
2025-03-26 02:03:27,265 - training - INFO - Epoch: 99/200000, Batch: 15/45, Loss: 5.0954, Throughput: 78.33 samples/sec
2025-03-26 02:03:37,757 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8668.0MB reserved
2025-03-26 02:03:37,757 - training - INFO - Epoch: 99/200000, Batch: 30/45, Loss: 5.1052, Throughput: 79.16 samples/sec
2025-03-26 02:03:47,634 - training - INFO - Epoch 99 completed in 31.81s. Average loss: 5.1154
2025-03-26 02:03:47,637 - training - INFO - Starting epoch 100/200000
2025-03-26 02:03:48,327 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8790.0MB reserved
2025-03-26 02:03:48,327 - training - INFO - Epoch: 100/200000, Batch: 0/45, Loss: 4.8589, Throughput: 81.33 samples/sec
2025-03-26 02:03:59,194 - training - INFO - Memory: GPU 0: 3185.3MB allocated, 8670.0MB reserved
2025-03-26 02:03:59,195 - training - INFO - Epoch: 100/200000, Batch: 15/45, Loss: 5.0681, Throughput: 77.54 samples/sec
2025-03-26 02:04:09,897 - training - INFO - Memory: GPU 0: 3185.3MB allocated, 8672.0MB reserved
2025-03-26 02:04:09,897 - training - INFO - Epoch: 100/200000, Batch: 30/45, Loss: 5.1234, Throughput: 77.99 samples/sec
2025-03-26 02:04:19,775 - training - INFO - Epoch 100 completed in 32.14s. Average loss: 5.1187
2025-03-26 02:04:19,778 - training - INFO - Starting validation...
2025-03-26 02:04:20,086 - training - INFO - Validation Loss: 6.5245
2025-03-26 02:04:20,086 - training - INFO - Validation loss did not improve. Counter: 8/10
2025-03-26 02:04:20,371 - training - INFO - Starting epoch 101/200000
2025-03-26 02:04:21,059 - training - INFO - Memory: GPU 0: 3196.5MB allocated, 7940.0MB reserved
2025-03-26 02:04:21,060 - training - INFO - Epoch: 101/200000, Batch: 0/45, Loss: 5.2707, Throughput: 81.62 samples/sec
2025-03-26 02:04:31,823 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8648.0MB reserved
2025-03-26 02:04:31,823 - training - INFO - Epoch: 101/200000, Batch: 15/45, Loss: 5.1410, Throughput: 78.26 samples/sec
2025-03-26 02:04:42,323 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8648.0MB reserved
2025-03-26 02:04:42,323 - training - INFO - Epoch: 101/200000, Batch: 30/45, Loss: 5.1778, Throughput: 79.09 samples/sec
2025-03-26 02:04:52,213 - training - INFO - Epoch 101 completed in 31.84s. Average loss: 5.1024
2025-03-26 02:04:52,217 - training - INFO - Starting epoch 102/200000
2025-03-26 02:04:52,889 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8770.0MB reserved
2025-03-26 02:04:52,889 - training - INFO - Epoch: 102/200000, Batch: 0/45, Loss: 4.9956, Throughput: 83.55 samples/sec
2025-03-26 02:05:03,657 - training - INFO - Memory: GPU 0: 3189.6MB allocated, 8656.0MB reserved
2025-03-26 02:05:03,658 - training - INFO - Epoch: 102/200000, Batch: 15/45, Loss: 5.0789, Throughput: 78.33 samples/sec
2025-03-26 02:05:14,188 - training - INFO - Memory: GPU 0: 3189.6MB allocated, 8656.0MB reserved
2025-03-26 02:05:14,188 - training - INFO - Epoch: 102/200000, Batch: 30/45, Loss: 5.0835, Throughput: 79.02 samples/sec
2025-03-26 02:05:24,113 - training - INFO - Epoch 102 completed in 31.90s. Average loss: 5.1123
2025-03-26 02:05:24,116 - training - INFO - Starting epoch 103/200000
2025-03-26 02:05:24,808 - training - INFO - Memory: GPU 0: 3191.1MB allocated, 8778.0MB reserved
2025-03-26 02:05:24,808 - training - INFO - Epoch: 103/200000, Batch: 0/45, Loss: 5.1486, Throughput: 81.10 samples/sec
2025-03-26 02:05:35,564 - training - INFO - Memory: GPU 0: 3186.5MB allocated, 8664.0MB reserved
2025-03-26 02:05:35,564 - training - INFO - Epoch: 103/200000, Batch: 15/45, Loss: 5.0774, Throughput: 78.28 samples/sec
2025-03-26 02:05:46,056 - training - INFO - Memory: GPU 0: 3186.5MB allocated, 8664.0MB reserved
2025-03-26 02:05:46,057 - training - INFO - Epoch: 103/200000, Batch: 30/45, Loss: 5.1052, Throughput: 79.13 samples/sec
2025-03-26 02:05:56,008 - training - INFO - Epoch 103 completed in 31.89s. Average loss: 5.1031
2025-03-26 02:05:56,012 - training - INFO - Starting epoch 104/200000
2025-03-26 02:05:56,685 - training - INFO - Memory: GPU 0: 3186.5MB allocated, 8788.0MB reserved
2025-03-26 02:05:56,685 - training - INFO - Epoch: 104/200000, Batch: 0/45, Loss: 5.2371, Throughput: 83.31 samples/sec
2025-03-26 02:06:07,448 - training - INFO - Memory: GPU 0: 3188.8MB allocated, 8662.0MB reserved
2025-03-26 02:06:07,448 - training - INFO - Epoch: 104/200000, Batch: 15/45, Loss: 5.1080, Throughput: 78.36 samples/sec
2025-03-26 02:06:18,064 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8662.0MB reserved
2025-03-26 02:06:18,064 - training - INFO - Epoch: 104/200000, Batch: 30/45, Loss: 5.0780, Throughput: 78.73 samples/sec
2025-03-26 02:06:28,049 - training - INFO - Epoch 104 completed in 32.04s. Average loss: 5.0966
2025-03-26 02:06:28,053 - training - INFO - Starting epoch 105/200000
2025-03-26 02:06:28,745 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8784.0MB reserved
2025-03-26 02:06:28,745 - training - INFO - Epoch: 105/200000, Batch: 0/45, Loss: 5.2290, Throughput: 81.07 samples/sec
2025-03-26 02:06:39,433 - training - INFO - Memory: GPU 0: 3188.4MB allocated, 8660.0MB reserved
2025-03-26 02:06:39,434 - training - INFO - Epoch: 105/200000, Batch: 15/45, Loss: 5.1141, Throughput: 78.74 samples/sec
2025-03-26 02:06:49,910 - training - INFO - Memory: GPU 0: 3188.4MB allocated, 8660.0MB reserved
2025-03-26 02:06:49,910 - training - INFO - Epoch: 105/200000, Batch: 30/45, Loss: 5.1091, Throughput: 79.43 samples/sec
2025-03-26 02:06:59,804 - training - INFO - Epoch 105 completed in 31.75s. Average loss: 5.0888
2025-03-26 02:06:59,808 - training - INFO - Starting epoch 106/200000
2025-03-26 02:07:00,509 - training - INFO - Memory: GPU 0: 3188.4MB allocated, 8782.0MB reserved
2025-03-26 02:07:00,510 - training - INFO - Epoch: 106/200000, Batch: 0/45, Loss: 4.4275, Throughput: 80.00 samples/sec
2025-03-26 02:07:11,312 - training - INFO - Memory: GPU 0: 3187.3MB allocated, 8664.0MB reserved
2025-03-26 02:07:11,313 - training - INFO - Epoch: 106/200000, Batch: 15/45, Loss: 4.9558, Throughput: 77.89 samples/sec
2025-03-26 02:07:21,852 - training - INFO - Memory: GPU 0: 3187.3MB allocated, 8664.0MB reserved
2025-03-26 02:07:21,853 - training - INFO - Epoch: 106/200000, Batch: 30/45, Loss: 5.0103, Throughput: 78.76 samples/sec
2025-03-26 02:07:31,737 - training - INFO - Epoch 106 completed in 31.93s. Average loss: 5.0744
2025-03-26 02:07:31,740 - training - INFO - Starting epoch 107/200000
2025-03-26 02:07:32,427 - training - INFO - Memory: GPU 0: 3187.3MB allocated, 8786.0MB reserved
2025-03-26 02:07:32,428 - training - INFO - Epoch: 107/200000, Batch: 0/45, Loss: 4.7992, Throughput: 81.68 samples/sec
2025-03-26 02:07:43,212 - training - INFO - Memory: GPU 0: 3185.8MB allocated, 8678.0MB reserved
2025-03-26 02:07:43,213 - training - INFO - Epoch: 107/200000, Batch: 15/45, Loss: 5.0433, Throughput: 78.11 samples/sec
2025-03-26 02:07:53,701 - training - INFO - Memory: GPU 0: 3185.8MB allocated, 8680.0MB reserved
2025-03-26 02:07:53,702 - training - INFO - Epoch: 107/200000, Batch: 30/45, Loss: 5.0461, Throughput: 79.05 samples/sec
2025-03-26 02:08:03,553 - training - INFO - Epoch 107 completed in 31.81s. Average loss: 5.0722
2025-03-26 02:08:03,557 - training - INFO - Starting epoch 108/200000
2025-03-26 02:08:04,236 - training - INFO - Memory: GPU 0: 3185.8MB allocated, 8802.0MB reserved
2025-03-26 02:08:04,236 - training - INFO - Epoch: 108/200000, Batch: 0/45, Loss: 5.0716, Throughput: 82.62 samples/sec
2025-03-26 02:08:14,958 - training - INFO - Memory: GPU 0: 3185.3MB allocated, 8670.0MB reserved
2025-03-26 02:08:14,958 - training - INFO - Epoch: 108/200000, Batch: 15/45, Loss: 5.0739, Throughput: 78.59 samples/sec
2025-03-26 02:08:25,389 - training - INFO - Memory: GPU 0: 3185.3MB allocated, 8670.0MB reserved
2025-03-26 02:08:25,389 - training - INFO - Epoch: 108/200000, Batch: 30/45, Loss: 5.0669, Throughput: 79.52 samples/sec
2025-03-26 02:08:35,241 - training - INFO - Epoch 108 completed in 31.68s. Average loss: 5.0690
2025-03-26 02:08:35,245 - training - INFO - Starting epoch 109/200000
2025-03-26 02:08:35,950 - training - INFO - Memory: GPU 0: 3185.3MB allocated, 8790.0MB reserved
2025-03-26 02:08:35,950 - training - INFO - Epoch: 109/200000, Batch: 0/45, Loss: 4.8366, Throughput: 79.56 samples/sec
2025-03-26 02:08:46,777 - training - INFO - Memory: GPU 0: 3189.8MB allocated, 8672.0MB reserved
2025-03-26 02:08:46,778 - training - INFO - Epoch: 109/200000, Batch: 15/45, Loss: 4.9640, Throughput: 77.71 samples/sec
2025-03-26 02:08:57,468 - training - INFO - Memory: GPU 0: 3189.8MB allocated, 8674.0MB reserved
2025-03-26 02:08:57,468 - training - INFO - Epoch: 109/200000, Batch: 30/45, Loss: 5.0529, Throughput: 78.12 samples/sec
2025-03-26 02:09:07,459 - training - INFO - Epoch 109 completed in 32.21s. Average loss: 5.0611
2025-03-26 02:09:07,463 - training - INFO - Starting epoch 110/200000
2025-03-26 02:09:08,157 - training - INFO - Memory: GPU 0: 3189.8MB allocated, 8794.0MB reserved
2025-03-26 02:09:08,157 - training - INFO - Epoch: 110/200000, Batch: 0/45, Loss: 5.0060, Throughput: 80.86 samples/sec
2025-03-26 02:09:19,045 - training - INFO - Memory: GPU 0: 3186.4MB allocated, 8670.0MB reserved
2025-03-26 02:09:19,045 - training - INFO - Epoch: 110/200000, Batch: 15/45, Loss: 5.0333, Throughput: 77.37 samples/sec
2025-03-26 02:09:29,601 - training - INFO - Memory: GPU 0: 3186.4MB allocated, 8672.0MB reserved
2025-03-26 02:09:29,601 - training - INFO - Epoch: 110/200000, Batch: 30/45, Loss: 5.0379, Throughput: 78.42 samples/sec
2025-03-26 02:09:39,512 - training - INFO - Epoch 110 completed in 32.05s. Average loss: 5.0605
2025-03-26 02:09:39,515 - training - INFO - Starting validation...
2025-03-26 02:09:39,814 - training - INFO - Validation Loss: 6.5019
2025-03-26 02:09:39,815 - training - INFO - Validation loss did not improve. Counter: 9/10
2025-03-26 02:09:40,106 - training - INFO - Starting epoch 111/200000
2025-03-26 02:09:40,814 - training - INFO - Memory: GPU 0: 3196.5MB allocated, 7940.0MB reserved
2025-03-26 02:09:40,814 - training - INFO - Epoch: 111/200000, Batch: 0/45, Loss: 5.1212, Throughput: 79.21 samples/sec
2025-03-26 02:09:51,461 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8648.0MB reserved
2025-03-26 02:09:51,461 - training - INFO - Epoch: 111/200000, Batch: 15/45, Loss: 5.1157, Throughput: 78.92 samples/sec
2025-03-26 02:10:02,113 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8648.0MB reserved
2025-03-26 02:10:02,114 - training - INFO - Epoch: 111/200000, Batch: 30/45, Loss: 5.0978, Throughput: 78.89 samples/sec
2025-03-26 02:10:12,019 - training - INFO - Epoch 111 completed in 31.91s. Average loss: 5.0420
2025-03-26 02:10:12,022 - training - INFO - Starting epoch 112/200000
2025-03-26 02:10:12,719 - training - INFO - Memory: GPU 0: 3188.0MB allocated, 8768.0MB reserved
2025-03-26 02:10:12,719 - training - INFO - Epoch: 112/200000, Batch: 0/45, Loss: 4.9275, Throughput: 80.58 samples/sec
2025-03-26 02:10:23,430 - training - INFO - Memory: GPU 0: 3188.9MB allocated, 8656.0MB reserved
2025-03-26 02:10:23,431 - training - INFO - Epoch: 112/200000, Batch: 15/45, Loss: 5.0447, Throughput: 78.56 samples/sec
2025-03-26 02:10:33,922 - training - INFO - Memory: GPU 0: 3188.9MB allocated, 8656.0MB reserved
2025-03-26 02:10:33,922 - training - INFO - Epoch: 112/200000, Batch: 30/45, Loss: 5.0167, Throughput: 79.28 samples/sec
2025-03-26 02:10:43,878 - training - INFO - Epoch 112 completed in 31.86s. Average loss: 5.0437
2025-03-26 02:10:43,881 - training - INFO - Starting epoch 113/200000
2025-03-26 02:10:44,570 - training - INFO - Memory: GPU 0: 3188.9MB allocated, 8778.0MB reserved
2025-03-26 02:10:44,571 - training - INFO - Epoch: 113/200000, Batch: 0/45, Loss: 5.0957, Throughput: 81.45 samples/sec
2025-03-26 02:10:55,453 - training - INFO - Memory: GPU 0: 3189.5MB allocated, 8672.0MB reserved
2025-03-26 02:10:55,453 - training - INFO - Epoch: 113/200000, Batch: 15/45, Loss: 4.9997, Throughput: 77.44 samples/sec
