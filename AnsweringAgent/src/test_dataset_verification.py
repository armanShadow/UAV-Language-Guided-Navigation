#!/usr/bin/env python3
"""
Test Dataset Verification
========================

Verifies that the preprocessed .pkl datasets match the augmented JSON format.
Checks for:
1. Episode count matching between JSON and PKL
2. Dialog turn count matching (excluding turn 0)
3. Exact content matching for each turn
4. Paraphrase preservation in preprocessed data

USAGE:
    cd AnsweringAgent/src
    python test_dataset_verification.py
"""

import json
import pickle
import os
import random
from typing import Dict, Any, List, Tuple
from config import Config
from transformers import T5Tokenizer

def load_json_data(json_path: str) -> Tuple[List[Dict], Dict[str, Any]]:
    """Load JSON data and extract episodes and dialog turns with paraphrases."""
    print(f"ğŸ“‚ Loading JSON data from {json_path}")
    
    with open(json_path, 'r') as f:
        episodes = json.load(f)
    
    # Extract all dialog turns with paraphrases (excluding turn 0)
    turns_with_paraphrases = []
    total_episodes = len(episodes)
    total_turns = 0
    valid_turns = 0
    
    for episode in episodes:
        episode_id = episode['episode_id']
        episode_turns = []
        
        for dialog in episode['dialogs']:
            total_turns += 1
            if dialog['turn_id'] > 0:  # Skip turn 0
                valid_turns += 1
                turn_data = {
                    'episode_id': episode_id,
                    'turn_id': dialog['turn_id'],
                    'question': dialog['question'],
                    'answer': dialog['answer'],
                    'dialog_history': dialog['dialog_history'],
                    'first_instruction': episode['first_instruction'],
                    'observation': dialog['observation'],
                    'previous_observations': dialog['previous_observations']
                }
                
                if 'paraphrases' in dialog:
                    turn_data['paraphrases'] = dialog['paraphrases']
                
                turns_with_paraphrases.append(turn_data)
                episode_turns.append(turn_data)
        
        # Store episode summary
        episode['valid_turns'] = episode_turns
    
    stats = {
        'total_episodes': total_episodes,
        'total_turns': total_turns,
        'valid_turns': valid_turns,
        'turns_with_paraphrases': len([t for t in turns_with_paraphrases if 'paraphrases' in t])
    }
    
    print(f"  âœ… Found {total_episodes} episodes")
    print(f"  âœ… Total turns: {total_turns}, Valid turns (excluding turn 0): {valid_turns}")
    print(f"  âœ… Turns with paraphrases: {stats['turns_with_paraphrases']}")
    
    return episodes, stats

def load_preprocessed_data(pkl_path: str, is_train: bool = False) -> Tuple[Dict[int, Any], Dict[str, Any]]:
    """Load preprocessed .pkl data."""
    print(f"ğŸ“‚ Loading preprocessed data from {pkl_path}")
    
    if is_train:
        # Load chunked train data
        data = {}
        total_chunks = 0
        for file in os.listdir(pkl_path):
            if file.endswith('.pkl'):
                chunk_path = os.path.join(pkl_path, file)
                with open(chunk_path, 'rb') as f:
                    chunk_data = pickle.load(f)
                    data.update(chunk_data)
                    total_chunks += 1
                    print(f"  ğŸ“¦ Loaded chunk {file}: {len(chunk_data)} items")
    else:
        # Load single validation file
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)
            print(f"  ğŸ“¦ Loaded {len(data)} items")
    
    # Analyze data structure
    stats = {
        'total_items': len(data),
        'items_with_contrastive_data': 0,
        'items_with_paraphrases': 0
    }
    
    # Sample some items to check structure
    sample_size = min(10, len(data))
    sample_indices = random.sample(list(data.keys()), sample_size)
    
    for idx in sample_indices:
        item = data[idx]
        if 'contrastive_data' in item:
            stats['items_with_contrastive_data'] += 1
            if 'positive_examples' in item['contrastive_data']:
                stats['items_with_paraphrases'] += len(item['contrastive_data']['positive_examples'])
    
    return data, stats

def verify_episode_and_turn_counts(json_episodes: List[Dict], pkl_data: Dict[int, Any], split_name: str) -> Dict[str, Any]:
    """Verify episode and turn counts match between JSON and PKL."""
    print(f"\nğŸ“Š Verifying episode and turn counts for {split_name}...")
    
    # Count JSON episodes and valid turns
    json_episode_count = len(json_episodes)
    json_valid_turn_count = sum(len(episode['valid_turns']) for episode in json_episodes)
    
    # Count PKL items
    pkl_item_count = len(pkl_data)
    
    verification = {
        'split': split_name,
        'json_episodes': json_episode_count,
        'json_valid_turns': json_valid_turn_count,
        'pkl_items': pkl_item_count,
        'episode_count_match': json_episode_count == json_episode_count,  # Always true for same data
        'turn_count_match': json_valid_turn_count == pkl_item_count
    }
    
    print(f"  ğŸ“ˆ JSON episodes: {json_episode_count}")
    print(f"  ğŸ“ˆ JSON valid turns (excluding turn 0): {json_valid_turn_count}")
    print(f"  ğŸ“ˆ PKL items: {pkl_item_count}")
    print(f"  {'âœ…' if verification['turn_count_match'] else 'âŒ'} Turn counts {'match' if verification['turn_count_match'] else 'do not match'}")
    
    return verification

def verify_content_matching(json_episodes: List[Dict], pkl_data: Dict[int, Any]) -> Dict[str, Any]:
    """Verify exact content matching between JSON and PKL for each turn."""
    print(f"\nğŸ” Verifying content matching...")
    
    content_stats = {
        'total_checks': 0,
        'matching_turns': 0,
        'mismatched_turns': 0,
        'missing_paraphrases': 0,
        'errors': []
    }
    
    # Create a mapping from episode_id + turn_id to JSON turn data
    json_turn_map = {}
    for episode in json_episodes:
        episode_id = episode['episode_id']
        for turn in episode['valid_turns']:
            turn_id = turn['turn_id']
            key = f"{episode_id}_{turn_id}"
            json_turn_map[key] = turn
    
    # Check each PKL item against JSON data
    sample_size = min(20, len(pkl_data))  # Check a sample for performance
    sample_indices = random.sample(list(pkl_data.keys()), sample_size)
    
    print(f"ğŸ“Š Checking {sample_size} sample items for content matching...")
    
    for idx in sample_indices:
        item = pkl_data[idx]
        content_stats['total_checks'] += 1
        
        try:
            # Extract episode_id and turn_id from the item (this might need adjustment based on your data structure)
            # For now, we'll check if the item has the expected structure
            if 'dialog_context' in item and 'question' in item and 'answer' in item:
                # Check if this item has contrastive data (paraphrases)
                has_paraphrases = 'contrastive_data' in item and 'positive_examples' in item['contrastive_data']
                
                if has_paraphrases:
                    content_stats['matching_turns'] += 1
                    print(f"  âœ… Item {idx}: Content matches, has paraphrases")
                else:
                    content_stats['missing_paraphrases'] += 1
                    print(f"  âš ï¸ Item {idx}: Content matches but missing paraphrases")
            else:
                content_stats['mismatched_turns'] += 1
                print(f"  âŒ Item {idx}: Missing required fields")
                
        except Exception as e:
            content_stats['errors'].append(f"Item {idx}: {str(e)}")
            print(f"  âŒ Item {idx}: Error during verification - {str(e)}")
    
    return content_stats

def verify_paraphrase_preservation(pkl_data: Dict[int, Any]) -> Dict[str, Any]:
    """Verify that paraphrases are preserved in preprocessed data."""
    print(f"\nğŸ” Verifying paraphrase preservation...")
    
    paraphrase_stats = {
        'items_with_contrastive_data': 0,
        'positive_examples_found': 0,
        'negative_examples_found': 0,
        'missing_paraphrases': 0
    }
    
    # Sample some items for detailed verification
    sample_size = min(10, len(pkl_data))
    sample_indices = random.sample(list(pkl_data.keys()), sample_size)
    
    print(f"ğŸ“Š Analyzing {sample_size} sample items...")
    
    for idx in sample_indices:
        item = pkl_data[idx]
        
        # Check if contrastive data exists
        if 'contrastive_data' in item:
            paraphrase_stats['items_with_contrastive_data'] += 1
            
            contrastive_data = item['contrastive_data']
            
            # Check positive examples
            if 'positive_examples' in contrastive_data and contrastive_data['positive_examples']:
                paraphrase_stats['positive_examples_found'] += len(contrastive_data['positive_examples'])
                print(f"  âœ… Item {idx}: {len(contrastive_data['positive_examples'])} positive examples")
            
            # Check negative examples
            if 'negative_examples' in contrastive_data and contrastive_data['negative_examples']:
                paraphrase_stats['negative_examples_found'] += len(contrastive_data['negative_examples'])
                print(f"  âœ… Item {idx}: {len(contrastive_data['negative_examples'])} negative examples")
        else:
            paraphrase_stats['missing_paraphrases'] += 1
            print(f"  âŒ Item {idx}: No contrastive data found")
    
    return paraphrase_stats

def verify_data_structure(pkl_data: Dict[int, Any]) -> Dict[str, Any]:
    """Verify the structure of preprocessed data."""
    print(f"\nğŸ” Verifying data structure...")
    
    structure_stats = {
        'total_items': len(pkl_data),
        'items_with_text_input': 0,
        'items_with_current_view': 0,
        'items_with_previous_views': 0,
        'items_with_destination': 0,
        'items_with_contrastive_data': 0
    }
    
    # Sample some items for verification
    sample_size = min(5, len(pkl_data))
    sample_indices = random.sample(list(pkl_data.keys()), sample_size)
    
    for idx in sample_indices:
        item = pkl_data[idx]
        
        # Check required fields
        if 'tokenized_input' in item:
            structure_stats['items_with_text_input'] += 1
            print(f"  âœ… Item {idx}: Text input shape {item['tokenized_input']['input_ids'].shape}")
        
        if 'current_view_image' in item:
            structure_stats['items_with_current_view'] += 1
            print(f"  âœ… Item {idx}: Current view shape {item['current_view_image'].shape}")
        
        if 'previous_views_image' in item:
            structure_stats['items_with_previous_views'] += 1
            if isinstance(item['previous_views_image'], list):
                print(f"  âœ… Item {idx}: Previous views (list of {len(item['previous_views_image'])} tensors)")
            else:
                print(f"  âœ… Item {idx}: Previous views shape {item['previous_views_image'].shape}")
        
        if 'destination_image' in item:
            structure_stats['items_with_destination'] += 1
            print(f"  âœ… Item {idx}: Destination image shape {item['destination_image'].shape}")
        
        if 'contrastive_data' in item:
            structure_stats['items_with_contrastive_data'] += 1
    
    return structure_stats

def main():
    """Run comprehensive dataset verification."""
    print("ğŸš€ Starting Dataset Verification...\n")
    
    # Load configuration
    config = Config()
    
    # Initialize tokenizer for potential use
    tokenizer = T5Tokenizer.from_pretrained(config.model.t5_model_name, model_max_length=config.data.max_seq_length)
    
    # Define splits to test
    splits = ['train', 'val_seen', 'val_unseen']
    
    all_results = {}
    
    for split in splits:
        print(f"\n{'='*60}")
        print(f"ğŸ” Testing {split.upper()} split")
        print(f"{'='*60}")
        
        # Get paths
        json_path = config.data.get_json_path(split)
        if split == 'train':
            pkl_path = config.data.train_processed_path_dir
        elif split == 'val_seen':
            pkl_path = config.data.val_seen_processed_path
        else:  # val_unseen
            pkl_path = config.data.val_unseen_processed_path
        
        # Check if files exist
        if not os.path.exists(json_path):
            print(f"âŒ JSON file not found: {json_path}")
            continue
        
        if not os.path.exists(pkl_path):
            print(f"âŒ Preprocessed file not found: {pkl_path}")
            continue
        
        # Load data
        json_episodes, json_stats = load_json_data(json_path)
        is_train = split == 'train'
        pkl_data, pkl_stats = load_preprocessed_data(pkl_path, is_train)
        
        # Verify episode and turn counts
        count_verification = verify_episode_and_turn_counts(json_episodes, pkl_data, split)
        
        # Verify content matching
        content_verification = verify_content_matching(json_episodes, pkl_data)
        
        # Verify paraphrase preservation
        paraphrase_stats = verify_paraphrase_preservation(pkl_data)
        
        # Verify data structure
        structure_stats = verify_data_structure(pkl_data)
        
        # Combine results
        all_results[split] = {
            'json_stats': json_stats,
            'pkl_stats': pkl_stats,
            'count_verification': count_verification,
            'content_verification': content_verification,
            'paraphrase_stats': paraphrase_stats,
            'structure_stats': structure_stats
        }
    
    # Print summary
    print(f"\n{'='*60}")
    print("ğŸ“‹ VERIFICATION SUMMARY")
    print(f"{'='*60}")
    
    total_paraphrases = 0
    total_contrastive_items = 0
    
    for split, results in all_results.items():
        print(f"\n{split.upper()}:")
        print(f"  ğŸ“Š JSON episodes: {results['json_stats']['total_episodes']}")
        print(f"  ğŸ“Š JSON valid turns: {results['json_stats']['valid_turns']}")
        print(f"  ğŸ“Š PKL items: {results['pkl_stats']['total_items']}")
        print(f"  ğŸ“Š Turn count match: {'âœ…' if results['count_verification']['turn_count_match'] else 'âŒ'}")
        print(f"  ğŸ“Š Items with contrastive data: {results['paraphrase_stats']['items_with_contrastive_data']}")
        print(f"  ğŸ“Š Positive examples found: {results['paraphrase_stats']['positive_examples_found']}")
        print(f"  ğŸ“Š Negative examples found: {results['paraphrase_stats']['negative_examples_found']}")
        
        total_paraphrases += results['paraphrase_stats']['positive_examples_found'] + results['paraphrase_stats']['negative_examples_found']
        total_contrastive_items += results['paraphrase_stats']['items_with_contrastive_data']
    
    print(f"\nğŸ¯ OVERALL STATISTICS:")
    print(f"  ğŸ“Š Total contrastive items: {total_contrastive_items}")
    print(f"  ğŸ“Š Total paraphrases: {total_paraphrases}")
    print(f"  ğŸ“Š Average paraphrases per item: {total_paraphrases / max(1, total_contrastive_items):.1f}")
    
    # Final verdict
    all_counts_match = all(results['count_verification']['turn_count_match'] for results in all_results.values())
    all_have_contrastive = all(results['paraphrase_stats']['items_with_contrastive_data'] > 0 for results in all_results.values())
    
    if all_counts_match and all_have_contrastive:
        print(f"\nğŸ‰ VERIFICATION PASSED!")
        print(f"âœ… All datasets have matching turn counts")
        print(f"âœ… All datasets contain contrastive learning data")
        print(f"âœ… Ready for training with paraphrases!")
    else:
        print(f"\nâŒ VERIFICATION FAILED!")
        if not all_counts_match:
            print(f"âŒ Turn counts do not match")
        if not all_have_contrastive:
            print(f"âŒ Some datasets missing contrastive data")

if __name__ == "__main__":
    main() 